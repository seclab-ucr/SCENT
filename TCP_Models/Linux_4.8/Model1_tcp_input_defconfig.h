#ifndef Model1_TCP_INPUT_DEFCONFIG_H
#define Model1_TCP_INPUT_DEFCONFIG_H
/*
 *
 * Automatically generated file; DO NOT EDIT.
 * Linux/x86_64 4.8.0 Kernel Configuration
 *
 */




/*
 * The use of "&&" / "||" is limited in certain expressions.
 * The followings enable to calculate "and" / "or" with macro expansion only.
 */
/*
 * Helper macros to use CONFIG_ options in C/CPP expressions. Note that
 * these only work with boolean and tristate options.
 */

/*
 * Getting something that works in C and CPP for an arg that may or may
 * not be defined is tricky.  Here, if we have "#define CONFIG_BOOGER 1"
 * we match on the placeholder define, insert the "0," for arg1 and generate
 * the triplet (0, 1, 0).  Then the last step cherry picks the 2nd arg (a one).
 * When CONFIG_BOOGER is not defined, we generate a (... 1, 0) pair, and when
 * the last step cherry picks the 2nd arg, we get a zero.
 */





/*
 * IS_BUILTIN(CONFIG_FOO) evaluates to 1 if CONFIG_FOO is set to 'y', 0
 * otherwise. For boolean options, this is equivalent to
 * IS_ENABLED(CONFIG_FOO).
 */


/*
 * IS_MODULE(CONFIG_FOO) evaluates to 1 if CONFIG_FOO is set to 'm', 0
 * otherwise.
 */


/*
 * IS_REACHABLE(CONFIG_FOO) evaluates to 1 if the currently compiled
 * code can call a function defined in code compiled based on CONFIG_FOO.
 * This is similar to IS_ENABLED(), but returns false when invoked from
 * built-in code when CONFIG_FOO is set to 'm'.
 */



/*
 * IS_ENABLED(CONFIG_FOO) evaluates to 1 if CONFIG_FOO is set to 'y' or 'm',
 * 0 otherwise.
 */
/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Implementation of the Transmission Control Protocol(TCP).
 *
 * Authors:	Ross Biro
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *		Mark Evans, <evansmp@uhura.aston.ac.uk>
 *		Corey Minyard <wf-rch!minyard@relay.EU.net>
 *		Florian La Roche, <flla@stud.uni-sb.de>
 *		Charles Hedrick, <hedrick@klinzhai.rutgers.edu>
 *		Linus Torvalds, <torvalds@cs.helsinki.fi>
 *		Alan Cox, <gw4pts@gw4pts.ampr.org>
 *		Matthew Dillon, <dillon@apollo.west.oic.com>
 *		Arnt Gulbrandsen, <agulbra@nvg.unit.no>
 *		Jorge Cwik, <jorge@laser.satlink.net>
 */

/*
 * Changes:
 *		Pedro Roque	:	Fast Retransmit/Recovery.
 *					Two receive queues.
 *					Retransmit queue handled by TCP.
 *					Better retransmit timer handling.
 *					New congestion avoidance.
 *					Header prediction.
 *					Variable renaming.
 *
 *		Eric		:	Fast Retransmit.
 *		Randy Scott	:	MSS option defines.
 *		Eric Schenk	:	Fixes to slow start algorithm.
 *		Eric Schenk	:	Yet another double ACK bug.
 *		Eric Schenk	:	Delayed ACK bug fixes.
 *		Eric Schenk	:	Floyd style fast retrans war avoidance.
 *		David S. Miller	:	Don't allow zero congestion window.
 *		Eric Schenk	:	Fix retransmitter so that it sends
 *					next packet on ack of previous packet.
 *		Andi Kleen	:	Moved open_request checking here
 *					and process RSTs for open_requests.
 *		Andi Kleen	:	Better prune_queue, and other fixes.
 *		Andrey Savochkin:	Fix RTT measurements in the presence of
 *					timestamps.
 *		Andrey Savochkin:	Check sequence numbers correctly when
 *					removing SACKs due to in sequence incoming
 *					data segments.
 *		Andi Kleen:		Make sure we never ack data there is not
 *					enough room for. Also make this condition
 *					a fatal error if it might still happen.
 *		Andi Kleen:		Add tcp_measure_rcv_mss to make
 *					connections with MSS<min(MTU,ann. MSS)
 *					work without delayed acks.
 *		Andi Kleen:		Process packets with PSH set in the
 *					fast path.
 *		J Hadi Salim:		ECN support
 *	 	Andrei Gurtov,
 *		Pasi Sarolahti,
 *		Panu Kuhlberg:		Experimental audit of TCP (re)transmission
 *					engine. Lots of bugs are found.
 *		Pasi Sarolahti:		F-RTO for dealing with spurious RTOs
 */


















/*
 * This error code is special: arch syscall entry code will return
 * -ENOSYS if users try to call a syscall that doesn't exist.  To keep
 * failures of syscalls that really do exist distinguishable from
 * failures due to attempts to use a nonexistent syscall, syscall
 * implementations should refrain from returning -ENOSYS.
 */
/* for robust mutexes */


/*
 * These should never be seen by user programs.  To return one of ERESTART*
 * codes, signal_pending() MUST be set.  Note that ptrace can observe these
 * at syscall exit tracing, but they will never be left for the debugged user
 * process to see.
 */
/* Defined for the NFSv3 protocol */












/* Indirect macros required for expanded argument pasting, eg. __LINE__. */











/*
 * Common definitions for all gcc versions go here.
 */




/* Optimization barrier */

/* The "volatile" is due to gcc bugs */

/*
 * This version is i.e. to prevent dead stores elimination on @ptr
 * where gcc and llvm may behave differently when otherwise using
 * normal barrier(): while gcc behavior gets along with a normal
 * barrier(), llvm needs an explicit input variable to be assumed
 * clobbered. The issue is as follows: while the inline asm might
 * access any memory it wants, the compiler could have fit all of
 * @ptr into memory registers instead, and since @ptr never escaped
 * from that, it proofed that the inline asm wasn't touching any of
 * it. This version works well with both compilers, i.e. we're telling
 * the compiler that the inline asm absolutely may see the contents
 * of @ptr. See also: https://llvm.org/bugs/show_bug.cgi?id=15495
 */


/*
 * This macro obfuscates arithmetic on a variable address so that gcc
 * shouldn't recognize the original var, and make assumptions about it.
 *
 * This is needed because the C standard makes it undefined to do
 * pointer arithmetic on "objects" outside their boundaries and the
 * gcc optimizers assume this is the case. In particular they
 * assume such arithmetic does not wrap.
 *
 * A miscompilation has been observed because of this on PPC.
 * To work around it we hide the relationship of the pointer and the object
 * using this macro.
 *
 * Versions of the ppc64 compiler before 4.1 had a bug where use of
 * RELOC_HIDE could trash r30. The bug can be worked around by changing
 * the inline assembly constraint from =g to =r, in this particular
 * case either is valid.
 */







/* Make the optimizer believe the variable can be manipulated arbitrarily. */






/* &a[0] degrades to a pointer: a different type from an array */



/*
 * Force always-inline if the user requests it so via the .config,
 * or if gcc is too old:
 */






/* A lot of inline functions can cause havoc with function tracing */
/*
 * it doesn't make sense on ARM (currently the only user of __naked)
 * to trace naked functions because then mcount is called without
 * stack and frame pointer being set up and there is no chance to
 * restore the lr register to the value before mcount was called.
 *
 * The asm() bodies of naked functions often depend on standard calling
 * conventions, therefore they must be noinline and noclone.
 *
 * GCC 4.[56] currently fail to enforce this, so we must do so ourselves.
 * See GCC PR44290.
 */




/*
 * From the GCC manual:
 *
 * Many functions have no effects except the return value and their
 * return value depends only on the parameters and/or global
 * variables.  Such a function can be subject to common subexpression
 * elimination and loop optimization just as an arithmetic operator
 * would be.
 * [...]
 */
/* gcc version specific checks */
/* GCC 4.1.[01] miscompiles __weak */
/*
 * GCC 'asm goto' miscompiles certain code sequences:
 *
 *   http://gcc.gnu.org/bugzilla/show_bug.cgi?id=58670
 *
 * Work it around via a compiler barrier quirk suggested by Jakub Jelinek.
 *
 * (asm goto is automatically volatile - the naming reflects this.)
 */


/*
 * sparse (__CHECKER__) pretends to be gcc, but can't do constant
 * folding in __builtin_bswap*() (yet), so don't set these for it.
 */
/*
 * A trick to suppress uninitialized variable warning without generating any
 * code
 */








/* Intel compiler defines __GNUC__. So we will overwrite implementations
 * coming from above header files here
 */




/* Clang compiler defines __GNUC__. So we will overwrite implementations
 * coming from above header files here
 */






/* Some compiler specific definitions are overwritten here
 * for Clang compiler
 */






/* same as gcc, this was present in clang-2.6 so we can assume it works
 * with any version that can compile the kernel
 */


/*
 * Generic compiler-dependent macros required for kernel
 * build go below this comment. Actual compiler/compiler version
 * specific implementations come from the above header files
 */

struct Model1_ftrace_branch_data {
 const char *func;
 const char *Model1_file;
 unsigned Model1_line;
 union {
  struct {
   unsigned long Model1_correct;
   unsigned long Model1_incorrect;
  };
  struct {
   unsigned long Model1_miss;
   unsigned long Model1_hit;
  };
  unsigned long Model1_miss_hit[2];
 };
};

/*
 * Note: DISABLE_BRANCH_PROFILING can be used by special lowlevel code
 * to disable branch tracing on a per file basis.
 */
/* Optimization barrier */
/* Unreachable code */
/* Not-quite-unique ID. */













/*
 * int-ll64 is used everywhere now.
 */

/*
 * asm-generic/int-ll64.h
 *
 * Integer declarations for architectures which use "long long"
 * for 64-bit types.
 */




/*
 * asm-generic/int-ll64.h
 *
 * Integer declarations for architectures which use "long long"
 * for 64-bit types.
 */











/*
 * There seems to be no way of detecting this automatically from user
 * space, so 64 bit architectures should override this in their
 * bitsperlong.h. In particular, an architecture that supports
 * both 32 and 64 bit user space must not rely on CONFIG_64BIT
 * to decide it, but rather check a compiler provided macro.
 */








/*
 * FIXME: The check currently breaks x86-64 build, so it's
 * temporarily disabled. Please fix x86-64 and reenable
 */


/*
 * __xx is ok: it doesn't pollute the POSIX namespace. Use these in the
 * header files exported to user space
 */

typedef __signed__ char Model1___s8;
typedef unsigned char __u8;

typedef __signed__ short Model1___s16;
typedef unsigned short Model1___u16;

typedef __signed__ int Model1___s32;
typedef unsigned int __u32;


__extension__ typedef __signed__ long long Model1___s64;
__extension__ typedef unsigned long long __u64;




typedef signed char Model1_s8;
typedef unsigned char Model1_u8;

typedef signed short Model1_s16;
typedef unsigned short Model1_u16;

typedef signed int Model1_s32;
typedef unsigned int Model1_u32;

typedef signed long long Model1_s64;
typedef unsigned long long Model1_u64;


















/*enum {
 false = 0,
 true = 1
};*/
/**
 * offsetofend(TYPE, MEMBER)
 *
 * @TYPE: The type of the structure
 * @MEMBER: The member within the structure to get the end offset of
 */

/*
 * This allows for 1024 file descriptors: if NR_OPEN is ever grown
 * beyond that you'll have to change this too. But 1024 fd's seem to be
 * enough even for such "real" unices like OSF/1, so hopefully this is
 * one limit that doesn't have to be changed [again].
 *
 * Note that POSIX wants the FD_CLEAR(fd,fdsetp) defines to be in
 * <sys/time.h> (and thus <linux/time.h>) - but this is a more logical
 * place for them. Solved by having dummy defines in <sys/time.h>.
 */

/*
 * This macro may have been defined in <gnu/types.h>. But we always
 * use the one here.
 */



typedef struct {
 unsigned long Model1_fds_bits[1024 / (8 * sizeof(long))];
} Model1___kernel_fd_set;

/* Type of a signal handler.  */
typedef void (*Model1___kernel_sighandler_t)(int);

/* Type of a SYSV IPC key.  */
typedef int Model1___kernel_key_t;
typedef int Model1___kernel_mqd_t;








/*
 * This file is generally used by user-level software, so you need to
 * be a little careful about namespace pollution etc.  Also, we cannot
 * assume GCC is being used.
 */

typedef unsigned short Model1___kernel_old_uid_t;
typedef unsigned short Model1___kernel_old_gid_t;


typedef unsigned long Model1___kernel_old_dev_t;







/*
 * This file is generally used by user-level software, so you need to
 * be a little careful about namespace pollution etc.
 *
 * First the types that are often defined in different ways across
 * architectures, so that you can override them.
 */


typedef long Model1___kernel_long_t;
typedef unsigned long Model1___kernel_ulong_t;



typedef Model1___kernel_ulong_t Model1___kernel_ino_t;



typedef unsigned int Model1___kernel_mode_t;



typedef int Model1___kernel_pid_t;



typedef int Model1___kernel_ipc_pid_t;



typedef unsigned int Model1___kernel_uid_t;
typedef unsigned int Model1___kernel_gid_t;



typedef Model1___kernel_long_t Model1___kernel_suseconds_t;



typedef int Model1___kernel_daddr_t;



typedef unsigned int Model1___kernel_uid32_t;
typedef unsigned int Model1___kernel_gid32_t;
/*
 * Most 32 bit architectures use "unsigned int" size_t,
 * and all 64 bit architectures use "unsigned long" size_t.
 */






typedef Model1___kernel_ulong_t Model1___kernel_size_t;
typedef Model1___kernel_long_t Model1___kernel_ssize_t;
typedef Model1___kernel_long_t Model1___kernel_ptrdiff_t;




typedef struct {
 int Model1_val[2];
} Model1___kernel_fsid_t;


/*
 * anything below here should be completely generic
 */
typedef Model1___kernel_long_t Model1___kernel_off_t;
typedef long long Model1___kernel_loff_t;
typedef Model1___kernel_long_t Model1___kernel_time_t;
typedef Model1___kernel_long_t Model1___kernel_clock_t;
typedef int Model1___kernel_timer_t;
typedef int Model1___kernel_clockid_t;
typedef char * Model1___kernel_caddr_t;
typedef unsigned short Model1___kernel_uid16_t;
typedef unsigned short Model1___kernel_gid16_t;


/*
 * Below are truly Linux-specific types that should never collide with
 * any application/library that wants linux/types.h.
 */
typedef Model1___u16 Model1___le16;
typedef Model1___u16 Model1___be16;
typedef __u32 Model1___le32;
typedef __u32 Model1___be32;
typedef __u64 Model1___le64;
typedef __u64 Model1___be64;

typedef Model1___u16 Model1___sum16;
typedef __u32 Model1___wsum;

/*
 * aligned_u64 should be used in defining kernel<->userspace ABIs to avoid
 * common 32/64-bit compat problems.
 * 64-bit values align to 4-byte boundaries on x86_32 (and possibly other
 * architectures) and to 8-byte boundaries on 64-bit architectures.  The new
 * aligned_64 type enforces 8-byte alignment so that structs containing
 * aligned_64 values have the same alignment on 32-bit and 64-bit architectures.
 * No conversions are necessary between 32-bit user-space and a 64-bit kernel.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline))
void Model1___read_once_size(const volatile void *Model1_p, void *Model1_res, int Model1_size)
{
 ({ switch (Model1_size) { case 1: *(__u8 *)Model1_res = *(volatile __u8 *)Model1_p; break; case 2: *(Model1___u16 *)Model1_res = *(volatile Model1___u16 *)Model1_p; break; case 4: *(__u32 *)Model1_res = *(volatile __u32 *)Model1_p; break; case 8: *(__u64 *)Model1_res = *(volatile __u64 *)Model1_p; break; default: __asm__ __volatile__("": : :"memory"); __builtin_memcpy((void *)Model1_res, (const void *)Model1_p, Model1_size); __asm__ __volatile__("": : :"memory"); } });
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline))
void Model1___read_once_size_nocheck(const volatile void *Model1_p, void *Model1_res, int Model1_size)
{
 ({ switch (Model1_size) { case 1: *(__u8 *)Model1_res = *(volatile __u8 *)Model1_p; break; case 2: *(Model1___u16 *)Model1_res = *(volatile Model1___u16 *)Model1_p; break; case 4: *(__u32 *)Model1_res = *(volatile __u32 *)Model1_p; break; case 8: *(__u64 *)Model1_res = *(volatile __u64 *)Model1_p; break; default: __asm__ __volatile__("": : :"memory"); __builtin_memcpy((void *)Model1_res, (const void *)Model1_p, Model1_size); __asm__ __volatile__("": : :"memory"); } });
}


static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___write_once_size(volatile void *Model1_p, void *Model1_res, int Model1_size)
{
 switch (Model1_size) {
 case 1: *(volatile __u8 *)Model1_p = *(__u8 *)Model1_res; break;
 case 2: *(volatile Model1___u16 *)Model1_p = *(Model1___u16 *)Model1_res; break;
 case 4: *(volatile __u32 *)Model1_p = *(__u32 *)Model1_res; break;
 case 8: *(volatile __u64 *)Model1_p = *(__u64 *)Model1_res; break;
 default:
  __asm__ __volatile__("": : :"memory");
  __builtin_memcpy((void *)Model1_p, (const void *)Model1_res, Model1_size);
  __asm__ __volatile__("": : :"memory");
 }
}

/*
 * Prevent the compiler from merging or refetching reads or writes. The
 * compiler is also forbidden from reordering successive instances of
 * READ_ONCE, WRITE_ONCE and ACCESS_ONCE (see below), but only when the
 * compiler is aware of some particular ordering.  One way to make the
 * compiler aware of ordering is to put the two invocations of READ_ONCE,
 * WRITE_ONCE or ACCESS_ONCE() in different C statements.
 *
 * In contrast to ACCESS_ONCE these two macros will also work on aggregate
 * data types like structs or unions. If the size of the accessed data
 * type exceeds the word size of the machine (e.g., 32 bits or 64 bits)
 * READ_ONCE() and WRITE_ONCE() will fall back to memcpy(). There's at
 * least two memcpy()s: one for the __builtin_memcpy() and then one for
 * the macro doing the copy of variable - '__u' allocated on the stack.
 *
 * Their two major use cases are: (1) Mediating communication between
 * process-level code and irq/NMI handlers, all running on the same CPU,
 * and (2) Ensuring that the compiler does not  fold, spindle, or otherwise
 * mutilate accesses that either do not require ordering or that interact
 * with an explicit memory barrier or atomic instruction that provides the
 * required ordering.
 */
/*
 * Use READ_ONCE_NOCHECK() instead of READ_ONCE() if you need
 * to hide memory access from KASAN.
 */
/*
 * Allow us to mark functions as 'deprecated' and have gcc emit a nice
 * warning for each use, in hopes of speeding the functions removal.
 * Usage is:
 * 		int __deprecated foo(void)
 */
/*
 * Allow us to avoid 'defined but not used' warnings on functions and data,
 * as well as force them to be emitted to the assembly file.
 *
 * As of gcc 3.4, static functions that are not marked with attribute((used))
 * may be elided from the assembly file.  As of gcc 3.4, static data not so
 * marked will not be elided, but this may change in a future gcc version.
 *
 * NOTE: Because distributions shipped with a backported unit-at-a-time
 * compiler in gcc 3.3, we must define __used to be __attribute__((used))
 * for gcc >=3.3 instead of 3.4.
 *
 * In prior versions of gcc, such functions and data would be emitted, but
 * would be warned about except with attribute((unused)).
 *
 * Mark functions that are referenced only in inline assembly as __used so
 * the code is emitted even though it appears to be unreferenced.
 */
/*
 * Rather then using noinline to prevent stack consumption, use
 * noinline_for_stack instead.  For documentation reasons.
 */
/*
 * From the GCC manual:
 *
 * Many functions do not examine any values except their arguments,
 * and have no effects except the return value.  Basically this is
 * just slightly more strict class than the `pure' attribute above,
 * since function is not allowed to read global memory.
 *
 * Note that a function that has pointer arguments and examines the
 * data pointed to must _not_ be declared `const'.  Likewise, a
 * function that calls a non-`const' function usually must not be
 * `const'.  It does not make sense for a `const' function to return
 * `void'.
 */




/*
 * Tell gcc if a function is cold. The compiler will assume any path
 * directly leading to the call is unlikely.
 */





/* Simple shorthand for a section definition */
/*
 * Assume alignment of return value.
 */





/* Are two types/vars the same type (ignoring qualifiers)? */




/* Is this type a native word size -- useful for atomic operations */




/* Compile time object size, -1 for unknown */
/*
 * Sparse complains of variable sized arrays due to the temporary variable in
 * __compiletime_assert. Unfortunately we can't just expand it out to make
 * sparse see a constant array size without breaking compiletime_assert on old
 * versions of GCC (e.g. 4.2.4), so hide the array from sparse altogether.
 */
/**
 * compiletime_assert - break build and emit msg if condition is false
 * @condition: a compile-time constant condition to check
 * @msg:       a message to emit if condition is false
 *
 * In tradition of POSIX assert, this macro will break the build if the
 * supplied condition is *false*, emitting the supplied error message if the
 * compiler has support to do so.
 */







/*
 * Prevent the compiler from merging or refetching accesses.  The compiler
 * is also forbidden from reordering successive instances of ACCESS_ONCE(),
 * but only when the compiler is aware of some particular ordering.  One way
 * to make the compiler aware of ordering is to put the two invocations of
 * ACCESS_ONCE() in different C statements.
 *
 * ACCESS_ONCE will only work on scalar types. For union types, ACCESS_ONCE
 * on a union member will work as long as the size of the member matches the
 * size of the union and the size is smaller than word size.
 *
 * The major use cases of ACCESS_ONCE used to be (1) Mediating communication
 * between process-level code and irq/NMI handlers, all running on the same CPU,
 * and (2) Ensuring that the compiler does not  fold, spindle, or otherwise
 * mutilate accesses that either do not require ordering or that interact
 * with an explicit memory barrier or atomic instruction that provides the
 * required ordering.
 *
 * If possible use READ_ONCE()/WRITE_ONCE() instead.
 */





/**
 * lockless_dereference() - safely load a pointer for later dereference
 * @p: The pointer to load
 *
 * Similar to rcu_dereference(), but for situations where the pointed-to
 * object's lifetime is managed by something other than RCU.  That
 * "something other" might be reference counting or simple immortality.
 *
 * The seemingly unused variable ___typecheck_p validates that @p is
 * indeed a pointer type by using a pointer to typeof(*p) as the type.
 * Taking a pointer to typeof(*p) again is needed in case p is void *.
 */
/* Ignore/forbid kprobes attach on very low level functions marked by this attribute: */












/*===---- stdarg.h - Variable argument handling ----------------------------===
 *
 * Copyright (c) 2008 Eli Friedman
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 * THE SOFTWARE.
 *
 *===-----------------------------------------------------------------------===
 */





typedef __builtin_va_list Model1_va_list;






/* GCC always defines __va_copy, but does not define va_copy unless in c99 mode
 * or -ansi is not specified, since it was not part of C90.
 */
typedef __builtin_va_list __gnuc_va_list;







/* Indirect stringification.  Doing two levels allows the parameter to be a
 * macro itself.  For example, compile with -DFOO=bar, __stringify(FOO)
 * converts to "bar".
 */


/*
 * Export symbols from the kernel to modules.  Forked from module.h
 * to reduce the amount of pointless cruft we feed to gcc when only
 * exporting a simple symbol or two.
 *
 * Try not to add #includes here.  It slows compilation and makes kernel
 * hackers place grumpy comments in header files.
 */

/* Some toolchains use a `_' prefix for all user symbols. */
/* Indirect, so macros are expanded before pasting. */




struct Model1_kernel_symbol
{
 unsigned long Model1_value;
 const char *Model1_name;
};
/* For every exported symbol, place a struct in the __ksymtab section */

/* Some toolchains use other characters (e.g. '`') to mark new line in macro */
/*
 * For assembly routines.
 *
 * Note when using these that you must specify the appropriate
 * alignment directives yourself
 */



/*
 * This is used by architectures to keep arguments on the stack
 * untouched by the compiler by keeping them live until the end.
 * The argument stack may be owned by the assembly-language
 * caller, not the callee, and gcc doesn't always understand
 * that.
 *
 * We have the return value, and a maximum of six arguments.
 *
 * This should always be followed by a "return ret" for the
 * protection to work (ie no more work that the compiler might
 * end up needing stack temporaries for).
 */
/* Assembly files may be compiled with -traditional .. */

typedef __u32 Model1___kernel_dev_t;

typedef Model1___kernel_fd_set Model1_fd_set;
typedef Model1___kernel_dev_t Model1_dev_t;
typedef Model1___kernel_ino_t Model1_ino_t;
typedef Model1___kernel_mode_t Model1_mode_t;
typedef unsigned short Model1_umode_t;
typedef __u32 Model1_nlink_t;
typedef Model1___kernel_off_t Model1_off_t;
typedef Model1___kernel_pid_t Model1_pid_t;
typedef Model1___kernel_daddr_t Model1_daddr_t;
typedef Model1___kernel_key_t Model1_key_t;
typedef Model1___kernel_suseconds_t Model1_suseconds_t;
typedef Model1___kernel_timer_t Model1_timer_t;
typedef Model1___kernel_clockid_t Model1_clockid_t;
typedef Model1___kernel_mqd_t Model1_mqd_t;

typedef _Bool bool;

typedef Model1___kernel_uid32_t Model1_uid_t;
typedef Model1___kernel_gid32_t Model1_gid_t;
typedef Model1___kernel_uid16_t Model1_uid16_t;
typedef Model1___kernel_gid16_t Model1_gid16_t;

typedef unsigned long Model1_uintptr_t;


/* This is defined by include/asm-{arch}/posix_types.h */
typedef Model1___kernel_old_uid_t Model1_old_uid_t;
typedef Model1___kernel_old_gid_t Model1_old_gid_t;



typedef Model1___kernel_loff_t Model1_loff_t;


/*
 * The following typedefs are also protected by individual ifdefs for
 * historical reasons:
 */


typedef Model1___kernel_size_t Model1_size_t;




typedef Model1___kernel_ssize_t Model1_ssize_t;




typedef Model1___kernel_ptrdiff_t Model1_ptrdiff_t;




typedef Model1___kernel_time_t Model1_time_t;




typedef Model1___kernel_clock_t Model1_clock_t;




typedef Model1___kernel_caddr_t Model1_caddr_t;


/* bsd */
typedef unsigned char Model1_u_char;
typedef unsigned short Model1_u_short;
typedef unsigned int Model1_u_int;
typedef unsigned long Model1_u_long;

/* sysv */
typedef unsigned char Model1_unchar;
typedef unsigned short Model1_ushort;
typedef unsigned int Model1_uint;
typedef unsigned long Model1_ulong;




typedef __u8 Model1_u_int8_t;
typedef Model1___s8 Model1_int8_t;
typedef Model1___u16 Model1_u_int16_t;
typedef Model1___s16 Model1_int16_t;
typedef __u32 Model1_u_int32_t;
typedef Model1___s32 Model1_int32_t;



typedef __u8 Model1_uint8_t;
typedef Model1___u16 Model1_uint16_t;
typedef __u32 Model1_uint32_t;


typedef __u64 Model1_uint64_t;
typedef __u64 Model1_u_int64_t;
typedef Model1___s64 Model1_int64_t;


/* this is a special 64bit data type that is 8-byte aligned */




/**
 * The type used for indexing onto a disc or disc partition.
 *
 * Linux always considers sectors to be 512 bytes long independently
 * of the devices real block size.
 *
 * blkcnt_t is the type of the inode's block count.
 */




typedef unsigned long Model1_sector_t;
typedef unsigned long Model1_blkcnt_t;


/*
 * The type of an index into the pagecache.
 */


/*
 * A dma_addr_t can hold any valid DMA address, i.e., any address returned
 * by the DMA API.
 *
 * If the DMA API only uses 32-bit addresses, dma_addr_t need only be 32
 * bits wide.  Bus addresses, e.g., PCI BARs, may be wider than 32 bits,
 * but drivers do memory-mapped I/O to ioremapped kernel virtual addresses,
 * so they don't care about the size of the actual bus addresses.
 */

typedef Model1_u64 Model1_dma_addr_t;




typedef unsigned Model1_gfp_t;
typedef unsigned Model1_fmode_t;


typedef Model1_u64 Model1_phys_addr_t;




typedef Model1_phys_addr_t Model1_resource_size_t;

/*
 * This type is the placeholder for a hardware interrupt number. It has to be
 * big enough to enclose whatever representation is used by a given platform.
 */
typedef unsigned long Model1_irq_hw_number_t;

typedef struct {
 int Model1_counter;
} Model1_atomic_t;


typedef struct {
 long Model1_counter;
} Model1_atomic64_t;


struct Model1_list_head {
 struct Model1_list_head *Model1_next, *Model1_prev;
};

struct Model1_hlist_head {
 struct Model1_hlist_node *Model1_first;
};

struct Model1_hlist_node {
 struct Model1_hlist_node *Model1_next, **Model1_pprev;
};

struct Model1_ustat {
 Model1___kernel_daddr_t Model1_f_tfree;
 Model1___kernel_ino_t Model1_f_tinode;
 char Model1_f_fname[6];
 char Model1_f_fpack[6];
};

/**
 * struct callback_head - callback structure for use with RCU and task_work
 * @next: next update requests in a list
 * @func: actual update function to call after the grace period.
 *
 * The struct is aligned to size of pointer. On most architectures it happens
 * naturally due ABI requirements, but some architectures (like CRIS) have
 * weird ABI and we need to ask it explicitly.
 *
 * The alignment is required to guarantee that bits 0 and 1 of @next will be
 * clear under normal conditions -- as long as we use call_rcu(),
 * call_rcu_bh(), call_rcu_sched(), or call_srcu() to queue callback.
 *
 * This guarantee is important for few reasons:
 *  - future call_rcu_lazy() will make use of lower bits in the pointer;
 *  - the structure shares storage spacer in struct page with @compound_head,
 *    which encode PageTail() in bit 0. The guarantee is needed to avoid
 *    false-positive PageTail().
 */
struct Model1_callback_head {
 struct Model1_callback_head *Model1_next;
 void (*func)(struct Model1_callback_head *Model1_head);
} __attribute__((aligned(sizeof(void *))));


typedef void (*Model1_rcu_callback_t)(struct Model1_callback_head *Model1_head);
typedef void (*Model1_call_rcu_func_t)(struct Model1_callback_head *Model1_head, Model1_rcu_callback_t func);

/* clocksource cycle base type */
typedef Model1_u64 Model1_cycle_t;

/*
 * Create a contiguous bitmask starting at bit position @l and ending at
 * position @h. For example
 * GENMASK_ULL(39, 21) gives us the 64bit vector 0x000000ffffe00000.
 */






extern unsigned int Model1___sw_hweight8(unsigned int Model1_w);
extern unsigned int Model1___sw_hweight16(unsigned int Model1_w);
extern unsigned int Model1___sw_hweight32(unsigned int Model1_w);
extern unsigned long Model1___sw_hweight64(__u64 Model1_w);

/*
 * Include this here because some architectures need generic_ffs/fls in
 * scope
 */




/*
 * Copyright 1992, Linus Torvalds.
 *
 * Note: inlines with more than a single statement should be marked
 * __always_inline to avoid problems with older gcc's inlining heuristics.
 */















/*
 * Macros to generate condition code outputs from inline assembly,
 * The output operand must be type "bool".
 */
/* Exception table entry */
/* For C file, we already have NOKPROBE_SYMBOL macro */

/*
 * Alternative inline assembly for SMP.
 *
 * The LOCK_PREFIX macro defined here replaces the LOCK and
 * LOCK_PREFIX macros used everywhere in the source tree.
 *
 * SMP alternatives use the same data structures as the other
 * alternatives and the X86_FEATURE_UP flag to indicate the case of a
 * UP system running a SMP kernel.  The existing apply_alternatives()
 * works fine for patching a SMP kernel for UP.
 *
 * The SMP alternative tables can be kept after boot and contain both
 * UP and SMP versions of the instructions to allow switching back to
 * SMP at runtime, when hotplugging in a new CPU, which is especially
 * useful in virtualized environments.
 *
 * The very common lock prefix is handled as special case in a
 * separate table which is a pure address list without replacement ptr
 * and size information.  That keeps the table sizes small.
 */
struct Model1_alt_instr {
 Model1_s32 Model1_instr_offset; /* original instruction */
 Model1_s32 Model1_repl_offset; /* offset to replacement instruction */
 Model1_u16 Model1_cpuid; /* cpuid bit set for replacement */
 Model1_u8 Model1_instrlen; /* length of original instruction */
 Model1_u8 Model1_replacementlen; /* length of new instruction */
 Model1_u8 Model1_padlen; /* length of build-time padding */
} __attribute__((packed));

/*
 * Debug flag that can be tested to see whether alternative
 * instructions were patched in already:
 */
extern int Model1_alternatives_patched;

extern void Model1_alternative_instructions(void);
extern void Model1_apply_alternatives(struct Model1_alt_instr *Model1_start, struct Model1_alt_instr *Model1_end);

struct Model1_module;


extern void Model1_alternatives_smp_module_add(struct Model1_module *Model1_mod, char *Model1_name,
     void *Model1_locks, void *Model1_locks_end,
     void *Model1_text, void *Model1_text_end);
extern void Model1_alternatives_smp_module_del(struct Model1_module *Model1_mod);
extern void Model1_alternatives_enable_smp(void);
extern int Model1_alternatives_text_reserved(void *Model1_start, void *Model1_end);
extern bool Model1_skip_smp_alternatives;
/*
 * max without conditionals. Idea adapted from:
 * http://graphics.stanford.edu/~seander/bithacks.html#IntegerMinOrMax
 *
 * The additional "-" is needed because gas works with s32s.
 */


/*
 * Pad the second replacement alternative with additional NOPs if it is
 * additionally longer than the first replacement alternative.
 */
/* alternative assembly primitive: */
/*
 * Alternative instructions for different CPU types or capabilities.
 *
 * This allows to use optimized instructions even on generic binary
 * kernels.
 *
 * length of oldinstr must be longer or equal the length of newinstr
 * It can be padded with nops as needed.
 *
 * For non barrier like inlines please define new variants
 * without volatile and memory clobber.
 */






/*
 * Alternative inline assembly with input.
 *
 * Pecularities:
 * No memory clobber here.
 * Argument numbers start with 1.
 * Best is to use constraints that are fixed size (like (%1) ... "r")
 * If you use variable sized constraints like "m" or "g" in the
 * replacement make sure to pad to the worst case length.
 * Leaving an unused argument 0 to keep API compatibility.
 */




/*
 * This is similar to alternative_input. But it has two features and
 * respective instructions.
 *
 * If CPU has feature2, newinstr2 is used.
 * Otherwise, if CPU has feature1, newinstr1 is used.
 * Otherwise, oldinstr is used.
 */






/* Like alternative_input, but with a single output argument */




/* Like alternative_io, but for replacing a direct call with another one. */




/*
 * Like alternative_call, but there are two features and respective functions.
 * If CPU has feature2, function2 is used.
 * Otherwise, if CPU has feature1, function1 is used.
 * Otherwise, old function is used.
 */







/*
 * use this macro(s) if you need more than one output parameter
 * in alternative_io
 */


/*
 * use this macro if you need clobbers but no inputs in
 * alternative_{input,io,call}()
 */
/* Use flags output or a set instruction */







/*
 * Define nops for use with alternative() and for tracing.
 *
 * *_NOP5_ATOMIC must be a single instruction.
 */



/* generic versions from gas
   1: nop
   the following instructions are NOT nops in 64-bit mode,
   for 64-bit mode use K8 or P6 nops instead
   2: movl %esi,%esi
   3: leal 0x00(%esi),%esi
   4: leal 0x00(,%esi,1),%esi
   6: leal 0x00000000(%esi),%esi
   7: leal 0x00000000(,%esi,1),%esi
*/
/* Opteron 64bit nops
   1: nop
   2: osp nop
   3: osp osp nop
   4: osp osp osp nop
*/
/* K7 nops
   uses eax dependencies (arbitrary choice)
   1: nop
   2: movl %eax,%eax
   3: leal (,%eax,1),%eax
   4: leal 0x00(,%eax,1),%eax
   6: leal 0x00000000(%eax),%eax
   7: leal 0x00000000(,%eax,1),%eax
*/
/* P6 nops
   uses eax dependencies (Intel-recommended choice)
   1: nop
   2: osp nop
   3: nopl (%eax)
   4: nopl 0x00(%eax)
   5: nopl 0x00(%eax,%eax,1)
   6: osp nopl 0x00(%eax,%eax,1)
   7: nopl 0x00000000(%eax)
   8: nopl 0x00000000(%eax,%eax,1)
   Note: All the above are assumed to be a single instruction.
	There is kernel code that depends on this.
*/
extern const unsigned char * const *Model1_ideal_nops;
extern void Model1_arch_init_ideal_nops(void);

/*
 * Force strict CPU ordering.
 * And yes, this might be required on UP too when we're talking
 * to devices.
 */
/* Atomic operations are already serializing on x86 */




/*
 * Generic barrier definitions, originally based on MN10300 definitions.
 *
 * It should be possible to use these on really simple architectures,
 * but it serves more as a starting point for new ports.
 *
 * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.
 * Written by David Howells (dhowells@redhat.com)
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public Licence
 * as published by the Free Software Foundation; either version
 * 2 of the Licence, or (at your option) any later version.
 */
/*
 * Force strict CPU ordering. And yes, this is required on UP too when we're
 * talking to devices.
 *
 * Fall back to compiler barriers if nothing better is provided.
 */
/* Barriers for virtual machine guests when talking to an SMP host */
/**
 * smp_acquire__after_ctrl_dep() - Provide ACQUIRE ordering after a control dependency
 *
 * A control dependency provides a LOAD->STORE order, the additional RMB
 * provides LOAD->LOAD order, together they provide LOAD->{LOAD,STORE} order,
 * aka. (load)-ACQUIRE.
 *
 * Architectures that do not do load speculation can have this be barrier().
 */




/**
 * smp_cond_load_acquire() - (Spin) wait for cond with ACQUIRE ordering
 * @ptr: pointer to the variable to wait on
 * @cond: boolean expression to wait for
 *
 * Equivalent to using smp_load_acquire() on the condition variable but employs
 * the control dependency of the wait to reduce the barrier on many platforms.
 *
 * Due to C lacking lambda expressions we load the value of *ptr into a
 * pre-named variable @VAL to be used in @cond.
 */
/*
 * These have to be done with inline assembly: that way the bit-setting
 * is guaranteed to be atomic. All bit operations return 0 if the bit
 * was cleared before the operation and != 0 if it was not.
 *
 * bit 0 is the LSB of addr; bit 32 is the LSB of (addr+1).
 */
/*
 * We do the locked ops that don't return the old value as
 * a mask operation on a byte.
 */




/**
 * set_bit - Atomically set a bit in memory
 * @nr: the bit to set
 * @addr: the address to start counting from
 *
 * This function is atomic and may not be reordered.  See __set_bit()
 * if you do not require the atomic guarantees.
 *
 * Note: there are no guarantees that this function will not be reordered
 * on non x86 architectures, so if you are writing portable code,
 * make sure not to rely on its reordering guarantees.
 *
 * Note that @nr may be almost arbitrarily large; this function is not
 * restricted to acting on a single-word quantity.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void
Model1_set_bit(long Model1_nr, volatile unsigned long *Model1_addr)
{
 if ((__builtin_constant_p(Model1_nr))) {
  asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "orb %1,%0"
   : "+m" (*(volatile long *) ((void *)(Model1_addr) + ((Model1_nr)>>3)))
   : "iq" ((Model1_u8)(1 << ((Model1_nr) & 7)))
   : "memory");
 } else {
  asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "bts %1,%0"
   : "+m" (*(volatile long *) (Model1_addr)) : "Ir" (Model1_nr) : "memory");
 }
}

/**
 * __set_bit - Set a bit in memory
 * @nr: the bit to set
 * @addr: the address to start counting from
 *
 * Unlike set_bit(), this function is non-atomic and may be reordered.
 * If it's called on the same region of memory simultaneously, the effect
 * may be that only one operation succeeds.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___set_bit(long Model1_nr, volatile unsigned long *Model1_addr)
{
 asm volatile("bts %1,%0" : "+m" (*(volatile long *) (Model1_addr)) : "Ir" (Model1_nr) : "memory");
}

/**
 * clear_bit - Clears a bit in memory
 * @nr: Bit to clear
 * @addr: Address to start counting from
 *
 * clear_bit() is atomic and may not be reordered.  However, it does
 * not contain a memory barrier, so if it is used for locking purposes,
 * you should call smp_mb__before_atomic() and/or smp_mb__after_atomic()
 * in order to ensure changes are visible on other processors.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void
Model1_clear_bit(long Model1_nr, volatile unsigned long *Model1_addr)
{
 if ((__builtin_constant_p(Model1_nr))) {
  asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "andb %1,%0"
   : "+m" (*(volatile long *) ((void *)(Model1_addr) + ((Model1_nr)>>3)))
   : "iq" ((Model1_u8)~(1 << ((Model1_nr) & 7))));
 } else {
  asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "btr %1,%0"
   : "+m" (*(volatile long *) (Model1_addr))
   : "Ir" (Model1_nr));
 }
}

/*
 * clear_bit_unlock - Clears a bit in memory
 * @nr: Bit to clear
 * @addr: Address to start counting from
 *
 * clear_bit() is atomic and implies release semantics before the memory
 * operation. It can be used for an unlock.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_clear_bit_unlock(long Model1_nr, volatile unsigned long *Model1_addr)
{
 __asm__ __volatile__("": : :"memory");
 Model1_clear_bit(Model1_nr, Model1_addr);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___clear_bit(long Model1_nr, volatile unsigned long *Model1_addr)
{
#if CY_ABSTRACT6
#define Model1_BITS_PER_LONG 64
    unsigned long Model1_mask = (1UL << ((Model1_nr) % Model1_BITS_PER_LONG));
    unsigned long *Model1_p = ((unsigned long *)Model1_addr) + ((Model1_nr) / Model1_BITS_PER_LONG);

    *Model1_p &= ~Model1_mask;
#else
 asm volatile("btr %1,%0" : "+m" (*(volatile long *) (Model1_addr)) : "Ir" (Model1_nr));
#endif
}

/*
 * __clear_bit_unlock - Clears a bit in memory
 * @nr: Bit to clear
 * @addr: Address to start counting from
 *
 * __clear_bit() is non-atomic and implies release semantics before the memory
 * operation. It can be used for an unlock if no other CPUs can concurrently
 * modify other bits in the word.
 *
 * No memory barrier is required here, because x86 cannot reorder stores past
 * older loads. Same principle as spin_unlock.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___clear_bit_unlock(long Model1_nr, volatile unsigned long *Model1_addr)
{
 __asm__ __volatile__("": : :"memory");
 Model1___clear_bit(Model1_nr, Model1_addr);
}

/**
 * __change_bit - Toggle a bit in memory
 * @nr: the bit to change
 * @addr: the address to start counting from
 *
 * Unlike change_bit(), this function is non-atomic and may be reordered.
 * If it's called on the same region of memory simultaneously, the effect
 * may be that only one operation succeeds.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___change_bit(long Model1_nr, volatile unsigned long *Model1_addr)
{
 asm volatile("btc %1,%0" : "+m" (*(volatile long *) (Model1_addr)) : "Ir" (Model1_nr));
}

/**
 * change_bit - Toggle a bit in memory
 * @nr: Bit to change
 * @addr: Address to start counting from
 *
 * change_bit() is atomic and may not be reordered.
 * Note that @nr may be almost arbitrarily large; this function is not
 * restricted to acting on a single-word quantity.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_change_bit(long Model1_nr, volatile unsigned long *Model1_addr)
{
 if ((__builtin_constant_p(Model1_nr))) {
  asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xorb %1,%0"
   : "+m" (*(volatile long *) ((void *)(Model1_addr) + ((Model1_nr)>>3)))
   : "iq" ((Model1_u8)(1 << ((Model1_nr) & 7))));
 } else {
  asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "btc %1,%0"
   : "+m" (*(volatile long *) (Model1_addr))
   : "Ir" (Model1_nr));
 }
}

/**
 * test_and_set_bit - Set a bit and return its old value
 * @nr: Bit to set
 * @addr: Address to count from
 *
 * This operation is atomic and cannot be reordered.
 * It also implies a memory barrier.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model1_test_and_set_bit(long Model1_nr, volatile unsigned long *Model1_addr)
{
 do { bool Model1_c; asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "bts" " %2, " "%0" ";" "\n\tset" "c" " %[_cc_" "c" "]\n" : "+m" (*Model1_addr), [_cc_c] "=qm" (Model1_c) : "Ir" (Model1_nr) : "memory"); return Model1_c; } while (0);
}

/**
 * test_and_set_bit_lock - Set a bit and return its old value for lock
 * @nr: Bit to set
 * @addr: Address to count from
 *
 * This is the same as test_and_set_bit on x86.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool
Model1_test_and_set_bit_lock(long Model1_nr, volatile unsigned long *Model1_addr)
{
 return Model1_test_and_set_bit(Model1_nr, Model1_addr);
}

/**
 * __test_and_set_bit - Set a bit and return its old value
 * @nr: Bit to set
 * @addr: Address to count from
 *
 * This operation is non-atomic and can be reordered.
 * If two examples of this operation race, one can appear to succeed
 * but actually fail.  You must protect multiple accesses with a lock.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model1___test_and_set_bit(long Model1_nr, volatile unsigned long *Model1_addr)
{
 bool Model1_oldbit;

 asm("bts %2,%1\n\t"
     "\n\tset" "c" " %[_cc_" "c" "]\n"
     : [_cc_c] "=qm" (Model1_oldbit), "+m" (*(volatile long *) (Model1_addr))
     : "Ir" (Model1_nr));
 return Model1_oldbit;
}

/**
 * test_and_clear_bit - Clear a bit and return its old value
 * @nr: Bit to clear
 * @addr: Address to count from
 *
 * This operation is atomic and cannot be reordered.
 * It also implies a memory barrier.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model1_test_and_clear_bit(long Model1_nr, volatile unsigned long *Model1_addr)
{
 do { bool Model1_c; asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "btr" " %2, " "%0" ";" "\n\tset" "c" " %[_cc_" "c" "]\n" : "+m" (*Model1_addr), [_cc_c] "=qm" (Model1_c) : "Ir" (Model1_nr) : "memory"); return Model1_c; } while (0);
}

/**
 * __test_and_clear_bit - Clear a bit and return its old value
 * @nr: Bit to clear
 * @addr: Address to count from
 *
 * This operation is non-atomic and can be reordered.
 * If two examples of this operation race, one can appear to succeed
 * but actually fail.  You must protect multiple accesses with a lock.
 *
 * Note: the operation is performed atomically with respect to
 * the local CPU, but not other CPUs. Portable code should not
 * rely on this behaviour.
 * KVM relies on this behaviour on x86 for modifying memory that is also
 * accessed from a hypervisor on the same CPU if running in a VM: don't change
 * this without also updating arch/x86/kernel/kvm.c
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model1___test_and_clear_bit(long Model1_nr, volatile unsigned long *Model1_addr)
{
 bool Model1_oldbit;

 asm volatile("btr %2,%1\n\t"
       "\n\tset" "c" " %[_cc_" "c" "]\n"
       : [_cc_c] "=qm" (Model1_oldbit), "+m" (*(volatile long *) (Model1_addr))
       : "Ir" (Model1_nr));
 return Model1_oldbit;
}

/* WARNING: non atomic and it can be reordered! */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model1___test_and_change_bit(long Model1_nr, volatile unsigned long *Model1_addr)
{
 bool Model1_oldbit;

 asm volatile("btc %2,%1\n\t"
       "\n\tset" "c" " %[_cc_" "c" "]\n"
       : [_cc_c] "=qm" (Model1_oldbit), "+m" (*(volatile long *) (Model1_addr))
       : "Ir" (Model1_nr) : "memory");

 return Model1_oldbit;
}

/**
 * test_and_change_bit - Change a bit and return its old value
 * @nr: Bit to change
 * @addr: Address to count from
 *
 * This operation is atomic and cannot be reordered.
 * It also implies a memory barrier.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model1_test_and_change_bit(long Model1_nr, volatile unsigned long *Model1_addr)
{
 do { bool Model1_c; asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "btc" " %2, " "%0" ";" "\n\tset" "c" " %[_cc_" "c" "]\n" : "+m" (*Model1_addr), [_cc_c] "=qm" (Model1_c) : "Ir" (Model1_nr) : "memory"); return Model1_c; } while (0);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model1_constant_test_bit(long Model1_nr, const volatile unsigned long *Model1_addr)
{
 return ((1UL << (Model1_nr & (64 -1))) &
  (Model1_addr[Model1_nr >> 6])) != 0;
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model1_variable_test_bit(long Model1_nr, volatile const unsigned long *Model1_addr)
{
 bool Model1_oldbit;

#if CY_ABSTRACT6
 Model1_oldbit = ((1UL << (Model1_nr)) & (*(unsigned long *)Model1_addr));
#else
 asm volatile("bt %2,%1\n\t"
       "\n\tset" "c" " %[_cc_" "c" "]\n"
       : [_cc_c] "=qm" (Model1_oldbit)
       : "m" (*(unsigned long *)Model1_addr), "Ir" (Model1_nr));
#endif
 return Model1_oldbit;
}
/**
 * __ffs - find first set bit in word
 * @word: The word to search
 *
 * Undefined if no bit exists, so code should check against 0 first.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) unsigned long Model1___ffs(unsigned long Model1_word)
{
 asm("rep; bsf %1,%0"
  : "=r" (Model1_word)
  : "rm" (Model1_word));
 return Model1_word;
}

/**
 * ffz - find first zero bit in word
 * @word: The word to search
 *
 * Undefined if no zero exists, so code should check against ~0UL first.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) unsigned long Model1_ffz(unsigned long Model1_word)
{
 asm("rep; bsf %1,%0"
  : "=r" (Model1_word)
  : "r" (~Model1_word));
 return Model1_word;
}

/*
 * __fls: find last set bit in word
 * @word: The word to search
 *
 * Undefined if no set bit exists, so code should check against 0 first.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) unsigned long Model1___fls(unsigned long Model1_word)
{
#if CY_ABSTRACT6
    return (sizeof(Model1_word)*8)-1-__builtin_clzl(Model1_word);
#else
 asm("bsr %1,%0"
     : "=r" (Model1_word)
     : "rm" (Model1_word));
 return Model1_word;
#endif
}




/**
 * ffs - find first set bit in word
 * @x: the word to search
 *
 * This is defined the same way as the libc and compiler builtin ffs
 * routines, therefore differs in spirit from the other bitops.
 *
 * ffs(value) returns 0 if value is 0 or the position of the first
 * set bit if value is nonzero. The first (least significant) bit
 * is at position 1.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_ffs(int Model1_x)
{
 int Model1_r;


 /*
	 * AMD64 says BSFL won't clobber the dest reg if x==0; Intel64 says the
	 * dest reg is undefined if x==0, but their CPU architect says its
	 * value is written to set it to the same as before, except that the
	 * top 32 bits will be cleared.
	 *
	 * We cannot do this on 32 bits because at the very least some
	 * 486 CPUs did not behave this way.
	 */
 asm("bsfl %1,%0"
     : "=r" (Model1_r)
     : "rm" (Model1_x), "0" (-1));
 return Model1_r + 1;
}

/**
 * fls - find last set bit in word
 * @x: the word to search
 *
 * This is defined in a similar way as the libc and compiler builtin
 * ffs, but returns the position of the most significant set bit.
 *
 * fls(value) returns 0 if value is 0 or the position of the last
 * set bit if value is nonzero. The last (most significant) bit is
 * at position 32.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_fls(int Model1_x)
{
 int Model1_r;


 /*
	 * AMD64 says BSRL won't clobber the dest reg if x==0; Intel64 says the
	 * dest reg is undefined if x==0, but their CPU architect says its
	 * value is written to set it to the same as before, except that the
	 * top 32 bits will be cleared.
	 *
	 * We cannot do this on 32 bits because at the very least some
	 * 486 CPUs did not behave this way.
	 */
 asm("bsrl %1,%0"
     : "=r" (Model1_r)
     : "rm" (Model1_x), "0" (-1));
 return Model1_r + 1;
}

/**
 * fls64 - find last set bit in a 64-bit word
 * @x: the word to search
 *
 * This is defined in a similar way as the libc and compiler builtin
 * ffsll, but returns the position of the most significant set bit.
 *
 * fls64(value) returns 0 if value is 0 or the position of the last
 * set bit if value is nonzero. The last (most significant) bit is
 * at position 64.
 */

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_fls64(__u64 Model1_x)
{
#if CY_ABSTRACT6
    if (Model1_x == 0)
        return 0;
    return Model1___fls(Model1_x)+1;
#else
 int Model1_bitpos = -1;
 /*
	 * AMD64 says BSRQ won't clobber the dest reg if x==0; Intel64 says the
	 * dest reg is undefined if x==0, but their CPU architect says its
	 * value is written to set it to the same as before.
	 */
 asm("bsrq %1,%q0"
     : "+r" (Model1_bitpos)
     : "rm" (Model1_x));
 return Model1_bitpos + 1;
#endif
}









/**
 * find_next_bit - find the next set bit in a memory region
 * @addr: The address to base the search on
 * @offset: The bitnumber to start searching at
 * @size: The bitmap size in bits
 *
 * Returns the bit number for the next set bit
 * If no bits are set, returns @size.
 */
extern unsigned long Model1_find_next_bit(const unsigned long *Model1_addr, unsigned long
  Model1_size, unsigned long Model1_offset);



/**
 * find_next_zero_bit - find the next cleared bit in a memory region
 * @addr: The address to base the search on
 * @offset: The bitnumber to start searching at
 * @size: The bitmap size in bits
 *
 * Returns the bit number of the next zero bit
 * If no bits are zero, returns @size.
 */
extern unsigned long Model1_find_next_zero_bit(const unsigned long *Model1_addr, unsigned
  long Model1_size, unsigned long Model1_offset);




/**
 * find_first_bit - find the first set bit in a memory region
 * @addr: The address to start the search at
 * @size: The maximum number of bits to search
 *
 * Returns the bit number of the first set bit.
 * If no bits are set, returns @size.
 */
extern unsigned long Model1_find_first_bit(const unsigned long *Model1_addr,
        unsigned long Model1_size);

/**
 * find_first_zero_bit - find the first cleared bit in a memory region
 * @addr: The address to start the search at
 * @size: The maximum number of bits to search
 *
 * Returns the bit number of the first cleared bit.
 * If no bits are zero, returns @size.
 */
extern unsigned long Model1_find_first_zero_bit(const unsigned long *Model1_addr,
      unsigned long Model1_size);







/*
 * Every architecture must define this function. It's the fastest
 * way of searching a 100-bit bitmap.  It's guaranteed that at least
 * one of the 100 bits is cleared.
 */
static inline __attribute__((no_instrument_function)) int Model1_sched_find_first_bit(const unsigned long *Model1_b)
{

 if (Model1_b[0])
  return Model1___ffs(Model1_b[0]);
 return Model1___ffs(Model1_b[1]) + 64;
}











/* Define minimum CPUID feature set for kernel These bits are checked
   really early to actually display a visible error message before the
   kernel dies.  Make sure to assign features to the proper mask!

   Some requirements that are not in CPUID yet are also in the
   CONFIG_X86_MINIMUM_CPU_FAMILY which is checked too.

   The real information is in arch/x86/Kconfig.cpu, this just converts
   the CONFIGs into a bitmask */






/* These features, although they might be available in a CPU
 * will not be used because the compile options to support
 * them are not present.
 *
 * This code allows them to be checked and disabled at
 * compile time without an explicit #ifdef.  Use
 * cpu_feature_enabled().
 */
/*
 * Make sure to add features to the correct mask
 */


/*
 * Defines x86 CPU feature bits
 */



/*
 * Note: If the comment begins with a quoted string, that string is used
 * in /proc/cpuinfo instead of the macro name.  If the string is "",
 * this feature bit is not displayed in /proc/cpuinfo at all.
 */

/* Intel-defined CPU features, CPUID level 0x00000001 (edx), word 0 */
       /* (plus FCMOVcc, FCOMI with FPU) */
/* AMD-defined CPU features, CPUID level 0x80000001, word 1 */
/* Don't duplicate feature flags which are redundant with Intel! */
/* Transmeta-defined CPU features, CPUID level 0x80860001, word 2 */




/* Other features, Linux-defined mapping, word 3 */
/* This range is used for feature bits which conflict or are synthesized */




/* cpu types for specific tunings: */
/* free, was #define X86_FEATURE_CLFLUSH_MONITOR ( 3*32+25) * "" clflush reqd with monitor */







/* Intel-defined CPU features, CPUID level 0x00000001 (ecx), word 4 */
/* VIA/Cyrix/Centaur-defined CPU features, CPUID level 0xC0000001, word 5 */
/* More extended AMD flags: CPUID level 0x80000001, ecx, word 6 */
/*
 * Auxiliary flags: Linux defined - For features scattered in various
 * CPUID levels like 0x6, 0xA etc, word 7.
 *
 * Reuse free bits when adding new feature flags!
 */
/* Virtualization flags: Linux defined, word 8 */
/* Intel-defined CPU features, CPUID level 0x00000007:0 (ebx), word 9 */
/* Extended state features, CPUID level 0x0000000d:1 (eax), word 10 */





/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:0 (edx), word 11 */


/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:1 (edx), word 12 */




/* AMD-defined CPU features, CPUID level 0x80000008 (ebx), word 13 */



/* Thermal and Power Management Leaf, CPUID level 0x00000006 (eax), word 14 */
/* AMD SVM Feature Identification, CPUID level 0x8000000a (edx), word 15 */
/* Intel-defined CPU features, CPUID level 0x00000007:0 (ecx), word 16 */



/* AMD-defined CPU features, CPUID level 0x80000007 (ebx), word 17 */




/*
 * BUG word(s)
 */


/* popcnt %edi, %eax */

/* popcnt %rdi, %rax */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) unsigned int Model1___arch_hweight32(unsigned int Model1_w)
{
 unsigned int Model1_res;

 asm ("661:\n\t" "call __sw_hweight32" "\n662:\n" ".skip -(((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")) > 0) * " "((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")),0x90\n" "663" ":\n" ".pushsection .altinstructions,\"a\"\n" " .long 661b - .\n" " .long " "664""1""f - .\n" " .word " "( 4*32+23)" "\n" " .byte " "663""b-661b" "\n" " .byte " "665""1""f-""664""1""f" "\n" " .byte " "663""b-662b" "\n" ".popsection\n" ".pushsection .altinstr_replacement, \"ax\"\n" "664""1"":\n\t" ".byte 0xf3,0x0f,0xb8,0xc7" "\n" "665""1" ":\n\t" ".popsection"
    : "=""a" (Model1_res)
    : "D" (Model1_w));

 return Model1_res;
}

static inline __attribute__((no_instrument_function)) unsigned int Model1___arch_hweight16(unsigned int Model1_w)
{
 return Model1___arch_hweight32(Model1_w & 0xffff);
}

static inline __attribute__((no_instrument_function)) unsigned int Model1___arch_hweight8(unsigned int Model1_w)
{
 return Model1___arch_hweight32(Model1_w & 0xff);
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) unsigned long Model1___arch_hweight64(__u64 Model1_w)
{
 unsigned long Model1_res;

 asm ("661:\n\t" "call __sw_hweight64" "\n662:\n" ".skip -(((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")) > 0) * " "((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")),0x90\n" "663" ":\n" ".pushsection .altinstructions,\"a\"\n" " .long 661b - .\n" " .long " "664""1""f - .\n" " .word " "( 4*32+23)" "\n" " .byte " "663""b-661b" "\n" " .byte " "665""1""f-""664""1""f" "\n" " .byte " "663""b-662b" "\n" ".popsection\n" ".pushsection .altinstr_replacement, \"ax\"\n" "664""1"":\n\t" ".byte 0xf3,0x48,0x0f,0xb8,0xc7" "\n" "665""1" ":\n\t" ".popsection"
    : "=""a" (Model1_res)
    : "D" (Model1_w));

 return Model1_res;
}




/*
 * Compile time versions of __arch_hweightN()
 */
/*
 * Generic interface.
 */





/*
 * Interface for known constant arguments
 */





/*
 * Type invariant interface to the compile time constant hweight functions.
 */

























static inline __attribute__((no_instrument_function)) __attribute__((__const__)) __u32 Model1___arch_swab32(__u32 Model1_val)
{
 asm("bswapl %0" : "=r" (Model1_val) : "0" (Model1_val));
 return Model1_val;
}


static inline __attribute__((no_instrument_function)) __attribute__((__const__)) __u64 Model1___arch_swab64(__u64 Model1_val)
{
 asm("bswapq %0" : "=r" (Model1_val) : "0" (Model1_val));
 return Model1_val;

}

/*
 * casts are necessary for constants, because we never know how for sure
 * how U/UL/ULL map to __u16, __u32, __u64. At least not in a portable way.
 */
/*
 * Implement the following as inlines, but define the interface using
 * macros to allow constant folding when possible:
 * ___swab16, ___swab32, ___swab64, ___swahw32, ___swahb32
 */

static inline __attribute__((no_instrument_function)) __attribute__((__const__)) Model1___u16 Model1___fswab16(Model1___u16 Model1_val)
{



 return ((Model1___u16)( (((Model1___u16)(Model1_val) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)(Model1_val) & (Model1___u16)0xff00U) >> 8)));

}

static inline __attribute__((no_instrument_function)) __attribute__((__const__)) __u32 Model1___fswab32(__u32 Model1_val)
{

 return Model1___arch_swab32(Model1_val);



}

static inline __attribute__((no_instrument_function)) __attribute__((__const__)) __u64 Model1___fswab64(__u64 Model1_val)
{

 return Model1___arch_swab64(Model1_val);







}

static inline __attribute__((no_instrument_function)) __attribute__((__const__)) __u32 Model1___fswahw32(__u32 Model1_val)
{



 return ((__u32)( (((__u32)(Model1_val) & (__u32)0x0000ffffUL) << 16) | (((__u32)(Model1_val) & (__u32)0xffff0000UL) >> 16)));

}

static inline __attribute__((no_instrument_function)) __attribute__((__const__)) __u32 Model1___fswahb32(__u32 Model1_val)
{



 return ((__u32)( (((__u32)(Model1_val) & (__u32)0x00ff00ffUL) << 8) | (((__u32)(Model1_val) & (__u32)0xff00ff00UL) >> 8)));

}

/**
 * __swab16 - return a byteswapped 16-bit value
 * @x: value to byteswap
 */
/**
 * __swab32 - return a byteswapped 32-bit value
 * @x: value to byteswap
 */
/**
 * __swab64 - return a byteswapped 64-bit value
 * @x: value to byteswap
 */
/**
 * __swahw32 - return a word-swapped 32-bit value
 * @x: value to wordswap
 *
 * __swahw32(0x12340000) is 0x00001234
 */





/**
 * __swahb32 - return a high and low byte-swapped 32-bit value
 * @x: value to byteswap
 *
 * __swahb32(0x12345678) is 0x34127856
 */





/**
 * __swab16p - return a byteswapped 16-bit value from a pointer
 * @p: pointer to a naturally-aligned 16-bit value
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model1___u16 Model1___swab16p(const Model1___u16 *Model1_p)
{



 return (__builtin_constant_p((Model1___u16)(*Model1_p)) ? ((Model1___u16)( (((Model1___u16)(*Model1_p) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)(*Model1_p) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16(*Model1_p));

}

/**
 * __swab32p - return a byteswapped 32-bit value from a pointer
 * @p: pointer to a naturally-aligned 32-bit value
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) __u32 Model1___swab32p(const __u32 *Model1_p)
{



 return (__builtin_constant_p((__u32)(*Model1_p)) ? ((__u32)( (((__u32)(*Model1_p) & (__u32)0x000000ffUL) << 24) | (((__u32)(*Model1_p) & (__u32)0x0000ff00UL) << 8) | (((__u32)(*Model1_p) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(*Model1_p) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32(*Model1_p));

}

/**
 * __swab64p - return a byteswapped 64-bit value from a pointer
 * @p: pointer to a naturally-aligned 64-bit value
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) __u64 Model1___swab64p(const __u64 *Model1_p)
{



 return (__builtin_constant_p((__u64)(*Model1_p)) ? ((__u64)( (((__u64)(*Model1_p) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(*Model1_p) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(*Model1_p) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(*Model1_p) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(*Model1_p) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(*Model1_p) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(*Model1_p) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(*Model1_p) & (__u64)0xff00000000000000ULL) >> 56))) : Model1___fswab64(*Model1_p));

}

/**
 * __swahw32p - return a wordswapped 32-bit value from a pointer
 * @p: pointer to a naturally-aligned 32-bit value
 *
 * See __swahw32() for details of wordswapping.
 */
static inline __attribute__((no_instrument_function)) __u32 Model1___swahw32p(const __u32 *Model1_p)
{



 return (__builtin_constant_p((__u32)(*Model1_p)) ? ((__u32)( (((__u32)(*Model1_p) & (__u32)0x0000ffffUL) << 16) | (((__u32)(*Model1_p) & (__u32)0xffff0000UL) >> 16))) : Model1___fswahw32(*Model1_p));

}

/**
 * __swahb32p - return a high and low byteswapped 32-bit value from a pointer
 * @p: pointer to a naturally-aligned 32-bit value
 *
 * See __swahb32() for details of high/low byteswapping.
 */
static inline __attribute__((no_instrument_function)) __u32 Model1___swahb32p(const __u32 *Model1_p)
{



 return (__builtin_constant_p((__u32)(*Model1_p)) ? ((__u32)( (((__u32)(*Model1_p) & (__u32)0x00ff00ffUL) << 8) | (((__u32)(*Model1_p) & (__u32)0xff00ff00UL) >> 8))) : Model1___fswahb32(*Model1_p));

}

/**
 * __swab16s - byteswap a 16-bit value in-place
 * @p: pointer to a naturally-aligned 16-bit value
 */
static inline __attribute__((no_instrument_function)) void Model1___swab16s(Model1___u16 *Model1_p)
{



 *Model1_p = Model1___swab16p(Model1_p);

}
/**
 * __swab32s - byteswap a 32-bit value in-place
 * @p: pointer to a naturally-aligned 32-bit value
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___swab32s(__u32 *Model1_p)
{



 *Model1_p = Model1___swab32p(Model1_p);

}

/**
 * __swab64s - byteswap a 64-bit value in-place
 * @p: pointer to a naturally-aligned 64-bit value
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___swab64s(__u64 *Model1_p)
{



 *Model1_p = Model1___swab64p(Model1_p);

}

/**
 * __swahw32s - wordswap a 32-bit value in-place
 * @p: pointer to a naturally-aligned 32-bit value
 *
 * See __swahw32() for details of wordswapping
 */
static inline __attribute__((no_instrument_function)) void Model1___swahw32s(__u32 *Model1_p)
{



 *Model1_p = Model1___swahw32p(Model1_p);

}

/**
 * __swahb32s - high and low byteswap a 32-bit value in-place
 * @p: pointer to a naturally-aligned 32-bit value
 *
 * See __swahb32() for details of high and low byte swapping
 */
static inline __attribute__((no_instrument_function)) void Model1___swahb32s(__u32 *Model1_p)
{



 *Model1_p = Model1___swahb32p(Model1_p);

}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model1___le64 Model1___cpu_to_le64p(const __u64 *Model1_p)
{
 return ( Model1___le64)*Model1_p;
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) __u64 Model1___le64_to_cpup(const Model1___le64 *Model1_p)
{
 return ( __u64)*Model1_p;
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model1___le32 Model1___cpu_to_le32p(const __u32 *Model1_p)
{
 return ( Model1___le32)*Model1_p;
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) __u32 Model1___le32_to_cpup(const Model1___le32 *Model1_p)
{
 return ( __u32)*Model1_p;
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model1___le16 Model1___cpu_to_le16p(const Model1___u16 *Model1_p)
{
 return ( Model1___le16)*Model1_p;
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model1___u16 Model1___le16_to_cpup(const Model1___le16 *Model1_p)
{
 return ( Model1___u16)*Model1_p;
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model1___be64 Model1___cpu_to_be64p(const __u64 *Model1_p)
{
 return ( Model1___be64)Model1___swab64p(Model1_p);
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) __u64 Model1___be64_to_cpup(const Model1___be64 *Model1_p)
{
 return Model1___swab64p((__u64 *)Model1_p);
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model1___be32 Model1___cpu_to_be32p(const __u32 *Model1_p)
{
 return ( Model1___be32)Model1___swab32p(Model1_p);
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) __u32 Model1___be32_to_cpup(const Model1___be32 *Model1_p)
{
 return Model1___swab32p((__u32 *)Model1_p);
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model1___be16 Model1___cpu_to_be16p(const Model1___u16 *Model1_p)
{
 return ( Model1___be16)Model1___swab16p(Model1_p);
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model1___u16 Model1___be16_to_cpup(const Model1___be16 *Model1_p)
{
 return Model1___swab16p((Model1___u16 *)Model1_p);
}




/*
 * linux/byteorder/generic.h
 * Generic Byte-reordering support
 *
 * The "... p" macros, like le64_to_cpup, can be used with pointers
 * to unaligned data, but there will be a performance penalty on 
 * some architectures.  Use get_unaligned for unaligned data.
 *
 * Francois-Rene Rideau <fare@tunes.org> 19970707
 *    gathered all the good ideas from all asm-foo/byteorder.h into one file,
 *    cleaned them up.
 *    I hope it is compliant with non-GCC compilers.
 *    I decided to put __BYTEORDER_HAS_U64__ in byteorder.h,
 *    because I wasn't sure it would be ok to put it in types.h
 *    Upgraded it to 2.1.43
 * Francois-Rene Rideau <fare@tunes.org> 19971012
 *    Upgraded it to 2.1.57
 *    to please Linus T., replaced huge #ifdef's between little/big endian
 *    by nestedly #include'd files.
 * Francois-Rene Rideau <fare@tunes.org> 19971205
 *    Made it to 2.1.71; now a facelift:
 *    Put files under include/linux/byteorder/
 *    Split swab from generic support.
 *
 * TODO:
 *   = Regular kernel maintainers could also replace all these manual
 *    byteswap macros that remain, disseminated among drivers,
 *    after some grep or the sources...
 *   = Linus might want to rename all these macros and files to fit his taste,
 *    to fit his personal naming scheme.
 *   = it seems that a few drivers would also appreciate
 *    nybble swapping support...
 *   = every architecture could add their byteswap macro in asm/byteorder.h
 *    see how some architectures already do (i386, alpha, ppc, etc)
 *   = cpu_to_beXX and beXX_to_cpu might some day need to be well
 *    distinguished throughout the kernel. This is not the case currently,
 *    since little endian, big endian, and pdp endian machines needn't it.
 *    But this might be the case for, say, a port of Linux to 20/21 bit
 *    architectures (and F21 Linux addict around?).
 */

/*
 * The following macros are to be defined by <asm/byteorder.h>:
 *
 * Conversion of long and short int between network and host format
 *	ntohl(__u32 x)
 *	ntohs(__u16 x)
 *	htonl(__u32 x)
 *	htons(__u16 x)
 * It seems that some programs (which? where? or perhaps a standard? POSIX?)
 * might like the above to be functions, not macros (why?).
 * if that's true, then detect them, and take measures.
 * Anyway, the measure is: define only ___ntohl as a macro instead,
 * and in a separate file, have
 * unsigned long inline ntohl(x){return ___ntohl(x);}
 *
 * The same for constant arguments
 *	__constant_ntohl(__u32 x)
 *	__constant_ntohs(__u16 x)
 *	__constant_htonl(__u32 x)
 *	__constant_htons(__u16 x)
 *
 * Conversion of XX-bit integers (16- 32- or 64-)
 * between native CPU format and little/big endian format
 * 64-bit stuff only defined for proper architectures
 *	cpu_to_[bl]eXX(__uXX x)
 *	[bl]eXX_to_cpu(__uXX x)
 *
 * The same, but takes a pointer to the value to convert
 *	cpu_to_[bl]eXXp(__uXX x)
 *	[bl]eXX_to_cpup(__uXX x)
 *
 * The same, but change in situ
 *	cpu_to_[bl]eXXs(__uXX x)
 *	[bl]eXX_to_cpus(__uXX x)
 *
 * See asm-foo/byteorder.h for examples of how to provide
 * architecture-optimized versions
 *
 */
/*
 * They have to be macros in order to do the constant folding
 * correctly - if the argument passed into a inline function
 * it is no longer constant according to gcc..
 */
static inline __attribute__((no_instrument_function)) void Model1_le16_add_cpu(Model1___le16 *Model1_var, Model1_u16 Model1_val)
{
 *Model1_var = (( Model1___le16)(Model1___u16)((( Model1___u16)(Model1___le16)(*Model1_var)) + Model1_val));
}

static inline __attribute__((no_instrument_function)) void Model1_le32_add_cpu(Model1___le32 *Model1_var, Model1_u32 Model1_val)
{
 *Model1_var = (( Model1___le32)(__u32)((( __u32)(Model1___le32)(*Model1_var)) + Model1_val));
}

static inline __attribute__((no_instrument_function)) void Model1_le64_add_cpu(Model1___le64 *Model1_var, Model1_u64 Model1_val)
{
 *Model1_var = (( Model1___le64)(__u64)((( __u64)(Model1___le64)(*Model1_var)) + Model1_val));
}

static inline __attribute__((no_instrument_function)) void Model1_be16_add_cpu(Model1___be16 *Model1_var, Model1_u16 Model1_val)
{
 *Model1_var = (( Model1___be16)(__builtin_constant_p((Model1___u16)(((__builtin_constant_p((Model1___u16)(( Model1___u16)(Model1___be16)(*Model1_var))) ? ((Model1___u16)( (((Model1___u16)(( Model1___u16)(Model1___be16)(*Model1_var)) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)(( Model1___u16)(Model1___be16)(*Model1_var)) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16(( Model1___u16)(Model1___be16)(*Model1_var))) + Model1_val))) ? ((Model1___u16)( (((Model1___u16)(((__builtin_constant_p((Model1___u16)(( Model1___u16)(Model1___be16)(*Model1_var))) ? ((Model1___u16)( (((Model1___u16)(( Model1___u16)(Model1___be16)(*Model1_var)) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)(( Model1___u16)(Model1___be16)(*Model1_var)) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16(( Model1___u16)(Model1___be16)(*Model1_var))) + Model1_val)) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)(((__builtin_constant_p((Model1___u16)(( Model1___u16)(Model1___be16)(*Model1_var))) ? ((Model1___u16)( (((Model1___u16)(( Model1___u16)(Model1___be16)(*Model1_var)) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)(( Model1___u16)(Model1___be16)(*Model1_var)) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16(( Model1___u16)(Model1___be16)(*Model1_var))) + Model1_val)) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16(((__builtin_constant_p((Model1___u16)(( Model1___u16)(Model1___be16)(*Model1_var))) ? ((Model1___u16)( (((Model1___u16)(( Model1___u16)(Model1___be16)(*Model1_var)) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)(( Model1___u16)(Model1___be16)(*Model1_var)) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16(( Model1___u16)(Model1___be16)(*Model1_var))) + Model1_val))));
}

static inline __attribute__((no_instrument_function)) void Model1_be32_add_cpu(Model1___be32 *Model1_var, Model1_u32 Model1_val)
{
 *Model1_var = (( Model1___be32)(__builtin_constant_p((__u32)(((__builtin_constant_p((__u32)(( __u32)(Model1___be32)(*Model1_var))) ? ((__u32)( (((__u32)(( __u32)(Model1___be32)(*Model1_var)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model1___be32)(*Model1_var)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model1___be32)(*Model1_var)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model1___be32)(*Model1_var)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32(( __u32)(Model1___be32)(*Model1_var))) + Model1_val))) ? ((__u32)( (((__u32)(((__builtin_constant_p((__u32)(( __u32)(Model1___be32)(*Model1_var))) ? ((__u32)( (((__u32)(( __u32)(Model1___be32)(*Model1_var)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model1___be32)(*Model1_var)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model1___be32)(*Model1_var)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model1___be32)(*Model1_var)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32(( __u32)(Model1___be32)(*Model1_var))) + Model1_val)) & (__u32)0x000000ffUL) << 24) | (((__u32)(((__builtin_constant_p((__u32)(( __u32)(Model1___be32)(*Model1_var))) ? ((__u32)( (((__u32)(( __u32)(Model1___be32)(*Model1_var)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model1___be32)(*Model1_var)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model1___be32)(*Model1_var)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model1___be32)(*Model1_var)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32(( __u32)(Model1___be32)(*Model1_var))) + Model1_val)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(((__builtin_constant_p((__u32)(( __u32)(Model1___be32)(*Model1_var))) ? ((__u32)( (((__u32)(( __u32)(Model1___be32)(*Model1_var)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model1___be32)(*Model1_var)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model1___be32)(*Model1_var)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model1___be32)(*Model1_var)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32(( __u32)(Model1___be32)(*Model1_var))) + Model1_val)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(((__builtin_constant_p((__u32)(( __u32)(Model1___be32)(*Model1_var))) ? ((__u32)( (((__u32)(( __u32)(Model1___be32)(*Model1_var)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model1___be32)(*Model1_var)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model1___be32)(*Model1_var)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model1___be32)(*Model1_var)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32(( __u32)(Model1___be32)(*Model1_var))) + Model1_val)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32(((__builtin_constant_p((__u32)(( __u32)(Model1___be32)(*Model1_var))) ? ((__u32)( (((__u32)(( __u32)(Model1___be32)(*Model1_var)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model1___be32)(*Model1_var)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model1___be32)(*Model1_var)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model1___be32)(*Model1_var)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32(( __u32)(Model1___be32)(*Model1_var))) + Model1_val))));
}

static inline __attribute__((no_instrument_function)) void Model1_be64_add_cpu(Model1___be64 *Model1_var, Model1_u64 Model1_val)
{
 *Model1_var = (( Model1___be64)(__builtin_constant_p((__u64)(((__builtin_constant_p((__u64)(( __u64)(Model1___be64)(*Model1_var))) ? ((__u64)( (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0xff00000000000000ULL) >> 56))) : Model1___fswab64(( __u64)(Model1___be64)(*Model1_var))) + Model1_val))) ? ((__u64)( (((__u64)(((__builtin_constant_p((__u64)(( __u64)(Model1___be64)(*Model1_var))) ? ((__u64)( (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0xff00000000000000ULL) >> 56))) : Model1___fswab64(( __u64)(Model1___be64)(*Model1_var))) + Model1_val)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(((__builtin_constant_p((__u64)(( __u64)(Model1___be64)(*Model1_var))) ? ((__u64)( (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0xff00000000000000ULL) >> 56))) : Model1___fswab64(( __u64)(Model1___be64)(*Model1_var))) + Model1_val)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(((__builtin_constant_p((__u64)(( __u64)(Model1___be64)(*Model1_var))) ? ((__u64)( (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0xff00000000000000ULL) >> 56))) : Model1___fswab64(( __u64)(Model1___be64)(*Model1_var))) + Model1_val)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(((__builtin_constant_p((__u64)(( __u64)(Model1___be64)(*Model1_var))) ? ((__u64)( (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0xff00000000000000ULL) >> 56))) : Model1___fswab64(( __u64)(Model1___be64)(*Model1_var))) + Model1_val)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(((__builtin_constant_p((__u64)(( __u64)(Model1___be64)(*Model1_var))) ? ((__u64)( (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0xff00000000000000ULL) >> 56))) : Model1___fswab64(( __u64)(Model1___be64)(*Model1_var))) + Model1_val)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(((__builtin_constant_p((__u64)(( __u64)(Model1___be64)(*Model1_var))) ? ((__u64)( (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0xff00000000000000ULL) >> 56))) : Model1___fswab64(( __u64)(Model1___be64)(*Model1_var))) + Model1_val)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(((__builtin_constant_p((__u64)(( __u64)(Model1___be64)(*Model1_var))) ? ((__u64)( (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0xff00000000000000ULL) >> 56))) : Model1___fswab64(( __u64)(Model1___be64)(*Model1_var))) + Model1_val)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(((__builtin_constant_p((__u64)(( __u64)(Model1___be64)(*Model1_var))) ? ((__u64)( (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0xff00000000000000ULL) >> 56))) : Model1___fswab64(( __u64)(Model1___be64)(*Model1_var))) + Model1_val)) & (__u64)0xff00000000000000ULL) >> 56))) : Model1___fswab64(((__builtin_constant_p((__u64)(( __u64)(Model1___be64)(*Model1_var))) ? ((__u64)( (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(Model1___be64)(*Model1_var)) & (__u64)0xff00000000000000ULL) >> 56))) : Model1___fswab64(( __u64)(Model1___be64)(*Model1_var))) + Model1_val))));
}





static inline __attribute__((no_instrument_function)) unsigned long Model1_find_next_zero_bit_le(const void *Model1_addr,
  unsigned long Model1_size, unsigned long Model1_offset)
{
 return Model1_find_next_zero_bit(Model1_addr, Model1_size, Model1_offset);
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_find_next_bit_le(const void *Model1_addr,
  unsigned long Model1_size, unsigned long Model1_offset)
{
 return Model1_find_next_bit(Model1_addr, Model1_size, Model1_offset);
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_find_first_zero_bit_le(const void *Model1_addr,
  unsigned long Model1_size)
{
 return Model1_find_first_zero_bit(Model1_addr, Model1_size);
}
static inline __attribute__((no_instrument_function)) int Model1_test_bit_le(int Model1_nr, const void *Model1_addr)
{
 return (__builtin_constant_p((Model1_nr ^ 0)) ? Model1_constant_test_bit((Model1_nr ^ 0), (Model1_addr)) : Model1_variable_test_bit((Model1_nr ^ 0), (Model1_addr)));
}

static inline __attribute__((no_instrument_function)) void Model1_set_bit_le(int Model1_nr, void *Model1_addr)
{
 Model1_set_bit(Model1_nr ^ 0, Model1_addr);
}

static inline __attribute__((no_instrument_function)) void Model1_clear_bit_le(int Model1_nr, void *Model1_addr)
{
 Model1_clear_bit(Model1_nr ^ 0, Model1_addr);
}

static inline __attribute__((no_instrument_function)) void Model1___set_bit_le(int Model1_nr, void *Model1_addr)
{
 Model1___set_bit(Model1_nr ^ 0, Model1_addr);
}

static inline __attribute__((no_instrument_function)) void Model1___clear_bit_le(int Model1_nr, void *Model1_addr)
{
 Model1___clear_bit(Model1_nr ^ 0, Model1_addr);
}

static inline __attribute__((no_instrument_function)) int Model1_test_and_set_bit_le(int Model1_nr, void *Model1_addr)
{
 return Model1_test_and_set_bit(Model1_nr ^ 0, Model1_addr);
}

static inline __attribute__((no_instrument_function)) int Model1_test_and_clear_bit_le(int Model1_nr, void *Model1_addr)
{
 return Model1_test_and_clear_bit(Model1_nr ^ 0, Model1_addr);
}

static inline __attribute__((no_instrument_function)) int Model1___test_and_set_bit_le(int Model1_nr, void *Model1_addr)
{
 return Model1___test_and_set_bit(Model1_nr ^ 0, Model1_addr);
}

static inline __attribute__((no_instrument_function)) int Model1___test_and_clear_bit_le(int Model1_nr, void *Model1_addr)
{
 return Model1___test_and_clear_bit(Model1_nr ^ 0, Model1_addr);
}




/*
 * Atomic bitops based version of ext2 atomic bitops
 */






/* same as for_each_set_bit() but use bit as value to start with */
/* same as for_each_clear_bit() but use bit as value to start with */





static inline __attribute__((no_instrument_function)) int Model1_get_bitmask_order(unsigned int Model1_count)
{
 int Model1_order;

 Model1_order = Model1_fls(Model1_count);
 return Model1_order; /* We could be slightly more clever with -1 here... */
}

static inline __attribute__((no_instrument_function)) int Model1_get_count_order(unsigned int Model1_count)
{
 int Model1_order;

 Model1_order = Model1_fls(Model1_count) - 1;
 if (Model1_count & (Model1_count - 1))
  Model1_order++;
 return Model1_order;
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) unsigned long Model1_hweight_long(unsigned long Model1_w)
{
 return sizeof(Model1_w) == 4 ? (__builtin_constant_p(Model1_w) ? ((((unsigned int) ((!!((Model1_w) & (1ULL << 0))) + (!!((Model1_w) & (1ULL << 1))) + (!!((Model1_w) & (1ULL << 2))) + (!!((Model1_w) & (1ULL << 3))) + (!!((Model1_w) & (1ULL << 4))) + (!!((Model1_w) & (1ULL << 5))) + (!!((Model1_w) & (1ULL << 6))) + (!!((Model1_w) & (1ULL << 7))))) + ((unsigned int) ((!!(((Model1_w) >> 8) & (1ULL << 0))) + (!!(((Model1_w) >> 8) & (1ULL << 1))) + (!!(((Model1_w) >> 8) & (1ULL << 2))) + (!!(((Model1_w) >> 8) & (1ULL << 3))) + (!!(((Model1_w) >> 8) & (1ULL << 4))) + (!!(((Model1_w) >> 8) & (1ULL << 5))) + (!!(((Model1_w) >> 8) & (1ULL << 6))) + (!!(((Model1_w) >> 8) & (1ULL << 7)))))) + (((unsigned int) ((!!(((Model1_w) >> 16) & (1ULL << 0))) + (!!(((Model1_w) >> 16) & (1ULL << 1))) + (!!(((Model1_w) >> 16) & (1ULL << 2))) + (!!(((Model1_w) >> 16) & (1ULL << 3))) + (!!(((Model1_w) >> 16) & (1ULL << 4))) + (!!(((Model1_w) >> 16) & (1ULL << 5))) + (!!(((Model1_w) >> 16) & (1ULL << 6))) + (!!(((Model1_w) >> 16) & (1ULL << 7))))) + ((unsigned int) ((!!((((Model1_w) >> 16) >> 8) & (1ULL << 0))) + (!!((((Model1_w) >> 16) >> 8) & (1ULL << 1))) + (!!((((Model1_w) >> 16) >> 8) & (1ULL << 2))) + (!!((((Model1_w) >> 16) >> 8) & (1ULL << 3))) + (!!((((Model1_w) >> 16) >> 8) & (1ULL << 4))) + (!!((((Model1_w) >> 16) >> 8) & (1ULL << 5))) + (!!((((Model1_w) >> 16) >> 8) & (1ULL << 6))) + (!!((((Model1_w) >> 16) >> 8) & (1ULL << 7))))))) : Model1___arch_hweight32(Model1_w)) : (__builtin_constant_p(Model1_w) ? (((((unsigned int) ((!!((Model1_w) & (1ULL << 0))) + (!!((Model1_w) & (1ULL << 1))) + (!!((Model1_w) & (1ULL << 2))) + (!!((Model1_w) & (1ULL << 3))) + (!!((Model1_w) & (1ULL << 4))) + (!!((Model1_w) & (1ULL << 5))) + (!!((Model1_w) & (1ULL << 6))) + (!!((Model1_w) & (1ULL << 7))))) + ((unsigned int) ((!!(((Model1_w) >> 8) & (1ULL << 0))) + (!!(((Model1_w) >> 8) & (1ULL << 1))) + (!!(((Model1_w) >> 8) & (1ULL << 2))) + (!!(((Model1_w) >> 8) & (1ULL << 3))) + (!!(((Model1_w) >> 8) & (1ULL << 4))) + (!!(((Model1_w) >> 8) & (1ULL << 5))) + (!!(((Model1_w) >> 8) & (1ULL << 6))) + (!!(((Model1_w) >> 8) & (1ULL << 7)))))) + (((unsigned int) ((!!(((Model1_w) >> 16) & (1ULL << 0))) + (!!(((Model1_w) >> 16) & (1ULL << 1))) + (!!(((Model1_w) >> 16) & (1ULL << 2))) + (!!(((Model1_w) >> 16) & (1ULL << 3))) + (!!(((Model1_w) >> 16) & (1ULL << 4))) + (!!(((Model1_w) >> 16) & (1ULL << 5))) + (!!(((Model1_w) >> 16) & (1ULL << 6))) + (!!(((Model1_w) >> 16) & (1ULL << 7))))) + ((unsigned int) ((!!((((Model1_w) >> 16) >> 8) & (1ULL << 0))) + (!!((((Model1_w) >> 16) >> 8) & (1ULL << 1))) + (!!((((Model1_w) >> 16) >> 8) & (1ULL << 2))) + (!!((((Model1_w) >> 16) >> 8) & (1ULL << 3))) + (!!((((Model1_w) >> 16) >> 8) & (1ULL << 4))) + (!!((((Model1_w) >> 16) >> 8) & (1ULL << 5))) + (!!((((Model1_w) >> 16) >> 8) & (1ULL << 6))) + (!!((((Model1_w) >> 16) >> 8) & (1ULL << 7))))))) + ((((unsigned int) ((!!(((Model1_w) >> 32) & (1ULL << 0))) + (!!(((Model1_w) >> 32) & (1ULL << 1))) + (!!(((Model1_w) >> 32) & (1ULL << 2))) + (!!(((Model1_w) >> 32) & (1ULL << 3))) + (!!(((Model1_w) >> 32) & (1ULL << 4))) + (!!(((Model1_w) >> 32) & (1ULL << 5))) + (!!(((Model1_w) >> 32) & (1ULL << 6))) + (!!(((Model1_w) >> 32) & (1ULL << 7))))) + ((unsigned int) ((!!((((Model1_w) >> 32) >> 8) & (1ULL << 0))) + (!!((((Model1_w) >> 32) >> 8) & (1ULL << 1))) + (!!((((Model1_w) >> 32) >> 8) & (1ULL << 2))) + (!!((((Model1_w) >> 32) >> 8) & (1ULL << 3))) + (!!((((Model1_w) >> 32) >> 8) & (1ULL << 4))) + (!!((((Model1_w) >> 32) >> 8) & (1ULL << 5))) + (!!((((Model1_w) >> 32) >> 8) & (1ULL << 6))) + (!!((((Model1_w) >> 32) >> 8) & (1ULL << 7)))))) + (((unsigned int) ((!!((((Model1_w) >> 32) >> 16) & (1ULL << 0))) + (!!((((Model1_w) >> 32) >> 16) & (1ULL << 1))) + (!!((((Model1_w) >> 32) >> 16) & (1ULL << 2))) + (!!((((Model1_w) >> 32) >> 16) & (1ULL << 3))) + (!!((((Model1_w) >> 32) >> 16) & (1ULL << 4))) + (!!((((Model1_w) >> 32) >> 16) & (1ULL << 5))) + (!!((((Model1_w) >> 32) >> 16) & (1ULL << 6))) + (!!((((Model1_w) >> 32) >> 16) & (1ULL << 7))))) + ((unsigned int) ((!!(((((Model1_w) >> 32) >> 16) >> 8) & (1ULL << 0))) + (!!(((((Model1_w) >> 32) >> 16) >> 8) & (1ULL << 1))) + (!!(((((Model1_w) >> 32) >> 16) >> 8) & (1ULL << 2))) + (!!(((((Model1_w) >> 32) >> 16) >> 8) & (1ULL << 3))) + (!!(((((Model1_w) >> 32) >> 16) >> 8) & (1ULL << 4))) + (!!(((((Model1_w) >> 32) >> 16) >> 8) & (1ULL << 5))) + (!!(((((Model1_w) >> 32) >> 16) >> 8) & (1ULL << 6))) + (!!(((((Model1_w) >> 32) >> 16) >> 8) & (1ULL << 7)))))))) : Model1___arch_hweight64(Model1_w));
}

/**
 * rol64 - rotate a 64-bit value left
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __attribute__((no_instrument_function)) __u64 Model1_rol64(__u64 Model1_word, unsigned int Model1_shift)
{
 return (Model1_word << Model1_shift) | (Model1_word >> (64 - Model1_shift));
}

/**
 * ror64 - rotate a 64-bit value right
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __attribute__((no_instrument_function)) __u64 Model1_ror64(__u64 Model1_word, unsigned int Model1_shift)
{
 return (Model1_word >> Model1_shift) | (Model1_word << (64 - Model1_shift));
}

/**
 * rol32 - rotate a 32-bit value left
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __attribute__((no_instrument_function)) __u32 Model1_rol32(__u32 Model1_word, unsigned int Model1_shift)
{
 return (Model1_word << Model1_shift) | (Model1_word >> ((-Model1_shift) & 31));
}

/**
 * ror32 - rotate a 32-bit value right
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __attribute__((no_instrument_function)) __u32 Model1_ror32(__u32 Model1_word, unsigned int Model1_shift)
{
 return (Model1_word >> Model1_shift) | (Model1_word << (32 - Model1_shift));
}

/**
 * rol16 - rotate a 16-bit value left
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __attribute__((no_instrument_function)) Model1___u16 Model1_rol16(Model1___u16 Model1_word, unsigned int Model1_shift)
{
 return (Model1_word << Model1_shift) | (Model1_word >> (16 - Model1_shift));
}

/**
 * ror16 - rotate a 16-bit value right
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __attribute__((no_instrument_function)) Model1___u16 Model1_ror16(Model1___u16 Model1_word, unsigned int Model1_shift)
{
 return (Model1_word >> Model1_shift) | (Model1_word << (16 - Model1_shift));
}

/**
 * rol8 - rotate an 8-bit value left
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __attribute__((no_instrument_function)) __u8 Model1_rol8(__u8 Model1_word, unsigned int Model1_shift)
{
 return (Model1_word << Model1_shift) | (Model1_word >> (8 - Model1_shift));
}

/**
 * ror8 - rotate an 8-bit value right
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __attribute__((no_instrument_function)) __u8 Model1_ror8(__u8 Model1_word, unsigned int Model1_shift)
{
 return (Model1_word >> Model1_shift) | (Model1_word << (8 - Model1_shift));
}

/**
 * sign_extend32 - sign extend a 32-bit value using specified bit as sign-bit
 * @value: value to sign extend
 * @index: 0 based bit index (0<=index<32) to sign bit
 *
 * This is safe to use for 16- and 8-bit types as well.
 */
static inline __attribute__((no_instrument_function)) Model1___s32 Model1_sign_extend32(__u32 Model1_value, int Model1_index)
{
 __u8 Model1_shift = 31 - Model1_index;
 return (Model1___s32)(Model1_value << Model1_shift) >> Model1_shift;
}

/**
 * sign_extend64 - sign extend a 64-bit value using specified bit as sign-bit
 * @value: value to sign extend
 * @index: 0 based bit index (0<=index<64) to sign bit
 */
static inline __attribute__((no_instrument_function)) Model1___s64 Model1_sign_extend64(__u64 Model1_value, int Model1_index)
{
 __u8 Model1_shift = 63 - Model1_index;
 return (Model1___s64)(Model1_value << Model1_shift) >> Model1_shift;
}

static inline __attribute__((no_instrument_function)) unsigned Model1_fls_long(unsigned long Model1_l)
{
 if (sizeof(Model1_l) == 4)
  return Model1_fls(Model1_l);
 return Model1_fls64(Model1_l);
}

/**
 * __ffs64 - find first set bit in a 64 bit word
 * @word: The 64 bit word
 *
 * On 64 bit arches this is a synomyn for __ffs
 * The result is not defined if no bits are set, so check that @word
 * is non-zero before calling this.
 */
static inline __attribute__((no_instrument_function)) unsigned long Model1___ffs64(Model1_u64 Model1_word)
{






 return Model1___ffs((unsigned long)Model1_word);
}
/**
 * find_last_bit - find the last set bit in a memory region
 * @addr: The address to start the search at
 * @size: The number of bits to search
 *
 * Returns the bit number of the last set bit, or size.
 */
extern unsigned long Model1_find_last_bit(const unsigned long *Model1_addr,
       unsigned long Model1_size);
/* Integer base 2 logarithm calculation
 *
 * Copyright (C) 2006 Red Hat, Inc. All Rights Reserved.
 * Written by David Howells (dhowells@redhat.com)
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public License
 * as published by the Free Software Foundation; either version
 * 2 of the License, or (at your option) any later version.
 */







/*
 * deal with unrepresentable constant logarithms
 */
extern __attribute__((const, noreturn))
int Model1_____ilog2_NaN(void);

/*
 * non-constant log of base 2 calculators
 * - the arch may override these in asm/bitops.h if they can be implemented
 *   more efficiently than using fls() and fls64()
 * - the arch is not required to handle n==0 if implementing the fallback
 */

static inline __attribute__((no_instrument_function)) __attribute__((const))
int Model1___ilog2_u32(Model1_u32 Model1_n)
{
 return Model1_fls(Model1_n) - 1;
}



static inline __attribute__((no_instrument_function)) __attribute__((const))
int Model1___ilog2_u64(Model1_u64 Model1_n)
{
 return Model1_fls64(Model1_n) - 1;
}


/*
 *  Determine whether some value is a power of two, where zero is
 * *not* considered a power of two.
 */

static inline __attribute__((no_instrument_function)) __attribute__((const))
bool Model1_is_power_of_2(unsigned long Model1_n)
{
 return (Model1_n != 0 && ((Model1_n & (Model1_n - 1)) == 0));
}

/*
 * round up to nearest power of two
 */
static inline __attribute__((no_instrument_function)) __attribute__((const))
unsigned long Model1___roundup_pow_of_two(unsigned long Model1_n)
{
 return 1UL << Model1_fls_long(Model1_n - 1);
}

/*
 * round down to nearest power of two
 */
static inline __attribute__((no_instrument_function)) __attribute__((const))
unsigned long Model1___rounddown_pow_of_two(unsigned long Model1_n)
{
 return 1UL << (Model1_fls_long(Model1_n) - 1);
}

/**
 * ilog2 - log of base 2 of 32-bit or a 64-bit unsigned value
 * @n - parameter
 *
 * constant-capable log of base 2 calculation
 * - this can be used to initialise global variables from constant data, hence
 *   the massive ternary operator construction
 *
 * selects the appropriately-sized optimised version depending on sizeof(n)
 */
/**
 * roundup_pow_of_two - round the given value up to nearest power of two
 * @n - parameter
 *
 * round the given value up to the nearest power of two
 * - the result is undefined when n == 0
 * - this can be used to initialise global variables from constant data
 */
/**
 * rounddown_pow_of_two - round the given value down to nearest power of two
 * @n - parameter
 *
 * round the given value down to the nearest power of two
 * - the result is undefined when n == 0
 * - this can be used to initialise global variables from constant data
 */







/**
 * order_base_2 - calculate the (rounded up) base 2 order of the argument
 * @n: parameter
 *
 * The first few values calculated by this routine:
 *  ob2(0) = 0
 *  ob2(1) = 0
 *  ob2(2) = 1
 *  ob2(3) = 2
 *  ob2(4) = 2
 *  ob2(5) = 3
 *  ... and so on.
 */



/*
 * Check at compile time that something is of a particular type.
 * Always evaluates to 1 so you may use it easily in comparisons.
 */







/*
 * Check at compile time that 'function' is a certain type, or is a pointer
 * to that type (needs to use typedef for the function type.)
 */










/* These macros are used to mark some functions or 
 * initialized data (doesn't apply to uninitialized data)
 * as `initialization' functions. The kernel can take this
 * as hint that the function is used only during the initialization
 * phase and free up used memory resources after
 *
 * Usage:
 * For functions:
 * 
 * You should add __init immediately before the function name, like:
 *
 * static void __init initme(int x, int y)
 * {
 *    extern int z; z = x * y;
 * }
 *
 * If the function has a prototype somewhere, you can also add
 * __init between closing brace of the prototype and semicolon:
 *
 * extern int initialize_foobar_device(int, int, int) __init;
 *
 * For initialized data:
 * You should insert __initdata or __initconst between the variable name
 * and equal sign followed by value, e.g.:
 *
 * static int init_variable __initdata = 0;
 * static const char linux_logo[] __initconst = { 0x32, 0x36, ... };
 *
 * Don't forget to initialize data not at file scope, i.e. within a function,
 * as gcc otherwise puts the data into the bss section and not into the init
 * section.
 */

/* These are for everybody (although not all archs will actually
   discard it in modules) */






/*
 * Some architecture have tool chains which do not handle rodata attributes
 * correctly. For those disable special sections for const, so that other
 * architectures can annotate correctly.
 */






/*
 * modpost check for section mismatches during the kernel build.
 * A section mismatch happens when there are references from a
 * code or data section to an init section (both code or data).
 * The init sections are (for most archs) discarded by the kernel
 * when early init has completed so all such references are potential bugs.
 * For exit sections the same issue exists.
 *
 * The following markers are used for the cases where the reference to
 * the *init / *exit section (code or data) is valid and will teach
 * modpost not to issue a warning.  Intended semantics is that a code or
 * data tagged __ref* can reference code or data from init section without
 * producing a warning (of course, no warning does not mean code is
 * correct, so optimally document why the __ref is needed and why it's OK).
 *
 * The markers follow same syntax rules as __init / __initdata.
 */
/* Used for MEMORY_HOTPLUG */







/* For assembly routines */
/* silence warnings when references are OK */





/*
 * Used for initialization calls..
 */
typedef int (*Model1_initcall_t)(void);
typedef void (*Model1_exitcall_t)(void);

extern Model1_initcall_t Model1___con_initcall_start[], Model1___con_initcall_end[];
extern Model1_initcall_t Model1___security_initcall_start[], Model1___security_initcall_end[];

/* Used for contructor calls. */
typedef void (*Model1_ctor_fn_t)(void);

/* Defined in init/main.c */
extern int Model1_do_one_initcall(Model1_initcall_t Model1_fn);
extern char __attribute__ ((__section__(".init.data"))) Model1_boot_command_line[];
extern char *Model1_saved_command_line;
extern unsigned int Model1_reset_devices;

/* used by init/main.c */
void Model1_setup_arch(char **);
void Model1_prepare_namespace(void);
void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_load_default_modules(void);
int __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_init_rootfs(void);


void Model1_mark_rodata_ro(void);


extern void (*Model1_late_time_init)(void);

extern bool Model1_initcall_debug;
/* initcalls are now grouped by functionality into separate 
 * subsections. Ordering inside the subsections is determined
 * by link order. 
 * For backwards compatibility, initcall() puts the call in 
 * the device init subsection.
 *
 * The `id' arg to __define_initcall() is needed so that multiple initcalls
 * can point at the same handler without causing duplicate-symbol build errors.
 */






/*
 * Early initcalls run before initializing SMP.
 *
 * Only for built-in code, not modules.
 */


/*
 * A "pure" initcall has no dependencies on anything else, and purely
 * initializes variables that couldn't be statically initialized.
 *
 * This only exists for built-in code, not for modules.
 * Keep main.c:initcall_level_names[] in sync.
 */
struct Model1_obs_kernel_param {
 const char *Model1_str;
 int (*Model1_setup_func)(char *);
 int Model1_early;
};

/*
 * Only for really core code.  See moduleparam.h for the normal way.
 *
 * Force the alignment so the compiler doesn't space elements of the
 * obs_kernel_param "array" too far apart in .init.setup.
 */
/*
 * NOTE: fn is as per module_param, not __setup!
 * Emits warning if fn returns non-zero.
 */
/* Relies on boot_command_line being set */
void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_parse_early_param(void);
void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_parse_early_options(char *Model1_cmdline);
/* Data marked not to be saved by software suspend */
/*
 * Annotation for a "continued" line of log printout (only done after a
 * line that had no enclosing \n). Only to be used by core/arch code
 * during early bootup (a continued line is not SMP-safe otherwise).
 */


/* integer equivalents of KERN_<LEVEL> */













struct Model1_sysinfo {
 Model1___kernel_long_t Model1_uptime; /* Seconds since boot */
 Model1___kernel_ulong_t Model1_loads[3]; /* 1, 5, and 15 minute load averages */
 Model1___kernel_ulong_t Model1_totalram; /* Total usable main memory size */
 Model1___kernel_ulong_t Model1_freeram; /* Available memory size */
 Model1___kernel_ulong_t Model1_sharedram; /* Amount of shared memory */
 Model1___kernel_ulong_t Model1_bufferram; /* Memory used by buffers */
 Model1___kernel_ulong_t Model1_totalswap; /* Total swap space size */
 Model1___kernel_ulong_t Model1_freeswap; /* swap space still available */
 Model1___u16 Model1_procs; /* Number of current processes */
 Model1___u16 Model1_pad; /* Explicit padding for m68k */
 Model1___kernel_ulong_t Model1_totalhigh; /* Total high memory size */
 Model1___kernel_ulong_t Model1_freehigh; /* Available high memory size */
 __u32 Model1_mem_unit; /* Memory unit size in bytes */
 char Model1__f[20-2*sizeof(Model1___kernel_ulong_t)-sizeof(__u32)]; /* Padding: libc5 uses this.. */
};

/*
 * 'kernel.h' contains some often-used function prototypes etc
 */





/* L1 cache line size */
/*
 * __read_mostly is used to keep rarely changing variables out of frequently
 * updated cachelines. If an architecture doesn't support it, ignore the
 * hint.
 */




/*
 * __ro_after_init is used to mark things that are read-only after init (i.e.
 * after mark_rodata_ro() has been called). These are effectively read-only,
 * but may get written to during init, so can't live in .rodata (via "const").
 */
/*
 * The maximum alignment needed for some critical structures
 * These could be inter-node cacheline sizes/L3 cacheline
 * size etc.  Define this in asm/cache.h for your arch
 */

extern const char Model1_linux_banner[];
extern const char Model1_linux_proc_banner[];

static inline __attribute__((no_instrument_function)) int Model1_printk_get_level(const char *Model1_buffer)
{
 if (Model1_buffer[0] == '\001' && Model1_buffer[1]) {
  switch (Model1_buffer[1]) {
  case '0' ... '7':
  case 'd': /* KERN_DEFAULT */
   return Model1_buffer[1];
  }
 }
 return 0;
}

static inline __attribute__((no_instrument_function)) const char *Model1_printk_skip_level(const char *Model1_buffer)
{
 if (Model1_printk_get_level(Model1_buffer))
  return Model1_buffer + 2;

 return Model1_buffer;
}



/* printk's without a loglevel use this.. */


/* We show everything that is MORE important than this.. */







extern int Model1_console_printk[];






static inline __attribute__((no_instrument_function)) void Model1_console_silent(void)
{
 (Model1_console_printk[0]) = 0;
}

static inline __attribute__((no_instrument_function)) void Model1_console_verbose(void)
{
 if ((Model1_console_printk[0]))
  (Model1_console_printk[0]) = 15;
}

/* strlen("ratelimit") + 1 */

extern char Model1_devkmsg_log_str[];
struct Model1_ctl_table;

struct Model1_va_format {
 const char *Model1_fmt;
 Model1_va_list *Model1_va;
};

/*
 * FW_BUG
 * Add this to a message where you are sure the firmware is buggy or behaves
 * really stupid or out of spec. Be aware that the responsible BIOS developer
 * should be able to fix this issue or at least get a concrete idea of the
 * problem by reading your message without the need of looking at the kernel
 * code.
 *
 * Use it for definite and high priority BIOS bugs.
 *
 * FW_WARN
 * Use it for not that clear (e.g. could the kernel messed up things already?)
 * and medium priority BIOS bugs.
 *
 * FW_INFO
 * Use this one if you want to tell the user or vendor about something
 * suspicious, but generally harmless related to the firmware.
 *
 * Use it for information or very low priority BIOS bugs.
 */




/*
 * HW_ERR
 * Add this to a message for hardware errors, so that user can report
 * it to hardware vendor instead of LKML or software vendor.
 */


/*
 * DEPRECATED
 * Add this to a message whenever you want to warn user space about the use
 * of a deprecated aspect of an API so they can stop using it
 */


/*
 * Dummy printk for disabled debugging statements to use whilst maintaining
 * gcc's format checking.
 */
extern __attribute__((format(printf, 1, 2)))
void Model1_early_printk(const char *Model1_fmt, ...);






extern void Model1_printk_nmi_init(void);
extern void Model1_printk_nmi_enter(void);
extern void Model1_printk_nmi_exit(void);
extern void Model1_printk_nmi_flush(void);
extern void Model1_printk_nmi_flush_on_panic(void);
           __attribute__((format(printf, 5, 0)))
int Model1_vprintk_emit(int Model1_facility, int Model1_level,
   const char *Model1_dict, Model1_size_t Model1_dictlen,
   const char *Model1_fmt, Model1_va_list Model1_args);

           __attribute__((format(printf, 1, 0)))
int Model1_vprintk(const char *Model1_fmt, Model1_va_list Model1_args);

           __attribute__((format(printf, 5, 6)))
int Model1_printk_emit(int Model1_facility, int Model1_level,
  const char *Model1_dict, Model1_size_t Model1_dictlen,
  const char *Model1_fmt, ...);

           __attribute__((format(printf, 1, 2)))
int Model1_printk(const char *Model1_fmt, ...);

/*
 * Special printk facility for scheduler/timekeeping use only, _DO_NOT_USE_ !
 */
__attribute__((format(printf, 1, 2))) int Model1_printk_deferred(const char *Model1_fmt, ...);

/*
 * Please don't use printk_ratelimit(), because it shares ratelimiting state
 * with all other unrelated printk_ratelimit() callsites.  Instead use
 * printk_ratelimited() or plain old __ratelimit().
 */
extern int Model1___printk_ratelimit(const char *func);

extern bool Model1_printk_timed_ratelimit(unsigned long *Model1_caller_jiffies,
       unsigned int Model1_interval_msec);

extern int Model1_printk_delay_msec;
extern int Model1_dmesg_restrict;
extern int Model1_kptr_restrict;

extern int
Model1_devkmsg_sysctl_set_loglvl(struct Model1_ctl_table *Model1_table, int Model1_write, void *Model1_buf,
     Model1_size_t *Model1_lenp, Model1_loff_t *Model1_ppos);

extern void Model1_wake_up_klogd(void);

char *Model1_log_buf_addr_get(void);
Model1_u32 Model1_log_buf_len_get(void);
void Model1_log_buf_kexec_setup(void);
void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_setup_log_buf(int Model1_early);
__attribute__((format(printf, 1, 2))) void Model1_dump_stack_set_arch_desc(const char *Model1_fmt, ...);
void Model1_dump_stack_print_info(const char *Model1_log_lvl);
void Model1_show_regs_print_info(const char *Model1_log_lvl);
extern void Model1_dump_stack(void) ;





/*
 * These can be used to print at the various log levels.
 * All of these will print unconditionally, although note that pr_debug()
 * and other debug macros are compiled out unless either DEBUG is defined
 * or CONFIG_DYNAMIC_DEBUG is set.
 */
/*
 * Like KERN_CONT, pr_cont() should only be used when continuing
 * a line with no newline ('\n') enclosed. Otherwise it defaults
 * back to KERN_DEFAULT.
 */



/* pr_devel() should produce zero code unless DEBUG is defined */
/* If you are writing a driver, please use dev_dbg instead */
/*
 * Print a one-time message (analogous to WARN_ONCE() et al):
 */
/* If you are writing a driver, please use dev_dbg instead */
/*
 * ratelimited messages with local ratelimit_state,
 * no local ratelimit_state used in the !PRINTK case
 */
/* no pr_cont_ratelimited, don't do that... */
/* If you are writing a driver, please use dev_dbg instead */
extern const struct Model1_file_operations Model1_kmsg_fops;

enum {
 Model1_DUMP_PREFIX_NONE,
 Model1_DUMP_PREFIX_ADDRESS,
 Model1_DUMP_PREFIX_OFFSET
};
extern int Model1_hex_dump_to_buffer(const void *Model1_buf, Model1_size_t Model1_len, int Model1_rowsize,
         int Model1_groupsize, char *Model1_linebuf, Model1_size_t Model1_linebuflen,
         bool Model1_ascii);

extern void Model1_print_hex_dump(const char *Model1_level, const char *Model1_prefix_str,
      int Model1_prefix_type, int Model1_rowsize, int Model1_groupsize,
      const void *Model1_buf, Model1_size_t Model1_len, bool Model1_ascii);




extern void Model1_print_hex_dump_bytes(const char *Model1_prefix_str, int Model1_prefix_type,
     const void *Model1_buf, Model1_size_t Model1_len);
static inline __attribute__((no_instrument_function)) void Model1_print_hex_dump_debug(const char *Model1_prefix_str, int Model1_prefix_type,
     int Model1_rowsize, int Model1_groupsize,
     const void *Model1_buf, Model1_size_t Model1_len, bool Model1_ascii)
{
}
/*
 * This looks more complex than it should be. But we need to
 * get the type for the ~ right in round_down (it needs to be
 * as wide as the result!), and we want to evaluate the macro
 * arguments just once each.
 */
/* The `const' in roundup() prevents gcc-3.3 from calling __divdi3 */
/*
 * Divide positive or negative dividend by positive divisor and round
 * to closest integer. Result is undefined for negative divisors and
 * for negative dividends if the divisor variable type is unsigned.
 */
/*
 * Same as above but for u64 dividends. divisor must be a 32-bit
 * number.
 */
/*
 * Multiplies an integer by a fraction, while avoiding unnecessary
 * overflow or loss of precision.
 */
/**
 * upper_32_bits - return bits 32-63 of a number
 * @n: the number we're accessing
 *
 * A basic shift-right of a 64- or 32-bit quantity.  Use this to suppress
 * the "right shift count >= width of type" warning when that quantity is
 * 32-bits.
 */


/**
 * lower_32_bits - return bits 0-31 of a number
 * @n: the number we're accessing
 */


struct Model1_completion;
struct Model1_pt_regs;
struct Model1_user;


extern int Model1__cond_resched(void);
  static inline __attribute__((no_instrument_function)) void Model1____might_sleep(const char *Model1_file, int Model1_line,
       int Model1_preempt_offset) { }
  static inline __attribute__((no_instrument_function)) void Model1___might_sleep(const char *Model1_file, int Model1_line,
       int Model1_preempt_offset) { }






/**
 * abs - return absolute value of an argument
 * @x: the value.  If it is unsigned type, it is converted to signed type first.
 *     char is treated as if it was signed (regardless of whether it really is)
 *     but the macro's return type is preserved as char.
 *
 * Return: an absolute value of x.
 */
/**
 * reciprocal_scale - "scale" a value into range [0, ep_ro)
 * @val: value
 * @ep_ro: right open interval endpoint
 *
 * Perform a "reciprocal multiplication" in order to "scale" a value into
 * range [0, ep_ro), where the upper interval endpoint is right-open.
 * This is useful, e.g. for accessing a index of an array containing
 * ep_ro elements, for example. Think of it as sort of modulus, only that
 * the result isn't that of modulo. ;) Note that if initial input is a
 * small value, then result will return 0.
 *
 * Return: a result based on val in interval [0, ep_ro).
 */
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_reciprocal_scale(Model1_u32 Model1_val, Model1_u32 Model1_ep_ro)
{
 return (Model1_u32)(((Model1_u64) Model1_val * Model1_ep_ro) >> 32);
}






static inline __attribute__((no_instrument_function)) void Model1_might_fault(void) { }


extern struct Model1_atomic_notifier_head Model1_panic_notifier_list;
extern long (*Model1_panic_blink)(int Model1_state);
__attribute__((format(printf, 1, 2)))
void Model1_panic(const char *Model1_fmt, ...)
 __attribute__((noreturn)) ;
void Model1_nmi_panic(struct Model1_pt_regs *Model1_regs, const char *Model1_msg);
extern void Model1_oops_enter(void);
extern void Model1_oops_exit(void);
void Model1_print_oops_end_marker(void);
extern int Model1_oops_may_print(void);
void Model1_do_exit(long Model1_error_code)
 __attribute__((noreturn));
void Model1_complete_and_exit(struct Model1_completion *, long)
 __attribute__((noreturn));

/* Internal, do not use. */
int __attribute__((warn_unused_result)) Model1__kstrtoul(const char *Model1_s, unsigned int Model1_base, unsigned long *Model1_res);
int __attribute__((warn_unused_result)) Model1__kstrtol(const char *Model1_s, unsigned int Model1_base, long *Model1_res);

int __attribute__((warn_unused_result)) Model1_kstrtoull(const char *Model1_s, unsigned int Model1_base, unsigned long long *Model1_res);
int __attribute__((warn_unused_result)) Model1_kstrtoll(const char *Model1_s, unsigned int Model1_base, long long *Model1_res);

/**
 * kstrtoul - convert a string to an unsigned long
 * @s: The start of the string. The string must be null-terminated, and may also
 *  include a single newline before its terminating null. The first character
 *  may also be a plus sign, but not a minus sign.
 * @base: The number base to use. The maximum supported base is 16. If base is
 *  given as 0, then the base of the string is automatically detected with the
 *  conventional semantics - If it begins with 0x the number will be parsed as a
 *  hexadecimal (case insensitive), if it otherwise begins with 0, it will be
 *  parsed as an octal number. Otherwise it will be parsed as a decimal.
 * @res: Where to write the result of the conversion on success.
 *
 * Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.
 * Used as a replacement for the obsolete simple_strtoull. Return code must
 * be checked.
*/
static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result)) Model1_kstrtoul(const char *Model1_s, unsigned int Model1_base, unsigned long *Model1_res)
{
 /*
	 * We want to shortcut function call, but
	 * __builtin_types_compatible_p(unsigned long, unsigned long long) = 0.
	 */
 if (sizeof(unsigned long) == sizeof(unsigned long long) &&
     __alignof__(unsigned long) == __alignof__(unsigned long long))
  return Model1_kstrtoull(Model1_s, Model1_base, (unsigned long long *)Model1_res);
 else
  return Model1__kstrtoul(Model1_s, Model1_base, Model1_res);
}

/**
 * kstrtol - convert a string to a long
 * @s: The start of the string. The string must be null-terminated, and may also
 *  include a single newline before its terminating null. The first character
 *  may also be a plus sign or a minus sign.
 * @base: The number base to use. The maximum supported base is 16. If base is
 *  given as 0, then the base of the string is automatically detected with the
 *  conventional semantics - If it begins with 0x the number will be parsed as a
 *  hexadecimal (case insensitive), if it otherwise begins with 0, it will be
 *  parsed as an octal number. Otherwise it will be parsed as a decimal.
 * @res: Where to write the result of the conversion on success.
 *
 * Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.
 * Used as a replacement for the obsolete simple_strtoull. Return code must
 * be checked.
 */
static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result)) Model1_kstrtol(const char *Model1_s, unsigned int Model1_base, long *Model1_res)
{
 /*
	 * We want to shortcut function call, but
	 * __builtin_types_compatible_p(long, long long) = 0.
	 */
 if (sizeof(long) == sizeof(long long) &&
     __alignof__(long) == __alignof__(long long))
  return Model1_kstrtoll(Model1_s, Model1_base, (long long *)Model1_res);
 else
  return Model1__kstrtol(Model1_s, Model1_base, Model1_res);
}

int __attribute__((warn_unused_result)) Model1_kstrtouint(const char *Model1_s, unsigned int Model1_base, unsigned int *Model1_res);
int __attribute__((warn_unused_result)) Model1_kstrtoint(const char *Model1_s, unsigned int Model1_base, int *Model1_res);

static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result)) Model1_kstrtou64(const char *Model1_s, unsigned int Model1_base, Model1_u64 *Model1_res)
{
 return Model1_kstrtoull(Model1_s, Model1_base, Model1_res);
}

static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result)) Model1_kstrtos64(const char *Model1_s, unsigned int Model1_base, Model1_s64 *Model1_res)
{
 return Model1_kstrtoll(Model1_s, Model1_base, Model1_res);
}

static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result)) Model1_kstrtou32(const char *Model1_s, unsigned int Model1_base, Model1_u32 *Model1_res)
{
 return Model1_kstrtouint(Model1_s, Model1_base, Model1_res);
}

static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result)) Model1_kstrtos32(const char *Model1_s, unsigned int Model1_base, Model1_s32 *Model1_res)
{
 return Model1_kstrtoint(Model1_s, Model1_base, Model1_res);
}

int __attribute__((warn_unused_result)) Model1_kstrtou16(const char *Model1_s, unsigned int Model1_base, Model1_u16 *Model1_res);
int __attribute__((warn_unused_result)) Model1_kstrtos16(const char *Model1_s, unsigned int Model1_base, Model1_s16 *Model1_res);
int __attribute__((warn_unused_result)) Model1_kstrtou8(const char *Model1_s, unsigned int Model1_base, Model1_u8 *Model1_res);
int __attribute__((warn_unused_result)) Model1_kstrtos8(const char *Model1_s, unsigned int Model1_base, Model1_s8 *Model1_res);
int __attribute__((warn_unused_result)) Model1_kstrtobool(const char *Model1_s, bool *Model1_res);

int __attribute__((warn_unused_result)) Model1_kstrtoull_from_user(const char *Model1_s, Model1_size_t Model1_count, unsigned int Model1_base, unsigned long long *Model1_res);
int __attribute__((warn_unused_result)) Model1_kstrtoll_from_user(const char *Model1_s, Model1_size_t Model1_count, unsigned int Model1_base, long long *Model1_res);
int __attribute__((warn_unused_result)) Model1_kstrtoul_from_user(const char *Model1_s, Model1_size_t Model1_count, unsigned int Model1_base, unsigned long *Model1_res);
int __attribute__((warn_unused_result)) Model1_kstrtol_from_user(const char *Model1_s, Model1_size_t Model1_count, unsigned int Model1_base, long *Model1_res);
int __attribute__((warn_unused_result)) Model1_kstrtouint_from_user(const char *Model1_s, Model1_size_t Model1_count, unsigned int Model1_base, unsigned int *Model1_res);
int __attribute__((warn_unused_result)) Model1_kstrtoint_from_user(const char *Model1_s, Model1_size_t Model1_count, unsigned int Model1_base, int *Model1_res);
int __attribute__((warn_unused_result)) Model1_kstrtou16_from_user(const char *Model1_s, Model1_size_t Model1_count, unsigned int Model1_base, Model1_u16 *Model1_res);
int __attribute__((warn_unused_result)) Model1_kstrtos16_from_user(const char *Model1_s, Model1_size_t Model1_count, unsigned int Model1_base, Model1_s16 *Model1_res);
int __attribute__((warn_unused_result)) Model1_kstrtou8_from_user(const char *Model1_s, Model1_size_t Model1_count, unsigned int Model1_base, Model1_u8 *Model1_res);
int __attribute__((warn_unused_result)) Model1_kstrtos8_from_user(const char *Model1_s, Model1_size_t Model1_count, unsigned int Model1_base, Model1_s8 *Model1_res);
int __attribute__((warn_unused_result)) Model1_kstrtobool_from_user(const char *Model1_s, Model1_size_t Model1_count, bool *Model1_res);

static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result)) Model1_kstrtou64_from_user(const char *Model1_s, Model1_size_t Model1_count, unsigned int Model1_base, Model1_u64 *Model1_res)
{
 return Model1_kstrtoull_from_user(Model1_s, Model1_count, Model1_base, Model1_res);
}

static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result)) Model1_kstrtos64_from_user(const char *Model1_s, Model1_size_t Model1_count, unsigned int Model1_base, Model1_s64 *Model1_res)
{
 return Model1_kstrtoll_from_user(Model1_s, Model1_count, Model1_base, Model1_res);
}

static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result)) Model1_kstrtou32_from_user(const char *Model1_s, Model1_size_t Model1_count, unsigned int Model1_base, Model1_u32 *Model1_res)
{
 return Model1_kstrtouint_from_user(Model1_s, Model1_count, Model1_base, Model1_res);
}

static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result)) Model1_kstrtos32_from_user(const char *Model1_s, Model1_size_t Model1_count, unsigned int Model1_base, Model1_s32 *Model1_res)
{
 return Model1_kstrtoint_from_user(Model1_s, Model1_count, Model1_base, Model1_res);
}

/* Obsolete, do not use.  Use kstrto<foo> instead */

extern unsigned long Model1_simple_strtoul(const char *,char **,unsigned int);
extern long Model1_simple_strtol(const char *,char **,unsigned int);
extern unsigned long long Model1_simple_strtoull(const char *,char **,unsigned int);
extern long long Model1_simple_strtoll(const char *,char **,unsigned int);

extern int Model1_num_to_str(char *Model1_buf, int Model1_size, unsigned long long Model1_num);

/* lib/printf utilities */

extern __attribute__((format(printf, 2, 3))) int Model1_sprintf(char *Model1_buf, const char * Model1_fmt, ...);
extern __attribute__((format(printf, 2, 0))) int Model1_vsprintf(char *Model1_buf, const char *, Model1_va_list);
extern __attribute__((format(printf, 3, 4)))
int Model1_snprintf(char *Model1_buf, Model1_size_t Model1_size, const char *Model1_fmt, ...);
extern __attribute__((format(printf, 3, 0)))
int Model1_vsnprintf(char *Model1_buf, Model1_size_t Model1_size, const char *Model1_fmt, Model1_va_list Model1_args);
extern __attribute__((format(printf, 3, 4)))
int Model1_scnprintf(char *Model1_buf, Model1_size_t Model1_size, const char *Model1_fmt, ...);
extern __attribute__((format(printf, 3, 0)))
int Model1_vscnprintf(char *Model1_buf, Model1_size_t Model1_size, const char *Model1_fmt, Model1_va_list Model1_args);
extern __attribute__((format(printf, 2, 3))) __attribute__((__malloc__))
char *Model1_kasprintf(Model1_gfp_t Model1_gfp, const char *Model1_fmt, ...);
extern __attribute__((format(printf, 2, 0))) __attribute__((__malloc__))
char *Model1_kvasprintf(Model1_gfp_t Model1_gfp, const char *Model1_fmt, Model1_va_list Model1_args);
extern __attribute__((format(printf, 2, 0)))
const char *Model1_kvasprintf_const(Model1_gfp_t Model1_gfp, const char *Model1_fmt, Model1_va_list Model1_args);

extern __attribute__((format(scanf, 2, 3)))
int Model1_sscanf(const char *, const char *, ...);
extern __attribute__((format(scanf, 2, 0)))
int Model1_vsscanf(const char *, const char *, Model1_va_list);

extern int Model1_get_option(char **Model1_str, int *Model1_pint);
extern char *Model1_get_options(const char *Model1_str, int Model1_nints, int *Model1_ints);
extern unsigned long long Model1_memparse(const char *Model1_ptr, char **Model1_retptr);
extern bool Model1_parse_option_str(const char *Model1_str, const char *Model1_option);

extern int Model1_core_kernel_text(unsigned long Model1_addr);
extern int Model1_core_kernel_data(unsigned long Model1_addr);
extern int Model1___kernel_text_address(unsigned long Model1_addr);
extern int Model1_kernel_text_address(unsigned long Model1_addr);
extern int Model1_func_ptr_is_kernel_text(void *Model1_ptr);

unsigned long Model1_int_sqrt(unsigned long);

extern void Model1_bust_spinlocks(int Model1_yes);
extern int Model1_oops_in_progress; /* If set, an oops, panic(), BUG() or die() is in progress */
extern int Model1_panic_timeout;
extern int Model1_panic_on_oops;
extern int Model1_panic_on_unrecovered_nmi;
extern int Model1_panic_on_io_nmi;
extern int Model1_panic_on_warn;
extern int Model1_sysctl_panic_on_rcu_stall;
extern int Model1_sysctl_panic_on_stackoverflow;

extern bool Model1_crash_kexec_post_notifiers;

/*
 * panic_cpu is used for synchronizing panic() and crash_kexec() execution. It
 * holds a CPU number which is executing panic() currently. A value of
 * PANIC_CPU_INVALID means no CPU has entered panic() or crash_kexec().
 */
extern Model1_atomic_t Model1_panic_cpu;


/*
 * Only to be used by arch init code. If the user over-wrote the default
 * CONFIG_PANIC_TIMEOUT, honor it.
 */
static inline __attribute__((no_instrument_function)) void Model1_set_arch_panic_timeout(int Model1_timeout, int Model1_arch_default_timeout)
{
 if (Model1_panic_timeout == Model1_arch_default_timeout)
  Model1_panic_timeout = Model1_timeout;
}
extern const char *Model1_print_tainted(void);
enum Model1_lockdep_ok {
 Model1_LOCKDEP_STILL_OK,
 Model1_LOCKDEP_NOW_UNRELIABLE
};
extern void Model1_add_taint(unsigned Model1_flag, enum Model1_lockdep_ok);
extern int Model1_test_taint(unsigned Model1_flag);
extern unsigned long Model1_get_taint(void);
extern int Model1_root_mountflags;

extern bool Model1_early_boot_irqs_disabled;

/* Values used for system_state */
extern enum Model1_system_states {
 Model1_SYSTEM_BOOTING,
 Model1_SYSTEM_RUNNING,
 Model1_SYSTEM_HALT,
 Model1_SYSTEM_POWER_OFF,
 Model1_SYSTEM_RESTART,
} Model1_system_state;
extern const char Model1_hex_asc[];



static inline __attribute__((no_instrument_function)) char *Model1_hex_byte_pack(char *Model1_buf, Model1_u8 Model1_byte)
{
 *Model1_buf++ = Model1_hex_asc[((Model1_byte) & 0xf0) >> 4];
 *Model1_buf++ = Model1_hex_asc[((Model1_byte) & 0x0f)];
 return Model1_buf;
}

extern const char Model1_hex_asc_upper[];



static inline __attribute__((no_instrument_function)) char *Model1_hex_byte_pack_upper(char *Model1_buf, Model1_u8 Model1_byte)
{
 *Model1_buf++ = Model1_hex_asc_upper[((Model1_byte) & 0xf0) >> 4];
 *Model1_buf++ = Model1_hex_asc_upper[((Model1_byte) & 0x0f)];
 return Model1_buf;
}

extern int Model1_hex_to_bin(char Model1_ch);
extern int __attribute__((warn_unused_result)) Model1_hex2bin(Model1_u8 *Model1_dst, const char *Model1_src, Model1_size_t Model1_count);
extern char *Model1_bin2hex(char *Model1_dst, const void *Model1_src, Model1_size_t Model1_count);

bool Model1_mac_pton(const char *Model1_s, Model1_u8 *Model1_mac);

/*
 * General tracing related utility functions - trace_printk(),
 * tracing_on/tracing_off and tracing_start()/tracing_stop
 *
 * Use tracing_on/tracing_off when you want to quickly turn on or off
 * tracing. It simply enables or disables the recording of the trace events.
 * This also corresponds to the user space /sys/kernel/debug/tracing/tracing_on
 * file, which gives a means for the kernel and userspace to interact.
 * Place a tracing_off() in the kernel where you want tracing to end.
 * From user space, examine the trace, and then echo 1 > tracing_on
 * to continue tracing.
 *
 * tracing_stop/tracing_start has slightly more overhead. It is used
 * by things like suspend to ram where disabling the recording of the
 * trace is not enough, but tracing must actually stop because things
 * like calling smp_processor_id() may crash the system.
 *
 * Most likely, you want to use tracing_on/tracing_off.
 */

enum Model1_ftrace_dump_mode {
 Model1_DUMP_NONE,
 Model1_DUMP_ALL,
 Model1_DUMP_ORIG,
};


void Model1_tracing_on(void);
void Model1_tracing_off(void);
int Model1_tracing_is_on(void);
void Model1_tracing_snapshot(void);
void Model1_tracing_snapshot_alloc(void);

extern void Model1_tracing_start(void);
extern void Model1_tracing_stop(void);

static inline __attribute__((no_instrument_function)) __attribute__((format(printf, 1, 2)))
void Model1_____trace_printk_check_format(const char *Model1_fmt, ...)
{
}






/**
 * trace_printk - printf formatting in the ftrace buffer
 * @fmt: the printf format for printing
 *
 * Note: __trace_printk is an internal function for trace_printk and
 *       the @ip is passed in via the trace_printk macro.
 *
 * This function allows a kernel developer to debug fast path sections
 * that printk is not appropriate for. By scattering in various
 * printk like tracing in the code, a developer can quickly see
 * where problems are occurring.
 *
 * This is intended as a debugging tool for the developer only.
 * Please refrain from leaving trace_printks scattered around in
 * your code. (Extra memory is used for special buffers that are
 * allocated when trace_printk() is used)
 *
 * A little optization trick is done here. If there's only one
 * argument, there's no need to scan the string for printf formats.
 * The trace_puts() will suffice. But how can we take advantage of
 * using trace_puts() when trace_printk() has only one argument?
 * By stringifying the args and checking the size we can tell
 * whether or not there are args. __stringify((__VA_ARGS__)) will
 * turn into "()\0" with a size of 3 when there are no args, anything
 * else will be bigger. All we need to do is define a string to this,
 * and then take its size and compare to 3. If it's bigger, use
 * do_trace_printk() otherwise, optimize it to trace_puts(). Then just
 * let gcc optimize the rest.
 */
extern __attribute__((format(printf, 2, 3)))
int Model1___trace_bprintk(unsigned long Model1_ip, const char *Model1_fmt, ...);

extern __attribute__((format(printf, 2, 3)))
int Model1___trace_printk(unsigned long Model1_ip, const char *Model1_fmt, ...);

/**
 * trace_puts - write a string into the ftrace buffer
 * @str: the string to record
 *
 * Note: __trace_bputs is an internal function for trace_puts and
 *       the @ip is passed in via the trace_puts macro.
 *
 * This is similar to trace_printk() but is made for those really fast
 * paths that a developer wants the least amount of "Heisenbug" affects,
 * where the processing of the print format is still too much.
 *
 * This function allows a kernel developer to debug fast path sections
 * that printk is not appropriate for. By scattering in various
 * printk like tracing in the code, a developer can quickly see
 * where problems are occurring.
 *
 * This is intended as a debugging tool for the developer only.
 * Please refrain from leaving trace_puts scattered around in
 * your code. (Extra memory is used for special buffers that are
 * allocated when trace_puts() is used)
 *
 * Returns: 0 if nothing was written, positive # if string was.
 *  (1 when __trace_bputs is used, strlen(str) when __trace_puts is used)
 */
extern int Model1___trace_bputs(unsigned long Model1_ip, const char *Model1_str);
extern int Model1___trace_puts(unsigned long Model1_ip, const char *Model1_str, int Model1_size);

extern void Model1_trace_dump_stack(int Model1_skip);

/*
 * The double __builtin_constant_p is because gcc will give us an error
 * if we try to allocate the static variable to fmt if it is not a
 * constant. Even with the outer if statement.
 */
extern __attribute__((format(printf, 2, 0))) int
Model1___ftrace_vbprintk(unsigned long Model1_ip, const char *Model1_fmt, Model1_va_list Model1_ap);

extern __attribute__((format(printf, 2, 0))) int
Model1___ftrace_vprintk(unsigned long Model1_ip, const char *Model1_fmt, Model1_va_list Model1_ap);

extern void Model1_ftrace_dump(enum Model1_ftrace_dump_mode Model1_oops_dump_mode);
/*
 * min()/max()/clamp() macros that also do
 * strict type-checking.. See the
 * "unnecessary" pointer comparison.
 */
/**
 * min_not_zero - return the minimum that is _not_ zero, unless both are zero
 * @x: value1
 * @y: value2
 */





/**
 * clamp - return a value clamped to a given range with strict typechecking
 * @val: current value
 * @lo: lowest allowable value
 * @hi: highest allowable value
 *
 * This macro does strict typechecking of lo/hi to make sure they are of the
 * same type as val.  See the unnecessary pointer comparisons.
 */


/*
 * ..and if you can't take the strict
 * types, you can specify one yourself.
 *
 * Or not use min/max/clamp at all, of course.
 */
/**
 * clamp_t - return a value clamped to a given range using a given type
 * @type: the type of variable to use
 * @val: current value
 * @lo: minimum allowable value
 * @hi: maximum allowable value
 *
 * This macro does no typechecking and uses temporary variables of type
 * 'type' to make all the comparisons.
 */


/**
 * clamp_val - return a value clamped to a given range using val's type
 * @val: current value
 * @lo: minimum allowable value
 * @hi: maximum allowable value
 *
 * This macro does no typechecking and uses temporary variables of whatever
 * type the input argument 'val' is.  This is useful when val is an unsigned
 * type and min and max are literals that will otherwise be assigned a signed
 * integer type.
 */



/*
 * swap - swap value of @a and @b
 */



/**
 * container_of - cast a member of a structure out to the containing structure
 * @ptr:	the pointer to the member.
 * @type:	the type of the container struct this is embedded in.
 * @member:	the name of the member within the struct.
 *
 */




/* Rebuild everything on CONFIG_FTRACE_MCOUNT_RECORD */




/* Permissions on a sysfs file: you didn't miss the 0 prefix did you? */




struct Model1_bug_entry {



 signed int Model1_bug_addr_disp;





 signed int Model1_file_disp;

 unsigned short Model1_line;

 unsigned short Model1_flags;
};


/*
 * Don't use BUG() or BUG_ON() unless there's really no way out; one
 * example might be detecting data structure corruption in the middle
 * of an operation that can't be backed out of.  If the (sub)system
 * can somehow continue operating, perhaps with reduced functionality,
 * it's probably not BUG-worthy.
 *
 * If you're tempted to BUG(), think again:  is completely giving up
 * really the *only* solution?  There are usually better options, where
 * users don't need to reboot ASAP and can mostly shut down cleanly.
 */
/*
 * WARN(), WARN_ON(), WARN_ON_ONCE, and so on can be used to report
 * significant issues that need prompt attention if they should ever
 * appear at runtime.  Use the versions with printk format strings
 * to provide better diagnostics.
 */

extern __attribute__((format(printf, 3, 4)))
void Model1_warn_slowpath_fmt(const char *Model1_file, const int Model1_line,
         const char *Model1_fmt, ...);
extern __attribute__((format(printf, 4, 5)))
void Model1_warn_slowpath_fmt_taint(const char *Model1_file, const int Model1_line, unsigned Model1_taint,
        const char *Model1_fmt, ...);
extern void Model1_warn_slowpath_null(const char *Model1_file, const int Model1_line);
/* used internally by panic.c */
struct Model1_warn_args;

void Model1___warn(const char *Model1_file, int Model1_line, void *Model1_caller, unsigned Model1_taint,
     struct Model1_pt_regs *Model1_regs, struct Model1_warn_args *Model1_args);
/*
 * WARN_ON_SMP() is for cases that the warning is either
 * meaningless for !SMP or may even cause failures.
 * This is usually used for cases that we have
 * WARN_ON(!spin_is_locked(&lock)) checks, as spin_is_locked()
 * returns 0 for uniprocessor settings.
 * It can also be used with values that are only defined
 * on SMP:
 *
 * struct foo {
 *  [...]
 * #ifdef CONFIG_SMP
 *	int bar;
 * #endif
 * };
 *
 * void func(struct foo *zoot)
 * {
 *	WARN_ON_SMP(!zoot->bar);
 *
 * For CONFIG_SMP, WARN_ON_SMP() should act the same as WARN_ON(),
 * and should be a nop and return false for uniprocessor.
 *
 * if (WARN_ON_SMP(x)) returns true only when CONFIG_SMP is set
 * and x is true.
 */


enum Model1_bug_trap_type {
 Model1_BUG_TRAP_TYPE_NONE = 0,
 Model1_BUG_TRAP_TYPE_WARN = 1,
 Model1_BUG_TRAP_TYPE_BUG = 2,
};

struct Model1_pt_regs;
/* Force a compilation error if a constant expression is not a power of 2 */



/* Force a compilation error if condition is true, but also produce a
   result (of value 0 and type size_t), so the expression can be used
   e.g. in a structure initializer (or where-ever else comma expressions
   aren't permitted). */



/*
 * BUILD_BUG_ON_INVALID() permits the compiler to check the validity of the
 * expression but avoids the generation of any code, even if that expression
 * has side-effects.
 */


/**
 * BUILD_BUG_ON_MSG - break compile if a condition is true & emit supplied
 *		      error message.
 * @condition: the condition which the compiler should know is false.
 *
 * See BUILD_BUG_ON for description.
 */


/**
 * BUILD_BUG_ON - break compile if a condition is true.
 * @condition: the condition which the compiler should know is false.
 *
 * If you have some code which relies on certain constants being equal, or
 * some other compile-time-evaluated condition, you should use BUILD_BUG_ON to
 * detect if someone changes it.
 *
 * The implementation uses gcc's reluctance to create a negative array, but gcc
 * (as of 4.4) only emits that error for obvious cases (e.g. not arguments to
 * inline functions).  Luckily, in 4.3 they added the "error" function
 * attribute just for this type of case.  Thus, we use a negative sized array
 * (should always create an error on gcc versions older than 4.4) and then call
 * an undefined function with the error attribute (should always create an
 * error on gcc 4.3 and later).  If for some reason, neither creates a
 * compile-time error, we'll still have a link-time error, which is harder to
 * track down.
 */







/**
 * BUILD_BUG - break compile if used.
 *
 * If you have some code that you expect the compiler to eliminate at
 * build time, you should use BUILD_BUG to detect if it is
 * unexpectedly used.
 */
static inline __attribute__((no_instrument_function)) int Model1_is_warning_bug(const struct Model1_bug_entry *Model1_bug)
{
 return Model1_bug->Model1_flags & (1 << 0);
}

const struct Model1_bug_entry *Model1_find_bug(unsigned long Model1_bugaddr);

enum Model1_bug_trap_type Model1_report_bug(unsigned long Model1_bug_addr, struct Model1_pt_regs *Model1_regs);

/* These are defined by the architecture */
int Model1_is_valid_bugaddr(unsigned long Model1_addr);


struct Model1_page;
struct Model1_vm_area_struct;
struct Model1_mm_struct;

extern void Model1_dump_page(struct Model1_page *Model1_page, const char *Model1_reason);
extern void Model1___dump_page(struct Model1_page *Model1_page, const char *Model1_reason);
void Model1_dump_vma(const struct Model1_vm_area_struct *Model1_vma);
void Model1_dump_mm(const struct Model1_mm_struct *Model1_mm);













/*
 * include/linux/spinlock.h - generic spinlock/rwlock declarations
 *
 * here's the role of the various spinlock/rwlock related include files:
 *
 * on SMP builds:
 *
 *  asm/spinlock_types.h: contains the arch_spinlock_t/arch_rwlock_t and the
 *                        initializers
 *
 *  linux/spinlock_types.h:
 *                        defines the generic type and initializers
 *
 *  asm/spinlock.h:       contains the arch_spin_*()/etc. lowlevel
 *                        implementations, mostly inline assembly code
 *
 *   (also included on UP-debug builds:)
 *
 *  linux/spinlock_api_smp.h:
 *                        contains the prototypes for the _spin_*() APIs.
 *
 *  linux/spinlock.h:     builds the final spin_*() APIs.
 *
 * on UP builds:
 *
 *  linux/spinlock_type_up.h:
 *                        contains the generic, simplified UP spinlock type.
 *                        (which is an empty structure on non-debug builds)
 *
 *  linux/spinlock_types.h:
 *                        defines the generic type and initializers
 *
 *  linux/spinlock_up.h:
 *                        contains the arch_spin_*()/etc. version of UP
 *                        builds. (which are NOPs on non-debug, non-preempt
 *                        builds)
 *
 *   (included on UP-non-debug builds:)
 *
 *  linux/spinlock_api_up.h:
 *                        builds the _spin_*() APIs.
 *
 *  linux/spinlock.h:     builds the final spin_*() APIs.
 */






/*
 * include/linux/preempt.h - macros for accessing and manipulating
 * preempt_count (used for kernel preemption, interrupt count, etc.)
 */











/********** include/linux/list.h **********/

/*
 * Architectures might want to move the poison pointer offset
 * into some well-recognized area such as 0xdead000000000000,
 * that is also not mappable by user-space exploits:
 */






/*
 * These are non-NULL pointers that will result in page faults
 * under normal circumstances, used to verify that nobody uses
 * non-initialized list entries.
 */



/********** include/linux/timer.h **********/
/*
 * Magic number "tsta" to indicate a static timer initializer
 * for the object debugging code.
 */


/********** mm/debug-pagealloc.c **********/






/********** mm/page_alloc.c ************/



/********** mm/slab.c **********/
/*
 * Magic nums for obj red zoning.
 * Placed in the first word before and the first word after an obj.
 */






/* ...and for poisoning */




/********** arch/$ARCH/mm/init.c **********/


/********** arch/ia64/hp/common/sba_iommu.c **********/
/*
 * arch/ia64/hp/common/sba_iommu.c uses a 16-byte poison string with a
 * value of "SBAIOMMU POISON\0" for spill-over poisoning.
 */

/********** fs/jbd/journal.c **********/



/********** drivers/base/dmapool.c **********/



/********** drivers/atm/ **********/



/********** kernel/mutexes **********/



/********** lib/flex_array.c **********/


/********** security/ **********/
/* const.h: Macros for dealing with constants.  */




/* Some constant macros are used in both assembler and
 * C code.  Therefore we cannot annotate them always with
 * 'UL' and other type specifiers unilaterally.  We
 * use the following macros to deal with this.
 *
 * Similarly, _AT() will cast an expression with a type in C, but
 * leave it unchanged in asm.
 */


/*
 * Simple doubly linked list implementation.
 *
 * Some of the internal functions ("__xxx") are useful when
 * manipulating whole lists rather than single entries, as
 * sometimes we already know the next/prev entries and we can
 * generate better code by using them directly rather than
 * using the generic single-entry routines.
 */






static inline __attribute__((no_instrument_function)) void Model1_INIT_LIST_HEAD(struct Model1_list_head *Model1_list)
{
 ({ union { typeof(Model1_list->Model1_next) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(Model1_list->Model1_next)) (Model1_list) }; Model1___write_once_size(&(Model1_list->Model1_next), Model1___u.Model1___c, sizeof(Model1_list->Model1_next)); Model1___u.Model1___val; });
 Model1_list->Model1_prev = Model1_list;
}

/*
 * Insert a new entry between two known consecutive entries.
 *
 * This is only for internal list manipulation where we know
 * the prev/next entries already!
 */

static inline __attribute__((no_instrument_function)) void Model1___list_add(struct Model1_list_head *Model1_new,
         struct Model1_list_head *Model1_prev,
         struct Model1_list_head *Model1_next)
{
 Model1_next->Model1_prev = Model1_new;
 Model1_new->Model1_next = Model1_next;
 Model1_new->Model1_prev = Model1_prev;
 ({ union { typeof(Model1_prev->Model1_next) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(Model1_prev->Model1_next)) (Model1_new) }; Model1___write_once_size(&(Model1_prev->Model1_next), Model1___u.Model1___c, sizeof(Model1_prev->Model1_next)); Model1___u.Model1___val; });
}






/**
 * list_add - add a new entry
 * @new: new entry to be added
 * @head: list head to add it after
 *
 * Insert a new entry after the specified head.
 * This is good for implementing stacks.
 */
static inline __attribute__((no_instrument_function)) void Model1_list_add(struct Model1_list_head *Model1_new, struct Model1_list_head *Model1_head)
{
 Model1___list_add(Model1_new, Model1_head, Model1_head->Model1_next);
}


/**
 * list_add_tail - add a new entry
 * @new: new entry to be added
 * @head: list head to add it before
 *
 * Insert a new entry before the specified head.
 * This is useful for implementing queues.
 */
static inline __attribute__((no_instrument_function)) void Model1_list_add_tail(struct Model1_list_head *Model1_new, struct Model1_list_head *Model1_head)
{
 Model1___list_add(Model1_new, Model1_head->Model1_prev, Model1_head);
}

/*
 * Delete a list entry by making the prev/next entries
 * point to each other.
 *
 * This is only for internal list manipulation where we know
 * the prev/next entries already!
 */
static inline __attribute__((no_instrument_function)) void Model1___list_del(struct Model1_list_head * Model1_prev, struct Model1_list_head * Model1_next)
{
 Model1_next->Model1_prev = Model1_prev;
 ({ union { typeof(Model1_prev->Model1_next) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(Model1_prev->Model1_next)) (Model1_next) }; Model1___write_once_size(&(Model1_prev->Model1_next), Model1___u.Model1___c, sizeof(Model1_prev->Model1_next)); Model1___u.Model1___val; });
}

/**
 * list_del - deletes entry from list.
 * @entry: the element to delete from the list.
 * Note: list_empty() on entry does not return true after this, the entry is
 * in an undefined state.
 */

static inline __attribute__((no_instrument_function)) void Model1___list_del_entry(struct Model1_list_head *Model1_entry)
{
 Model1___list_del(Model1_entry->Model1_prev, Model1_entry->Model1_next);
}

static inline __attribute__((no_instrument_function)) void Model1_list_del(struct Model1_list_head *Model1_entry)
{
 Model1___list_del(Model1_entry->Model1_prev, Model1_entry->Model1_next);
 Model1_entry->Model1_next = ((void *) 0x100 + (0xdead000000000000UL));
 Model1_entry->Model1_prev = ((void *) 0x200 + (0xdead000000000000UL));
}





/**
 * list_replace - replace old entry by new one
 * @old : the element to be replaced
 * @new : the new element to insert
 *
 * If @old was empty, it will be overwritten.
 */
static inline __attribute__((no_instrument_function)) void Model1_list_replace(struct Model1_list_head *old,
    struct Model1_list_head *Model1_new)
{
 Model1_new->Model1_next = old->Model1_next;
 Model1_new->Model1_next->Model1_prev = Model1_new;
 Model1_new->Model1_prev = old->Model1_prev;
 Model1_new->Model1_prev->Model1_next = Model1_new;
}

static inline __attribute__((no_instrument_function)) void Model1_list_replace_init(struct Model1_list_head *old,
     struct Model1_list_head *Model1_new)
{
 Model1_list_replace(old, Model1_new);
 Model1_INIT_LIST_HEAD(old);
}

/**
 * list_del_init - deletes entry from list and reinitialize it.
 * @entry: the element to delete from the list.
 */
static inline __attribute__((no_instrument_function)) void Model1_list_del_init(struct Model1_list_head *Model1_entry)
{
 Model1___list_del_entry(Model1_entry);
 Model1_INIT_LIST_HEAD(Model1_entry);
}

/**
 * list_move - delete from one list and add as another's head
 * @list: the entry to move
 * @head: the head that will precede our entry
 */
static inline __attribute__((no_instrument_function)) void Model1_list_move(struct Model1_list_head *Model1_list, struct Model1_list_head *Model1_head)
{
 Model1___list_del_entry(Model1_list);
 Model1_list_add(Model1_list, Model1_head);
}

/**
 * list_move_tail - delete from one list and add as another's tail
 * @list: the entry to move
 * @head: the head that will follow our entry
 */
static inline __attribute__((no_instrument_function)) void Model1_list_move_tail(struct Model1_list_head *Model1_list,
      struct Model1_list_head *Model1_head)
{
 Model1___list_del_entry(Model1_list);
 Model1_list_add_tail(Model1_list, Model1_head);
}

/**
 * list_is_last - tests whether @list is the last entry in list @head
 * @list: the entry to test
 * @head: the head of the list
 */
static inline __attribute__((no_instrument_function)) int Model1_list_is_last(const struct Model1_list_head *Model1_list,
    const struct Model1_list_head *Model1_head)
{
 return Model1_list->Model1_next == Model1_head;
}

/**
 * list_empty - tests whether a list is empty
 * @head: the list to test.
 */
static inline __attribute__((no_instrument_function)) int Model1_list_empty(const struct Model1_list_head *Model1_head)
{
 return ({ union { typeof(Model1_head->Model1_next) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&(Model1_head->Model1_next), Model1___u.Model1___c, sizeof(Model1_head->Model1_next)); else Model1___read_once_size_nocheck(&(Model1_head->Model1_next), Model1___u.Model1___c, sizeof(Model1_head->Model1_next)); Model1___u.Model1___val; }) == Model1_head;
}

/**
 * list_empty_careful - tests whether a list is empty and not being modified
 * @head: the list to test
 *
 * Description:
 * tests whether a list is empty _and_ checks that no other CPU might be
 * in the process of modifying either member (next or prev)
 *
 * NOTE: using list_empty_careful() without synchronization
 * can only be safe if the only activity that can happen
 * to the list entry is list_del_init(). Eg. it cannot be used
 * if another CPU could re-list_add() it.
 */
static inline __attribute__((no_instrument_function)) int Model1_list_empty_careful(const struct Model1_list_head *Model1_head)
{
 struct Model1_list_head *Model1_next = Model1_head->Model1_next;
 return (Model1_next == Model1_head) && (Model1_next == Model1_head->Model1_prev);
}

/**
 * list_rotate_left - rotate the list to the left
 * @head: the head of the list
 */
static inline __attribute__((no_instrument_function)) void Model1_list_rotate_left(struct Model1_list_head *Model1_head)
{
 struct Model1_list_head *Model1_first;

 if (!Model1_list_empty(Model1_head)) {
  Model1_first = Model1_head->Model1_next;
  Model1_list_move_tail(Model1_first, Model1_head);
 }
}

/**
 * list_is_singular - tests whether a list has just one entry.
 * @head: the list to test.
 */
static inline __attribute__((no_instrument_function)) int Model1_list_is_singular(const struct Model1_list_head *Model1_head)
{
 return !Model1_list_empty(Model1_head) && (Model1_head->Model1_next == Model1_head->Model1_prev);
}

static inline __attribute__((no_instrument_function)) void Model1___list_cut_position(struct Model1_list_head *Model1_list,
  struct Model1_list_head *Model1_head, struct Model1_list_head *Model1_entry)
{
 struct Model1_list_head *Model1_new_first = Model1_entry->Model1_next;
 Model1_list->Model1_next = Model1_head->Model1_next;
 Model1_list->Model1_next->Model1_prev = Model1_list;
 Model1_list->Model1_prev = Model1_entry;
 Model1_entry->Model1_next = Model1_list;
 Model1_head->Model1_next = Model1_new_first;
 Model1_new_first->Model1_prev = Model1_head;
}

/**
 * list_cut_position - cut a list into two
 * @list: a new list to add all removed entries
 * @head: a list with entries
 * @entry: an entry within head, could be the head itself
 *	and if so we won't cut the list
 *
 * This helper moves the initial part of @head, up to and
 * including @entry, from @head to @list. You should
 * pass on @entry an element you know is on @head. @list
 * should be an empty list or a list you do not care about
 * losing its data.
 *
 */
static inline __attribute__((no_instrument_function)) void Model1_list_cut_position(struct Model1_list_head *Model1_list,
  struct Model1_list_head *Model1_head, struct Model1_list_head *Model1_entry)
{
 if (Model1_list_empty(Model1_head))
  return;
 if (Model1_list_is_singular(Model1_head) &&
  (Model1_head->Model1_next != Model1_entry && Model1_head != Model1_entry))
  return;
 if (Model1_entry == Model1_head)
  Model1_INIT_LIST_HEAD(Model1_list);
 else
  Model1___list_cut_position(Model1_list, Model1_head, Model1_entry);
}

static inline __attribute__((no_instrument_function)) void Model1___list_splice(const struct Model1_list_head *Model1_list,
     struct Model1_list_head *Model1_prev,
     struct Model1_list_head *Model1_next)
{
 struct Model1_list_head *Model1_first = Model1_list->Model1_next;
 struct Model1_list_head *Model1_last = Model1_list->Model1_prev;

 Model1_first->Model1_prev = Model1_prev;
 Model1_prev->Model1_next = Model1_first;

 Model1_last->Model1_next = Model1_next;
 Model1_next->Model1_prev = Model1_last;
}

/**
 * list_splice - join two lists, this is designed for stacks
 * @list: the new list to add.
 * @head: the place to add it in the first list.
 */
static inline __attribute__((no_instrument_function)) void Model1_list_splice(const struct Model1_list_head *Model1_list,
    struct Model1_list_head *Model1_head)
{
 if (!Model1_list_empty(Model1_list))
  Model1___list_splice(Model1_list, Model1_head, Model1_head->Model1_next);
}

/**
 * list_splice_tail - join two lists, each list being a queue
 * @list: the new list to add.
 * @head: the place to add it in the first list.
 */
static inline __attribute__((no_instrument_function)) void Model1_list_splice_tail(struct Model1_list_head *Model1_list,
    struct Model1_list_head *Model1_head)
{
 if (!Model1_list_empty(Model1_list))
  Model1___list_splice(Model1_list, Model1_head->Model1_prev, Model1_head);
}

/**
 * list_splice_init - join two lists and reinitialise the emptied list.
 * @list: the new list to add.
 * @head: the place to add it in the first list.
 *
 * The list at @list is reinitialised
 */
static inline __attribute__((no_instrument_function)) void Model1_list_splice_init(struct Model1_list_head *Model1_list,
        struct Model1_list_head *Model1_head)
{
 if (!Model1_list_empty(Model1_list)) {
  Model1___list_splice(Model1_list, Model1_head, Model1_head->Model1_next);
  Model1_INIT_LIST_HEAD(Model1_list);
 }
}

/**
 * list_splice_tail_init - join two lists and reinitialise the emptied list
 * @list: the new list to add.
 * @head: the place to add it in the first list.
 *
 * Each of the lists is a queue.
 * The list at @list is reinitialised
 */
static inline __attribute__((no_instrument_function)) void Model1_list_splice_tail_init(struct Model1_list_head *Model1_list,
      struct Model1_list_head *Model1_head)
{
 if (!Model1_list_empty(Model1_list)) {
  Model1___list_splice(Model1_list, Model1_head->Model1_prev, Model1_head);
  Model1_INIT_LIST_HEAD(Model1_list);
 }
}

/**
 * list_entry - get the struct for this entry
 * @ptr:	the &struct list_head pointer.
 * @type:	the type of the struct this is embedded in.
 * @member:	the name of the list_head within the struct.
 */



/**
 * list_first_entry - get the first element from a list
 * @ptr:	the list head to take the element from.
 * @type:	the type of the struct this is embedded in.
 * @member:	the name of the list_head within the struct.
 *
 * Note, that list is expected to be not empty.
 */



/**
 * list_last_entry - get the last element from a list
 * @ptr:	the list head to take the element from.
 * @type:	the type of the struct this is embedded in.
 * @member:	the name of the list_head within the struct.
 *
 * Note, that list is expected to be not empty.
 */



/**
 * list_first_entry_or_null - get the first element from a list
 * @ptr:	the list head to take the element from.
 * @type:	the type of the struct this is embedded in.
 * @member:	the name of the list_head within the struct.
 *
 * Note that if the list is empty, it returns NULL.
 */



/**
 * list_next_entry - get the next element in list
 * @pos:	the type * to cursor
 * @member:	the name of the list_head within the struct.
 */



/**
 * list_prev_entry - get the prev element in list
 * @pos:	the type * to cursor
 * @member:	the name of the list_head within the struct.
 */



/**
 * list_for_each	-	iterate over a list
 * @pos:	the &struct list_head to use as a loop cursor.
 * @head:	the head for your list.
 */



/**
 * list_for_each_prev	-	iterate over a list backwards
 * @pos:	the &struct list_head to use as a loop cursor.
 * @head:	the head for your list.
 */



/**
 * list_for_each_safe - iterate over a list safe against removal of list entry
 * @pos:	the &struct list_head to use as a loop cursor.
 * @n:		another &struct list_head to use as temporary storage
 * @head:	the head for your list.
 */




/**
 * list_for_each_prev_safe - iterate over a list backwards safe against removal of list entry
 * @pos:	the &struct list_head to use as a loop cursor.
 * @n:		another &struct list_head to use as temporary storage
 * @head:	the head for your list.
 */





/**
 * list_for_each_entry	-	iterate over list of given type
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 */





/**
 * list_for_each_entry_reverse - iterate backwards over list of given type.
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 */





/**
 * list_prepare_entry - prepare a pos entry for use in list_for_each_entry_continue()
 * @pos:	the type * to use as a start point
 * @head:	the head of the list
 * @member:	the name of the list_head within the struct.
 *
 * Prepares a pos entry for use as a start point in list_for_each_entry_continue().
 */



/**
 * list_for_each_entry_continue - continue iteration over list of given type
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 *
 * Continue to iterate over list of given type, continuing after
 * the current position.
 */





/**
 * list_for_each_entry_continue_reverse - iterate backwards from the given point
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 *
 * Start to iterate over list of given type backwards, continuing after
 * the current position.
 */





/**
 * list_for_each_entry_from - iterate over list of given type from the current point
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 *
 * Iterate over list of given type, continuing from current position.
 */




/**
 * list_for_each_entry_safe - iterate over list of given type safe against removal of list entry
 * @pos:	the type * to use as a loop cursor.
 * @n:		another type * to use as temporary storage
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 */






/**
 * list_for_each_entry_safe_continue - continue list iteration safe against removal
 * @pos:	the type * to use as a loop cursor.
 * @n:		another type * to use as temporary storage
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 *
 * Iterate over list of given type, continuing after current point,
 * safe against removal of list entry.
 */






/**
 * list_for_each_entry_safe_from - iterate over list from current point safe against removal
 * @pos:	the type * to use as a loop cursor.
 * @n:		another type * to use as temporary storage
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 *
 * Iterate over list of given type from current point, safe against
 * removal of list entry.
 */





/**
 * list_for_each_entry_safe_reverse - iterate backwards over list safe against removal
 * @pos:	the type * to use as a loop cursor.
 * @n:		another type * to use as temporary storage
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 *
 * Iterate backwards over list of given type, safe against removal
 * of list entry.
 */






/**
 * list_safe_reset_next - reset a stale list_for_each_entry_safe loop
 * @pos:	the loop cursor used in the list_for_each_entry_safe loop
 * @n:		temporary storage used in list_for_each_entry_safe
 * @member:	the name of the list_head within the struct.
 *
 * list_safe_reset_next is not safe to use in general if the list may be
 * modified concurrently (eg. the lock is dropped in the loop body). An
 * exception to this is if the cursor element (pos) is pinned in the list,
 * and list_safe_reset_next is called after re-taking the lock and before
 * completing the current iteration of the loop body.
 */



/*
 * Double linked lists with a single pointer list head.
 * Mostly useful for hash tables where the two pointer list head is
 * too wasteful.
 * You lose the ability to access the tail in O(1).
 */




static inline __attribute__((no_instrument_function)) void Model1_INIT_HLIST_NODE(struct Model1_hlist_node *Model1_h)
{
 Model1_h->Model1_next = ((void *)0);
 Model1_h->Model1_pprev = ((void *)0);
}

static inline __attribute__((no_instrument_function)) int Model1_hlist_unhashed(const struct Model1_hlist_node *Model1_h)
{
 return !Model1_h->Model1_pprev;
}

static inline __attribute__((no_instrument_function)) int Model1_hlist_empty(const struct Model1_hlist_head *Model1_h)
{
 return !({ union { typeof(Model1_h->Model1_first) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&(Model1_h->Model1_first), Model1___u.Model1___c, sizeof(Model1_h->Model1_first)); else Model1___read_once_size_nocheck(&(Model1_h->Model1_first), Model1___u.Model1___c, sizeof(Model1_h->Model1_first)); Model1___u.Model1___val; });
}

static inline __attribute__((no_instrument_function)) void Model1___hlist_del(struct Model1_hlist_node *Model1_n)
{
 struct Model1_hlist_node *Model1_next = Model1_n->Model1_next;
 struct Model1_hlist_node **Model1_pprev = Model1_n->Model1_pprev;

 ({ union { typeof(*Model1_pprev) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(*Model1_pprev)) (Model1_next) }; Model1___write_once_size(&(*Model1_pprev), Model1___u.Model1___c, sizeof(*Model1_pprev)); Model1___u.Model1___val; });
 if (Model1_next)
  Model1_next->Model1_pprev = Model1_pprev;
}

static inline __attribute__((no_instrument_function)) void Model1_hlist_del(struct Model1_hlist_node *Model1_n)
{
 Model1___hlist_del(Model1_n);
 Model1_n->Model1_next = ((void *) 0x100 + (0xdead000000000000UL));
 Model1_n->Model1_pprev = ((void *) 0x200 + (0xdead000000000000UL));
}

static inline __attribute__((no_instrument_function)) void Model1_hlist_del_init(struct Model1_hlist_node *Model1_n)
{
 if (!Model1_hlist_unhashed(Model1_n)) {
  Model1___hlist_del(Model1_n);
  Model1_INIT_HLIST_NODE(Model1_n);
 }
}

static inline __attribute__((no_instrument_function)) void Model1_hlist_add_head(struct Model1_hlist_node *Model1_n, struct Model1_hlist_head *Model1_h)
{
 struct Model1_hlist_node *Model1_first = Model1_h->Model1_first;
 Model1_n->Model1_next = Model1_first;
 if (Model1_first)
  Model1_first->Model1_pprev = &Model1_n->Model1_next;
 ({ union { typeof(Model1_h->Model1_first) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(Model1_h->Model1_first)) (Model1_n) }; Model1___write_once_size(&(Model1_h->Model1_first), Model1___u.Model1___c, sizeof(Model1_h->Model1_first)); Model1___u.Model1___val; });
 Model1_n->Model1_pprev = &Model1_h->Model1_first;
}

/* next must be != NULL */
static inline __attribute__((no_instrument_function)) void Model1_hlist_add_before(struct Model1_hlist_node *Model1_n,
     struct Model1_hlist_node *Model1_next)
{
 Model1_n->Model1_pprev = Model1_next->Model1_pprev;
 Model1_n->Model1_next = Model1_next;
 Model1_next->Model1_pprev = &Model1_n->Model1_next;
 ({ union { typeof(*(Model1_n->Model1_pprev)) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(*(Model1_n->Model1_pprev))) (Model1_n) }; Model1___write_once_size(&(*(Model1_n->Model1_pprev)), Model1___u.Model1___c, sizeof(*(Model1_n->Model1_pprev))); Model1___u.Model1___val; });
}

static inline __attribute__((no_instrument_function)) void Model1_hlist_add_behind(struct Model1_hlist_node *Model1_n,
        struct Model1_hlist_node *Model1_prev)
{
 Model1_n->Model1_next = Model1_prev->Model1_next;
 ({ union { typeof(Model1_prev->Model1_next) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(Model1_prev->Model1_next)) (Model1_n) }; Model1___write_once_size(&(Model1_prev->Model1_next), Model1___u.Model1___c, sizeof(Model1_prev->Model1_next)); Model1___u.Model1___val; });
 Model1_n->Model1_pprev = &Model1_prev->Model1_next;

 if (Model1_n->Model1_next)
  Model1_n->Model1_next->Model1_pprev = &Model1_n->Model1_next;
}

/* after that we'll appear to be on some hlist and hlist_del will work */
static inline __attribute__((no_instrument_function)) void Model1_hlist_add_fake(struct Model1_hlist_node *Model1_n)
{
 Model1_n->Model1_pprev = &Model1_n->Model1_next;
}

static inline __attribute__((no_instrument_function)) bool Model1_hlist_fake(struct Model1_hlist_node *Model1_h)
{
 return Model1_h->Model1_pprev == &Model1_h->Model1_next;
}

/*
 * Check whether the node is the only node of the head without
 * accessing head:
 */
static inline __attribute__((no_instrument_function)) bool
Model1_hlist_is_singular_node(struct Model1_hlist_node *Model1_n, struct Model1_hlist_head *Model1_h)
{
 return !Model1_n->Model1_next && Model1_n->Model1_pprev == &Model1_h->Model1_first;
}

/*
 * Move a list from one list head to another. Fixup the pprev
 * reference of the first entry if it exists.
 */
static inline __attribute__((no_instrument_function)) void Model1_hlist_move_list(struct Model1_hlist_head *old,
       struct Model1_hlist_head *Model1_new)
{
 Model1_new->Model1_first = old->Model1_first;
 if (Model1_new->Model1_first)
  Model1_new->Model1_first->Model1_pprev = &Model1_new->Model1_first;
 old->Model1_first = ((void *)0);
}
/**
 * hlist_for_each_entry	- iterate over list of given type
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the hlist_node within the struct.
 */





/**
 * hlist_for_each_entry_continue - iterate over a hlist continuing after current point
 * @pos:	the type * to use as a loop cursor.
 * @member:	the name of the hlist_node within the struct.
 */





/**
 * hlist_for_each_entry_from - iterate over a hlist continuing from current point
 * @pos:	the type * to use as a loop cursor.
 * @member:	the name of the hlist_node within the struct.
 */




/**
 * hlist_for_each_entry_safe - iterate over list of given type safe against removal of list entry
 * @pos:	the type * to use as a loop cursor.
 * @n:		another &struct hlist_node to use as temporary storage
 * @head:	the head for your list.
 * @member:	the name of the hlist_node within the struct.
 */

/*
 * We put the hardirq and softirq counter into the preemption
 * counter. The bitmask has the following meaning:
 *
 * - bits 0-7 are the preemption count (max preemption depth: 256)
 * - bits 8-15 are the softirq count (max # of softirqs: 256)
 *
 * The hardirq count could in theory be the same as the number of
 * interrupts in the system, but we run all interrupt handlers with
 * interrupts disabled, so we cannot have nesting interrupts. Though
 * there are a few palaeontologic drivers which reenable interrupts in
 * the handler, so we need more than one bit here.
 *
 *         PREEMPT_MASK:	0x000000ff
 *         SOFTIRQ_MASK:	0x0000ff00
 *         HARDIRQ_MASK:	0x000f0000
 *             NMI_MASK:	0x00100000
 * PREEMPT_NEED_RESCHED:	0x80000000
 */
/* We use the MSB mostly because its available */


/* preempt_count() and related functions, depends on PREEMPT_NEED_RESCHED */





/*
 * Compared to the generic __my_cpu_offset version, the following
 * saves one instruction and avoids clobbering a temp register.
 */
/*
 * Initialized pointers to per-cpu variables needed for the boot
 * processor need to use these macros to get the proper address
 * offset from __per_cpu_load on SMP.
 *
 * There also must be an entry in vmlinux_64.lds.S
 */
/* For arch-specific code, we can use direct single-insn ops (they
 * don't give an lvalue though). */
extern void Model1___bad_percpu_size(void);
/*
 * Generate a percpu add to memory instruction and optimize code
 * if one is added or subtracted.
 */
/*
 * Add return operation
 */
/*
 * xchg is implemented using cmpxchg without a lock prefix. xchg is
 * expensive due to the implied lock prefix.  The processor cannot prefetch
 * cachelines if xchg is used.
 */
/*
 * cmpxchg has no such implied lock semantics as a result it is much
 * more efficient for cpu local operations.
 */
/*
 * this_cpu_read() makes gcc load the percpu variable every time it is
 * accessed while this_cpu_read_stable() allows the value to be cached.
 * this_cpu_read_stable() is more efficient and can be used if its value
 * is guaranteed to be valid across cpus.  The current users include
 * get_current() and get_thread_info() both of which are actually
 * per-thread variables implemented as per-cpu variables and thus
 * stable for the duration of the respective task.
 */
/*
 * Per cpu atomic 64 bit operations are only available under 64 bit.
 * 32 bit must fall back to generic operations.
 */
/*
 * Pretty complex macro to generate cmpxchg16 instruction.  The instruction
 * is not supported on early AMD64 processors so we must be able to emulate
 * it in software.  The address used in the cmpxchg16 instruction must be
 * aligned to a 16 byte boundary.
 */
/* This is not atomic against other CPUs -- CPU preemption needs to be off */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model1_x86_this_cpu_constant_test_bit(unsigned int Model1_nr,
                        const unsigned long *Model1_addr)
{
 unsigned long *Model1_a = (unsigned long *)Model1_addr + Model1_nr / 64;


 return ((1UL << (Model1_nr % 64)) & ({ typeof(*Model1_a) Model1_pfo_ret__; switch (sizeof(*Model1_a)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (*Model1_a)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (*Model1_a)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (*Model1_a)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (*Model1_a)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; })) != 0;



}

static inline __attribute__((no_instrument_function)) bool Model1_x86_this_cpu_variable_test_bit(int Model1_nr,
                        const unsigned long *Model1_addr)
{
 bool Model1_oldbit;

 asm volatile("bt ""%%""gs"":" "%" "2"",%1\n\t"
   "\n\tset" "c" " %[_cc_" "c" "]\n"
   : [_cc_c] "=qm" (Model1_oldbit)
   : "m" (*(unsigned long *)Model1_addr), "Ir" (Model1_nr));

 return Model1_oldbit;
}
















/*
 * The default limit for the nr of threads is now in
 * /proc/sys/kernel/threads-max.
 */

/*
 * Maximum supported processors.  Setting this smaller saves quite a
 * bit of memory.  Use nr_cpu_ids instead of this except for static bitmaps.
 */





/* Places which use this should consider cpumask_var_t. */




/*
 * This controls the default maximum pid allocated to a process
 */


/*
 * A maximum of 4 million PIDs should be enough for a while.
 * [NOTE: PID/TIDs are limited to 2^29 ~= 500+ million, see futex.h.]
 */



/*
 * Define a minimum number of pids per cpu.  Heuristically based
 * on original pid max of 32k for 32 cpus.  Also, increase the
 * minimum settable value for pid_max on the running system based
 * on similar defaults.  See kernel/pid.c:pidmap_init() for details.
 */
/*
 * linux/percpu-defs.h - basic definitions for percpu areas
 *
 * DO NOT INCLUDE DIRECTLY OUTSIDE PERCPU IMPLEMENTATION PROPER.
 *
 * This file is separate from linux/percpu.h to avoid cyclic inclusion
 * dependency from arch header files.  Only to be included from
 * asm/percpu.h.
 *
 * This file includes macros necessary to declare percpu sections and
 * variables, and definitions of percpu accessors and operations.  It
 * should provide enough percpu features to arch header files even when
 * they can only include asm/percpu.h to avoid cyclic inclusion dependency.
 */
/*
 * Base implementations of per-CPU variable declarations and definitions, where
 * the section in which the variable is to be placed is provided by the
 * 'sec' argument.  This may be used to affect the parameters governing the
 * variable's storage.
 *
 * NOTE!  The sections for the DECLARE and for the DEFINE must match, lest
 * linkage errors occur due the compiler generating the wrong code to access
 * that section.
 */







/*
 * s390 and alpha modules require percpu variables to be defined as
 * weak to force the compiler to generate GOT based external
 * references for them.  This is necessary because percpu sections
 * will be located outside of the usually addressable area.
 *
 * This definition puts the following two extra restrictions when
 * defining percpu variables.
 *
 * 1. The symbol must be globally unique, even the static ones.
 * 2. Static percpu variables cannot be defined inside a function.
 *
 * Archs which need weak percpu definitions should define
 * ARCH_NEEDS_WEAK_PER_CPU in asm/percpu.h when necessary.
 *
 * To ensure that the generic code observes the above two
 * restrictions, if CONFIG_DEBUG_FORCE_WEAK_PER_CPU is set weak
 * definition is used for all cases.
 */
/*
 * Normal declaration and definition macros.
 */
/*
 * Variant on the per-CPU variable declaration/definition theme used for
 * ordinary per-CPU variables.
 */






/*
 * Declaration/definition used for per-CPU variables that must come first in
 * the set of variables.
 */






/*
 * Declaration/definition used for per-CPU variables that must be cacheline
 * aligned under SMP conditions so that, whilst a particular instance of the
 * data corresponds to a particular CPU, inefficiencies due to direct access by
 * other CPUs are reduced by preventing the data from unnecessarily spanning
 * cachelines.
 *
 * An example of this would be statistical data, where each CPU's set of data
 * is updated by that CPU alone, but the data from across all CPUs is collated
 * by a CPU processing a read from a proc file.
 */
/*
 * Declaration/definition used for per-CPU variables that must be page aligned.
 */
/*
 * Declaration/definition used for per-CPU variables that must be read mostly.
 */






/*
 * Intermodule exports for per-CPU variables.  sparse forgets about
 * address space across EXPORT_SYMBOL(), change EXPORT_SYMBOL() to
 * noop if __CHECKER__.
 */
/*
 * Accessors and operations.
 */


/*
 * __verify_pcpu_ptr() verifies @ptr is a percpu pointer without evaluating
 * @ptr and is invoked once before a percpu area is accessed by all
 * accessors and operations.  This is performed in the generic part of
 * percpu and arch overrides don't need to worry about it; however, if an
 * arch wants to implement an arch-specific percpu accessor or operation,
 * it may use __verify_pcpu_ptr() to verify the parameters.
 *
 * + 0 is required in order to convert the pointer type from a
 * potential array type to a pointer to a single item of the array.
 */
/*
 * Add an offset to a pointer but keep the pointer as-is.  Use RELOC_HIDE()
 * to prevent the compiler from making incorrect assumptions about the
 * pointer value.  The weird cast keeps both GCC and sparse happy.
 */
/*
 * Must be an lvalue. Since @var must be a simple identifier,
 * we force a syntax error here if it isn't.
 */






/*
 * The weird & is necessary because sparse considers (void)(var) to be
 * a direct dereference of percpu variable (var).
 */
/*
 * Branching function to split up a function into a set of functions that
 * are called for different scalar sizes of the objects handled.
 */

extern void Model1___bad_size_call_parameter(void);




static inline __attribute__((no_instrument_function)) void Model1___this_cpu_preempt_check(const char *Model1_op) { }
/*
 * Special handling for cmpxchg_double.  cmpxchg_double is passed two
 * percpu variables.  The first has to be aligned to a double word
 * boundary and the second has to follow directly thereafter.
 * We enforce this on all architectures even if they don't support
 * a double cmpxchg instruction, since it's a cheap requirement, and it
 * avoids breaking the requirement for architectures with the instruction.
 */
/*
 * this_cpu operations (C) 2008-2013 Christoph Lameter <cl@linux.com>
 *
 * Optimized manipulation for memory allocated through the per cpu
 * allocator or for addresses of per cpu variables.
 *
 * These operation guarantee exclusivity of access for other operations
 * on the *same* processor. The assumption is that per cpu data is only
 * accessed by a single processor instance (the current one).
 *
 * The arch code can provide optimized implementation by defining macros
 * for certain scalar sizes. F.e. provide this_cpu_add_2() to provide per
 * cpu atomic operations for 2 byte sized RMW actions. If arch code does
 * not provide operations for a scalar size then the fallback in the
 * generic code will be used.
 *
 * cmpxchg_double replaces two adjacent scalars at once.  The first two
 * parameters are per cpu variables which have to be of the same size.  A
 * truth value is returned to indicate success or failure (since a double
 * register result is difficult to handle).  There is very limited hardware
 * support for these operations, so only certain sizes may work.
 */

/*
 * Operations for contexts where we do not want to do any checks for
 * preemptions.  Unless strictly necessary, always use [__]this_cpu_*()
 * instead.
 *
 * If there is no other protection through preempt disable and/or disabling
 * interupts then one of these RMW operations can show unexpected behavior
 * because the execution thread was rescheduled on another processor or an
 * interrupt occurred and the same percpu variable was modified from the
 * interrupt context.
 */
/*
 * Operations for contexts that are safe from preemption/interrupts.  These
 * operations verify that preemption is disabled.
 */
/*
 * Operations with implied preemption/interrupt protection.  These
 * operations can be used without worrying about preemption or interrupt.
 */



/*
 * per_cpu_offset() is the offset that has to be added to a
 * percpu variable to get to the instance for a certain processor.
 *
 * Most arches use the __per_cpu_offset array for those offsets but
 * some arches have their own ways of determining the offset (x86_64, s390).
 */

extern unsigned long Model1___per_cpu_offset[64];




/*
 * Determine the offset for the currently active processor.
 * An arch may define __my_cpu_offset to provide a more effective
 * means of obtaining the offset to the per cpu variables of the
 * current processor.
 */
/*
 * Arch may define arch_raw_cpu_ptr() to provide more efficient address
 * translations for raw_cpu_ptr().
 */





extern void Model1_setup_per_cpu_areas(void);

/* We can use this directly for local CPU (faster). */
extern __attribute__((section(".data..percpu" "..read_mostly"))) __typeof__(unsigned long) Model1_this_cpu_off;





/*
 * Define the "EARLY_PER_CPU" macros.  These are used for some per_cpu
 * variables that are initialized and accessed before there are per_cpu
 * areas allocated.
 */
/* thread_info.h: common low-level thread information accessors
 *
 * Copyright (C) 2002  David Howells (dhowells@redhat.com)
 * - Incorporating suggestions made by Linus Torvalds
 */







struct Model1_timespec;
struct Model1_compat_timespec;

/*
 * System call restart block.
 */
struct Model1_restart_block {
 long (*Model1_fn)(struct Model1_restart_block *);
 union {
  /* For futex_wait and futex_wait_requeue_pi */
  struct {
   Model1_u32 *Model1_uaddr;
   Model1_u32 Model1_val;
   Model1_u32 Model1_flags;
   Model1_u32 Model1_bitset;
   Model1_u64 Model1_time;
   Model1_u32 *Model1_uaddr2;
  } Model1_futex;
  /* For nanosleep */
  struct {
   Model1_clockid_t Model1_clockid;
   struct Model1_timespec *Model1_rmtp;

   struct Model1_compat_timespec *Model1_compat_rmtp;

   Model1_u64 Model1_expires;
  } Model1_nanosleep;
  /* For poll */
  struct {
   struct Model1_pollfd *Model1_ufds;
   int Model1_nfds;
   int Model1_has_timeout;
   unsigned long Model1_tv_sec;
   unsigned long Model1_tv_nsec;
  } Model1_poll;
 };
};

extern long Model1_do_no_restart_syscall(struct Model1_restart_block *Model1_parm);



/* thread_info.h: low-level thread information
 *
 * Copyright (C) 2002  David Howells (dhowells@redhat.com)
 * - Incorporating suggestions made by Linus Torvalds and Dave Miller
 */



















/* PAGE_SHIFT determines the page size */
/* Cast *PAGE_MASK to a signed type so that it is sign-extended if
   virtual addresses are 32-bits but physical addresses are larger
   (ie, 32-bit PAE). */







unsigned long Model1_kaslr_get_random_long(const char *Model1_purpose);







static inline __attribute__((no_instrument_function)) void Model1_kernel_randomize_memory(void) { }
/*
 * Set __PAGE_OFFSET to the most negative possible address +
 * PGDIR_SIZE*16 (pgd slot 272).  The gap is to allow a space for a
 * hypervisor to fit.  Choosing 16 slots here is arbitrary, but it's
 * what Xen requires.
 */
/* See Documentation/x86/x86_64/mm.txt for a description of the memory map. */



/*
 * Kernel image size is limited to 1GiB due to the fixmap living in the
 * next 1GiB (see level2_kernel_pgt in arch/x86/kernel/head_64.S). Use
 * 512MiB by default, leaving 1.5GiB for modules once the page tables
 * are fully set up. If kernel ASLR is configured, it can extend the
 * kernel page table mapping, reducing the size of the modules area.
 */








extern int Model1_devmem_is_allowed(unsigned long Model1_pagenr);

extern unsigned long Model1_max_low_pfn_mapped;
extern unsigned long Model1_max_pfn_mapped;

static inline __attribute__((no_instrument_function)) Model1_phys_addr_t Model1_get_max_mapped(void)
{
 return (Model1_phys_addr_t)Model1_max_pfn_mapped << 12;
}

bool Model1_pfn_range_is_mapped(unsigned long Model1_start_pfn, unsigned long Model1_end_pfn);

extern unsigned long Model1_init_memory_mapping(unsigned long Model1_start,
      unsigned long Model1_end);

extern void Model1_initmem_init(void);









/* duplicated to the one in bootmem.h */
extern unsigned long Model1_max_pfn;
extern unsigned long Model1_phys_base;

static inline __attribute__((no_instrument_function)) unsigned long Model1___phys_addr_nodebug(unsigned long Model1_x)
{
 unsigned long Model1_y = Model1_x - (0xffffffff80000000UL);

 /* use the carry flag to determine if x was < __START_KERNEL_map */
 Model1_x = Model1_y + ((Model1_x > Model1_y) ? Model1_phys_base : ((0xffffffff80000000UL) - ((unsigned long)(0xffff880000000000UL))));

 return Model1_x;
}
void Model1_clear_page(void *Model1_page);
void Model1_copy_page(void *Model1_to, void *Model1_from);






struct Model1_page;





struct Model1_range {
 Model1_u64 Model1_start;
 Model1_u64 Model1_end;
};

int Model1_add_range(struct Model1_range *Model1_range, int Model1_az, int Model1_nr_range,
  Model1_u64 Model1_start, Model1_u64 Model1_end);


int Model1_add_range_with_merge(struct Model1_range *Model1_range, int Model1_az, int Model1_nr_range,
    Model1_u64 Model1_start, Model1_u64 Model1_end);

void Model1_subtract_range(struct Model1_range *Model1_range, int Model1_az, Model1_u64 Model1_start, Model1_u64 Model1_end);

int Model1_clean_sort_range(struct Model1_range *Model1_range, int Model1_az);

void Model1_sort_range(struct Model1_range *Model1_range, int Model1_nr_range);


static inline __attribute__((no_instrument_function)) Model1_resource_size_t Model1_cap_resource(Model1_u64 Model1_val)
{
 if (Model1_val > ((Model1_resource_size_t)~0))
  return ((Model1_resource_size_t)~0);

 return Model1_val;
}
extern struct Model1_range Model1_pfn_mapped[];
extern int Model1_nr_pfn_mapped;

static inline __attribute__((no_instrument_function)) void Model1_clear_user_page(void *Model1_page, unsigned long Model1_vaddr,
       struct Model1_page *Model1_pg)
{
 Model1_clear_page(Model1_page);
}

static inline __attribute__((no_instrument_function)) void Model1_copy_user_page(void *Model1_to, void *Model1_from, unsigned long Model1_vaddr,
      struct Model1_page *Model1_topage)
{
 Model1_copy_page(Model1_to, Model1_from);
}
/* __pa_symbol should be used for C visible symbols.
   This seems to be the official gcc blessed way to do such arithmetic. */
/*
 * We need __phys_reloc_hide() here because gcc may assume that there is no
 * overflow during __pa() calculation and can optimize it unexpectedly.
 * Newer versions of gcc provide -fno-strict-overflow switch to handle this
 * case properly. Once all supported versions of gcc understand it, we can
 * remove this Voodoo magic stuff. (i.e. once gcc3.x is deprecated)
 */
/*
 * virt_to_page(kaddr) returns a valid pointer if and only if
 * virt_addr_valid(kaddr) returns true.
 */


extern bool Model1___virt_addr_valid(unsigned long Model1_kaddr);














/*
 * pfn_t: encapsulates a page-frame number that is optionally backed
 * by memmap (struct page).  Whether a pfn_t has a 'struct page'
 * backing is indicated by flags in the high bits of the value.
 */
typedef struct {
 Model1_u64 Model1_val;
} Model1_pfn_t;
/*
 * supports 3 memory models.
 */
/* memmap is virtually contiguous.  */
/*
 * Convert a physical address to a Page Frame Number and back
 */








/*
 * Runtime evaluation of get_order()
 */
static inline __attribute__((no_instrument_function)) __attribute__((__const__))
int Model1___get_order(unsigned long Model1_size)
{
 int Model1_order;

 Model1_size--;
 Model1_size >>= 12;



 Model1_order = Model1_fls64(Model1_size);

 return Model1_order;
}

/**
 * get_order - Determine the allocation order of a memory size
 * @size: The size for which to get the order
 *
 * Determine the allocation order of a particular sized block of memory.  This
 * is on a logarithmic scale, where:
 *
 *	0 -> 2^0 * PAGE_SIZE and below
 *	1 -> 2^1 * PAGE_SIZE to 2^0 * PAGE_SIZE + 1
 *	2 -> 2^2 * PAGE_SIZE to 2^1 * PAGE_SIZE + 1
 *	3 -> 2^3 * PAGE_SIZE to 2^2 * PAGE_SIZE + 1
 *	4 -> 2^4 * PAGE_SIZE to 2^3 * PAGE_SIZE + 1
 *	...
 *
 * The order returned is used to find the smallest allocation granule required
 * to hold an object of the specified size.
 *
 * The result is undefined if the size is 0.
 *
 * This function may be used to initialise variables with compile time
 * evaluations of constants.
 */



/*
 * TOP_OF_KERNEL_STACK_PADDING is a number of unused bytes that we
 * reserve at the top of the kernel stack.  We do it because of a nasty
 * 32-bit corner case.  On x86_32, the hardware stack frame is
 * variable-length.  Except for vm86 mode, struct pt_regs assumes a
 * maximum-length frame.  If we enter from CPL 0, the top 8 bytes of
 * pt_regs don't actually exist.  Ordinarily this doesn't matter, but it
 * does in at least one case:
 *
 * If we take an NMI early enough in SYSENTER, then we can end up with
 * pt_regs that extends above sp0.  On the way out, in the espfix code,
 * we can read the saved SS value, but that value will be above sp0.
 * Without this offset, that can result in a page fault.  (We are
 * careful that, in this case, the value we read doesn't matter.)
 *
 * In vm86 mode, the hardware frame is much longer still, so add 16
 * bytes to make room for the real-mode segments.
 *
 * x86_64 has a fixed-length stack frame.
 */
/*
 * low level task data that entry.S needs immediate access to
 * - this struct should fit entirely inside of one cache line
 * - this struct shares the supervisor stack pages
 */

struct Model1_task_struct;












/* Various flags defined: can be included from assembler. */



/*
 * EFLAGS bits
 */
/*
 * Basic CPU control in CR0
 */
/*
 * Paging options in CR3
 */






/*
 * Intel CPU features in CR4
 */
/*
 * x86-64 Task Priority Register, CR8
 */


/*
 * AMD and Transmeta use MSRs for configuration; see <asm/msr-index.h>
 */

/*
 *      NSC/Cyrix CPU configuration register indexes
 */

/* Forward declaration, a strange C thing */
struct Model1_task_struct;
struct Model1_mm_struct;
struct Model1_vm86;














/*
 * Constructor for a conventional segment GDT (or LDT) entry.
 * This is a macro so it can be used in initializers.
 */







/* Simple and small GDT entries for booting only: */
/*
 * Bottom two bits of selector give the ring
 * privilege level
 */


/* User mode is privilege level 3: */


/* Bit 2 is Table Indicator (TI): selects between LDT or GDT */

/* LDT segment has TI set ... */

/* ... GDT has it cleared */
/*
 * We cannot use the same code segment descriptor for user and kernel mode,
 * not even in long flat mode, because of different DPL.
 *
 * GDT layout to get 64-bit SYSCALL/SYSRET support right. SYSRET hardcodes
 * selectors:
 *
 *   if returning to 32-bit userspace: cs = STAR.SYSRET_CS,
 *   if returning to 64-bit userspace: cs = STAR.SYSRET_CS+16,
 *
 * ss = STAR.SYSRET_CS+8 (in either case)
 *
 * thus USER_DS should be between 32-bit and 64-bit code selectors:
 */




/* Needs two entries */

/* Needs two entries */





/* Abused to load per CPU data from limit */


/*
 * Number of entries in the GDT table:
 */


/*
 * Segment selector values corresponding to the above entries:
 *
 * Note, selectors also need to have a correct RPL,
 * expressed with the +3 value for user-space selectors:
 */
/* Bitmask of exception vectors which push an error code on the stack: */
/*
 * early_idt_handler_array is an array of entry points referenced in the
 * early IDT.  For simplicity, it's a real array with one entry point
 * every nine bytes.  That leaves room for an optional 'push $0' if the
 * vector has no error code (two bytes), a 'push $vector_number' (two
 * bytes), and a jump to the common entry code (up to five bytes).
 */




extern const char Model1_early_idt_handler_array[32][9];




/*
 * Load a segment. Fall back on loading the zero segment if something goes
 * wrong.  This variant assumes that loading zero fully clears the segment.
 * This is always the case on Intel CPUs and, even on 64-bit AMD CPUs, any
 * failure to fully clear the cached descriptor is only observable for
 * FS and GS.
 */
static inline __attribute__((no_instrument_function)) void Model1___loadsegment_fs(unsigned short Model1_value)
{
 asm volatile("						\n"
       "1:	movw %0, %%fs			\n"
       "2:					\n"

       " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "2b" ") - .\n" " .long (" "ex_handler_clear_fs" ") - .\n" " .popsection\n"

       : : "rm" (Model1_value) : "memory");
}

/* __loadsegment_gs is intentionally undefined.  Use load_gs_index instead. */





/*
 * Save a segment register away:
 */



/*
 * x86-32 user GS accessors:
 */





/* top of stack page */




/* Arbitrarily choose the same ptrace numbers as used by the Sparc code. */
/* only useful for access 32bit programs / kernels */
struct Model1_pt_regs {
/*
 * C ABI says these regs are callee-preserved. They aren't saved on kernel entry
 * unless syscall needs a complete, fully filled "struct pt_regs".
 */
 unsigned long Model1_r15;
 unsigned long Model1_r14;
 unsigned long Model1_r13;
 unsigned long Model1_r12;
 unsigned long Model1_bp;
 unsigned long Model1_bx;
/* These regs are callee-clobbered. Always saved on kernel entry. */
 unsigned long Model1_r11;
 unsigned long Model1_r10;
 unsigned long Model1_r9;
 unsigned long Model1_r8;
 unsigned long Model1_ax;
 unsigned long Model1_cx;
 unsigned long Model1_dx;
 unsigned long Model1_si;
 unsigned long Model1_di;
/*
 * On syscall entry, this is syscall#. On CPU exception, this is error code.
 * On hw interrupt, it's IRQ number:
 */
 unsigned long Model1_orig_ax;
/* Return frame for iretq */
 unsigned long Model1_ip;
 unsigned long Model1_cs;
 unsigned long Model1_flags;
 unsigned long Model1_sp;
 unsigned long Model1_ss;
/* top of stack page */
};







struct Model1_cpuinfo_x86;
struct Model1_task_struct;

extern unsigned long Model1_profile_pc(struct Model1_pt_regs *Model1_regs);


extern unsigned long
Model1_convert_ip_to_linear(struct Model1_task_struct *Model1_child, struct Model1_pt_regs *Model1_regs);
extern void Model1_send_sigtrap(struct Model1_task_struct *Model1_tsk, struct Model1_pt_regs *Model1_regs,
    int Model1_error_code, int Model1_si_code);


static inline __attribute__((no_instrument_function)) unsigned long Model1_regs_return_value(struct Model1_pt_regs *Model1_regs)
{
 return Model1_regs->Model1_ax;
}

/*
 * user_mode(regs) determines whether a register set came from user
 * mode.  On x86_32, this is true if V8086 mode was enabled OR if the
 * register set was from protected mode with RPL-3 CS value.  This
 * tricky test checks that with one comparison.
 *
 * On x86_64, vm86 mode is mercifully nonexistent, and we don't need
 * the extra check.
 */
static inline __attribute__((no_instrument_function)) int Model1_user_mode(struct Model1_pt_regs *Model1_regs)
{



 return !!(Model1_regs->Model1_cs & 3);

}

static inline __attribute__((no_instrument_function)) int Model1_v8086_mode(struct Model1_pt_regs *Model1_regs)
{



 return 0; /* No V86 mode support in long mode */

}


static inline __attribute__((no_instrument_function)) bool Model1_user_64bit_mode(struct Model1_pt_regs *Model1_regs)
{

 /*
	 * On non-paravirt systems, this is the only long mode CPL 3
	 * selector.  We do not allow long mode selectors in the LDT.
	 */
 return Model1_regs->Model1_cs == (6*8 + 3);




}
static inline __attribute__((no_instrument_function)) unsigned long Model1_kernel_stack_pointer(struct Model1_pt_regs *Model1_regs)
{
 return Model1_regs->Model1_sp;
}







/*
 * Common low level (register) ptrace helpers
 *
 * Copyright 2004-2011 Analog Devices Inc.
 *
 * Licensed under the GPL-2 or later.
 */






/* Helpers for working with the instruction pointer */







static inline __attribute__((no_instrument_function)) unsigned long Model1_instruction_pointer(struct Model1_pt_regs *Model1_regs)
{
 return ((Model1_regs)->Model1_ip);
}
static inline __attribute__((no_instrument_function)) void Model1_instruction_pointer_set(struct Model1_pt_regs *Model1_regs,
                                           unsigned long Model1_val)
{
 (((Model1_regs)->Model1_ip) = (Model1_val));
}





/* Helpers for working with the user stack pointer */







static inline __attribute__((no_instrument_function)) unsigned long Model1_user_stack_pointer(struct Model1_pt_regs *Model1_regs)
{
 return ((Model1_regs)->Model1_sp);
}
static inline __attribute__((no_instrument_function)) void Model1_user_stack_pointer_set(struct Model1_pt_regs *Model1_regs,
                                          unsigned long Model1_val)
{
 (((Model1_regs)->Model1_sp) = (Model1_val));
}

/* Helpers for working with the frame pointer */







static inline __attribute__((no_instrument_function)) unsigned long Model1_frame_pointer(struct Model1_pt_regs *Model1_regs)
{
 return ((Model1_regs)->Model1_bp);
}
static inline __attribute__((no_instrument_function)) void Model1_frame_pointer_set(struct Model1_pt_regs *Model1_regs,
                                     unsigned long Model1_val)
{
 (((Model1_regs)->Model1_bp) = (Model1_val));
}

/* Query offset/name of register from its name/offset */
extern int Model1_regs_query_register_offset(const char *Model1_name);
extern const char *Model1_regs_query_register_name(unsigned int Model1_offset);


/**
 * regs_get_register() - get register value from its offset
 * @regs:	pt_regs from which register value is gotten.
 * @offset:	offset number of the register.
 *
 * regs_get_register returns the value of a register. The @offset is the
 * offset of the register in struct pt_regs address which specified by @regs.
 * If @offset is bigger than MAX_REG_OFFSET, this returns 0.
 */
static inline __attribute__((no_instrument_function)) unsigned long Model1_regs_get_register(struct Model1_pt_regs *Model1_regs,
           unsigned int Model1_offset)
{
 if (__builtin_expect(!!(Model1_offset > (__builtin_offsetof(struct Model1_pt_regs, Model1_ss))), 0))
  return 0;
 return *(unsigned long *)((unsigned long)Model1_regs + Model1_offset);
}

/**
 * regs_within_kernel_stack() - check the address in the stack
 * @regs:	pt_regs which contains kernel stack pointer.
 * @addr:	address which is checked.
 *
 * regs_within_kernel_stack() checks @addr is within the kernel stack page(s).
 * If @addr is within the kernel stack, it returns true. If not, returns false.
 */
static inline __attribute__((no_instrument_function)) int Model1_regs_within_kernel_stack(struct Model1_pt_regs *Model1_regs,
        unsigned long Model1_addr)
{
 return ((Model1_addr & ~((((1UL) << 12) << (2 + 0)) - 1)) ==
  (Model1_kernel_stack_pointer(Model1_regs) & ~((((1UL) << 12) << (2 + 0)) - 1)));
}

/**
 * regs_get_kernel_stack_nth() - get Nth entry of the stack
 * @regs:	pt_regs which contains kernel stack pointer.
 * @n:		stack entry number.
 *
 * regs_get_kernel_stack_nth() returns @n th entry of the kernel stack which
 * is specified by @regs. If the @n th entry is NOT in the kernel stack,
 * this returns 0.
 */
static inline __attribute__((no_instrument_function)) unsigned long Model1_regs_get_kernel_stack_nth(struct Model1_pt_regs *Model1_regs,
            unsigned int Model1_n)
{
 unsigned long *Model1_addr = (unsigned long *)Model1_kernel_stack_pointer(Model1_regs);
 Model1_addr += Model1_n;
 if (Model1_regs_within_kernel_stack(Model1_regs, (unsigned long)Model1_addr))
  return *Model1_addr;
 else
  return 0;
}
/*
 * When hitting ptrace_stop(), we cannot return using SYSRET because
 * that does not restore the full CPU state, only a minimal set.  The
 * ptracer can change arbitrary register values, which is usually okay
 * because the usual ptrace stops run off the signal delivery path which
 * forces IRET; however, ptrace_event() stops happen in arbitrary places
 * in the kernel and don't force IRET path.
 *
 * So force IRET path after a ptrace stop.
 */






struct Model1_user_desc;
extern int Model1_do_get_thread_area(struct Model1_task_struct *Model1_p, int Model1_idx,
         struct Model1_user_desc *Model1_info);
extern int Model1_do_set_thread_area(struct Model1_task_struct *Model1_p, int Model1_idx,
         struct Model1_user_desc *Model1_info, int Model1_can_allocate);

/* This structure matches the layout of the data saved to the stack
   following a device-not-present interrupt, part of it saved
   automatically by the 80386/80486.
   */
struct Model1_math_emu_info {
 long Model1____orig_eip;
 struct Model1_pt_regs *Model1_regs;
};





/*
 * Linux signal context definitions. The sigcontext includes a complex
 * hierarchy of CPU and FPU state, available to user-space (on the stack) when
 * a signal handler is executed.
 *
 * As over the years this ABI grew from its very simple roots towards
 * supporting more and more CPU state organically, some of the details (which
 * were rather clever hacks back in the days) became a bit quirky by today.
 *
 * The current ABI includes flexible provisions for future extensions, so we
 * won't have to grow new quirks for quite some time. Promise!
 */
/*
 * Bytes 464..511 in the current 512-byte layout of the FXSAVE/FXRSTOR frame
 * are reserved for SW usage. On CPUs supporting XSAVE/XRSTOR, these bytes are
 * used to extend the fpstate pointer in the sigcontext, which now includes the
 * extended state information along with fpstate information.
 *
 * If sw_reserved.magic1 == FP_XSTATE_MAGIC1 then there's a
 * sw_reserved.extended_size bytes large extended context area present. (The
 * last 32-bit word of this extended area (at the
 * fpstate+extended_size-FP_XSTATE_MAGIC2_SIZE address) is set to
 * FP_XSTATE_MAGIC2 so that you can sanity check your size calculations.)
 *
 * This extended area typically grows with newer CPUs that have larger and
 * larger XSAVE areas.
 */
struct Model1__fpx_sw_bytes {
 /*
	 * If set to FP_XSTATE_MAGIC1 then this is an xstate context.
	 * 0 if a legacy frame.
	 */
 __u32 Model1_magic1;

 /*
	 * Total size of the fpstate area:
	 *
	 *  - if magic1 == 0 then it's sizeof(struct _fpstate)
	 *  - if magic1 == FP_XSTATE_MAGIC1 then it's sizeof(struct _xstate)
	 *    plus extensions (if any)
	 */
 __u32 Model1_extended_size;

 /*
	 * Feature bit mask (including FP/SSE/extended state) that is present
	 * in the memory layout:
	 */
 __u64 Model1_xfeatures;

 /*
	 * Actual XSAVE state size, based on the xfeatures saved in the layout.
	 * 'extended_size' is greater than 'xstate_size':
	 */
 __u32 Model1_xstate_size;

 /* For future use: */
 __u32 Model1_padding[7];
};

/*
 * As documented in the iBCS2 standard:
 *
 * The first part of "struct _fpstate" is just the normal i387 hardware setup,
 * the extra "status" word is used to save the coprocessor status word before
 * entering the handler.
 *
 * The FPU state data structure has had to grow to accommodate the extended FPU
 * state required by the Streaming SIMD Extensions.  There is no documented
 * standard to accomplish this at the moment.
 */

/* 10-byte legacy floating point register: */
struct Model1__fpreg {
 Model1___u16 Model1_significand[4];
 Model1___u16 Model1_exponent;
};

/* 16-byte floating point register: */
struct Model1__fpxreg {
 Model1___u16 Model1_significand[4];
 Model1___u16 Model1_exponent;
 Model1___u16 Model1_padding[3];
};

/* 16-byte XMM register: */
struct Model1__xmmreg {
 __u32 Model1_element[4];
};



/*
 * The 32-bit FPU frame:
 */
struct Model1__fpstate_32 {
 /* Legacy FPU environment: */
 __u32 Model1_cw;
 __u32 Model1_sw;
 __u32 Model1_tag;
 __u32 Model1_ipoff;
 __u32 Model1_cssel;
 __u32 Model1_dataoff;
 __u32 Model1_datasel;
 struct Model1__fpreg Model1__st[8];
 Model1___u16 Model1_status;
 Model1___u16 Model1_magic; /* 0xffff: regular FPU data only */
       /* 0x0000: FXSR FPU data */

 /* FXSR FPU environment */
 __u32 Model1__fxsr_env[6]; /* FXSR FPU env is ignored */
 __u32 Model1_mxcsr;
 __u32 Model1_reserved;
 struct Model1__fpxreg Model1__fxsr_st[8]; /* FXSR FPU reg data is ignored */
 struct Model1__xmmreg Model1__xmm[8]; /* First 8 XMM registers */
 union {
  __u32 Model1_padding1[44]; /* Second 8 XMM registers plus padding */
  __u32 Model1_padding[44]; /* Alias name for old user-space */
 };

 union {
  __u32 Model1_padding2[12];
  struct Model1__fpx_sw_bytes Model1_sw_reserved; /* Potential extended state is encoded here */
 };
};

/*
 * The 64-bit FPU frame. (FXSAVE format and later)
 *
 * Note1: If sw_reserved.magic1 == FP_XSTATE_MAGIC1 then the structure is
 *        larger: 'struct _xstate'. Note that 'struct _xstate' embedds
 *        'struct _fpstate' so that you can always assume the _fpstate portion
 *        exists so that you can check the magic value.
 *
 * Note2: Reserved fields may someday contain valuable data. Always
 *	  save/restore them when you change signal frames.
 */
struct Model1__fpstate_64 {
 Model1___u16 Model1_cwd;
 Model1___u16 Model1_swd;
 /* Note this is not the same as the 32-bit/x87/FSAVE twd: */
 Model1___u16 Model1_twd;
 Model1___u16 Model1_fop;
 __u64 Model1_rip;
 __u64 Model1_rdp;
 __u32 Model1_mxcsr;
 __u32 Model1_mxcsr_mask;
 __u32 Model1_st_space[32]; /*  8x  FP registers, 16 bytes each */
 __u32 Model1_xmm_space[64]; /* 16x XMM registers, 16 bytes each */
 __u32 Model1_reserved2[12];
 union {
  __u32 Model1_reserved3[12];
  struct Model1__fpx_sw_bytes Model1_sw_reserved; /* Potential extended state is encoded here */
 };
};







struct Model1__header {
 __u64 Model1_xfeatures;
 __u64 Model1_reserved1[2];
 __u64 Model1_reserved2[5];
};

struct Model1__ymmh_state {
 /* 16x YMM registers, 16 bytes each: */
 __u32 Model1_ymmh_space[64];
};

/*
 * Extended state pointed to by sigcontext::fpstate.
 *
 * In addition to the fpstate, information encoded in _xstate::xstate_hdr
 * indicates the presence of other extended state information supported
 * by the CPU and kernel:
 */
struct Model1__xstate {
 struct Model1__fpstate_64 Model1_fpstate;
 struct Model1__header Model1_xstate_hdr;
 struct Model1__ymmh_state Model1_ymmh;
 /* New processor state extensions go here: */
};

/*
 * The 32-bit signal frame:
 */
struct Model1_sigcontext_32 {
 Model1___u16 Model1_gs, Model1___gsh;
 Model1___u16 Model1_fs, Model1___fsh;
 Model1___u16 Model1_es, Model1___esh;
 Model1___u16 Model1_ds, Model1___dsh;
 __u32 Model1_di;
 __u32 Model1_si;
 __u32 Model1_bp;
 __u32 Model1_sp;
 __u32 Model1_bx;
 __u32 Model1_dx;
 __u32 Model1_cx;
 __u32 Model1_ax;
 __u32 Model1_trapno;
 __u32 err;
 __u32 Model1_ip;
 Model1___u16 Model1_cs, Model1___csh;
 __u32 Model1_flags;
 __u32 Model1_sp_at_signal;
 Model1___u16 Model1_ss, Model1___ssh;

 /*
	 * fpstate is really (struct _fpstate *) or (struct _xstate *)
	 * depending on the FP_XSTATE_MAGIC1 encoded in the SW reserved
	 * bytes of (struct _fpstate) and FP_XSTATE_MAGIC2 present at the end
	 * of extended memory layout. See comments at the definition of
	 * (struct _fpx_sw_bytes)
	 */
 __u32 Model1_fpstate; /* Zero when no FPU/extended context */
 __u32 Model1_oldmask;
 __u32 Model1_cr2;
};

/*
 * The 64-bit signal frame:
 */
struct Model1_sigcontext_64 {
 __u64 Model1_r8;
 __u64 Model1_r9;
 __u64 Model1_r10;
 __u64 Model1_r11;
 __u64 Model1_r12;
 __u64 Model1_r13;
 __u64 Model1_r14;
 __u64 Model1_r15;
 __u64 Model1_di;
 __u64 Model1_si;
 __u64 Model1_bp;
 __u64 Model1_bx;
 __u64 Model1_dx;
 __u64 Model1_ax;
 __u64 Model1_cx;
 __u64 Model1_sp;
 __u64 Model1_ip;
 __u64 Model1_flags;
 Model1___u16 Model1_cs;
 Model1___u16 Model1_gs;
 Model1___u16 Model1_fs;
 Model1___u16 Model1_ss;
 __u64 err;
 __u64 Model1_trapno;
 __u64 Model1_oldmask;
 __u64 Model1_cr2;

 /*
	 * fpstate is really (struct _fpstate *) or (struct _xstate *)
	 * depending on the FP_XSTATE_MAGIC1 encoded in the SW reserved
	 * bytes of (struct _fpstate) and FP_XSTATE_MAGIC2 present at the end
	 * of extended memory layout. See comments at the definition of
	 * (struct _fpx_sw_bytes)
	 */
 __u64 Model1_fpstate; /* Zero when no FPU/extended context */
 __u64 Model1_reserved1[8];
};

/*
 * Create the real 'struct sigcontext' type:
 */
/*
 * The old user-space sigcontext definition, just in case user-space still
 * relies on it. The kernel definition (in asm/sigcontext.h) has unified
 * field names but otherwise the same layout.
 */







struct Model1_task_struct;

extern __attribute__((section(".data..percpu" ""))) __typeof__(struct Model1_task_struct *) Model1_current_task;

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) struct Model1_task_struct *Model1_get_current(void)
{
 return ({ typeof(Model1_current_task) Model1_pfo_ret__; switch (sizeof(Model1_current_task)) { case 1: asm("mov" "b ""%%""gs"":" "%" "P1"",%0" : "=q" (Model1_pfo_ret__) : "p" (&(Model1_current_task))); break; case 2: asm("mov" "w ""%%""gs"":" "%" "P1"",%0" : "=r" (Model1_pfo_ret__) : "p" (&(Model1_current_task))); break; case 4: asm("mov" "l ""%%""gs"":" "%" "P1"",%0" : "=r" (Model1_pfo_ret__) : "p" (&(Model1_current_task))); break; case 8: asm("mov" "q ""%%""gs"":" "%" "P1"",%0" : "=r" (Model1_pfo_ret__) : "p" (&(Model1_current_task))); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; });
}


/* If _PAGE_BIT_PRESENT is clear, we use these: */
/* - if the user mapped it with PROT_NONE; pte_present gives true */
/*
 * The same hidden bit is used by kmemcheck, but since kmemcheck
 * works on kernel pages while soft-dirty engine on user space,
 * they do not conflict with each other.
 */







/*
 * Tracking soft dirty bit when a page goes to a swap is tricky.
 * We need a bit which can be stored in pte _and_ not conflict
 * with swap entry format. On x86 bits 6 and 7 are *not* involved
 * into swap entry computation, but bit 6 is used for nonlinear
 * file mapping, so we borrow bit 7 for soft dirty tracking.
 *
 * Please note that this bit must be treated as swap dirty page
 * mark if and only if the PTE has present bit clear!
 */
/*
 * Set of bits not changed in pte_modify.  The pte's
 * protection key is treated like _PAGE_RW, for
 * instance, and is *not* included in this mask since
 * pte_modify() does modify it.
 */





/*
 * The cache modes defined here are used to translate between pure SW usage
 * and the HW defined cache mode bits and/or PAT entries.
 *
 * The resulting bits for PWT, PCD and PAT should be chosen in a way
 * to have the WB mode at index 0 (all bits clear). This is the default
 * right now and likely would break too much if changed.
 */

enum Model1_page_cache_mode {
 Model1__PAGE_CACHE_MODE_WB = 0,
 Model1__PAGE_CACHE_MODE_WC = 1,
 Model1__PAGE_CACHE_MODE_UC_MINUS = 2,
 Model1__PAGE_CACHE_MODE_UC = 3,
 Model1__PAGE_CACHE_MODE_WT = 4,
 Model1__PAGE_CACHE_MODE_WP = 5,
 Model1__PAGE_CACHE_MODE_NUM = 8
};
/*         xwr */
/*
 * early identity mapping  pte attrib macros.
 */







/*
 * generic non-linear memory support:
 *
 * 1) we will not split memory into more chunks than will fit into the flags
 *    field of the struct page
 *
 * SECTION_SIZE_BITS		2^n: size of each section
 * MAX_PHYSADDR_BITS		2^n: max size of physical address space
 * MAX_PHYSMEM_BITS		2^n: how much memory we can have in that space
 *
 */





/*
 * These are used to make use of C type-checking..
 */
typedef unsigned long Model1_pteval_t;
typedef unsigned long Model1_pmdval_t;
typedef unsigned long Model1_pudval_t;
typedef unsigned long Model1_pgdval_t;
typedef unsigned long Model1_pgprotval_t;

typedef struct { Model1_pteval_t Model1_pte; } Model1_pte_t;





/*
 * PGDIR_SHIFT determines what a top-level page table entry can map
 */



/*
 * 3rd level page
 */



/*
 * PMD_SHIFT determines the size of the area a middle-level
 * page table can map
 */



/*
 * entries per page directory level
 */
/* See Documentation/x86/x86_64/mm.txt for a description of the memory map. */






/* Extracts the PFN from a (pte|pmd|pud|pgd)val_t of a 4KB page */


/*
 *  Extracts the flags from a (pte|pmd|pud|pgd)val_t
 *  This includes the protection key value.
 */


typedef struct Model1_pgprot { Model1_pgprotval_t Model1_pgprot; } Model1_pgprot_t;

typedef struct { Model1_pgdval_t Model1_pgd; } Model1_pgd_t;

static inline __attribute__((no_instrument_function)) Model1_pgd_t Model1_native_make_pgd(Model1_pgdval_t Model1_val)
{
 return (Model1_pgd_t) { Model1_val };
}

static inline __attribute__((no_instrument_function)) Model1_pgdval_t Model1_native_pgd_val(Model1_pgd_t Model1_pgd)
{
 return Model1_pgd.Model1_pgd;
}

static inline __attribute__((no_instrument_function)) Model1_pgdval_t Model1_pgd_flags(Model1_pgd_t Model1_pgd)
{
 return Model1_native_pgd_val(Model1_pgd) & (~((Model1_pteval_t)(((signed long)(~(((1UL) << 12)-1))) & ((Model1_phys_addr_t)((1ULL << 46) - 1)))));
}


typedef struct { Model1_pudval_t Model1_pud; } Model1_pud_t;

static inline __attribute__((no_instrument_function)) Model1_pud_t Model1_native_make_pud(Model1_pmdval_t Model1_val)
{
 return (Model1_pud_t) { Model1_val };
}

static inline __attribute__((no_instrument_function)) Model1_pudval_t Model1_native_pud_val(Model1_pud_t Model1_pud)
{
 return Model1_pud.Model1_pud;
}
typedef struct { Model1_pmdval_t Model1_pmd; } Model1_pmd_t;

static inline __attribute__((no_instrument_function)) Model1_pmd_t Model1_native_make_pmd(Model1_pmdval_t Model1_val)
{
 return (Model1_pmd_t) { Model1_val };
}

static inline __attribute__((no_instrument_function)) Model1_pmdval_t Model1_native_pmd_val(Model1_pmd_t Model1_pmd)
{
 return Model1_pmd.Model1_pmd;
}
static inline __attribute__((no_instrument_function)) Model1_pudval_t Model1_pud_pfn_mask(Model1_pud_t Model1_pud)
{
 if (Model1_native_pud_val(Model1_pud) & (((Model1_pteval_t)(1)) << 7))
  return (((signed long)(~(((1UL) << 30)-1))) & ((Model1_phys_addr_t)((1ULL << 46) - 1)));
 else
  return ((Model1_pteval_t)(((signed long)(~(((1UL) << 12)-1))) & ((Model1_phys_addr_t)((1ULL << 46) - 1))));
}

static inline __attribute__((no_instrument_function)) Model1_pudval_t Model1_pud_flags_mask(Model1_pud_t Model1_pud)
{
 return ~Model1_pud_pfn_mask(Model1_pud);
}

static inline __attribute__((no_instrument_function)) Model1_pudval_t Model1_pud_flags(Model1_pud_t Model1_pud)
{
 return Model1_native_pud_val(Model1_pud) & Model1_pud_flags_mask(Model1_pud);
}

static inline __attribute__((no_instrument_function)) Model1_pmdval_t Model1_pmd_pfn_mask(Model1_pmd_t Model1_pmd)
{
 if (Model1_native_pmd_val(Model1_pmd) & (((Model1_pteval_t)(1)) << 7))
  return (((signed long)(~(((1UL) << 21)-1))) & ((Model1_phys_addr_t)((1ULL << 46) - 1)));
 else
  return ((Model1_pteval_t)(((signed long)(~(((1UL) << 12)-1))) & ((Model1_phys_addr_t)((1ULL << 46) - 1))));
}

static inline __attribute__((no_instrument_function)) Model1_pmdval_t Model1_pmd_flags_mask(Model1_pmd_t Model1_pmd)
{
 return ~Model1_pmd_pfn_mask(Model1_pmd);
}

static inline __attribute__((no_instrument_function)) Model1_pmdval_t Model1_pmd_flags(Model1_pmd_t Model1_pmd)
{
 return Model1_native_pmd_val(Model1_pmd) & Model1_pmd_flags_mask(Model1_pmd);
}

static inline __attribute__((no_instrument_function)) Model1_pte_t Model1_native_make_pte(Model1_pteval_t Model1_val)
{
 return (Model1_pte_t) { .Model1_pte = Model1_val };
}

static inline __attribute__((no_instrument_function)) Model1_pteval_t Model1_native_pte_val(Model1_pte_t Model1_pte)
{
 return Model1_pte.Model1_pte;
}

static inline __attribute__((no_instrument_function)) Model1_pteval_t Model1_pte_flags(Model1_pte_t Model1_pte)
{
 return Model1_native_pte_val(Model1_pte) & (~((Model1_pteval_t)(((signed long)(~(((1UL) << 12)-1))) & ((Model1_phys_addr_t)((1ULL << 46) - 1)))));
}




extern Model1_uint16_t Model1___cachemode2pte_tbl[Model1__PAGE_CACHE_MODE_NUM];
extern Model1_uint8_t Model1___pte2cachemode_tbl[8];
static inline __attribute__((no_instrument_function)) unsigned long Model1_cachemode2protval(enum Model1_page_cache_mode Model1_pcm)
{
 if (__builtin_expect(!!(Model1_pcm == 0), 1))
  return 0;
 return Model1___cachemode2pte_tbl[Model1_pcm];
}
static inline __attribute__((no_instrument_function)) Model1_pgprot_t Model1_cachemode2pgprot(enum Model1_page_cache_mode Model1_pcm)
{
 return ((Model1_pgprot_t) { (Model1_cachemode2protval(Model1_pcm)) } );
}
static inline __attribute__((no_instrument_function)) enum Model1_page_cache_mode Model1_pgprot2cachemode(Model1_pgprot_t Model1_pgprot)
{
 unsigned long Model1_masked;

 Model1_masked = ((Model1_pgprot).Model1_pgprot) & ((((Model1_pteval_t)(1)) << 7) | (((Model1_pteval_t)(1)) << 4) | (((Model1_pteval_t)(1)) << 3));
 if (__builtin_expect(!!(Model1_masked == 0), 1))
  return 0;
 return Model1___pte2cachemode_tbl[((((Model1_masked) >> (7 - 2)) & 4) | (((Model1_masked) >> (4 - 1)) & 2) | (((Model1_masked) >> 3) & 1))];
}
static inline __attribute__((no_instrument_function)) Model1_pgprot_t Model1_pgprot_4k_2_large(Model1_pgprot_t Model1_pgprot)
{
 Model1_pgprotval_t Model1_val = ((Model1_pgprot).Model1_pgprot);
 Model1_pgprot_t Model1_new;

 ((Model1_new).Model1_pgprot) = (Model1_val & ~((((Model1_pteval_t)(1)) << 7) | (((Model1_pteval_t)(1)) << 12))) |
  ((Model1_val & (((Model1_pteval_t)(1)) << 7)) << (12 - 7));
 return Model1_new;
}
static inline __attribute__((no_instrument_function)) Model1_pgprot_t Model1_pgprot_large_2_4k(Model1_pgprot_t Model1_pgprot)
{
 Model1_pgprotval_t Model1_val = ((Model1_pgprot).Model1_pgprot);
 Model1_pgprot_t Model1_new;

 ((Model1_new).Model1_pgprot) = (Model1_val & ~((((Model1_pteval_t)(1)) << 7) | (((Model1_pteval_t)(1)) << 12))) |
     ((Model1_val & (((Model1_pteval_t)(1)) << 12)) >>
      (12 - 7));
 return Model1_new;
}


typedef struct Model1_page *Model1_pgtable_t;

extern Model1_pteval_t Model1___supported_pte_mask;
extern void Model1_set_nx(void);
extern int Model1_nx_enabled;


extern Model1_pgprot_t Model1_pgprot_writecombine(Model1_pgprot_t Model1_prot);


extern Model1_pgprot_t Model1_pgprot_writethrough(Model1_pgprot_t Model1_prot);

/* Indicate that x86 has its own track and untrack pfn vma functions */



struct Model1_file;
Model1_pgprot_t Model1_phys_mem_access_prot(struct Model1_file *Model1_file, unsigned long Model1_pfn,
                              unsigned long Model1_size, Model1_pgprot_t Model1_vma_prot);
int Model1_phys_mem_access_prot_allowed(struct Model1_file *Model1_file, unsigned long Model1_pfn,
                              unsigned long Model1_size, Model1_pgprot_t *Model1_vma_prot);

/* Install a pte for a particular vaddr in kernel space. */
void Model1_set_pte_vaddr(unsigned long Model1_vaddr, Model1_pte_t Model1_pte);







struct Model1_seq_file;
extern void Model1_arch_report_meminfo(struct Model1_seq_file *Model1_m);

enum Model1_pg_level {
 Model1_PG_LEVEL_NONE,
 Model1_PG_LEVEL_4K,
 Model1_PG_LEVEL_2M,
 Model1_PG_LEVEL_1G,
 Model1_PG_LEVEL_NUM
};


extern void Model1_update_page_count(int Model1_level, unsigned long Model1_pages);




/*
 * Helper function that returns the kernel pagetable entry controlling
 * the virtual address 'address'. NULL means no pagetable entry present.
 * NOTE: the return type is pte_t but if the pmd is PSE then we return it
 * as a pte too.
 */
extern Model1_pte_t *Model1_lookup_address(unsigned long Model1_address, unsigned int *Model1_level);
extern Model1_pte_t *Model1_lookup_address_in_pgd(Model1_pgd_t *Model1_pgd, unsigned long Model1_address,
        unsigned int *Model1_level);
extern Model1_pmd_t *Model1_lookup_pmd_address(unsigned long Model1_address);
extern Model1_phys_addr_t Model1_slow_virt_to_phys(void *Model1___address);
extern int Model1_kernel_map_pages_in_pgd(Model1_pgd_t *Model1_pgd, Model1_u64 Model1_pfn, unsigned long Model1_address,
       unsigned Model1_numpages, unsigned long Model1_page_flags);







/*
 * CPU model specific register (MSR) numbers.
 *
 * Do not add new entries to this file unless the definitions are shared
 * between multiple compilation units.
 */

/* x86-64 specific MSRs */
/* EFER bits: */
/* Intel MSRs. Some also available on other CPUs */
/* DEBUGCTLMSR bits (others vary by model): */
/* C-state Residency Counters */
/* Interrupt Response Limit */







/* Run Time Average Power Limiting (RAPL) Interface */
/* Config TDP MSRs */
/* Hardware P state interface */
/* CPUID.6.EAX */






/* IA32_HWP_CAPABILITIES */





/* IA32_HWP_REQUEST */







/* IA32_HWP_STATUS */



/* IA32_HWP_INTERRUPT */
/* These are consecutive and not in the normal 4er MCE bank block */
/* Alternative perfctr range with full access. */


/* AMD64 MSRs. Not complete. See the architecture manual for a more
   complete list. */
/* Fam 17h MSRs */


/* Fam 16h MSRs */







/* Fam 15h MSRs */







/* Fam 10h MSRs */
/* K8 MSRs */




/* C1E active bits in int pending message */







/* K7 MSRs */
/* K6 MSRs */






/* Centaur-Hauls/IDT defined MSRs. */
/* VIA Cyrix defined MSRs*/





/* Transmeta defined MSRs */





/* Intel defined MSRs. */
/* Thermal Thresholds Support */
/* MISC_ENABLE bits: architectural */
/* MISC_ENABLE bits: model-specific, meaning may vary from core to core */
/* P4/Xeon+ specific */
/* Pentium IV performance counter MSRs */
/* Intel Core-based CPU performance counters */
/* Geode defined MSRs */


/* Intel VT MSRs */
/* VMX_BASIC bits and bitmasks */
/* MSR_IA32_VMX_MISC bits */


/* AMD-V MSRs */










/*
 * Cpumasks provide a bitmap suitable for representing the
 * set of CPU's in a system, one bit position per CPU number.  In general,
 * only nr_cpu_ids (<= NR_CPUS) bits are valid.
 */





















/* We don't want strings.h stuff being used by user stuff by accident */

extern char *Model1_strndup_user(const char *, long);
extern void *Model1_memdup_user(const void *, Model1_size_t);
extern void *Model1_memdup_user_nul(const void *, Model1_size_t);

/*
 * Include machine specific inline routines
 */









/* Written 2002 by Andi Kleen */

/* Only used for special circumstances. Stolen from i386/string.h */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void *Model1___inline_memcpy(void *Model1_to, const void *Model1_from, Model1_size_t Model1_n)
{
 unsigned long Model1_d0, Model1_d1, Model1_d2;
 asm volatile("rep ; movsl\n\t"
       "testb $2,%b4\n\t"
       "je 1f\n\t"
       "movsw\n"
       "1:\ttestb $1,%b4\n\t"
       "je 2f\n\t"
       "movsb\n"
       "2:"
       : "=&c" (Model1_d0), "=&D" (Model1_d1), "=&S" (Model1_d2)
       : "0" (Model1_n / 4), "q" (Model1_n), "1" ((long)Model1_to), "2" ((long)Model1_from)
       : "memory");
 return Model1_to;
}

/* Even with __builtin_ the compiler may decide to use the out of line
   function. */


extern void *memcpy(void *Model1_to, const void *Model1_from, Model1_size_t Model1_len);
extern void *Model1___memcpy(void *Model1_to, const void *Model1_from, Model1_size_t Model1_len);
void *memset(void *Model1_s, int Model1_c, Model1_size_t Model1_n);
void *Model1___memset(void *Model1_s, int Model1_c, Model1_size_t Model1_n);


void *Model1_memmove(void *Model1_dest, const void *Model1_src, Model1_size_t Model1_count);
void *Model1___memmove(void *Model1_dest, const void *Model1_src, Model1_size_t Model1_count);

int Model1_memcmp(const void *Model1_cs, const void *Model1_ct, Model1_size_t Model1_count);
Model1_size_t Model1_strlen(const char *Model1_s);
char *Model1_strcpy(char *Model1_dest, const char *Model1_src);
char *Model1_strcat(char *Model1_dest, const char *Model1_src);
int Model1_strcmp(const char *Model1_cs, const char *Model1_ct);
/**
 * memcpy_mcsafe - copy memory with indication if a machine check happened
 *
 * @dst:	destination address
 * @src:	source address
 * @cnt:	number of bytes to copy
 *
 * Low level memory copy function that catches machine checks
 *
 * Return 0 for success, -EFAULT for fail
 */
int Model1_memcpy_mcsafe(void *Model1_dst, const void *Model1_src, Model1_size_t Model1_cnt);


extern char * Model1_strcpy(char *,const char *);


extern char * Model1_strncpy(char *,const char *, Model1___kernel_size_t);


Model1_size_t Model1_strlcpy(char *, const char *, Model1_size_t);


Model1_ssize_t __attribute__((warn_unused_result)) Model1_strscpy(char *, const char *, Model1_size_t);


extern char * Model1_strcat(char *, const char *);


extern char * Model1_strncat(char *, const char *, Model1___kernel_size_t);


extern Model1_size_t Model1_strlcat(char *, const char *, Model1___kernel_size_t);


extern int Model1_strcmp(const char *,const char *);


extern int Model1_strncmp(const char *,const char *,Model1___kernel_size_t);


extern int Model1_strcasecmp(const char *Model1_s1, const char *Model1_s2);


extern int Model1_strncasecmp(const char *Model1_s1, const char *Model1_s2, Model1_size_t Model1_n);


extern char * Model1_strchr(const char *,int);


extern char * Model1_strchrnul(const char *,int);


extern char * Model1_strnchr(const char *, Model1_size_t, int);


extern char * Model1_strrchr(const char *,int);

extern char * __attribute__((warn_unused_result)) Model1_skip_spaces(const char *);

extern char *Model1_strim(char *);

static inline __attribute__((no_instrument_function)) __attribute__((warn_unused_result)) char *Model1_strstrip(char *Model1_str)
{
 return Model1_strim(Model1_str);
}


extern char * Model1_strstr(const char *, const char *);


extern char * Model1_strnstr(const char *, const char *, Model1_size_t);


extern Model1___kernel_size_t Model1_strlen(const char *);


extern Model1___kernel_size_t Model1_strnlen(const char *,Model1___kernel_size_t);


extern char * Model1_strpbrk(const char *,const char *);


extern char * Model1_strsep(char **,const char *);


extern Model1___kernel_size_t Model1_strspn(const char *,const char *);


extern Model1___kernel_size_t Model1_strcspn(const char *,const char *);
extern void * Model1_memscan(void *,int,Model1___kernel_size_t);


extern int Model1_memcmp(const void *,const void *,Model1___kernel_size_t);


extern void * Model1_memchr(const void *,int,Model1___kernel_size_t);

void *Model1_memchr_inv(const void *Model1_s, int Model1_c, Model1_size_t Model1_n);
char *Model1_strreplace(char *Model1_s, char old, char Model1_new);

extern void Model1_kfree_const(const void *Model1_x);

extern char *Model1_kstrdup(const char *Model1_s, Model1_gfp_t Model1_gfp) __attribute__((__malloc__));
extern const char *Model1_kstrdup_const(const char *Model1_s, Model1_gfp_t Model1_gfp);
extern char *Model1_kstrndup(const char *Model1_s, Model1_size_t Model1_len, Model1_gfp_t Model1_gfp);
extern void *Model1_kmemdup(const void *Model1_src, Model1_size_t Model1_len, Model1_gfp_t Model1_gfp);

extern char **Model1_argv_split(Model1_gfp_t Model1_gfp, const char *Model1_str, int *Model1_argcp);
extern void Model1_argv_free(char **Model1_argv);

extern bool Model1_sysfs_streq(const char *Model1_s1, const char *Model1_s2);
extern int Model1_kstrtobool(const char *Model1_s, bool *Model1_res);
static inline __attribute__((no_instrument_function)) int Model1_strtobool(const char *Model1_s, bool *Model1_res)
{
 return Model1_kstrtobool(Model1_s, Model1_res);
}

int Model1_match_string(const char * const *Model1_array, Model1_size_t Model1_n, const char *Model1_string);


int Model1_vbin_printf(Model1_u32 *Model1_bin_buf, Model1_size_t Model1_size, const char *Model1_fmt, Model1_va_list Model1_args);
int Model1_bstr_printf(char *Model1_buf, Model1_size_t Model1_size, const char *Model1_fmt, const Model1_u32 *Model1_bin_buf);
int Model1_bprintf(Model1_u32 *Model1_bin_buf, Model1_size_t Model1_size, const char *Model1_fmt, ...) __attribute__((format(printf, 3, 4)));


extern Model1_ssize_t Model1_memory_read_from_buffer(void *Model1_to, Model1_size_t Model1_count, Model1_loff_t *Model1_ppos,
           const void *Model1_from, Model1_size_t Model1_available);

/**
 * strstarts - does @str start with @prefix?
 * @str: string to examine
 * @prefix: prefix to look for.
 */
static inline __attribute__((no_instrument_function)) bool Model1_strstarts(const char *Model1_str, const char *Model1_prefix)
{
 return Model1_strncmp(Model1_str, Model1_prefix, Model1_strlen(Model1_prefix)) == 0;
}

Model1_size_t Model1_memweight(const void *Model1_ptr, Model1_size_t Model1_bytes);
void Model1_memzero_explicit(void *Model1_s, Model1_size_t Model1_count);

/**
 * kbasename - return the last part of a pathname.
 *
 * @path: path to extract the filename from.
 */
static inline __attribute__((no_instrument_function)) const char *Model1_kbasename(const char *Model1_path)
{
 const char *Model1_tail = Model1_strrchr(Model1_path, '/');
 return Model1_tail ? Model1_tail + 1 : Model1_path;
}


/*
 * bitmaps provide bit arrays that consume one or more unsigned
 * longs.  The bitmap interface and available operations are listed
 * here, in bitmap.h
 *
 * Function implementations generic to all architectures are in
 * lib/bitmap.c.  Functions implementations that are architecture
 * specific are in various include/asm-<arch>/bitops.h headers
 * and other arch/<arch> specific files.
 *
 * See lib/bitmap.c for more details.
 */

/*
 * The available bitmap operations and their rough meaning in the
 * case that the bitmap is a single unsigned long are thus:
 *
 * Note that nbits should be always a compile time evaluable constant.
 * Otherwise many inlines will generate horrible code.
 *
 * bitmap_zero(dst, nbits)			*dst = 0UL
 * bitmap_fill(dst, nbits)			*dst = ~0UL
 * bitmap_copy(dst, src, nbits)			*dst = *src
 * bitmap_and(dst, src1, src2, nbits)		*dst = *src1 & *src2
 * bitmap_or(dst, src1, src2, nbits)		*dst = *src1 | *src2
 * bitmap_xor(dst, src1, src2, nbits)		*dst = *src1 ^ *src2
 * bitmap_andnot(dst, src1, src2, nbits)	*dst = *src1 & ~(*src2)
 * bitmap_complement(dst, src, nbits)		*dst = ~(*src)
 * bitmap_equal(src1, src2, nbits)		Are *src1 and *src2 equal?
 * bitmap_intersects(src1, src2, nbits) 	Do *src1 and *src2 overlap?
 * bitmap_subset(src1, src2, nbits)		Is *src1 a subset of *src2?
 * bitmap_empty(src, nbits)			Are all bits zero in *src?
 * bitmap_full(src, nbits)			Are all bits set in *src?
 * bitmap_weight(src, nbits)			Hamming Weight: number set bits
 * bitmap_set(dst, pos, nbits)			Set specified bit area
 * bitmap_clear(dst, pos, nbits)		Clear specified bit area
 * bitmap_find_next_zero_area(buf, len, pos, n, mask)	Find bit free area
 * bitmap_find_next_zero_area_off(buf, len, pos, n, mask)	as above
 * bitmap_shift_right(dst, src, n, nbits)	*dst = *src >> n
 * bitmap_shift_left(dst, src, n, nbits)	*dst = *src << n
 * bitmap_remap(dst, src, old, new, nbits)	*dst = map(old, new)(src)
 * bitmap_bitremap(oldbit, old, new, nbits)	newbit = map(old, new)(oldbit)
 * bitmap_onto(dst, orig, relmap, nbits)	*dst = orig relative to relmap
 * bitmap_fold(dst, orig, sz, nbits)		dst bits = orig bits mod sz
 * bitmap_parse(buf, buflen, dst, nbits)	Parse bitmap dst from kernel buf
 * bitmap_parse_user(ubuf, ulen, dst, nbits)	Parse bitmap dst from user buf
 * bitmap_parselist(buf, dst, nbits)		Parse bitmap dst from kernel buf
 * bitmap_parselist_user(buf, dst, nbits)	Parse bitmap dst from user buf
 * bitmap_find_free_region(bitmap, bits, order)	Find and allocate bit region
 * bitmap_release_region(bitmap, pos, order)	Free specified bit region
 * bitmap_allocate_region(bitmap, pos, order)	Allocate specified bit region
 * bitmap_from_u32array(dst, nbits, buf, nwords) *dst = *buf (nwords 32b words)
 * bitmap_to_u32array(buf, nwords, src, nbits)	*buf = *dst (nwords 32b words)
 */

/*
 * Also the following operations in asm/bitops.h apply to bitmaps.
 *
 * set_bit(bit, addr)			*addr |= bit
 * clear_bit(bit, addr)			*addr &= ~bit
 * change_bit(bit, addr)		*addr ^= bit
 * test_bit(bit, addr)			Is bit set in *addr?
 * test_and_set_bit(bit, addr)		Set bit and return old value
 * test_and_clear_bit(bit, addr)	Clear bit and return old value
 * test_and_change_bit(bit, addr)	Change bit and return old value
 * find_first_zero_bit(addr, nbits)	Position first zero bit in *addr
 * find_first_bit(addr, nbits)		Position first set bit in *addr
 * find_next_zero_bit(addr, nbits, bit)	Position next zero bit in *addr >= bit
 * find_next_bit(addr, nbits, bit)	Position next set bit in *addr >= bit
 */

/*
 * The DECLARE_BITMAP(name,bits) macro, in linux/types.h, can be used
 * to declare an array named 'name' of just enough unsigned longs to
 * contain all bit positions from 0 to 'bits' - 1.
 */

/*
 * lib/bitmap.c provides these functions:
 */

extern int Model1___bitmap_empty(const unsigned long *Model1_bitmap, unsigned int Model1_nbits);
extern int Model1___bitmap_full(const unsigned long *Model1_bitmap, unsigned int Model1_nbits);
extern int Model1___bitmap_equal(const unsigned long *Model1_bitmap1,
     const unsigned long *Model1_bitmap2, unsigned int Model1_nbits);
extern void Model1___bitmap_complement(unsigned long *Model1_dst, const unsigned long *Model1_src,
   unsigned int Model1_nbits);
extern void Model1___bitmap_shift_right(unsigned long *Model1_dst, const unsigned long *Model1_src,
    unsigned int Model1_shift, unsigned int Model1_nbits);
extern void Model1___bitmap_shift_left(unsigned long *Model1_dst, const unsigned long *Model1_src,
    unsigned int Model1_shift, unsigned int Model1_nbits);
extern int Model1___bitmap_and(unsigned long *Model1_dst, const unsigned long *Model1_bitmap1,
   const unsigned long *Model1_bitmap2, unsigned int Model1_nbits);
extern void Model1___bitmap_or(unsigned long *Model1_dst, const unsigned long *Model1_bitmap1,
   const unsigned long *Model1_bitmap2, unsigned int Model1_nbits);
extern void Model1___bitmap_xor(unsigned long *Model1_dst, const unsigned long *Model1_bitmap1,
   const unsigned long *Model1_bitmap2, unsigned int Model1_nbits);
extern int Model1___bitmap_andnot(unsigned long *Model1_dst, const unsigned long *Model1_bitmap1,
   const unsigned long *Model1_bitmap2, unsigned int Model1_nbits);
extern int Model1___bitmap_intersects(const unsigned long *Model1_bitmap1,
   const unsigned long *Model1_bitmap2, unsigned int Model1_nbits);
extern int Model1___bitmap_subset(const unsigned long *Model1_bitmap1,
   const unsigned long *Model1_bitmap2, unsigned int Model1_nbits);
extern int Model1___bitmap_weight(const unsigned long *Model1_bitmap, unsigned int Model1_nbits);

extern void Model1_bitmap_set(unsigned long *Model1_map, unsigned int Model1_start, int Model1_len);
extern void Model1_bitmap_clear(unsigned long *Model1_map, unsigned int Model1_start, int Model1_len);

extern unsigned long Model1_bitmap_find_next_zero_area_off(unsigned long *Model1_map,
          unsigned long Model1_size,
          unsigned long Model1_start,
          unsigned int Model1_nr,
          unsigned long Model1_align_mask,
          unsigned long Model1_align_offset);

/**
 * bitmap_find_next_zero_area - find a contiguous aligned zero area
 * @map: The address to base the search on
 * @size: The bitmap size in bits
 * @start: The bitnumber to start searching at
 * @nr: The number of zeroed bits we're looking for
 * @align_mask: Alignment mask for zero area
 *
 * The @align_mask should be one less than a power of 2; the effect is that
 * the bit offset of all zero areas this function finds is multiples of that
 * power of 2. A @align_mask of 0 means no alignment is required.
 */
static inline __attribute__((no_instrument_function)) unsigned long
Model1_bitmap_find_next_zero_area(unsigned long *Model1_map,
      unsigned long Model1_size,
      unsigned long Model1_start,
      unsigned int Model1_nr,
      unsigned long Model1_align_mask)
{
 return Model1_bitmap_find_next_zero_area_off(Model1_map, Model1_size, Model1_start, Model1_nr,
           Model1_align_mask, 0);
}

extern int Model1___bitmap_parse(const char *Model1_buf, unsigned int Model1_buflen, int Model1_is_user,
   unsigned long *Model1_dst, int Model1_nbits);
extern int Model1_bitmap_parse_user(const char *Model1_ubuf, unsigned int Model1_ulen,
   unsigned long *Model1_dst, int Model1_nbits);
extern int Model1_bitmap_parselist(const char *Model1_buf, unsigned long *Model1_maskp,
   int Model1_nmaskbits);
extern int Model1_bitmap_parselist_user(const char *Model1_ubuf, unsigned int Model1_ulen,
   unsigned long *Model1_dst, int Model1_nbits);
extern void Model1_bitmap_remap(unsigned long *Model1_dst, const unsigned long *Model1_src,
  const unsigned long *old, const unsigned long *Model1_new, unsigned int Model1_nbits);
extern int Model1_bitmap_bitremap(int Model1_oldbit,
  const unsigned long *old, const unsigned long *Model1_new, int Model1_bits);
extern void Model1_bitmap_onto(unsigned long *Model1_dst, const unsigned long *Model1_orig,
  const unsigned long *Model1_relmap, unsigned int Model1_bits);
extern void Model1_bitmap_fold(unsigned long *Model1_dst, const unsigned long *Model1_orig,
  unsigned int Model1_sz, unsigned int Model1_nbits);
extern int Model1_bitmap_find_free_region(unsigned long *Model1_bitmap, unsigned int Model1_bits, int Model1_order);
extern void Model1_bitmap_release_region(unsigned long *Model1_bitmap, unsigned int Model1_pos, int Model1_order);
extern int Model1_bitmap_allocate_region(unsigned long *Model1_bitmap, unsigned int Model1_pos, int Model1_order);
extern unsigned int Model1_bitmap_from_u32array(unsigned long *Model1_bitmap,
      unsigned int Model1_nbits,
      const Model1_u32 *Model1_buf,
      unsigned int Model1_nwords);
extern unsigned int Model1_bitmap_to_u32array(Model1_u32 *Model1_buf,
           unsigned int Model1_nwords,
           const unsigned long *Model1_bitmap,
           unsigned int Model1_nbits);





extern unsigned int Model1_bitmap_ord_to_pos(const unsigned long *Model1_bitmap, unsigned int Model1_ord, unsigned int Model1_nbits);
extern int Model1_bitmap_print_to_pagebuf(bool Model1_list, char *Model1_buf,
       const unsigned long *Model1_maskp, int Model1_nmaskbits);







static inline __attribute__((no_instrument_function)) void Model1_bitmap_zero(unsigned long *Model1_dst, unsigned int Model1_nbits)
{
 if ((__builtin_constant_p(Model1_nbits) && (Model1_nbits) <= 64))
  *Model1_dst = 0UL;
 else {
  unsigned int Model1_len = (((Model1_nbits) + (8 * sizeof(long)) - 1) / (8 * sizeof(long))) * sizeof(unsigned long);
  memset(Model1_dst, 0, Model1_len);
 }
}

static inline __attribute__((no_instrument_function)) void Model1_bitmap_fill(unsigned long *Model1_dst, unsigned int Model1_nbits)
{
 unsigned int Model1_nlongs = (((Model1_nbits) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)));
 if (!(__builtin_constant_p(Model1_nbits) && (Model1_nbits) <= 64)) {
  unsigned int Model1_len = (Model1_nlongs - 1) * sizeof(unsigned long);
  memset(Model1_dst, 0xff, Model1_len);
 }
 Model1_dst[Model1_nlongs - 1] = (~0UL >> (-(Model1_nbits) & (64 - 1)));
}

static inline __attribute__((no_instrument_function)) void Model1_bitmap_copy(unsigned long *Model1_dst, const unsigned long *Model1_src,
   unsigned int Model1_nbits)
{
 if ((__builtin_constant_p(Model1_nbits) && (Model1_nbits) <= 64))
  *Model1_dst = *Model1_src;
 else {
  unsigned int Model1_len = (((Model1_nbits) + (8 * sizeof(long)) - 1) / (8 * sizeof(long))) * sizeof(unsigned long);
  ({ Model1_size_t Model1___len = (Model1_len); void *Model1___ret; if (__builtin_constant_p(Model1_len) && Model1___len >= 64) Model1___ret = Model1___memcpy((Model1_dst), (Model1_src), Model1___len); else Model1___ret = __builtin_memcpy((Model1_dst), (Model1_src), Model1___len); Model1___ret; });
 }
}

static inline __attribute__((no_instrument_function)) int Model1_bitmap_and(unsigned long *Model1_dst, const unsigned long *Model1_src1,
   const unsigned long *Model1_src2, unsigned int Model1_nbits)
{
 if ((__builtin_constant_p(Model1_nbits) && (Model1_nbits) <= 64))
  return (*Model1_dst = *Model1_src1 & *Model1_src2 & (~0UL >> (-(Model1_nbits) & (64 - 1)))) != 0;
 return Model1___bitmap_and(Model1_dst, Model1_src1, Model1_src2, Model1_nbits);
}

static inline __attribute__((no_instrument_function)) void Model1_bitmap_or(unsigned long *Model1_dst, const unsigned long *Model1_src1,
   const unsigned long *Model1_src2, unsigned int Model1_nbits)
{
 if ((__builtin_constant_p(Model1_nbits) && (Model1_nbits) <= 64))
  *Model1_dst = *Model1_src1 | *Model1_src2;
 else
  Model1___bitmap_or(Model1_dst, Model1_src1, Model1_src2, Model1_nbits);
}

static inline __attribute__((no_instrument_function)) void Model1_bitmap_xor(unsigned long *Model1_dst, const unsigned long *Model1_src1,
   const unsigned long *Model1_src2, unsigned int Model1_nbits)
{
 if ((__builtin_constant_p(Model1_nbits) && (Model1_nbits) <= 64))
  *Model1_dst = *Model1_src1 ^ *Model1_src2;
 else
  Model1___bitmap_xor(Model1_dst, Model1_src1, Model1_src2, Model1_nbits);
}

static inline __attribute__((no_instrument_function)) int Model1_bitmap_andnot(unsigned long *Model1_dst, const unsigned long *Model1_src1,
   const unsigned long *Model1_src2, unsigned int Model1_nbits)
{
 if ((__builtin_constant_p(Model1_nbits) && (Model1_nbits) <= 64))
  return (*Model1_dst = *Model1_src1 & ~(*Model1_src2) & (~0UL >> (-(Model1_nbits) & (64 - 1)))) != 0;
 return Model1___bitmap_andnot(Model1_dst, Model1_src1, Model1_src2, Model1_nbits);
}

static inline __attribute__((no_instrument_function)) void Model1_bitmap_complement(unsigned long *Model1_dst, const unsigned long *Model1_src,
   unsigned int Model1_nbits)
{
 if ((__builtin_constant_p(Model1_nbits) && (Model1_nbits) <= 64))
  *Model1_dst = ~(*Model1_src);
 else
  Model1___bitmap_complement(Model1_dst, Model1_src, Model1_nbits);
}

static inline __attribute__((no_instrument_function)) int Model1_bitmap_equal(const unsigned long *Model1_src1,
   const unsigned long *Model1_src2, unsigned int Model1_nbits)
{
 if ((__builtin_constant_p(Model1_nbits) && (Model1_nbits) <= 64))
  return !((*Model1_src1 ^ *Model1_src2) & (~0UL >> (-(Model1_nbits) & (64 - 1))));




 return Model1___bitmap_equal(Model1_src1, Model1_src2, Model1_nbits);
}

static inline __attribute__((no_instrument_function)) int Model1_bitmap_intersects(const unsigned long *Model1_src1,
   const unsigned long *Model1_src2, unsigned int Model1_nbits)
{
 if ((__builtin_constant_p(Model1_nbits) && (Model1_nbits) <= 64))
  return ((*Model1_src1 & *Model1_src2) & (~0UL >> (-(Model1_nbits) & (64 - 1)))) != 0;
 else
  return Model1___bitmap_intersects(Model1_src1, Model1_src2, Model1_nbits);
}

static inline __attribute__((no_instrument_function)) int Model1_bitmap_subset(const unsigned long *Model1_src1,
   const unsigned long *Model1_src2, unsigned int Model1_nbits)
{
 if ((__builtin_constant_p(Model1_nbits) && (Model1_nbits) <= 64))
  return ! ((*Model1_src1 & ~(*Model1_src2)) & (~0UL >> (-(Model1_nbits) & (64 - 1))));
 else
  return Model1___bitmap_subset(Model1_src1, Model1_src2, Model1_nbits);
}

static inline __attribute__((no_instrument_function)) int Model1_bitmap_empty(const unsigned long *Model1_src, unsigned Model1_nbits)
{
 if ((__builtin_constant_p(Model1_nbits) && (Model1_nbits) <= 64))
  return ! (*Model1_src & (~0UL >> (-(Model1_nbits) & (64 - 1))));

 return Model1_find_first_bit(Model1_src, Model1_nbits) == Model1_nbits;
}

static inline __attribute__((no_instrument_function)) int Model1_bitmap_full(const unsigned long *Model1_src, unsigned int Model1_nbits)
{
 if ((__builtin_constant_p(Model1_nbits) && (Model1_nbits) <= 64))
  return ! (~(*Model1_src) & (~0UL >> (-(Model1_nbits) & (64 - 1))));

 return Model1_find_first_zero_bit(Model1_src, Model1_nbits) == Model1_nbits;
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_bitmap_weight(const unsigned long *Model1_src, unsigned int Model1_nbits)
{
 if ((__builtin_constant_p(Model1_nbits) && (Model1_nbits) <= 64))
  return Model1_hweight_long(*Model1_src & (~0UL >> (-(Model1_nbits) & (64 - 1))));
 return Model1___bitmap_weight(Model1_src, Model1_nbits);
}

static inline __attribute__((no_instrument_function)) void Model1_bitmap_shift_right(unsigned long *Model1_dst, const unsigned long *Model1_src,
    unsigned int Model1_shift, int Model1_nbits)
{
 if ((__builtin_constant_p(Model1_nbits) && (Model1_nbits) <= 64))
  *Model1_dst = (*Model1_src & (~0UL >> (-(Model1_nbits) & (64 - 1)))) >> Model1_shift;
 else
  Model1___bitmap_shift_right(Model1_dst, Model1_src, Model1_shift, Model1_nbits);
}

static inline __attribute__((no_instrument_function)) void Model1_bitmap_shift_left(unsigned long *Model1_dst, const unsigned long *Model1_src,
    unsigned int Model1_shift, unsigned int Model1_nbits)
{
 if ((__builtin_constant_p(Model1_nbits) && (Model1_nbits) <= 64))
  *Model1_dst = (*Model1_src << Model1_shift) & (~0UL >> (-(Model1_nbits) & (64 - 1)));
 else
  Model1___bitmap_shift_left(Model1_dst, Model1_src, Model1_shift, Model1_nbits);
}

static inline __attribute__((no_instrument_function)) int Model1_bitmap_parse(const char *Model1_buf, unsigned int Model1_buflen,
   unsigned long *Model1_maskp, int Model1_nmaskbits)
{
 return Model1___bitmap_parse(Model1_buf, Model1_buflen, 0, Model1_maskp, Model1_nmaskbits);
}


/* Don't assign or return these: may not be this big! */
typedef struct Model1_cpumask { unsigned long Model1_bits[(((64) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))]; } Model1_cpumask_t;

/**
 * cpumask_bits - get the bits in a cpumask
 * @maskp: the struct cpumask *
 *
 * You should only assume nr_cpu_ids bits of this mask are valid.  This is
 * a macro so it's const-correct.
 */


/**
 * cpumask_pr_args - printf args to output a cpumask
 * @maskp: cpumask to be printed
 *
 * Can be used to provide arguments for '%*pb[l]' when printing a cpumask.
 */





extern int Model1_nr_cpu_ids;
/*
 * The following particular system cpumasks and operations manage
 * possible, present, active and online cpus.
 *
 *     cpu_possible_mask- has bit 'cpu' set iff cpu is populatable
 *     cpu_present_mask - has bit 'cpu' set iff cpu is populated
 *     cpu_online_mask  - has bit 'cpu' set iff cpu available to scheduler
 *     cpu_active_mask  - has bit 'cpu' set iff cpu available to migration
 *
 *  If !CONFIG_HOTPLUG_CPU, present == possible, and active == online.
 *
 *  The cpu_possible_mask is fixed at boot time, as the set of CPU id's
 *  that it is possible might ever be plugged in at anytime during the
 *  life of that system boot.  The cpu_present_mask is dynamic(*),
 *  representing which CPUs are currently plugged in.  And
 *  cpu_online_mask is the dynamic subset of cpu_present_mask,
 *  indicating those CPUs available for scheduling.
 *
 *  If HOTPLUG is enabled, then cpu_possible_mask is forced to have
 *  all NR_CPUS bits set, otherwise it is just the set of CPUs that
 *  ACPI reports present at boot.
 *
 *  If HOTPLUG is enabled, then cpu_present_mask varies dynamically,
 *  depending on what ACPI reports as currently plugged in, otherwise
 *  cpu_present_mask is just a copy of cpu_possible_mask.
 *
 *  (*) Well, cpu_present_mask is dynamic in the hotplug case.  If not
 *      hotplug, it's a copy of cpu_possible_mask, hence fixed at boot.
 *
 * Subtleties:
 * 1) UP arch's (NR_CPUS == 1, CONFIG_SMP not defined) hardcode
 *    assumption that their single CPU is online.  The UP
 *    cpu_{online,possible,present}_masks are placebos.  Changing them
 *    will have no useful affect on the following num_*_cpus()
 *    and cpu_*() macros in the UP case.  This ugliness is a UP
 *    optimization - don't waste any instructions or memory references
 *    asking if you're online or how many CPUs there are if there is
 *    only one CPU.
 */

extern struct Model1_cpumask Model1___cpu_possible_mask;
extern struct Model1_cpumask Model1___cpu_online_mask;
extern struct Model1_cpumask Model1___cpu_present_mask;
extern struct Model1_cpumask Model1___cpu_active_mask;
/* verify cpu argument to cpumask_* operators */
static inline __attribute__((no_instrument_function)) unsigned int Model1_cpumask_check(unsigned int Model1_cpu)
{



 return Model1_cpu;
}
/**
 * cpumask_first - get the first cpu in a cpumask
 * @srcp: the cpumask pointer
 *
 * Returns >= nr_cpu_ids if no cpus set.
 */
static inline __attribute__((no_instrument_function)) unsigned int Model1_cpumask_first(const struct Model1_cpumask *Model1_srcp)
{
 return Model1_find_first_bit(((Model1_srcp)->Model1_bits), 64);
}

/**
 * cpumask_next - get the next cpu in a cpumask
 * @n: the cpu prior to the place to search (ie. return will be > @n)
 * @srcp: the cpumask pointer
 *
 * Returns >= nr_cpu_ids if no further cpus set.
 */
static inline __attribute__((no_instrument_function)) unsigned int Model1_cpumask_next(int Model1_n, const struct Model1_cpumask *Model1_srcp)
{
 /* -1 is a legal arg here. */
 if (Model1_n != -1)
  Model1_cpumask_check(Model1_n);
 return Model1_find_next_bit(((Model1_srcp)->Model1_bits), 64, Model1_n+1);
}

/**
 * cpumask_next_zero - get the next unset cpu in a cpumask
 * @n: the cpu prior to the place to search (ie. return will be > @n)
 * @srcp: the cpumask pointer
 *
 * Returns >= nr_cpu_ids if no further cpus unset.
 */
static inline __attribute__((no_instrument_function)) unsigned int Model1_cpumask_next_zero(int Model1_n, const struct Model1_cpumask *Model1_srcp)
{
 /* -1 is a legal arg here. */
 if (Model1_n != -1)
  Model1_cpumask_check(Model1_n);
 return Model1_find_next_zero_bit(((Model1_srcp)->Model1_bits), 64, Model1_n+1);
}

int Model1_cpumask_next_and(int Model1_n, const struct Model1_cpumask *, const struct Model1_cpumask *);
int Model1_cpumask_any_but(const struct Model1_cpumask *Model1_mask, unsigned int Model1_cpu);
unsigned int Model1_cpumask_local_spread(unsigned int Model1_i, int Model1_node);

/**
 * for_each_cpu - iterate over every cpu in a mask
 * @cpu: the (optionally unsigned) integer iterator
 * @mask: the cpumask pointer
 *
 * After the loop, cpu is >= nr_cpu_ids.
 */





/**
 * for_each_cpu_not - iterate over every cpu in a complemented mask
 * @cpu: the (optionally unsigned) integer iterator
 * @mask: the cpumask pointer
 *
 * After the loop, cpu is >= nr_cpu_ids.
 */





/**
 * for_each_cpu_and - iterate over every cpu in both masks
 * @cpu: the (optionally unsigned) integer iterator
 * @mask: the first cpumask pointer
 * @and: the second cpumask pointer
 *
 * This saves a temporary CPU mask in many places.  It is equivalent to:
 *	struct cpumask tmp;
 *	cpumask_and(&tmp, &mask, &and);
 *	for_each_cpu(cpu, &tmp)
 *		...
 *
 * After the loop, cpu is >= nr_cpu_ids.
 */
/**
 * cpumask_set_cpu - set a cpu in a cpumask
 * @cpu: cpu number (< nr_cpu_ids)
 * @dstp: the cpumask pointer
 */
static inline __attribute__((no_instrument_function)) void Model1_cpumask_set_cpu(unsigned int Model1_cpu, struct Model1_cpumask *Model1_dstp)
{
 Model1_set_bit(Model1_cpumask_check(Model1_cpu), ((Model1_dstp)->Model1_bits));
}

/**
 * cpumask_clear_cpu - clear a cpu in a cpumask
 * @cpu: cpu number (< nr_cpu_ids)
 * @dstp: the cpumask pointer
 */
static inline __attribute__((no_instrument_function)) void Model1_cpumask_clear_cpu(int Model1_cpu, struct Model1_cpumask *Model1_dstp)
{
 Model1_clear_bit(Model1_cpumask_check(Model1_cpu), ((Model1_dstp)->Model1_bits));
}

/**
 * cpumask_test_cpu - test for a cpu in a cpumask
 * @cpu: cpu number (< nr_cpu_ids)
 * @cpumask: the cpumask pointer
 *
 * Returns 1 if @cpu is set in @cpumask, else returns 0
 */
static inline __attribute__((no_instrument_function)) int Model1_cpumask_test_cpu(int Model1_cpu, const struct Model1_cpumask *Model1_cpumask)
{
 return (__builtin_constant_p((Model1_cpumask_check(Model1_cpu))) ? Model1_constant_test_bit((Model1_cpumask_check(Model1_cpu)), ((((Model1_cpumask))->Model1_bits))) : Model1_variable_test_bit((Model1_cpumask_check(Model1_cpu)), ((((Model1_cpumask))->Model1_bits))));
}

/**
 * cpumask_test_and_set_cpu - atomically test and set a cpu in a cpumask
 * @cpu: cpu number (< nr_cpu_ids)
 * @cpumask: the cpumask pointer
 *
 * Returns 1 if @cpu is set in old bitmap of @cpumask, else returns 0
 *
 * test_and_set_bit wrapper for cpumasks.
 */
static inline __attribute__((no_instrument_function)) int Model1_cpumask_test_and_set_cpu(int Model1_cpu, struct Model1_cpumask *Model1_cpumask)
{
 return Model1_test_and_set_bit(Model1_cpumask_check(Model1_cpu), ((Model1_cpumask)->Model1_bits));
}

/**
 * cpumask_test_and_clear_cpu - atomically test and clear a cpu in a cpumask
 * @cpu: cpu number (< nr_cpu_ids)
 * @cpumask: the cpumask pointer
 *
 * Returns 1 if @cpu is set in old bitmap of @cpumask, else returns 0
 *
 * test_and_clear_bit wrapper for cpumasks.
 */
static inline __attribute__((no_instrument_function)) int Model1_cpumask_test_and_clear_cpu(int Model1_cpu, struct Model1_cpumask *Model1_cpumask)
{
 return Model1_test_and_clear_bit(Model1_cpumask_check(Model1_cpu), ((Model1_cpumask)->Model1_bits));
}

/**
 * cpumask_setall - set all cpus (< nr_cpu_ids) in a cpumask
 * @dstp: the cpumask pointer
 */
static inline __attribute__((no_instrument_function)) void Model1_cpumask_setall(struct Model1_cpumask *Model1_dstp)
{
 Model1_bitmap_fill(((Model1_dstp)->Model1_bits), 64);
}

/**
 * cpumask_clear - clear all cpus (< nr_cpu_ids) in a cpumask
 * @dstp: the cpumask pointer
 */
static inline __attribute__((no_instrument_function)) void Model1_cpumask_clear(struct Model1_cpumask *Model1_dstp)
{
 Model1_bitmap_zero(((Model1_dstp)->Model1_bits), 64);
}

/**
 * cpumask_and - *dstp = *src1p & *src2p
 * @dstp: the cpumask result
 * @src1p: the first input
 * @src2p: the second input
 *
 * If *@dstp is empty, returns 0, else returns 1
 */
static inline __attribute__((no_instrument_function)) int Model1_cpumask_and(struct Model1_cpumask *Model1_dstp,
          const struct Model1_cpumask *Model1_src1p,
          const struct Model1_cpumask *Model1_src2p)
{
 return Model1_bitmap_and(((Model1_dstp)->Model1_bits), ((Model1_src1p)->Model1_bits),
           ((Model1_src2p)->Model1_bits), 64);
}

/**
 * cpumask_or - *dstp = *src1p | *src2p
 * @dstp: the cpumask result
 * @src1p: the first input
 * @src2p: the second input
 */
static inline __attribute__((no_instrument_function)) void Model1_cpumask_or(struct Model1_cpumask *Model1_dstp, const struct Model1_cpumask *Model1_src1p,
         const struct Model1_cpumask *Model1_src2p)
{
 Model1_bitmap_or(((Model1_dstp)->Model1_bits), ((Model1_src1p)->Model1_bits),
          ((Model1_src2p)->Model1_bits), 64);
}

/**
 * cpumask_xor - *dstp = *src1p ^ *src2p
 * @dstp: the cpumask result
 * @src1p: the first input
 * @src2p: the second input
 */
static inline __attribute__((no_instrument_function)) void Model1_cpumask_xor(struct Model1_cpumask *Model1_dstp,
          const struct Model1_cpumask *Model1_src1p,
          const struct Model1_cpumask *Model1_src2p)
{
 Model1_bitmap_xor(((Model1_dstp)->Model1_bits), ((Model1_src1p)->Model1_bits),
           ((Model1_src2p)->Model1_bits), 64);
}

/**
 * cpumask_andnot - *dstp = *src1p & ~*src2p
 * @dstp: the cpumask result
 * @src1p: the first input
 * @src2p: the second input
 *
 * If *@dstp is empty, returns 0, else returns 1
 */
static inline __attribute__((no_instrument_function)) int Model1_cpumask_andnot(struct Model1_cpumask *Model1_dstp,
      const struct Model1_cpumask *Model1_src1p,
      const struct Model1_cpumask *Model1_src2p)
{
 return Model1_bitmap_andnot(((Model1_dstp)->Model1_bits), ((Model1_src1p)->Model1_bits),
       ((Model1_src2p)->Model1_bits), 64);
}

/**
 * cpumask_complement - *dstp = ~*srcp
 * @dstp: the cpumask result
 * @srcp: the input to invert
 */
static inline __attribute__((no_instrument_function)) void Model1_cpumask_complement(struct Model1_cpumask *Model1_dstp,
          const struct Model1_cpumask *Model1_srcp)
{
 Model1_bitmap_complement(((Model1_dstp)->Model1_bits), ((Model1_srcp)->Model1_bits),
           64);
}

/**
 * cpumask_equal - *src1p == *src2p
 * @src1p: the first input
 * @src2p: the second input
 */
static inline __attribute__((no_instrument_function)) bool Model1_cpumask_equal(const struct Model1_cpumask *Model1_src1p,
    const struct Model1_cpumask *Model1_src2p)
{
 return Model1_bitmap_equal(((Model1_src1p)->Model1_bits), ((Model1_src2p)->Model1_bits),
       64);
}

/**
 * cpumask_intersects - (*src1p & *src2p) != 0
 * @src1p: the first input
 * @src2p: the second input
 */
static inline __attribute__((no_instrument_function)) bool Model1_cpumask_intersects(const struct Model1_cpumask *Model1_src1p,
         const struct Model1_cpumask *Model1_src2p)
{
 return Model1_bitmap_intersects(((Model1_src1p)->Model1_bits), ((Model1_src2p)->Model1_bits),
            64);
}

/**
 * cpumask_subset - (*src1p & ~*src2p) == 0
 * @src1p: the first input
 * @src2p: the second input
 *
 * Returns 1 if *@src1p is a subset of *@src2p, else returns 0
 */
static inline __attribute__((no_instrument_function)) int Model1_cpumask_subset(const struct Model1_cpumask *Model1_src1p,
     const struct Model1_cpumask *Model1_src2p)
{
 return Model1_bitmap_subset(((Model1_src1p)->Model1_bits), ((Model1_src2p)->Model1_bits),
        64);
}

/**
 * cpumask_empty - *srcp == 0
 * @srcp: the cpumask to that all cpus < nr_cpu_ids are clear.
 */
static inline __attribute__((no_instrument_function)) bool Model1_cpumask_empty(const struct Model1_cpumask *Model1_srcp)
{
 return Model1_bitmap_empty(((Model1_srcp)->Model1_bits), 64);
}

/**
 * cpumask_full - *srcp == 0xFFFFFFFF...
 * @srcp: the cpumask to that all cpus < nr_cpu_ids are set.
 */
static inline __attribute__((no_instrument_function)) bool Model1_cpumask_full(const struct Model1_cpumask *Model1_srcp)
{
 return Model1_bitmap_full(((Model1_srcp)->Model1_bits), 64);
}

/**
 * cpumask_weight - Count of bits in *srcp
 * @srcp: the cpumask to count bits (< nr_cpu_ids) in.
 */
static inline __attribute__((no_instrument_function)) unsigned int Model1_cpumask_weight(const struct Model1_cpumask *Model1_srcp)
{
 return Model1_bitmap_weight(((Model1_srcp)->Model1_bits), 64);
}

/**
 * cpumask_shift_right - *dstp = *srcp >> n
 * @dstp: the cpumask result
 * @srcp: the input to shift
 * @n: the number of bits to shift by
 */
static inline __attribute__((no_instrument_function)) void Model1_cpumask_shift_right(struct Model1_cpumask *Model1_dstp,
           const struct Model1_cpumask *Model1_srcp, int Model1_n)
{
 Model1_bitmap_shift_right(((Model1_dstp)->Model1_bits), ((Model1_srcp)->Model1_bits), Model1_n,
            64);
}

/**
 * cpumask_shift_left - *dstp = *srcp << n
 * @dstp: the cpumask result
 * @srcp: the input to shift
 * @n: the number of bits to shift by
 */
static inline __attribute__((no_instrument_function)) void Model1_cpumask_shift_left(struct Model1_cpumask *Model1_dstp,
          const struct Model1_cpumask *Model1_srcp, int Model1_n)
{
 Model1_bitmap_shift_left(((Model1_dstp)->Model1_bits), ((Model1_srcp)->Model1_bits), Model1_n,
           64);
}

/**
 * cpumask_copy - *dstp = *srcp
 * @dstp: the result
 * @srcp: the input cpumask
 */
static inline __attribute__((no_instrument_function)) void Model1_cpumask_copy(struct Model1_cpumask *Model1_dstp,
    const struct Model1_cpumask *Model1_srcp)
{
 Model1_bitmap_copy(((Model1_dstp)->Model1_bits), ((Model1_srcp)->Model1_bits), 64);
}

/**
 * cpumask_any - pick a "random" cpu from *srcp
 * @srcp: the input cpumask
 *
 * Returns >= nr_cpu_ids if no cpus set.
 */


/**
 * cpumask_first_and - return the first cpu from *srcp1 & *srcp2
 * @src1p: the first input
 * @src2p: the second input
 *
 * Returns >= nr_cpu_ids if no cpus set in both.  See also cpumask_next_and().
 */


/**
 * cpumask_any_and - pick a "random" cpu from *mask1 & *mask2
 * @mask1: the first input cpumask
 * @mask2: the second input cpumask
 *
 * Returns >= nr_cpu_ids if no cpus set.
 */


/**
 * cpumask_of - the cpumask containing just a given cpu
 * @cpu: the cpu (<= nr_cpu_ids)
 */


/**
 * cpumask_parse_user - extract a cpumask from a user string
 * @buf: the buffer to extract from
 * @len: the length of the buffer
 * @dstp: the cpumask to set.
 *
 * Returns -errno, or 0 for success.
 */
static inline __attribute__((no_instrument_function)) int Model1_cpumask_parse_user(const char *Model1_buf, int Model1_len,
         struct Model1_cpumask *Model1_dstp)
{
 return Model1_bitmap_parse_user(Model1_buf, Model1_len, ((Model1_dstp)->Model1_bits), Model1_nr_cpu_ids);
}

/**
 * cpumask_parselist_user - extract a cpumask from a user string
 * @buf: the buffer to extract from
 * @len: the length of the buffer
 * @dstp: the cpumask to set.
 *
 * Returns -errno, or 0 for success.
 */
static inline __attribute__((no_instrument_function)) int Model1_cpumask_parselist_user(const char *Model1_buf, int Model1_len,
         struct Model1_cpumask *Model1_dstp)
{
 return Model1_bitmap_parselist_user(Model1_buf, Model1_len, ((Model1_dstp)->Model1_bits),
         Model1_nr_cpu_ids);
}

/**
 * cpumask_parse - extract a cpumask from a string
 * @buf: the buffer to extract from
 * @dstp: the cpumask to set.
 *
 * Returns -errno, or 0 for success.
 */
static inline __attribute__((no_instrument_function)) int Model1_cpumask_parse(const char *Model1_buf, struct Model1_cpumask *Model1_dstp)
{
 char *Model1_nl = Model1_strchr(Model1_buf, '\n');
 unsigned int Model1_len = Model1_nl ? (unsigned int)(Model1_nl - Model1_buf) : Model1_strlen(Model1_buf);

 return Model1_bitmap_parse(Model1_buf, Model1_len, ((Model1_dstp)->Model1_bits), Model1_nr_cpu_ids);
}

/**
 * cpulist_parse - extract a cpumask from a user string of ranges
 * @buf: the buffer to extract from
 * @dstp: the cpumask to set.
 *
 * Returns -errno, or 0 for success.
 */
static inline __attribute__((no_instrument_function)) int Model1_cpulist_parse(const char *Model1_buf, struct Model1_cpumask *Model1_dstp)
{
 return Model1_bitmap_parselist(Model1_buf, ((Model1_dstp)->Model1_bits), Model1_nr_cpu_ids);
}

/**
 * cpumask_size - size to allocate for a 'struct cpumask' in bytes
 */
static inline __attribute__((no_instrument_function)) Model1_size_t Model1_cpumask_size(void)
{
 return (((64) + (8 * sizeof(long)) - 1) / (8 * sizeof(long))) * sizeof(long);
}

/*
 * cpumask_var_t: struct cpumask for stack usage.
 *
 * Oh, the wicked games we play!  In order to make kernel coding a
 * little more difficult, we typedef cpumask_var_t to an array or a
 * pointer: doing &mask on an array is a noop, so it still works.
 *
 * ie.
 *	cpumask_var_t tmpmask;
 *	if (!alloc_cpumask_var(&tmpmask, GFP_KERNEL))
 *		return -ENOMEM;
 *
 *	  ... use 'tmpmask' like a normal struct cpumask * ...
 *
 *	free_cpumask_var(tmpmask);
 *
 *
 * However, one notable exception is there. alloc_cpumask_var() allocates
 * only nr_cpumask_bits bits (in the other hand, real cpumask_t always has
 * NR_CPUS bits). Therefore you don't have to dereference cpumask_var_t.
 *
 *	cpumask_var_t tmpmask;
 *	if (!alloc_cpumask_var(&tmpmask, GFP_KERNEL))
 *		return -ENOMEM;
 *
 *	var = *tmpmask;
 *
 * This code makes NR_CPUS length memcopy and brings to a memory corruption.
 * cpumask_copy() provide safe copy functionality.
 *
 * Note that there is another evil here: If you define a cpumask_var_t
 * as a percpu variable then the way to obtain the address of the cpumask
 * structure differently influences what this_cpu_* operation needs to be
 * used. Please use this_cpu_cpumask_var_t in those cases. The direct use
 * of this_cpu_ptr() or this_cpu_read() will lead to failures when the
 * other type of cpumask_var_t implementation is configured.
 */
typedef struct Model1_cpumask Model1_cpumask_var_t[1];



static inline __attribute__((no_instrument_function)) bool Model1_alloc_cpumask_var(Model1_cpumask_var_t *Model1_mask, Model1_gfp_t Model1_flags)
{
 return true;
}

static inline __attribute__((no_instrument_function)) bool Model1_alloc_cpumask_var_node(Model1_cpumask_var_t *Model1_mask, Model1_gfp_t Model1_flags,
       int Model1_node)
{
 return true;
}

static inline __attribute__((no_instrument_function)) bool Model1_zalloc_cpumask_var(Model1_cpumask_var_t *Model1_mask, Model1_gfp_t Model1_flags)
{
 Model1_cpumask_clear(*Model1_mask);
 return true;
}

static inline __attribute__((no_instrument_function)) bool Model1_zalloc_cpumask_var_node(Model1_cpumask_var_t *Model1_mask, Model1_gfp_t Model1_flags,
       int Model1_node)
{
 Model1_cpumask_clear(*Model1_mask);
 return true;
}

static inline __attribute__((no_instrument_function)) void Model1_alloc_bootmem_cpumask_var(Model1_cpumask_var_t *Model1_mask)
{
}

static inline __attribute__((no_instrument_function)) void Model1_free_cpumask_var(Model1_cpumask_var_t Model1_mask)
{
}

static inline __attribute__((no_instrument_function)) void Model1_free_bootmem_cpumask_var(Model1_cpumask_var_t Model1_mask)
{
}


/* It's common to want to use cpu_all_mask in struct member initializers,
 * so it has to refer to an address rather than a pointer. */
extern const unsigned long Model1_cpu_all_bits[(((64) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];


/* First bits of cpu_bit_bitmap are in fact unset. */






/* Wrappers for arch boot code to manipulate normally-constant masks */
void Model1_init_cpu_present(const struct Model1_cpumask *Model1_src);
void Model1_init_cpu_possible(const struct Model1_cpumask *Model1_src);
void Model1_init_cpu_online(const struct Model1_cpumask *Model1_src);

static inline __attribute__((no_instrument_function)) void
Model1_set_cpu_possible(unsigned int Model1_cpu, bool Model1_possible)
{
 if (Model1_possible)
  Model1_cpumask_set_cpu(Model1_cpu, &Model1___cpu_possible_mask);
 else
  Model1_cpumask_clear_cpu(Model1_cpu, &Model1___cpu_possible_mask);
}

static inline __attribute__((no_instrument_function)) void
Model1_set_cpu_present(unsigned int Model1_cpu, bool Model1_present)
{
 if (Model1_present)
  Model1_cpumask_set_cpu(Model1_cpu, &Model1___cpu_present_mask);
 else
  Model1_cpumask_clear_cpu(Model1_cpu, &Model1___cpu_present_mask);
}

static inline __attribute__((no_instrument_function)) void
Model1_set_cpu_online(unsigned int Model1_cpu, bool Model1_online)
{
 if (Model1_online)
  Model1_cpumask_set_cpu(Model1_cpu, &Model1___cpu_online_mask);
 else
  Model1_cpumask_clear_cpu(Model1_cpu, &Model1___cpu_online_mask);
}

static inline __attribute__((no_instrument_function)) void
Model1_set_cpu_active(unsigned int Model1_cpu, bool Model1_active)
{
 if (Model1_active)
  Model1_cpumask_set_cpu(Model1_cpu, &Model1___cpu_active_mask);
 else
  Model1_cpumask_clear_cpu(Model1_cpu, &Model1___cpu_active_mask);
}


/**
 * to_cpumask - convert an NR_CPUS bitmap to a struct cpumask *
 * @bitmap: the bitmap
 *
 * There are a few places where cpumask_var_t isn't appropriate and
 * static cpumasks must be used (eg. very early boot), yet we don't
 * expose the definition of 'struct cpumask'.
 *
 * This does the conversion, and can be used as a constant initializer.
 */




static inline __attribute__((no_instrument_function)) int Model1___check_is_bitmap(const unsigned long *Model1_bitmap)
{
 return 1;
}

/*
 * Special-case data structure for "single bit set only" constant CPU masks.
 *
 * We pre-generate all the 64 (or 32) possible bit positions, with enough
 * padding to the left and the right, and return the constant pointer
 * appropriately offset.
 */
extern const unsigned long
 Model1_cpu_bit_bitmap[64 +1][(((64) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];

static inline __attribute__((no_instrument_function)) const struct Model1_cpumask *Model1_get_cpu_mask(unsigned int Model1_cpu)
{
 const unsigned long *Model1_p = Model1_cpu_bit_bitmap[1 + Model1_cpu % 64];
 Model1_p -= Model1_cpu / 64;
 return ((struct Model1_cpumask *)(1 ? (Model1_p) : (void *)sizeof(Model1___check_is_bitmap(Model1_p))));
}
/**
 * cpumap_print_to_pagebuf  - copies the cpumask into the buffer either
 *	as comma-separated list of cpus or hex values of cpumask
 * @list: indicates whether the cpumap must be list
 * @mask: the cpumask to copy
 * @buf: the buffer to copy into
 *
 * Returns the length of the (null-terminated) @buf string, zero if
 * nothing is copied.
 */
static inline __attribute__((no_instrument_function)) Model1_ssize_t
Model1_cpumap_print_to_pagebuf(bool Model1_list, char *Model1_buf, const struct Model1_cpumask *Model1_mask)
{
 return Model1_bitmap_print_to_pagebuf(Model1_list, Model1_buf, ((Model1_mask)->Model1_bits),
          Model1_nr_cpu_ids);
}

extern Model1_cpumask_var_t Model1_cpu_callin_mask;
extern Model1_cpumask_var_t Model1_cpu_callout_mask;
extern Model1_cpumask_var_t Model1_cpu_initialized_mask;
extern Model1_cpumask_var_t Model1_cpu_sibling_setup_mask;

extern void Model1_setup_cpu_local_masks(void);















/* ioctl command encoding: 32 bits total, command in lower 16 bits,
 * size of the parameter structure in the lower 14 bits of the
 * upper 16 bits.
 * Encoding the size of the parameter structure in the ioctl request
 * is useful for catching programs compiled with old versions
 * and to avoid overwriting user space outside the user buffer area.
 * The highest 2 bits are reserved for indicating the ``access mode''.
 * NOTE: This limits the max parameter size to 16kB -1 !
 */

/*
 * The following is for compatibility across the various Linux
 * platforms.  The generic ioctl numbering scheme doesn't really enforce
 * a type field.  De facto, however, the top 8 bits of the lower 16
 * bits are indeed used as a type field, so we might just as well make
 * this explicit here.  Please be sure to use the decoding macros
 * below from now on.
 */



/*
 * Let any architecture override either of the following before
 * including this file.
 */
/*
 * Direction bits, which any architecture can choose to override
 * before including this file.
 */
/* used to create numbers */
/* used to decode ioctl numbers.. */





/* ...and for the drivers/sound files... */




/* provoke compile error for invalid uses of size argument */
extern unsigned int Model1___invalid_size_argument_for_IOC;

struct Model1_msr {
 union {
  struct {
   Model1_u32 Model1_l;
   Model1_u32 Model1_h;
  };
  Model1_u64 Model1_q;
 };
};

struct Model1_msr_info {
 Model1_u32 Model1_msr_no;
 struct Model1_msr Model1_reg;
 struct Model1_msr *Model1_msrs;
 int err;
};

struct Model1_msr_regs_info {
 Model1_u32 *Model1_regs;
 int err;
};

struct Model1_saved_msr {
 bool Model1_valid;
 struct Model1_msr_info Model1_info;
};

struct Model1_saved_msrs {
 unsigned int Model1_num;
 struct Model1_saved_msr *Model1_array;
};

/*
 * both i386 and x86_64 returns 64-bit value in edx:eax, but gcc's "A"
 * constraint has different meanings. For i386, "A" means exactly
 * edx:eax, while for x86_64 it doesn't mean rdx:rax or edx:eax. Instead,
 * it means rax *or* rdx.
 */

/* Using 64-bit values saves one instruction clearing the high half of low */
/*
 * Be very careful with includes. This header is prone to include loops.
 */














/*
 * Non-existant functions to indicate usage errors at link time
 * (or compile-time if the compiler implements __compiletime_error().
 */
extern void Model1___xchg_wrong_size(void)
                                                  ;
extern void Model1___cmpxchg_wrong_size(void)
                                                     ;
extern void Model1___xadd_wrong_size(void)
                                                  ;
extern void Model1___add_wrong_size(void)
                                                 ;

/*
 * Constants for operation sizes. On 32-bit, the 64-bit size it set to
 * -1 because sizeof will never return -1, thereby making those switch
 * case statements guaranteeed dead code which the compiler will
 * eliminate, and allowing the "missing symbol in the default case" to
 * indicate a usage error.
 */
/* 
 * An exchange-type operation, which takes a value and a pointer, and
 * returns the old value.
 */
/*
 * Note: no "lock" prefix even on SMP: xchg always implies lock anyway.
 * Since this is generally used to protect other memory information, we
 * use "asm volatile" and "memory" clobbers to prevent gcc from moving
 * information around.
 */


/*
 * Atomic compare and exchange.  Compare OLD with MEM, if identical,
 * store NEW in MEM.  Return the initial value in MEM.  Success is
 * indicated by comparing RETURN with OLD.
 */



static inline __attribute__((no_instrument_function)) void Model1_set_64bit(volatile Model1_u64 *Model1_ptr, Model1_u64 Model1_val)
{
 *Model1_ptr = Model1_val;
}
/*
 * xadd() adds "inc" to "*ptr" and atomically returns the previous
 * value of "*ptr".
 *
 * xadd() is locked when multiple CPUs are online
 * xadd_sync() is always locked
 * xadd_local() is never locked
 */
/*
 * add_*() adds "inc" to "*ptr"
 *
 * __add() takes a lock prefix
 * add_smp() is locked when multiple CPUs are online
 * add_sync() is always locked
 */



/*
 * Atomic operations that C can't guarantee us.  Useful for
 * resource counting etc..
 */



/**
 * atomic_read - read atomic variable
 * @v: pointer of type atomic_t
 *
 * Atomically reads the value of @v.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_atomic_read(const Model1_atomic_t *Model1_v)
{
 return ({ union { typeof((Model1_v)->Model1_counter) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&((Model1_v)->Model1_counter), Model1___u.Model1___c, sizeof((Model1_v)->Model1_counter)); else Model1___read_once_size_nocheck(&((Model1_v)->Model1_counter), Model1___u.Model1___c, sizeof((Model1_v)->Model1_counter)); Model1___u.Model1___val; });
}

/**
 * atomic_set - set atomic variable
 * @v: pointer of type atomic_t
 * @i: required value
 *
 * Atomically sets the value of @v to @i.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_atomic_set(Model1_atomic_t *Model1_v, int Model1_i)
{
 ({ union { typeof(Model1_v->Model1_counter) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(Model1_v->Model1_counter)) (Model1_i) }; Model1___write_once_size(&(Model1_v->Model1_counter), Model1___u.Model1___c, sizeof(Model1_v->Model1_counter)); Model1___u.Model1___val; });
}

/**
 * atomic_add - add integer to atomic variable
 * @i: integer value to add
 * @v: pointer of type atomic_t
 *
 * Atomically adds @i to @v.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_atomic_add(int Model1_i, Model1_atomic_t *Model1_v)
{
#if CY_ABSTRACT6
    Model1_v->Model1_counter += Model1_i;
#else
 asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "addl %1,%0"
       : "+m" (Model1_v->Model1_counter)
       : "ir" (Model1_i));
#endif
}

/**
 * atomic_sub - subtract integer from atomic variable
 * @i: integer value to subtract
 * @v: pointer of type atomic_t
 *
 * Atomically subtracts @i from @v.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_atomic_sub(int Model1_i, Model1_atomic_t *Model1_v)
{
#if CY_ABSTRACT6
    Model1_v->Model1_counter -= Model1_i;
#else
 asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "subl %1,%0"
       : "+m" (Model1_v->Model1_counter)
       : "ir" (Model1_i));
#endif
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_atomic_sub_return(int Model1_i, Model1_atomic_t *Model1_v);
/**
 * atomic_sub_and_test - subtract value from variable and test result
 * @i: integer value to subtract
 * @v: pointer of type atomic_t
 *
 * Atomically subtracts @i from @v and returns
 * true if the result is zero, or false for all
 * other cases.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model1_atomic_sub_and_test(int Model1_i, Model1_atomic_t *Model1_v)
{
#if CY_ABSTRACT6
    return Model1_atomic_sub_return(Model1_i, Model1_v) == 0;
#else
 do { bool Model1_c; asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "subl" " %2, " "%0" ";" "\n\tset" "e" " %[_cc_" "e" "]\n" : "+m" (Model1_v->Model1_counter), [_cc_e] "=qm" (Model1_c) : "er" (Model1_i) : "memory"); return Model1_c; } while (0);
#endif
}

/**
 * atomic_inc - increment atomic variable
 * @v: pointer of type atomic_t
 *
 * Atomically increments @v by 1.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_atomic_inc(Model1_atomic_t *Model1_v)
{
#if CY_ABSTRACT6
    ++(Model1_v->Model1_counter);
#else
 asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "incl %0"
       : "+m" (Model1_v->Model1_counter));
#endif
}

/**
 * atomic_dec - decrement atomic variable
 * @v: pointer of type atomic_t
 *
 * Atomically decrements @v by 1.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_atomic_dec(Model1_atomic_t *Model1_v)
{
#if CY_ABSTRACT6
    --(Model1_v->Model1_counter);
#else
 asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "decl %0"
       : "+m" (Model1_v->Model1_counter));
#endif
}

/**
 * atomic_dec_and_test - decrement and test
 * @v: pointer of type atomic_t
 *
 * Atomically decrements @v by 1 and
 * returns true if the result is 0, or false for all other
 * cases.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model1_atomic_dec_and_test(Model1_atomic_t *Model1_v)
{
#if CY_ABSTRACT6
    return --(Model1_v->Model1_counter) == 0;
#else
 do { bool Model1_c; asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "decl" " " "%0" ";" "\n\tset" "e" " %[_cc_" "e" "]\n" : "+m" (Model1_v->Model1_counter), [_cc_e] "=qm" (Model1_c) : : "memory"); return Model1_c; } while (0);
#endif
}

/**
 * atomic_inc_and_test - increment and test
 * @v: pointer of type atomic_t
 *
 * Atomically increments @v by 1
 * and returns true if the result is zero, or false for all
 * other cases.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model1_atomic_inc_and_test(Model1_atomic_t *Model1_v)
{
#if CY_ABSTRACT6
    return ++(Model1_v->Model1_counter) == 0;
#else
 do { bool Model1_c; asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "incl" " " "%0" ";" "\n\tset" "e" " %[_cc_" "e" "]\n" : "+m" (Model1_v->Model1_counter), [_cc_e] "=qm" (Model1_c) : : "memory"); return Model1_c; } while (0);
#endif
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_atomic_add_return(int Model1_i, Model1_atomic_t *Model1_v);
/**
 * atomic_add_negative - add and test if negative
 * @i: integer value to add
 * @v: pointer of type atomic_t
 *
 * Atomically adds @i to @v and returns true
 * if the result is negative, or false when
 * result is greater than or equal to zero.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model1_atomic_add_negative(int Model1_i, Model1_atomic_t *Model1_v)
{
#if CY_ABSTRACT6
    return Model1_atomic_add_return(Model1_i, Model1_v) < 0;
#else
 do { bool Model1_c; asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "addl" " %2, " "%0" ";" "\n\tset" "s" " %[_cc_" "s" "]\n" : "+m" (Model1_v->Model1_counter), [_cc_s] "=qm" (Model1_c) : "er" (Model1_i) : "memory"); return Model1_c; } while (0);
#endif
}

/**
 * atomic_add_return - add integer and return
 * @i: integer value to add
 * @v: pointer of type atomic_t
 *
 * Atomically adds @i to @v and returns @i + @v
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_atomic_add_return(int Model1_i, Model1_atomic_t *Model1_v)
{
#if CY_ABSTRACT6
    return Model1_v->Model1_counter += Model1_i;
#else
 return Model1_i + ({ __typeof__ (*(((&Model1_v->Model1_counter)))) Model1___ret = (((Model1_i))); switch (sizeof(*(((&Model1_v->Model1_counter))))) { case 1: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "b %b0, %1\n" : "+q" (Model1___ret), "+m" (*(((&Model1_v->Model1_counter)))) : : "memory", "cc"); break; case 2: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "w %w0, %1\n" : "+r" (Model1___ret), "+m" (*(((&Model1_v->Model1_counter)))) : : "memory", "cc"); break; case 4: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "l %0, %1\n" : "+r" (Model1___ret), "+m" (*(((&Model1_v->Model1_counter)))) : : "memory", "cc"); break; case 8: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "q %q0, %1\n" : "+r" (Model1___ret), "+m" (*(((&Model1_v->Model1_counter)))) : : "memory", "cc"); break; default: Model1___xadd_wrong_size(); } Model1___ret; });
#endif
}

/**
 * atomic_sub_return - subtract integer and return
 * @v: pointer of type atomic_t
 * @i: integer value to subtract
 *
 * Atomically subtracts @i from @v and returns @v - @i
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_atomic_sub_return(int Model1_i, Model1_atomic_t *Model1_v)
{
 return Model1_atomic_add_return(-Model1_i, Model1_v);
}




static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_atomic_fetch_add(int Model1_i, Model1_atomic_t *Model1_v)
{
 return ({ __typeof__ (*(((&Model1_v->Model1_counter)))) Model1___ret = (((Model1_i))); switch (sizeof(*(((&Model1_v->Model1_counter))))) { case 1: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "b %b0, %1\n" : "+q" (Model1___ret), "+m" (*(((&Model1_v->Model1_counter)))) : : "memory", "cc"); break; case 2: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "w %w0, %1\n" : "+r" (Model1___ret), "+m" (*(((&Model1_v->Model1_counter)))) : : "memory", "cc"); break; case 4: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "l %0, %1\n" : "+r" (Model1___ret), "+m" (*(((&Model1_v->Model1_counter)))) : : "memory", "cc"); break; case 8: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "q %q0, %1\n" : "+r" (Model1___ret), "+m" (*(((&Model1_v->Model1_counter)))) : : "memory", "cc"); break; default: Model1___xadd_wrong_size(); } Model1___ret; });
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_atomic_fetch_sub(int Model1_i, Model1_atomic_t *Model1_v)
{
 return ({ __typeof__ (*(((&Model1_v->Model1_counter)))) Model1___ret = (((-Model1_i))); switch (sizeof(*(((&Model1_v->Model1_counter))))) { case 1: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "b %b0, %1\n" : "+q" (Model1___ret), "+m" (*(((&Model1_v->Model1_counter)))) : : "memory", "cc"); break; case 2: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "w %w0, %1\n" : "+r" (Model1___ret), "+m" (*(((&Model1_v->Model1_counter)))) : : "memory", "cc"); break; case 4: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "l %0, %1\n" : "+r" (Model1___ret), "+m" (*(((&Model1_v->Model1_counter)))) : : "memory", "cc"); break; case 8: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "q %q0, %1\n" : "+r" (Model1___ret), "+m" (*(((&Model1_v->Model1_counter)))) : : "memory", "cc"); break; default: Model1___xadd_wrong_size(); } Model1___ret; });
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_atomic_cmpxchg(Model1_atomic_t *Model1_v, int old, int Model1_new)
{
#if CY_ABSTRACT6
    //reference [https://www.khronos.org/registry/OpenCL/sdk/1.1/docs/man/xhtml/atomic_cmpxchg.html]
    printf("Model1_new: %d\n", Model1_new);
    printf("old: %d\n", old);
    Model1_v->Model1_counter = (Model1_atomic_read(Model1_v) == old)? Model1_new : old;
    return old;
#else
 return ({ __typeof__(*((&Model1_v->Model1_counter))) Model1___ret; __typeof__(*((&Model1_v->Model1_counter))) Model1___old = ((old)); __typeof__(*((&Model1_v->Model1_counter))) Model1___new = ((Model1_new)); switch ((sizeof(*(&Model1_v->Model1_counter)))) { case 1: { volatile Model1_u8 *Model1___ptr = (volatile Model1_u8 *)((&Model1_v->Model1_counter)); asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "cmpxchgb %2,%1" : "=a" (Model1___ret), "+m" (*Model1___ptr) : "q" (Model1___new), "0" (Model1___old) : "memory"); break; } case 2: { volatile Model1_u16 *Model1___ptr = (volatile Model1_u16 *)((&Model1_v->Model1_counter)); asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "cmpxchgw %2,%1" : "=a" (Model1___ret), "+m" (*Model1___ptr) : "r" (Model1___new), "0" (Model1___old) : "memory"); break; } case 4: { volatile Model1_u32 *Model1___ptr = (volatile Model1_u32 *)((&Model1_v->Model1_counter)); asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "cmpxchgl %2,%1" : "=a" (Model1___ret), "+m" (*Model1___ptr) : "r" (Model1___new), "0" (Model1___old) : "memory"); break; } case 8: { volatile Model1_u64 *Model1___ptr = (volatile Model1_u64 *)((&Model1_v->Model1_counter)); asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "cmpxchgq %2,%1" : "=a" (Model1___ret), "+m" (*Model1___ptr) : "r" (Model1___new), "0" (Model1___old) : "memory"); break; } default: Model1___cmpxchg_wrong_size(); } Model1___ret; });
#endif
}

static inline __attribute__((no_instrument_function)) int Model1_atomic_xchg(Model1_atomic_t *Model1_v, int Model1_new)
{
 return ({ __typeof__ (*((&Model1_v->Model1_counter))) Model1___ret = ((Model1_new)); switch (sizeof(*((&Model1_v->Model1_counter)))) { case 1: asm volatile ("" "xchg" "b %b0, %1\n" : "+q" (Model1___ret), "+m" (*((&Model1_v->Model1_counter))) : : "memory", "cc"); break; case 2: asm volatile ("" "xchg" "w %w0, %1\n" : "+r" (Model1___ret), "+m" (*((&Model1_v->Model1_counter))) : : "memory", "cc"); break; case 4: asm volatile ("" "xchg" "l %0, %1\n" : "+r" (Model1___ret), "+m" (*((&Model1_v->Model1_counter))) : : "memory", "cc"); break; case 8: asm volatile ("" "xchg" "q %q0, %1\n" : "+r" (Model1___ret), "+m" (*((&Model1_v->Model1_counter))) : : "memory", "cc"); break; default: Model1___xchg_wrong_size(); } Model1___ret; });
}
static inline __attribute__((no_instrument_function)) void Model1_atomic_and(int Model1_i, Model1_atomic_t *Model1_v) { asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "and""l %1,%0" : "+m" (Model1_v->Model1_counter) : "ir" (Model1_i) : "memory"); } static inline __attribute__((no_instrument_function)) int Model1_atomic_fetch_and(int Model1_i, Model1_atomic_t *Model1_v) { int old, Model1_val = Model1_atomic_read(Model1_v); for (;;) { old = Model1_atomic_cmpxchg(Model1_v, Model1_val, Model1_val & Model1_i); if (old == Model1_val) break; Model1_val = old; } return old; }
static inline __attribute__((no_instrument_function)) void Model1_atomic_or(int Model1_i, Model1_atomic_t *Model1_v) { asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "or""l %1,%0" : "+m" (Model1_v->Model1_counter) : "ir" (Model1_i) : "memory"); } static inline __attribute__((no_instrument_function)) int Model1_atomic_fetch_or(int Model1_i, Model1_atomic_t *Model1_v) { int old, Model1_val = Model1_atomic_read(Model1_v); for (;;) { old = Model1_atomic_cmpxchg(Model1_v, Model1_val, Model1_val | Model1_i); if (old == Model1_val) break; Model1_val = old; } return old; }
static inline __attribute__((no_instrument_function)) void Model1_atomic_xor(int Model1_i, Model1_atomic_t *Model1_v) { asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xor""l %1,%0" : "+m" (Model1_v->Model1_counter) : "ir" (Model1_i) : "memory"); } static inline __attribute__((no_instrument_function)) int Model1_atomic_fetch_xor(int Model1_i, Model1_atomic_t *Model1_v) { int old, Model1_val = Model1_atomic_read(Model1_v); for (;;) { old = Model1_atomic_cmpxchg(Model1_v, Model1_val, Model1_val ^ Model1_i); if (old == Model1_val) break; Model1_val = old; } return old; }





/**
 * __atomic_add_unless - add unless the number is already a given value
 * @v: pointer of type atomic_t
 * @a: the amount to add to v...
 * @u: ...unless v is equal to u.
 *
 * Atomically adds @a to @v, so long as @v was not already @u.
 * Returns the old value of @v.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1___atomic_add_unless(Model1_atomic_t *Model1_v, int Model1_a, int Model1_u)
{
 int Model1_c, old;
 Model1_c = Model1_atomic_read(Model1_v);
 for (;;) {
  if (__builtin_expect(!!(Model1_c == (Model1_u)), 0))
   break;
  old = Model1_atomic_cmpxchg((Model1_v), Model1_c, Model1_c + (Model1_a));
  if (__builtin_expect(!!(old == Model1_c), 1))
   break;
  Model1_c = old;
 }
 return Model1_c;
}

/**
 * atomic_inc_short - increment of a short integer
 * @v: pointer to type int
 *
 * Atomically adds 1 to @v
 * Returns the new value of @u
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) short int Model1_atomic_inc_short(short int *Model1_v)
{
 asm(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "addw $1, %0" : "+m" (*Model1_v));
 return *Model1_v;
}












/* The 64-bit atomic type */



/**
 * atomic64_read - read atomic64 variable
 * @v: pointer of type atomic64_t
 *
 * Atomically reads the value of @v.
 * Doesn't imply a read memory barrier.
 */
static inline __attribute__((no_instrument_function)) long Model1_atomic64_read(const Model1_atomic64_t *Model1_v)
{
 return ({ union { typeof((Model1_v)->Model1_counter) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&((Model1_v)->Model1_counter), Model1___u.Model1___c, sizeof((Model1_v)->Model1_counter)); else Model1___read_once_size_nocheck(&((Model1_v)->Model1_counter), Model1___u.Model1___c, sizeof((Model1_v)->Model1_counter)); Model1___u.Model1___val; });
}

/**
 * atomic64_set - set atomic64 variable
 * @v: pointer to type atomic64_t
 * @i: required value
 *
 * Atomically sets the value of @v to @i.
 */
static inline __attribute__((no_instrument_function)) void Model1_atomic64_set(Model1_atomic64_t *Model1_v, long Model1_i)
{
 ({ union { typeof(Model1_v->Model1_counter) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(Model1_v->Model1_counter)) (Model1_i) }; Model1___write_once_size(&(Model1_v->Model1_counter), Model1___u.Model1___c, sizeof(Model1_v->Model1_counter)); Model1___u.Model1___val; });
}

/**
 * atomic64_add - add integer to atomic64 variable
 * @i: integer value to add
 * @v: pointer to type atomic64_t
 *
 * Atomically adds @i to @v.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_atomic64_add(long Model1_i, Model1_atomic64_t *Model1_v)
{
 asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "addq %1,%0"
       : "=m" (Model1_v->Model1_counter)
       : "er" (Model1_i), "m" (Model1_v->Model1_counter));
}

/**
 * atomic64_sub - subtract the atomic64 variable
 * @i: integer value to subtract
 * @v: pointer to type atomic64_t
 *
 * Atomically subtracts @i from @v.
 */
static inline __attribute__((no_instrument_function)) void Model1_atomic64_sub(long Model1_i, Model1_atomic64_t *Model1_v)
{
 asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "subq %1,%0"
       : "=m" (Model1_v->Model1_counter)
       : "er" (Model1_i), "m" (Model1_v->Model1_counter));
}

static inline __attribute__((no_instrument_function)) long Model1_atomic64_sub_return(long Model1_i, Model1_atomic64_t *Model1_v);
/**
 * atomic64_sub_and_test - subtract value from variable and test result
 * @i: integer value to subtract
 * @v: pointer to type atomic64_t
 *
 * Atomically subtracts @i from @v and returns
 * true if the result is zero, or false for all
 * other cases.
 */
static inline __attribute__((no_instrument_function)) bool Model1_atomic64_sub_and_test(long Model1_i, Model1_atomic64_t *Model1_v)
{
#if CY_ABSTRACT6
    return Model1_atomic64_sub_return(Model1_i, Model1_v) == 0;
#else
 do { bool Model1_c; asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "subq" " %2, " "%0" ";" "\n\tset" "e" " %[_cc_" "e" "]\n" : "+m" (Model1_v->Model1_counter), [_cc_e] "=qm" (Model1_c) : "er" (Model1_i) : "memory"); return Model1_c; } while (0);
#endif
}

/**
 * atomic64_inc - increment atomic64 variable
 * @v: pointer to type atomic64_t
 *
 * Atomically increments @v by 1.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_atomic64_inc(Model1_atomic64_t *Model1_v)
{
 asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "incq %0"
       : "=m" (Model1_v->Model1_counter)
       : "m" (Model1_v->Model1_counter));
}

/**
 * atomic64_dec - decrement atomic64 variable
 * @v: pointer to type atomic64_t
 *
 * Atomically decrements @v by 1.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_atomic64_dec(Model1_atomic64_t *Model1_v)
{
 asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "decq %0"
       : "=m" (Model1_v->Model1_counter)
       : "m" (Model1_v->Model1_counter));
}

/**
 * atomic64_dec_and_test - decrement and test
 * @v: pointer to type atomic64_t
 *
 * Atomically decrements @v by 1 and
 * returns true if the result is 0, or false for all other
 * cases.
 */
static inline __attribute__((no_instrument_function)) bool Model1_atomic64_dec_and_test(Model1_atomic64_t *Model1_v)
{
#if CY_ABSTRACT6
    return --(Model1_v->Model1_counter) == 0;
#else
 do { bool Model1_c; asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "decq" " " "%0" ";" "\n\tset" "e" " %[_cc_" "e" "]\n" : "+m" (Model1_v->Model1_counter), [_cc_e] "=qm" (Model1_c) : : "memory"); return Model1_c; } while (0);
#endif
}

/**
 * atomic64_inc_and_test - increment and test
 * @v: pointer to type atomic64_t
 *
 * Atomically increments @v by 1
 * and returns true if the result is zero, or false for all
 * other cases.
 */
static inline __attribute__((no_instrument_function)) bool Model1_atomic64_inc_and_test(Model1_atomic64_t *Model1_v)
{
#if CY_ABSTRACT6
    return ++(Model1_v->Model1_counter) == 0;
#else
 do { bool Model1_c; asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "incq" " " "%0" ";" "\n\tset" "e" " %[_cc_" "e" "]\n" : "+m" (Model1_v->Model1_counter), [_cc_e] "=qm" (Model1_c) : : "memory"); return Model1_c; } while (0);
#endif
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) long Model1_atomic64_add_return(long Model1_i, Model1_atomic64_t *Model1_v);
/**
 * atomic64_add_negative - add and test if negative
 * @i: integer value to add
 * @v: pointer to type atomic64_t
 *
 * Atomically adds @i to @v and returns true
 * if the result is negative, or false when
 * result is greater than or equal to zero.
 */
static inline __attribute__((no_instrument_function)) bool Model1_atomic64_add_negative(long Model1_i, Model1_atomic64_t *Model1_v)
{
#if CY_ABSTRACT6
    return Model1_atomic64_add_return(Model1_i, Model1_v) < 0;
#else
 do { bool Model1_c; asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "addq" " %2, " "%0" ";" "\n\tset" "s" " %[_cc_" "s" "]\n" : "+m" (Model1_v->Model1_counter), [_cc_s] "=qm" (Model1_c) : "er" (Model1_i) : "memory"); return Model1_c; } while (0);
#endif
}

/**
 * atomic64_add_return - add and return
 * @i: integer value to add
 * @v: pointer to type atomic64_t
 *
 * Atomically adds @i to @v and returns @i + @v
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) long Model1_atomic64_add_return(long Model1_i, Model1_atomic64_t *Model1_v)
{
#if CY_ABSTRACT6
    Model1_v->Model1_counter += Model1_i;
    return Model1_v->Model1_counter;
#else
 return Model1_i + ({ __typeof__ (*(((&Model1_v->Model1_counter)))) Model1___ret = (((Model1_i))); switch (sizeof(*(((&Model1_v->Model1_counter))))) { case 1: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "b %b0, %1\n" : "+q" (Model1___ret), "+m" (*(((&Model1_v->Model1_counter)))) : : "memory", "cc"); break; case 2: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "w %w0, %1\n" : "+r" (Model1___ret), "+m" (*(((&Model1_v->Model1_counter)))) : : "memory", "cc"); break; case 4: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "l %0, %1\n" : "+r" (Model1___ret), "+m" (*(((&Model1_v->Model1_counter)))) : : "memory", "cc"); break; case 8: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "q %q0, %1\n" : "+r" (Model1___ret), "+m" (*(((&Model1_v->Model1_counter)))) : : "memory", "cc"); break; default: Model1___xadd_wrong_size(); } Model1___ret; });
#endif
}

static inline __attribute__((no_instrument_function)) long Model1_atomic64_sub_return(long Model1_i, Model1_atomic64_t *Model1_v)
{
 return Model1_atomic64_add_return(-Model1_i, Model1_v);
}

static inline __attribute__((no_instrument_function)) long Model1_atomic64_fetch_add(long Model1_i, Model1_atomic64_t *Model1_v)
{
 return ({ __typeof__ (*(((&Model1_v->Model1_counter)))) Model1___ret = (((Model1_i))); switch (sizeof(*(((&Model1_v->Model1_counter))))) { case 1: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "b %b0, %1\n" : "+q" (Model1___ret), "+m" (*(((&Model1_v->Model1_counter)))) : : "memory", "cc"); break; case 2: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "w %w0, %1\n" : "+r" (Model1___ret), "+m" (*(((&Model1_v->Model1_counter)))) : : "memory", "cc"); break; case 4: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "l %0, %1\n" : "+r" (Model1___ret), "+m" (*(((&Model1_v->Model1_counter)))) : : "memory", "cc"); break; case 8: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "q %q0, %1\n" : "+r" (Model1___ret), "+m" (*(((&Model1_v->Model1_counter)))) : : "memory", "cc"); break; default: Model1___xadd_wrong_size(); } Model1___ret; });
}

static inline __attribute__((no_instrument_function)) long Model1_atomic64_fetch_sub(long Model1_i, Model1_atomic64_t *Model1_v)
{
 return ({ __typeof__ (*(((&Model1_v->Model1_counter)))) Model1___ret = (((-Model1_i))); switch (sizeof(*(((&Model1_v->Model1_counter))))) { case 1: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "b %b0, %1\n" : "+q" (Model1___ret), "+m" (*(((&Model1_v->Model1_counter)))) : : "memory", "cc"); break; case 2: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "w %w0, %1\n" : "+r" (Model1___ret), "+m" (*(((&Model1_v->Model1_counter)))) : : "memory", "cc"); break; case 4: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "l %0, %1\n" : "+r" (Model1___ret), "+m" (*(((&Model1_v->Model1_counter)))) : : "memory", "cc"); break; case 8: asm volatile (".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xadd" "q %q0, %1\n" : "+r" (Model1___ret), "+m" (*(((&Model1_v->Model1_counter)))) : : "memory", "cc"); break; default: Model1___xadd_wrong_size(); } Model1___ret; });
}




static inline __attribute__((no_instrument_function)) long Model1_atomic64_cmpxchg(Model1_atomic64_t *Model1_v, long old, long Model1_new)
{
 return ({ __typeof__(*((&Model1_v->Model1_counter))) Model1___ret; __typeof__(*((&Model1_v->Model1_counter))) Model1___old = ((old)); __typeof__(*((&Model1_v->Model1_counter))) Model1___new = ((Model1_new)); switch ((sizeof(*(&Model1_v->Model1_counter)))) { case 1: { volatile Model1_u8 *Model1___ptr = (volatile Model1_u8 *)((&Model1_v->Model1_counter)); asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "cmpxchgb %2,%1" : "=a" (Model1___ret), "+m" (*Model1___ptr) : "q" (Model1___new), "0" (Model1___old) : "memory"); break; } case 2: { volatile Model1_u16 *Model1___ptr = (volatile Model1_u16 *)((&Model1_v->Model1_counter)); asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "cmpxchgw %2,%1" : "=a" (Model1___ret), "+m" (*Model1___ptr) : "r" (Model1___new), "0" (Model1___old) : "memory"); break; } case 4: { volatile Model1_u32 *Model1___ptr = (volatile Model1_u32 *)((&Model1_v->Model1_counter)); asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "cmpxchgl %2,%1" : "=a" (Model1___ret), "+m" (*Model1___ptr) : "r" (Model1___new), "0" (Model1___old) : "memory"); break; } case 8: { volatile Model1_u64 *Model1___ptr = (volatile Model1_u64 *)((&Model1_v->Model1_counter)); asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "cmpxchgq %2,%1" : "=a" (Model1___ret), "+m" (*Model1___ptr) : "r" (Model1___new), "0" (Model1___old) : "memory"); break; } default: Model1___cmpxchg_wrong_size(); } Model1___ret; });
}

static inline __attribute__((no_instrument_function)) long Model1_atomic64_xchg(Model1_atomic64_t *Model1_v, long Model1_new)
{
 return ({ __typeof__ (*((&Model1_v->Model1_counter))) Model1___ret = ((Model1_new)); switch (sizeof(*((&Model1_v->Model1_counter)))) { case 1: asm volatile ("" "xchg" "b %b0, %1\n" : "+q" (Model1___ret), "+m" (*((&Model1_v->Model1_counter))) : : "memory", "cc"); break; case 2: asm volatile ("" "xchg" "w %w0, %1\n" : "+r" (Model1___ret), "+m" (*((&Model1_v->Model1_counter))) : : "memory", "cc"); break; case 4: asm volatile ("" "xchg" "l %0, %1\n" : "+r" (Model1___ret), "+m" (*((&Model1_v->Model1_counter))) : : "memory", "cc"); break; case 8: asm volatile ("" "xchg" "q %q0, %1\n" : "+r" (Model1___ret), "+m" (*((&Model1_v->Model1_counter))) : : "memory", "cc"); break; default: Model1___xchg_wrong_size(); } Model1___ret; });
}

/**
 * atomic64_add_unless - add unless the number is a given value
 * @v: pointer of type atomic64_t
 * @a: the amount to add to v...
 * @u: ...unless v is equal to u.
 *
 * Atomically adds @a to @v, so long as it was not @u.
 * Returns the old value of @v.
 */
static inline __attribute__((no_instrument_function)) bool Model1_atomic64_add_unless(Model1_atomic64_t *Model1_v, long Model1_a, long Model1_u)
{
 long Model1_c, old;
 Model1_c = Model1_atomic64_read(Model1_v);
 for (;;) {
  if (__builtin_expect(!!(Model1_c == (Model1_u)), 0))
   break;
  old = Model1_atomic64_cmpxchg((Model1_v), Model1_c, Model1_c + (Model1_a));
  if (__builtin_expect(!!(old == Model1_c), 1))
   break;
  Model1_c = old;
 }
 return Model1_c != (Model1_u);
}



/*
 * atomic64_dec_if_positive - decrement by 1 if old value positive
 * @v: pointer of type atomic_t
 *
 * The function returns the old value of *v minus 1, even if
 * the atomic variable, v, was not decremented.
 */
static inline __attribute__((no_instrument_function)) long Model1_atomic64_dec_if_positive(Model1_atomic64_t *Model1_v)
{
 long Model1_c, old, Model1_dec;
 Model1_c = Model1_atomic64_read(Model1_v);
 for (;;) {
  Model1_dec = Model1_c - 1;
  if (__builtin_expect(!!(Model1_dec < 0), 0))
   break;
  old = Model1_atomic64_cmpxchg((Model1_v), Model1_c, Model1_dec);
  if (__builtin_expect(!!(old == Model1_c), 1))
   break;
  Model1_c = old;
 }
 return Model1_dec;
}
static inline __attribute__((no_instrument_function)) void Model1_atomic64_and(long Model1_i, Model1_atomic64_t *Model1_v) { asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "and""q %1,%0" : "+m" (Model1_v->Model1_counter) : "er" (Model1_i) : "memory"); } static inline __attribute__((no_instrument_function)) long Model1_atomic64_fetch_and(long Model1_i, Model1_atomic64_t *Model1_v) { long old, Model1_val = Model1_atomic64_read(Model1_v); for (;;) { old = Model1_atomic64_cmpxchg(Model1_v, Model1_val, Model1_val & Model1_i); if (old == Model1_val) break; Model1_val = old; } return old; }
static inline __attribute__((no_instrument_function)) void Model1_atomic64_or(long Model1_i, Model1_atomic64_t *Model1_v) { asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "or""q %1,%0" : "+m" (Model1_v->Model1_counter) : "er" (Model1_i) : "memory"); } static inline __attribute__((no_instrument_function)) long Model1_atomic64_fetch_or(long Model1_i, Model1_atomic64_t *Model1_v) { long old, Model1_val = Model1_atomic64_read(Model1_v); for (;;) { old = Model1_atomic64_cmpxchg(Model1_v, Model1_val, Model1_val | Model1_i); if (old == Model1_val) break; Model1_val = old; } return old; }
static inline __attribute__((no_instrument_function)) void Model1_atomic64_xor(long Model1_i, Model1_atomic64_t *Model1_v) { asm volatile(".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "xor""q %1,%0" : "+m" (Model1_v->Model1_counter) : "er" (Model1_i) : "memory"); } static inline __attribute__((no_instrument_function)) long Model1_atomic64_fetch_xor(long Model1_i, Model1_atomic64_t *Model1_v) { long old, Model1_val = Model1_atomic64_read(Model1_v); for (;;) { old = Model1_atomic64_cmpxchg(Model1_v, Model1_val, Model1_val ^ Model1_i); if (old == Model1_val) break; Model1_val = old; } return old; }



/*
 * File can be included directly by headers who only want to access
 * tracepoint->key to guard out of line trace calls, or the definition of
 * trace_print_flags{_u64}. Otherwise linux/tracepoint.h should be used.
 */


/* Atomic operations usable in machine independent code */





/*
 * Relaxed variants of xchg, cmpxchg and some atomic operations.
 *
 * We support four variants:
 *
 * - Fully ordered: The default implementation, no suffix required.
 * - Acquire: Provides ACQUIRE semantics, _acquire suffix.
 * - Release: Provides RELEASE semantics, _release suffix.
 * - Relaxed: No ordering guarantees, _relaxed suffix.
 *
 * For compound atomics performing both a load and a store, ACQUIRE
 * semantics apply only to the load and RELEASE semantics only to the
 * store portion of the operation. Note that a failed cmpxchg_acquire
 * does -not- imply any memory ordering constraints.
 *
 * See Documentation/memory-barriers.txt for ACQUIRE/RELEASE definitions.
 */
/*
 * The idea here is to build acquire/release variants by adding explicit
 * barriers on top of the relaxed variant. In the case where the relaxed
 * variant is already fully ordered, no additional barriers are needed.
 *
 * Besides, if an arch has a special barrier for acquire/release, it could
 * implement its own __atomic_op_* and use the same framework for building
 * variants
 */
/* atomic_add_return_relaxed */
/* atomic_inc_return_relaxed */
/* atomic_sub_return_relaxed */
/* atomic_dec_return_relaxed */
/* atomic_fetch_add_relaxed */
/* atomic_fetch_inc_relaxed */
/* atomic_fetch_sub_relaxed */
/* atomic_fetch_dec_relaxed */
/* atomic_fetch_or_relaxed */
/* atomic_fetch_and_relaxed */
/* atomic_fetch_xor_relaxed */
/* atomic_xchg_relaxed */
/* atomic_cmpxchg_relaxed */
/* cmpxchg_relaxed */
/* cmpxchg64_relaxed */
/* xchg_relaxed */
/**
 * atomic_add_unless - add unless the number is already a given value
 * @v: pointer of type atomic_t
 * @a: the amount to add to v...
 * @u: ...unless v is equal to u.
 *
 * Atomically adds @a to @v, so long as @v was not already @u.
 * Returns non-zero if @v was not @u, and zero otherwise.
 */
static inline __attribute__((no_instrument_function)) int Model1_atomic_add_unless(Model1_atomic_t *Model1_v, int Model1_a, int Model1_u)
{
 return Model1___atomic_add_unless(Model1_v, Model1_a, Model1_u) != Model1_u;
}

/**
 * atomic_inc_not_zero - increment unless the number is zero
 * @v: pointer of type atomic_t
 *
 * Atomically increments @v by 1, so long as @v is non-zero.
 * Returns non-zero if @v was non-zero, and zero otherwise.
 */





static inline __attribute__((no_instrument_function)) void Model1_atomic_andnot(int Model1_i, Model1_atomic_t *Model1_v)
{
 Model1_atomic_and(~Model1_i, Model1_v);
}

static inline __attribute__((no_instrument_function)) int Model1_atomic_fetch_andnot(int Model1_i, Model1_atomic_t *Model1_v)
{
 return Model1_atomic_fetch_and(~Model1_i, Model1_v);
}

static inline __attribute__((no_instrument_function)) int Model1_atomic_fetch_andnot_relaxed(int Model1_i, Model1_atomic_t *Model1_v)
{
 return Model1_atomic_fetch_and(~Model1_i, Model1_v);
}

static inline __attribute__((no_instrument_function)) int Model1_atomic_fetch_andnot_acquire(int Model1_i, Model1_atomic_t *Model1_v)
{
 return Model1_atomic_fetch_and(~Model1_i, Model1_v);
}

static inline __attribute__((no_instrument_function)) int Model1_atomic_fetch_andnot_release(int Model1_i, Model1_atomic_t *Model1_v)
{
 return Model1_atomic_fetch_and(~Model1_i, Model1_v);
}


/**
 * atomic_inc_not_zero_hint - increment if not null
 * @v: pointer of type atomic_t
 * @hint: probable value of the atomic before the increment
 *
 * This version of atomic_inc_not_zero() gives a hint of probable
 * value of the atomic. This helps processor to not read the memory
 * before doing the atomic read/modify/write cycle, lowering
 * number of bus transactions on some arches.
 *
 * Returns: 0 if increment was not done, 1 otherwise.
 */

static inline __attribute__((no_instrument_function)) int Model1_atomic_inc_not_zero_hint(Model1_atomic_t *Model1_v, int Model1_hint)
{
 int Model1_val, Model1_c = Model1_hint;

 /* sanity test, should be removed by compiler if hint is a constant */
 if (!Model1_hint)
  return Model1_atomic_add_unless((Model1_v), 1, 0);

 do {
  Model1_val = Model1_atomic_cmpxchg(Model1_v, Model1_c, Model1_c + 1);
  if (Model1_val == Model1_c)
   return 1;
  Model1_c = Model1_val;
 } while (Model1_c);

 return 0;
}



static inline __attribute__((no_instrument_function)) int Model1_atomic_inc_unless_negative(Model1_atomic_t *Model1_p)
{
 int Model1_v, Model1_v1;
 for (Model1_v = 0; Model1_v >= 0; Model1_v = Model1_v1) {
  Model1_v1 = Model1_atomic_cmpxchg(Model1_p, Model1_v, Model1_v + 1);
  if (__builtin_expect(!!(Model1_v1 == Model1_v), 1))
   return 1;
 }
 return 0;
}



static inline __attribute__((no_instrument_function)) int Model1_atomic_dec_unless_positive(Model1_atomic_t *Model1_p)
{
 int Model1_v, Model1_v1;
 for (Model1_v = 0; Model1_v <= 0; Model1_v = Model1_v1) {
  Model1_v1 = Model1_atomic_cmpxchg(Model1_p, Model1_v, Model1_v - 1);
  if (__builtin_expect(!!(Model1_v1 == Model1_v), 1))
   return 1;
 }
 return 0;
}


/*
 * atomic_dec_if_positive - decrement by 1 if old value positive
 * @v: pointer of type atomic_t
 *
 * The function returns the old value of *v minus 1, even if
 * the atomic variable, v, was not decremented.
 */

static inline __attribute__((no_instrument_function)) int Model1_atomic_dec_if_positive(Model1_atomic_t *Model1_v)
{
 int Model1_c, old, Model1_dec;
 Model1_c = Model1_atomic_read(Model1_v);
 for (;;) {
  Model1_dec = Model1_c - 1;
  if (__builtin_expect(!!(Model1_dec < 0), 0))
   break;
  old = Model1_atomic_cmpxchg((Model1_v), Model1_c, Model1_dec);
  if (__builtin_expect(!!(old == Model1_c), 1))
   break;
  Model1_c = old;
 }
 return Model1_dec;
}
/* atomic64_add_return_relaxed */
/* atomic64_inc_return_relaxed */
/* atomic64_sub_return_relaxed */
/* atomic64_dec_return_relaxed */
/* atomic64_fetch_add_relaxed */
/* atomic64_fetch_inc_relaxed */
/* atomic64_fetch_sub_relaxed */
/* atomic64_fetch_dec_relaxed */
/* atomic64_fetch_or_relaxed */
/* atomic64_fetch_and_relaxed */
/* atomic64_fetch_xor_relaxed */
/* atomic64_xchg_relaxed */
/* atomic64_cmpxchg_relaxed */
static inline __attribute__((no_instrument_function)) void Model1_atomic64_andnot(long long Model1_i, Model1_atomic64_t *Model1_v)
{
 Model1_atomic64_and(~Model1_i, Model1_v);
}

static inline __attribute__((no_instrument_function)) long long Model1_atomic64_fetch_andnot(long long Model1_i, Model1_atomic64_t *Model1_v)
{
 return Model1_atomic64_fetch_and(~Model1_i, Model1_v);
}

static inline __attribute__((no_instrument_function)) long long Model1_atomic64_fetch_andnot_relaxed(long long Model1_i, Model1_atomic64_t *Model1_v)
{
 return Model1_atomic64_fetch_and(~Model1_i, Model1_v);
}

static inline __attribute__((no_instrument_function)) long long Model1_atomic64_fetch_andnot_acquire(long long Model1_i, Model1_atomic64_t *Model1_v)
{
 return Model1_atomic64_fetch_and(~Model1_i, Model1_v);
}

static inline __attribute__((no_instrument_function)) long long Model1_atomic64_fetch_andnot_release(long long Model1_i, Model1_atomic64_t *Model1_v)
{
 return Model1_atomic64_fetch_and(~Model1_i, Model1_v);
}





/*
 * Copyright (C) 2005 Silicon Graphics, Inc.
 *	Christoph Lameter
 *
 * Allows to provide arch independent atomic definitions without the need to
 * edit all arch specific atomic.h files.
 */



/*
 * Suppport for atomic_long_t
 *
 * Casts for parameters are avoided for existing atomic functions in order to
 * avoid issues with cast-as-lval under gcc 4.x and other limitations that the
 * macros of a platform may have.
 */



typedef Model1_atomic64_t Model1_atomic_long_t;
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_read(const Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_read(Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_read_acquire(const Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)({ typeof(*&(Model1_v)->Model1_counter) Model1____p1 = ({ union { typeof(*&(Model1_v)->Model1_counter) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&(*&(Model1_v)->Model1_counter), Model1___u.Model1___c, sizeof(*&(Model1_v)->Model1_counter)); else Model1___read_once_size_nocheck(&(*&(Model1_v)->Model1_counter), Model1___u.Model1___c, sizeof(*&(Model1_v)->Model1_counter)); Model1___u.Model1___val; }); do { bool Model1___cond = !((sizeof(*&(Model1_v)->Model1_counter) == sizeof(char) || sizeof(*&(Model1_v)->Model1_counter) == sizeof(short) || sizeof(*&(Model1_v)->Model1_counter) == sizeof(int) || sizeof(*&(Model1_v)->Model1_counter) == sizeof(long))); extern void Model1___compiletime_assert_45(void) ; if (Model1___cond) Model1___compiletime_assert_45(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); Model1____p1; }); }
static inline __attribute__((no_instrument_function)) void Model1_atomic_long_set(Model1_atomic_long_t *Model1_l, long Model1_i) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; Model1_atomic64_set(Model1_v, Model1_i); }
static inline __attribute__((no_instrument_function)) void Model1_atomic_long_set_release(Model1_atomic_long_t *Model1_l, long Model1_i) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; do { do { bool Model1___cond = !((sizeof(*&(Model1_v)->Model1_counter) == sizeof(char) || sizeof(*&(Model1_v)->Model1_counter) == sizeof(short) || sizeof(*&(Model1_v)->Model1_counter) == sizeof(int) || sizeof(*&(Model1_v)->Model1_counter) == sizeof(long))); extern void Model1___compiletime_assert_57(void) ; if (Model1___cond) Model1___compiletime_assert_57(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&(Model1_v)->Model1_counter) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(*&(Model1_v)->Model1_counter)) ((Model1_i)) }; Model1___write_once_size(&(*&(Model1_v)->Model1_counter), Model1___u.Model1___c, sizeof(*&(Model1_v)->Model1_counter)); Model1___u.Model1___val; }); } while (0); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_add_return(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_add_return(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_add_return_relaxed(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_add_return(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_add_return_acquire(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_add_return(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_add_return_release(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_add_return(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_sub_return(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_sub_return(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_sub_return_relaxed(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_sub_return(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_sub_return_acquire(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_sub_return(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_sub_return_release(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_sub_return(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_atomic_long_inc(Model1_atomic_long_t *Model1_l)
{
 Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l;

 Model1_atomic64_inc(Model1_v);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_atomic_long_dec(Model1_atomic_long_t *Model1_l)
{
 Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l;

 Model1_atomic64_dec(Model1_v);
}
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_add(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_add(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_add_relaxed(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_add(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_add_acquire(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_add(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_add_release(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_add(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_sub(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_sub(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_sub_relaxed(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_sub(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_sub_acquire(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_sub(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_sub_release(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_sub(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_and(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_and(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_and_relaxed(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_and(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_and_acquire(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_and(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_and_release(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_and(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_andnot(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_andnot(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_andnot_relaxed(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_andnot_relaxed(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_andnot_acquire(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_andnot_acquire(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_andnot_release(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_andnot_release(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_or(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_or(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_or_relaxed(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_or(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_or_acquire(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_or(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_or_release(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_or(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_xor(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_xor(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_xor_relaxed(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_xor(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_xor_acquire(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_xor(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_xor_release(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_xor(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_inc(Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_add(1, (Model1_v)); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_inc_relaxed(Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_add(1, (Model1_v)); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_inc_acquire(Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_add(1, (Model1_v)); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_inc_release(Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_add(1, (Model1_v)); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_dec(Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_sub(1, (Model1_v)); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_dec_relaxed(Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_sub(1, (Model1_v)); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_dec_acquire(Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_sub(1, (Model1_v)); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_fetch_dec_release(Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)Model1_atomic64_fetch_sub(1, (Model1_v)); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_atomic_long_add(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; Model1_atomic64_add(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_atomic_long_sub(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; Model1_atomic64_sub(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_atomic_long_and(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; Model1_atomic64_and(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_atomic_long_andnot(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; Model1_atomic64_andnot(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_atomic_long_or(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; Model1_atomic64_or(Model1_i, Model1_v); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_atomic_long_xor(long Model1_i, Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; Model1_atomic64_xor(Model1_i, Model1_v); }



static inline __attribute__((no_instrument_function)) int Model1_atomic_long_sub_and_test(long Model1_i, Model1_atomic_long_t *Model1_l)
{
 Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l;

 return Model1_atomic64_sub_and_test(Model1_i, Model1_v);
}

static inline __attribute__((no_instrument_function)) int Model1_atomic_long_dec_and_test(Model1_atomic_long_t *Model1_l)
{
 Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l;

 return Model1_atomic64_dec_and_test(Model1_v);
}

static inline __attribute__((no_instrument_function)) int Model1_atomic_long_inc_and_test(Model1_atomic_long_t *Model1_l)
{
 Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l;

 return Model1_atomic64_inc_and_test(Model1_v);
}

static inline __attribute__((no_instrument_function)) int Model1_atomic_long_add_negative(long Model1_i, Model1_atomic_long_t *Model1_l)
{
 Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l;

 return Model1_atomic64_add_negative(Model1_i, Model1_v);
}
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_inc_return(Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)(Model1_atomic64_add_return(1, (Model1_v))); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_inc_return_relaxed(Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)(Model1_atomic64_add_return(1, (Model1_v))); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_inc_return_acquire(Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)(Model1_atomic64_add_return(1, (Model1_v))); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_inc_return_release(Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)(Model1_atomic64_add_return(1, (Model1_v))); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_dec_return(Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)(Model1_atomic64_sub_return(1, (Model1_v))); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_dec_return_relaxed(Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)(Model1_atomic64_sub_return(1, (Model1_v))); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_dec_return_acquire(Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)(Model1_atomic64_sub_return(1, (Model1_v))); }
static inline __attribute__((no_instrument_function)) long Model1_atomic_long_dec_return_release(Model1_atomic_long_t *Model1_l) { Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l; return (long)(Model1_atomic64_sub_return(1, (Model1_v))); }



static inline __attribute__((no_instrument_function)) long Model1_atomic_long_add_unless(Model1_atomic_long_t *Model1_l, long Model1_a, long Model1_u)
{
 Model1_atomic64_t *Model1_v = (Model1_atomic64_t *)Model1_l;

 return (long)Model1_atomic64_add_unless(Model1_v, Model1_a, Model1_u);
}



/*
 * Jump label support
 *
 * Copyright (C) 2009-2012 Jason Baron <jbaron@redhat.com>
 * Copyright (C) 2011-2012 Red Hat, Inc., Peter Zijlstra
 *
 * DEPRECATED API:
 *
 * The use of 'struct static_key' directly, is now DEPRECATED. In addition
 * static_key_{true,false}() is also DEPRECATED. IE DO NOT use the following:
 *
 * struct static_key false = STATIC_KEY_INIT_FALSE;
 * struct static_key true = STATIC_KEY_INIT_TRUE;
 * static_key_true()
 * static_key_false()
 *
 * The updated API replacements are:
 *
 * DEFINE_STATIC_KEY_TRUE(key);
 * DEFINE_STATIC_KEY_FALSE(key);
 * static_branch_likely()
 * static_branch_unlikely()
 *
 * Jump labels provide an interface to generate dynamic branches using
 * self-modifying code. Assuming toolchain and architecture support, if we
 * define a "key" that is initially false via "DEFINE_STATIC_KEY_FALSE(key)",
 * an "if (static_branch_unlikely(&key))" statement is an unconditional branch
 * (which defaults to false - and the true block is placed out of line).
 * Similarly, we can define an initially true key via
 * "DEFINE_STATIC_KEY_TRUE(key)", and use it in the same
 * "if (static_branch_unlikely(&key))", in which case we will generate an
 * unconditional branch to the out-of-line true branch. Keys that are
 * initially true or false can be using in both static_branch_unlikely()
 * and static_branch_likely() statements.
 *
 * At runtime we can change the branch target by setting the key
 * to true via a call to static_branch_enable(), or false using
 * static_branch_disable(). If the direction of the branch is switched by
 * these calls then we run-time modify the branch target via a
 * no-op -> jump or jump -> no-op conversion. For example, for an
 * initially false key that is used in an "if (static_branch_unlikely(&key))"
 * statement, setting the key to true requires us to patch in a jump
 * to the out-of-line of true branch.
 *
 * In addition to static_branch_{enable,disable}, we can also reference count
 * the key or branch direction via static_branch_{inc,dec}. Thus,
 * static_branch_inc() can be thought of as a 'make more true' and
 * static_branch_dec() as a 'make more false'.
 *
 * Since this relies on modifying code, the branch modifying functions
 * must be considered absolute slow paths (machine wide synchronization etc.).
 * OTOH, since the affected branches are unconditional, their runtime overhead
 * will be absolutely minimal, esp. in the default (off) case where the total
 * effect is a single NOP of appropriate size. The on case will patch in a jump
 * to the out-of-line block.
 *
 * When the control is directly exposed to userspace, it is prudent to delay the
 * decrement to avoid high frequency code modifications which can (and do)
 * cause significant performance degradation. Struct static_key_deferred and
 * static_key_slow_dec_deferred() provide for this.
 *
 * Lacking toolchain and or architecture support, static keys fall back to a
 * simple conditional branch.
 *
 * Additional babbling in: Documentation/static-keys.txt
 */
extern bool Model1_static_key_initialized;
struct Model1_static_key {
 Model1_atomic_t Model1_enabled;
};
enum Model1_jump_label_type {
 Model1_JUMP_LABEL_NOP = 0,
 Model1_JUMP_LABEL_JMP,
};

struct Model1_module;
static inline __attribute__((no_instrument_function)) int Model1_static_key_count(struct Model1_static_key *Model1_key)
{
 return Model1_atomic_read(&Model1_key->Model1_enabled);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_jump_label_init(void)
{
 Model1_static_key_initialized = true;
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model1_static_key_false(struct Model1_static_key *Model1_key)
{
 if (__builtin_expect(!!(Model1_static_key_count(Model1_key) > 0), 0))
  return true;
 return false;
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model1_static_key_true(struct Model1_static_key *Model1_key)
{
 if (__builtin_expect(!!(Model1_static_key_count(Model1_key) > 0), 1))
  return true;
 return false;
}

static inline __attribute__((no_instrument_function)) void Model1_static_key_slow_inc(struct Model1_static_key *Model1_key)
{
 ({ int Model1___ret_warn_on = !!(!Model1_static_key_initialized); if (__builtin_expect(!!(Model1___ret_warn_on), 0)) Model1_warn_slowpath_fmt("./include/linux/jump_label.h", 196, "%s used before call to jump_label_init", __func__); __builtin_expect(!!(Model1___ret_warn_on), 0); });
 Model1_atomic_inc(&Model1_key->Model1_enabled);
}

static inline __attribute__((no_instrument_function)) void Model1_static_key_slow_dec(struct Model1_static_key *Model1_key)
{
 ({ int Model1___ret_warn_on = !!(!Model1_static_key_initialized); if (__builtin_expect(!!(Model1___ret_warn_on), 0)) Model1_warn_slowpath_fmt("./include/linux/jump_label.h", 202, "%s used before call to jump_label_init", __func__); __builtin_expect(!!(Model1___ret_warn_on), 0); });
 Model1_atomic_dec(&Model1_key->Model1_enabled);
}

static inline __attribute__((no_instrument_function)) int Model1_jump_label_text_reserved(void *Model1_start, void *Model1_end)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model1_jump_label_lock(void) {}
static inline __attribute__((no_instrument_function)) void Model1_jump_label_unlock(void) {}

static inline __attribute__((no_instrument_function)) int Model1_jump_label_apply_nops(struct Model1_module *Model1_mod)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model1_static_key_enable(struct Model1_static_key *Model1_key)
{
 int Model1_count = Model1_static_key_count(Model1_key);

 ({ static bool __attribute__ ((__section__(".data.unlikely"))) Model1___warned; int Model1___ret_warn_once = !!(Model1_count < 0 || Model1_count > 1); if (__builtin_expect(!!(Model1___ret_warn_once && !Model1___warned), 0)) { Model1___warned = true; ({ int Model1___ret_warn_on = !!(1); if (__builtin_expect(!!(Model1___ret_warn_on), 0)) Model1_warn_slowpath_null("./include/linux/jump_label.h", 223); __builtin_expect(!!(Model1___ret_warn_on), 0); }); } __builtin_expect(!!(Model1___ret_warn_once), 0); });

 if (!Model1_count)
  Model1_static_key_slow_inc(Model1_key);
}

static inline __attribute__((no_instrument_function)) void Model1_static_key_disable(struct Model1_static_key *Model1_key)
{
 int Model1_count = Model1_static_key_count(Model1_key);

 ({ static bool __attribute__ ((__section__(".data.unlikely"))) Model1___warned; int Model1___ret_warn_once = !!(Model1_count < 0 || Model1_count > 1); if (__builtin_expect(!!(Model1___ret_warn_once && !Model1___warned), 0)) { Model1___warned = true; ({ int Model1___ret_warn_on = !!(1); if (__builtin_expect(!!(Model1___ret_warn_on), 0)) Model1_warn_slowpath_null("./include/linux/jump_label.h", 233); __builtin_expect(!!(Model1___ret_warn_on), 0); }); } __builtin_expect(!!(Model1___ret_warn_once), 0); });

 if (Model1_count)
  Model1_static_key_slow_dec(Model1_key);
}
/* -------------------------------------------------------------------------- */

/*
 * Two type wrappers around static_key, such that we can use compile time
 * type differentiation to emit the right code.
 *
 * All the below code is macros in order to play type games.
 */

struct Model1_static_key_true {
 struct Model1_static_key Model1_key;
};

struct Model1_static_key_false {
 struct Model1_static_key Model1_key;
};
extern bool Model1_____wrong_branch_error(void);
/*
 * Advanced usage; refcount, branch is enabled when: count != 0
 */




/*
 * Normal usage; boolean enable/disable.
 */

struct Model1_trace_print_flags {
 unsigned long Model1_mask;
 const char *Model1_name;
};

struct Model1_trace_print_flags_u64 {
 unsigned long long Model1_mask;
 const char *Model1_name;
};

struct Model1_tracepoint_func {
 void *func;
 void *Model1_data;
 int Model1_prio;
};

struct Model1_tracepoint {
 const char *Model1_name; /* Tracepoint name */
 struct Model1_static_key Model1_key;
 void (*Model1_regfunc)(void);
 void (*Model1_unregfunc)(void);
 struct Model1_tracepoint_func *Model1_funcs;
};

extern struct Model1_tracepoint Model1___tracepoint_read_msr;
extern struct Model1_tracepoint Model1___tracepoint_write_msr;
extern struct Model1_tracepoint Model1___tracepoint_rdpmc;

extern void Model1_do_trace_write_msr(unsigned Model1_msr, Model1_u64 Model1_val, int Model1_failed);
extern void Model1_do_trace_read_msr(unsigned Model1_msr, Model1_u64 Model1_val, int Model1_failed);
extern void Model1_do_trace_rdpmc(unsigned Model1_msr, Model1_u64 Model1_val, int Model1_failed);







static inline __attribute__((no_instrument_function)) unsigned long long Model1_native_read_msr(unsigned int Model1_msr)
{
 unsigned long Model1_low, Model1_high;

 asm volatile("1: rdmsr\n"
       "2:\n"
       " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "2b" ") - .\n" " .long (" "ex_handler_rdmsr_unsafe" ") - .\n" " .popsection\n"
       : "=a" (Model1_low), "=d" (Model1_high) : "c" (Model1_msr));
 if (Model1_static_key_false(&(Model1___tracepoint_read_msr).Model1_key))
  Model1_do_trace_read_msr(Model1_msr, ((Model1_low) | (Model1_high) << 32), 0);
 return ((Model1_low) | (Model1_high) << 32);
}

static inline __attribute__((no_instrument_function)) unsigned long long Model1_native_read_msr_safe(unsigned int Model1_msr,
            int *err)
{
 unsigned long Model1_low, Model1_high;

 asm volatile("2: rdmsr ; xor %[err],%[err]\n"
       "1:\n\t"
       ".section .fixup,\"ax\"\n\t"
       "3: mov %[fault],%[err]\n\t"
       "xorl %%eax, %%eax\n\t"
       "xorl %%edx, %%edx\n\t"
       "jmp 1b\n\t"
       ".previous\n\t"
       " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "2b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n"
       : [err] "=r" (*err), "=a" (Model1_low), "=d" (Model1_high)
       : "c" (Model1_msr), [fault] "i" (-5));
 if (Model1_static_key_false(&(Model1___tracepoint_read_msr).Model1_key))
  Model1_do_trace_read_msr(Model1_msr, ((Model1_low) | (Model1_high) << 32), *err);
 return ((Model1_low) | (Model1_high) << 32);
}

/* Can be uninlined because referenced by paravirt */
__attribute__((no_instrument_function)) static inline __attribute__((no_instrument_function)) void Model1_native_write_msr(unsigned int Model1_msr,
         unsigned Model1_low, unsigned Model1_high)
{
 asm volatile("1: wrmsr\n"
       "2:\n"
       " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "2b" ") - .\n" " .long (" "ex_handler_wrmsr_unsafe" ") - .\n" " .popsection\n"
       : : "c" (Model1_msr), "a"(Model1_low), "d" (Model1_high) : "memory");
 if (Model1_static_key_false(&(Model1___tracepoint_write_msr).Model1_key))
  Model1_do_trace_write_msr(Model1_msr, ((Model1_u64)Model1_high << 32 | Model1_low), 0);
}

/* Can be uninlined because referenced by paravirt */
__attribute__((no_instrument_function)) static inline __attribute__((no_instrument_function)) int Model1_native_write_msr_safe(unsigned int Model1_msr,
     unsigned Model1_low, unsigned Model1_high)
{
 int err;
 asm volatile("2: wrmsr ; xor %[err],%[err]\n"
       "1:\n\t"
       ".section .fixup,\"ax\"\n\t"
       "3:  mov %[fault],%[err] ; jmp 1b\n\t"
       ".previous\n\t"
       " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "2b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n"
       : [err] "=a" (err)
       : "c" (Model1_msr), "0" (Model1_low), "d" (Model1_high),
         [fault] "i" (-5)
       : "memory");
 if (Model1_static_key_false(&(Model1___tracepoint_write_msr).Model1_key))
  Model1_do_trace_write_msr(Model1_msr, ((Model1_u64)Model1_high << 32 | Model1_low), err);
 return err;
}

extern int Model1_rdmsr_safe_regs(Model1_u32 Model1_regs[8]);
extern int Model1_wrmsr_safe_regs(Model1_u32 Model1_regs[8]);

/**
 * rdtsc() - returns the current TSC without ordering constraints
 *
 * rdtsc() returns the result of RDTSC as a 64-bit integer.  The
 * only ordering constraint it supplies is the ordering implied by
 * "asm volatile": it will put the RDTSC in the place you expect.  The
 * CPU can and will speculatively execute that RDTSC, though, so the
 * results can be non-monotonic if compared on different CPUs.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) unsigned long long Model1_rdtsc(void)
{
 unsigned long Model1_low, Model1_high;

 asm volatile("rdtsc" : "=a" (Model1_low), "=d" (Model1_high));

 return ((Model1_low) | (Model1_high) << 32);
}

/**
 * rdtsc_ordered() - read the current TSC in program order
 *
 * rdtsc_ordered() returns the result of RDTSC as a 64-bit integer.
 * It is ordered like a load to a global in-memory counter.  It should
 * be impossible to observe non-monotonic rdtsc_unordered() behavior
 * across multiple CPUs as long as the TSC is synced.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) unsigned long long Model1_rdtsc_ordered(void)
{
 /*
	 * The RDTSC instruction is not ordered relative to memory
	 * access.  The Intel SDM and the AMD APM are both vague on this
	 * point, but empirically an RDTSC instruction can be
	 * speculatively executed before prior loads.  An RDTSC
	 * immediately after an appropriate barrier appears to be
	 * ordered as a normal load, that is, it provides the same
	 * ordering guarantees as reading from a global memory location
	 * that some other imaginary CPU is updating continuously with a
	 * time stamp.
	 */
 asm volatile("661:\n\t" "" "\n662:\n" ".skip -((" "((" "665""1""f-""664""1""f" ") ^ (((" "665""1""f-""664""1""f" ") ^ (" "665""2""f-""664""2""f" ")) & -(-((" "665""1""f-""664""1""f" ") - (" "665""2""f-""664""2""f" ")))))" " - (" "662b-661b" ")) > 0) * " "(" "((" "665""1""f-""664""1""f" ") ^ (((" "665""1""f-""664""1""f" ") ^ (" "665""2""f-""664""2""f" ")) & -(-((" "665""1""f-""664""1""f" ") - (" "665""2""f-""664""2""f" ")))))" " - (" "662b-661b" ")), 0x90\n" "663" ":\n" ".pushsection .altinstructions,\"a\"\n" " .long 661b - .\n" " .long " "664""1""f - .\n" " .word " "( 3*32+17)" "\n" " .byte " "663""b-661b" "\n" " .byte " "665""1""f-""664""1""f" "\n" " .byte " "663""b-662b" "\n" " .long 661b - .\n" " .long " "664""2""f - .\n" " .word " "( 3*32+18)" "\n" " .byte " "663""b-661b" "\n" " .byte " "665""2""f-""664""2""f" "\n" " .byte " "663""b-662b" "\n" ".popsection\n" ".pushsection .altinstr_replacement, \"ax\"\n" "664""1"":\n\t" "mfence" "\n" "665""1" ":\n\t" "664""2"":\n\t" "lfence" "\n" "665""2" ":\n\t" ".popsection" ::: "memory");

 return Model1_rdtsc();
}

/* Deprecated, keep it for a cycle for easier merging: */


static inline __attribute__((no_instrument_function)) unsigned long long Model1_native_read_pmc(int Model1_counter)
{
 unsigned long Model1_low, Model1_high;

 asm volatile("rdpmc" : "=a" (Model1_low), "=d" (Model1_high) : "c" (Model1_counter));
 if (Model1_static_key_false(&(Model1___tracepoint_rdpmc).Model1_key))
  Model1_do_trace_rdpmc(Model1_counter, ((Model1_low) | (Model1_high) << 32), 0);
 return ((Model1_low) | (Model1_high) << 32);
}





/*
 * Access to machine-specific registers (available on 586 and better only)
 * Note: the rd* operations modify the parameters directly (without using
 * pointer indirection), this allows gcc to optimize better
 */
static inline __attribute__((no_instrument_function)) void Model1_wrmsr(unsigned Model1_msr, unsigned Model1_low, unsigned Model1_high)
{
 Model1_native_write_msr(Model1_msr, Model1_low, Model1_high);
}




static inline __attribute__((no_instrument_function)) void Model1_wrmsrl(unsigned Model1_msr, Model1_u64 Model1_val)
{
 Model1_native_write_msr(Model1_msr, (Model1_u32)(Model1_val & 0xffffffffULL), (Model1_u32)(Model1_val >> 32));
}

/* wrmsr with exception handling */
static inline __attribute__((no_instrument_function)) int Model1_wrmsr_safe(unsigned Model1_msr, unsigned Model1_low, unsigned Model1_high)
{
 return Model1_native_write_msr_safe(Model1_msr, Model1_low, Model1_high);
}

/* rdmsr with exception handling */
static inline __attribute__((no_instrument_function)) int Model1_rdmsrl_safe(unsigned Model1_msr, unsigned long long *Model1_p)
{
 int err;

 *Model1_p = Model1_native_read_msr_safe(Model1_msr, &err);
 return err;
}
/*
 * 64-bit version of wrmsr_safe():
 */
static inline __attribute__((no_instrument_function)) int Model1_wrmsrl_safe(Model1_u32 Model1_msr, Model1_u64 Model1_val)
{
 return Model1_wrmsr_safe(Model1_msr, (Model1_u32)Model1_val, (Model1_u32)(Model1_val >> 32));
}





struct Model1_msr *Model1_msrs_alloc(void);
void Model1_msrs_free(struct Model1_msr *Model1_msrs);
int Model1_msr_set_bit(Model1_u32 Model1_msr, Model1_u8 Model1_bit);
int Model1_msr_clear_bit(Model1_u32 Model1_msr, Model1_u8 Model1_bit);


int Model1_rdmsr_on_cpu(unsigned int Model1_cpu, Model1_u32 Model1_msr_no, Model1_u32 *Model1_l, Model1_u32 *Model1_h);
int Model1_wrmsr_on_cpu(unsigned int Model1_cpu, Model1_u32 Model1_msr_no, Model1_u32 Model1_l, Model1_u32 Model1_h);
int Model1_rdmsrl_on_cpu(unsigned int Model1_cpu, Model1_u32 Model1_msr_no, Model1_u64 *Model1_q);
int Model1_wrmsrl_on_cpu(unsigned int Model1_cpu, Model1_u32 Model1_msr_no, Model1_u64 Model1_q);
void Model1_rdmsr_on_cpus(const struct Model1_cpumask *Model1_mask, Model1_u32 Model1_msr_no, struct Model1_msr *Model1_msrs);
void Model1_wrmsr_on_cpus(const struct Model1_cpumask *Model1_mask, Model1_u32 Model1_msr_no, struct Model1_msr *Model1_msrs);
int Model1_rdmsr_safe_on_cpu(unsigned int Model1_cpu, Model1_u32 Model1_msr_no, Model1_u32 *Model1_l, Model1_u32 *Model1_h);
int Model1_wrmsr_safe_on_cpu(unsigned int Model1_cpu, Model1_u32 Model1_msr_no, Model1_u32 Model1_l, Model1_u32 Model1_h);
int Model1_rdmsrl_safe_on_cpu(unsigned int Model1_cpu, Model1_u32 Model1_msr_no, Model1_u64 *Model1_q);
int Model1_wrmsrl_safe_on_cpu(unsigned int Model1_cpu, Model1_u32 Model1_msr_no, Model1_u64 Model1_q);
int Model1_rdmsr_safe_regs_on_cpu(unsigned int Model1_cpu, Model1_u32 Model1_regs[8]);
int Model1_wrmsr_safe_regs_on_cpu(unsigned int Model1_cpu, Model1_u32 Model1_regs[8]);
/* Written 2000 by Andi Kleen */



/*
 * Segment descriptor structure definitions, usable from both x86_64 and i386
 * archs.
 */





/*
 * FIXME: Accessing the desc_struct through its fields is more elegant,
 * and should be the one valid thing to do. However, a lot of open code
 * still touches the a and b accessors, and doing this allow us to do it
 * incrementally. We keep the signature as a struct, rather than an union,
 * so we can get rid of it transparently in the future -- glommer
 */
/* 8 byte segment descriptor */
struct Model1_desc_struct {
 union {
  struct {
   unsigned int Model1_a;
   unsigned int Model1_b;
  };
  struct {
   Model1_u16 Model1_limit0;
   Model1_u16 Model1_base0;
   unsigned Model1_base1: 8, Model1_type: 4, Model1_s: 1, Model1_dpl: 2, Model1_p: 1;
   unsigned Model1_limit: 4, Model1_avl: 1, Model1_l: 1, Model1_d: 1, Model1_g: 1, Model1_base2: 8;
  };
 };
} __attribute__((packed));







enum {
 Model1_GATE_INTERRUPT = 0xE,
 Model1_GATE_TRAP = 0xF,
 Model1_GATE_CALL = 0xC,
 Model1_GATE_TASK = 0x5,
};

/* 16byte gate */
struct Model1_gate_struct64 {
 Model1_u16 Model1_offset_low;
 Model1_u16 Model1_segment;
 unsigned Model1_ist : 3, Model1_zero0 : 5, Model1_type : 5, Model1_dpl : 2, Model1_p : 1;
 Model1_u16 Model1_offset_middle;
 Model1_u32 Model1_offset_high;
 Model1_u32 Model1_zero1;
} __attribute__((packed));





enum {
 Model1_DESC_TSS = 0x9,
 Model1_DESC_LDT = 0x2,
 Model1_DESCTYPE_S = 0x10, /* !system */
};

/* LDT or TSS descriptor in the GDT. 16 bytes. */
struct Model1_ldttss_desc64 {
 Model1_u16 Model1_limit0;
 Model1_u16 Model1_base0;
 unsigned Model1_base1 : 8, Model1_type : 5, Model1_dpl : 2, Model1_p : 1;
 unsigned Model1_limit1 : 4, Model1_zero0 : 3, Model1_g : 1, Model1_base2 : 8;
 Model1_u32 Model1_base3;
 Model1_u32 Model1_zero1;
} __attribute__((packed));


typedef struct Model1_gate_struct64 Model1_gate_desc;
typedef struct Model1_ldttss_desc64 Model1_ldt_desc;
typedef struct Model1_ldttss_desc64 Model1_tss_desc;
struct Model1_desc_ptr {
 unsigned short Model1_size;
 unsigned long Model1_address;
} __attribute__((packed)) ;



/* Access rights as returned by LAR */









static inline __attribute__((no_instrument_function)) void Model1_native_clts(void)
{
 asm volatile("clts");
}

/*
 * Volatile isn't enough to prevent the compiler from reordering the
 * read/write functions for the control registers and messing everything up.
 * A memory clobber would solve the problem, but would prevent reordering of
 * all loads stores around it, which can hurt performance. Solution is to
 * use a variable and mimic reads and writes to it to enforce serialization
 */
extern unsigned long Model1___force_order;

static inline __attribute__((no_instrument_function)) unsigned long Model1_native_read_cr0(void)
{
 unsigned long Model1_val;
 asm volatile("mov %%cr0,%0\n\t" : "=r" (Model1_val), "=m" (Model1___force_order));
 return Model1_val;
}

static inline __attribute__((no_instrument_function)) void Model1_native_write_cr0(unsigned long Model1_val)
{
 asm volatile("mov %0,%%cr0": : "r" (Model1_val), "m" (Model1___force_order));
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_native_read_cr2(void)
{
 unsigned long Model1_val;
 asm volatile("mov %%cr2,%0\n\t" : "=r" (Model1_val), "=m" (Model1___force_order));
 return Model1_val;
}

static inline __attribute__((no_instrument_function)) void Model1_native_write_cr2(unsigned long Model1_val)
{
 asm volatile("mov %0,%%cr2": : "r" (Model1_val), "m" (Model1___force_order));
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_native_read_cr3(void)
{
 unsigned long Model1_val;
 asm volatile("mov %%cr3,%0\n\t" : "=r" (Model1_val), "=m" (Model1___force_order));
 return Model1_val;
}

static inline __attribute__((no_instrument_function)) void Model1_native_write_cr3(unsigned long Model1_val)
{
 asm volatile("mov %0,%%cr3": : "r" (Model1_val), "m" (Model1___force_order));
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_native_read_cr4(void)
{
 unsigned long Model1_val;
 asm volatile("mov %%cr4,%0\n\t" : "=r" (Model1_val), "=m" (Model1___force_order));
 return Model1_val;
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_native_read_cr4_safe(void)
{
 unsigned long Model1_val;
 /* This could fault if %cr4 does not exist. In x86_64, a cr4 always
	 * exists, so it will never fail. */






 Model1_val = Model1_native_read_cr4();

 return Model1_val;
}

static inline __attribute__((no_instrument_function)) void Model1_native_write_cr4(unsigned long Model1_val)
{
 asm volatile("mov %0,%%cr4": : "r" (Model1_val), "m" (Model1___force_order));
}


static inline __attribute__((no_instrument_function)) unsigned long Model1_native_read_cr8(void)
{
 unsigned long Model1_cr8;
 asm volatile("movq %%cr8,%0" : "=r" (Model1_cr8));
 return Model1_cr8;
}

static inline __attribute__((no_instrument_function)) void Model1_native_write_cr8(unsigned long Model1_val)
{
 asm volatile("movq %0,%%cr8" :: "r" (Model1_val) : "memory");
}



static inline __attribute__((no_instrument_function)) Model1_u32 Model1___read_pkru(void)
{
 Model1_u32 Model1_ecx = 0;
 Model1_u32 Model1_edx, Model1_pkru;

 /*
	 * "rdpkru" instruction.  Places PKRU contents in to EAX,
	 * clears EDX and requires that ecx=0.
	 */
 asm volatile(".byte 0x0f,0x01,0xee\n\t"
       : "=a" (Model1_pkru), "=d" (Model1_edx)
       : "c" (Model1_ecx));
 return Model1_pkru;
}

static inline __attribute__((no_instrument_function)) void Model1___write_pkru(Model1_u32 Model1_pkru)
{
 Model1_u32 Model1_ecx = 0, Model1_edx = 0;

 /*
	 * "wrpkru" instruction.  Loads contents in EAX to PKRU,
	 * requires that ecx = edx = 0.
	 */
 asm volatile(".byte 0x0f,0x01,0xef\n\t"
       : : "a" (Model1_pkru), "c"(Model1_ecx), "d"(Model1_edx));
}
static inline __attribute__((no_instrument_function)) void Model1_native_wbinvd(void)
{
 asm volatile("wbinvd": : :"memory");
}

extern void Model1_native_load_gs_index(unsigned);





static inline __attribute__((no_instrument_function)) unsigned long Model1_read_cr0(void)
{
 return Model1_native_read_cr0();
}

static inline __attribute__((no_instrument_function)) void Model1_write_cr0(unsigned long Model1_x)
{
 Model1_native_write_cr0(Model1_x);
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_read_cr2(void)
{
 return Model1_native_read_cr2();
}

static inline __attribute__((no_instrument_function)) void Model1_write_cr2(unsigned long Model1_x)
{
 Model1_native_write_cr2(Model1_x);
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_read_cr3(void)
{
 return Model1_native_read_cr3();
}

static inline __attribute__((no_instrument_function)) void Model1_write_cr3(unsigned long Model1_x)
{
 Model1_native_write_cr3(Model1_x);
}

static inline __attribute__((no_instrument_function)) unsigned long Model1___read_cr4(void)
{
 return Model1_native_read_cr4();
}

static inline __attribute__((no_instrument_function)) unsigned long Model1___read_cr4_safe(void)
{
 return Model1_native_read_cr4_safe();
}

static inline __attribute__((no_instrument_function)) void Model1___write_cr4(unsigned long Model1_x)
{
 Model1_native_write_cr4(Model1_x);
}

static inline __attribute__((no_instrument_function)) void Model1_wbinvd(void)
{
 Model1_native_wbinvd();
}



static inline __attribute__((no_instrument_function)) unsigned long Model1_read_cr8(void)
{
 return Model1_native_read_cr8();
}

static inline __attribute__((no_instrument_function)) void Model1_write_cr8(unsigned long Model1_x)
{
 Model1_native_write_cr8(Model1_x);
}

static inline __attribute__((no_instrument_function)) void Model1_load_gs_index(unsigned Model1_selector)
{
 Model1_native_load_gs_index(Model1_selector);
}



/* Clear the 'TS' bit */
static inline __attribute__((no_instrument_function)) void Model1_clts(void)
{
 Model1_native_clts();
}





static inline __attribute__((no_instrument_function)) void Model1_clflush(volatile void *Model1___p)
{
 asm volatile("clflush %0" : "+m" (*(volatile char *)Model1___p));
}

static inline __attribute__((no_instrument_function)) void Model1_clflushopt(volatile void *Model1___p)
{
 asm volatile ("661:\n\t" ".byte " "0x3e" "; clflush %P0" "\n662:\n" ".skip -(((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")) > 0) * " "((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")),0x90\n" "663" ":\n" ".pushsection .altinstructions,\"a\"\n" " .long 661b - .\n" " .long " "664""1""f - .\n" " .word " "( 9*32+23)" "\n" " .byte " "663""b-661b" "\n" " .byte " "665""1""f-""664""1""f" "\n" " .byte " "663""b-662b" "\n" ".popsection\n" ".pushsection .altinstr_replacement, \"ax\"\n" "664""1"":\n\t" ".byte 0x66; clflush %P0" "\n" "665""1" ":\n\t" ".popsection" : "+m" (*(volatile char *)Model1___p) : "i" (0));



}

static inline __attribute__((no_instrument_function)) void Model1_clwb(volatile void *Model1___p)
{
 volatile struct { char Model1_x[64]; } *Model1_p = Model1___p;

 asm volatile("661:\n\t" ".byte " "0x3e" "; clflush (%[pax])" "\n662:\n" ".skip -((" "((" "665""1""f-""664""1""f" ") ^ (((" "665""1""f-""664""1""f" ") ^ (" "665""2""f-""664""2""f" ")) & -(-((" "665""1""f-""664""1""f" ") - (" "665""2""f-""664""2""f" ")))))" " - (" "662b-661b" ")) > 0) * " "(" "((" "665""1""f-""664""1""f" ") ^ (((" "665""1""f-""664""1""f" ") ^ (" "665""2""f-""664""2""f" ")) & -(-((" "665""1""f-""664""1""f" ") - (" "665""2""f-""664""2""f" ")))))" " - (" "662b-661b" ")), 0x90\n" "663" ":\n" ".pushsection .altinstructions,\"a\"\n" " .long 661b - .\n" " .long " "664""1""f - .\n" " .word " "( 9*32+23)" "\n" " .byte " "663""b-661b" "\n" " .byte " "665""1""f-""664""1""f" "\n" " .byte " "663""b-662b" "\n" " .long 661b - .\n" " .long " "664""2""f - .\n" " .word " "( 9*32+24)" "\n" " .byte " "663""b-661b" "\n" " .byte " "665""2""f-""664""2""f" "\n" " .byte " "663""b-662b" "\n" ".popsection\n" ".pushsection .altinstr_replacement, \"ax\"\n" "664""1"":\n\t" ".byte 0x66; clflush (%[pax])" "\n" "665""1" ":\n\t" "664""2"":\n\t" ".byte 0x66, 0x0f, 0xae, 0x30" "\n" "665""2" ":\n\t" ".popsection"





  : [Model1_p] "+m" (*Model1_p)
  : [pax] "a" (Model1_p));
}
/*
 * FPU data structures:
 */



/*
 * The legacy x87 FPU state format, as saved by FSAVE and
 * restored by the FRSTOR instructions:
 */
struct Model1_fregs_state {
 Model1_u32 Model1_cwd; /* FPU Control Word		*/
 Model1_u32 Model1_swd; /* FPU Status Word		*/
 Model1_u32 Model1_twd; /* FPU Tag Word			*/
 Model1_u32 Model1_fip; /* FPU IP Offset		*/
 Model1_u32 Model1_fcs; /* FPU IP Selector		*/
 Model1_u32 Model1_foo; /* FPU Operand Pointer Offset	*/
 Model1_u32 Model1_fos; /* FPU Operand Pointer Selector	*/

 /* 8*10 bytes for each FP-reg = 80 bytes:			*/
 Model1_u32 Model1_st_space[20];

 /* Software status information [not touched by FSAVE]:		*/
 Model1_u32 Model1_status;
};

/*
 * The legacy fx SSE/MMX FPU state format, as saved by FXSAVE and
 * restored by the FXRSTOR instructions. It's similar to the FSAVE
 * format, but differs in some areas, plus has extensions at
 * the end for the XMM registers.
 */
struct Model1_fxregs_state {
 Model1_u16 Model1_cwd; /* Control Word			*/
 Model1_u16 Model1_swd; /* Status Word			*/
 Model1_u16 Model1_twd; /* Tag Word			*/
 Model1_u16 Model1_fop; /* Last Instruction Opcode		*/
 union {
  struct {
   Model1_u64 Model1_rip; /* Instruction Pointer		*/
   Model1_u64 Model1_rdp; /* Data Pointer			*/
  };
  struct {
   Model1_u32 Model1_fip; /* FPU IP Offset			*/
   Model1_u32 Model1_fcs; /* FPU IP Selector			*/
   Model1_u32 Model1_foo; /* FPU Operand Offset		*/
   Model1_u32 Model1_fos; /* FPU Operand Selector		*/
  };
 };
 Model1_u32 Model1_mxcsr; /* MXCSR Register State */
 Model1_u32 Model1_mxcsr_mask; /* MXCSR Mask		*/

 /* 8*16 bytes for each FP-reg = 128 bytes:			*/
 Model1_u32 Model1_st_space[32];

 /* 16*16 bytes for each XMM-reg = 256 bytes:			*/
 Model1_u32 Model1_xmm_space[64];

 Model1_u32 Model1_padding[12];

 union {
  Model1_u32 Model1_padding1[12];
  Model1_u32 Model1_sw_reserved[12];
 };

} __attribute__((aligned(16)));

/* Default value for fxregs_state.mxcsr: */


/*
 * Software based FPU emulation state. This is arbitrary really,
 * it matches the x87 format to make it easier to understand:
 */
struct Model1_swregs_state {
 Model1_u32 Model1_cwd;
 Model1_u32 Model1_swd;
 Model1_u32 Model1_twd;
 Model1_u32 Model1_fip;
 Model1_u32 Model1_fcs;
 Model1_u32 Model1_foo;
 Model1_u32 Model1_fos;
 /* 8*10 bytes for each FP-reg = 80 bytes: */
 Model1_u32 Model1_st_space[20];
 Model1_u8 Model1_ftop;
 Model1_u8 Model1_changed;
 Model1_u8 Model1_lookahead;
 Model1_u8 Model1_no_update;
 Model1_u8 Model1_rm;
 Model1_u8 Model1_alimit;
 struct Model1_math_emu_info *Model1_info;
 Model1_u32 Model1_entry_eip;
};

/*
 * List of XSAVE features Linux knows about:
 */
enum Model1_xfeature {
 Model1_XFEATURE_FP,
 Model1_XFEATURE_SSE,
 /*
	 * Values above here are "legacy states".
	 * Those below are "extended states".
	 */
 Model1_XFEATURE_YMM,
 Model1_XFEATURE_BNDREGS,
 Model1_XFEATURE_BNDCSR,
 Model1_XFEATURE_OPMASK,
 Model1_XFEATURE_ZMM_Hi256,
 Model1_XFEATURE_Hi16_ZMM,
 Model1_XFEATURE_PT_UNIMPLEMENTED_SO_FAR,
 Model1_XFEATURE_PKRU,

 Model1_XFEATURE_MAX,
};
struct Model1_reg_128_bit {
 Model1_u8 Model1_regbytes[128/8];
};
struct Model1_reg_256_bit {
 Model1_u8 Model1_regbytes[256/8];
};
struct Model1_reg_512_bit {
 Model1_u8 Model1_regbytes[512/8];
};

/*
 * State component 2:
 *
 * There are 16x 256-bit AVX registers named YMM0-YMM15.
 * The low 128 bits are aliased to the 16 SSE registers (XMM0-XMM15)
 * and are stored in 'struct fxregs_state::xmm_space[]' in the
 * "legacy" area.
 *
 * The high 128 bits are stored here.
 */
struct Model1_ymmh_struct {
 struct Model1_reg_128_bit Model1_hi_ymm[16];
} __attribute__((packed));

/* Intel MPX support: */

struct Model1_mpx_bndreg {
 Model1_u64 Model1_lower_bound;
 Model1_u64 Model1_upper_bound;
} __attribute__((packed));
/*
 * State component 3 is used for the 4 128-bit bounds registers
 */
struct Model1_mpx_bndreg_state {
 struct Model1_mpx_bndreg Model1_bndreg[4];
} __attribute__((packed));

/*
 * State component 4 is used for the 64-bit user-mode MPX
 * configuration register BNDCFGU and the 64-bit MPX status
 * register BNDSTATUS.  We call the pair "BNDCSR".
 */
struct Model1_mpx_bndcsr {
 Model1_u64 Model1_bndcfgu;
 Model1_u64 Model1_bndstatus;
} __attribute__((packed));

/*
 * The BNDCSR state is padded out to be 64-bytes in size.
 */
struct Model1_mpx_bndcsr_state {
 union {
  struct Model1_mpx_bndcsr Model1_bndcsr;
  Model1_u8 Model1_pad_to_64_bytes[64];
 };
} __attribute__((packed));

/* AVX-512 Components: */

/*
 * State component 5 is used for the 8 64-bit opmask registers
 * k0-k7 (opmask state).
 */
struct Model1_avx_512_opmask_state {
 Model1_u64 Model1_opmask_reg[8];
} __attribute__((packed));

/*
 * State component 6 is used for the upper 256 bits of the
 * registers ZMM0-ZMM15. These 16 256-bit values are denoted
 * ZMM0_H-ZMM15_H (ZMM_Hi256 state).
 */
struct Model1_avx_512_zmm_uppers_state {
 struct Model1_reg_256_bit Model1_zmm_upper[16];
} __attribute__((packed));

/*
 * State component 7 is used for the 16 512-bit registers
 * ZMM16-ZMM31 (Hi16_ZMM state).
 */
struct Model1_avx_512_hi16_state {
 struct Model1_reg_512_bit Model1_hi16_zmm[16];
} __attribute__((packed));

/*
 * State component 9: 32-bit PKRU register.  The state is
 * 8 bytes long but only 4 bytes is used currently.
 */
struct Model1_pkru_state {
 Model1_u32 Model1_pkru;
 Model1_u32 Model1_pad;
} __attribute__((packed));

struct Model1_xstate_header {
 Model1_u64 Model1_xfeatures;
 Model1_u64 Model1_xcomp_bv;
 Model1_u64 Model1_reserved[6];
} __attribute__((packed));

/*
 * xstate_header.xcomp_bv[63] indicates that the extended_state_area
 * is in compacted format.
 */


/*
 * This is our most modern FPU state format, as saved by the XSAVE
 * and restored by the XRSTOR instructions.
 *
 * It consists of a legacy fxregs portion, an xstate header and
 * subsequent areas as defined by the xstate header.  Not all CPUs
 * support all the extensions, so the size of the extended area
 * can vary quite a bit between CPUs.
 */
struct Model1_xregs_state {
 struct Model1_fxregs_state Model1_i387;
 struct Model1_xstate_header Model1_header;
 Model1_u8 Model1_extended_state_area[0];
} __attribute__ ((packed, aligned (64)));

/*
 * This is a union of all the possible FPU state formats
 * put together, so that we can pick the right one runtime.
 *
 * The size of the structure is determined by the largest
 * member - which is the xsave area.  The padding is there
 * to ensure that statically-allocated task_structs (just
 * the init_task today) have enough space.
 */
union Model1_fpregs_state {
 struct Model1_fregs_state Model1_fsave;
 struct Model1_fxregs_state Model1_fxsave;
 struct Model1_swregs_state Model1_soft;
 struct Model1_xregs_state Model1_xsave;
 Model1_u8 Model1___padding[((1UL) << 12)];
};

/*
 * Highest level per task FPU state data structure that
 * contains the FPU register state plus various FPU
 * state fields:
 */
struct Model1_fpu {
 /*
	 * @last_cpu:
	 *
	 * Records the last CPU on which this context was loaded into
	 * FPU registers. (In the lazy-restore case we might be
	 * able to reuse FPU registers across multiple context switches
	 * this way, if no intermediate task used the FPU.)
	 *
	 * A value of -1 is used to indicate that the FPU state in context
	 * memory is newer than the FPU state in registers, and that the
	 * FPU state should be reloaded next time the task is run.
	 */
 unsigned int Model1_last_cpu;

 /*
	 * @fpstate_active:
	 *
	 * This flag indicates whether this context is active: if the task
	 * is not running then we can restore from this context, if the task
	 * is running then we should save into this context.
	 */
 unsigned char Model1_fpstate_active;

 /*
	 * @fpregs_active:
	 *
	 * This flag determines whether a given context is actively
	 * loaded into the FPU's registers and that those registers
	 * represent the task's current FPU state.
	 *
	 * Note the interaction with fpstate_active:
	 *
	 *   # task does not use the FPU:
	 *   fpstate_active == 0
	 *
	 *   # task uses the FPU and regs are active:
	 *   fpstate_active == 1 && fpregs_active == 1
	 *
	 *   # the regs are inactive but still match fpstate:
	 *   fpstate_active == 1 && fpregs_active == 0 && fpregs_owner == fpu
	 *
	 * The third state is what we use for the lazy restore optimization
	 * on lazy-switching CPUs.
	 */
 unsigned char Model1_fpregs_active;

 /*
	 * @counter:
	 *
	 * This counter contains the number of consecutive context switches
	 * during which the FPU stays used. If this is over a threshold, the
	 * lazy FPU restore logic becomes eager, to save the trap overhead.
	 * This is an unsigned char so that after 256 iterations the counter
	 * wraps and the context switch behavior turns lazy again; this is to
	 * deal with bursty apps that only use the FPU for a short time:
	 */
 unsigned char Model1_counter;
 /*
	 * @state:
	 *
	 * In-memory copy of all FPU registers that we save/restore
	 * over context switches. If the task is using the FPU then
	 * the registers in the FPU are more recent than this state
	 * copy. If the task context-switches away then they get
	 * saved here and represent the FPU state.
	 *
	 * After context switches there may be a (short) time period
	 * during which the in-FPU hardware registers are unchanged
	 * and still perfectly match this state, if the tasks
	 * scheduled afterwards are not using the FPU.
	 *
	 * This is the 'lazy restore' window of optimization, which
	 * we track though 'fpu_fpregs_owner_ctx' and 'fpu->last_cpu'.
	 *
	 * We detect whether a subsequent task uses the FPU via setting
	 * CR0::TS to 1, which causes any FPU use to raise a #NM fault.
	 *
	 * During this window, if the task gets scheduled again, we
	 * might be able to skip having to do a restore from this
	 * memory buffer to the hardware registers - at the cost of
	 * incurring the overhead of #NM fault traps.
	 *
	 * Note that on modern CPUs that support the XSAVEOPT (or other
	 * optimized XSAVE instructions), we don't use #NM traps anymore,
	 * as the hardware can track whether FPU registers need saving
	 * or not. On such CPUs we activate the non-lazy ('eagerfpu')
	 * logic, which unconditionally saves/restores all FPU state
	 * across context switches. (if FPU state exists.)
	 */
 union Model1_fpregs_state Model1_state;
 /*
	 * WARNING: 'state' is dynamically-sized.  Do not put
	 * anything after it here.
	 */
};








/*
 * Flags for bug emulation.
 *
 * These occupy the top three bytes.
 */
enum {
 Model1_UNAME26 = 0x0020000,
 Model1_ADDR_NO_RANDOMIZE = 0x0040000, /* disable randomization of VA space */
 Model1_FDPIC_FUNCPTRS = 0x0080000, /* userspace function ptrs point to descriptors
						 * (signal handling)
						 */
 Model1_MMAP_PAGE_ZERO = 0x0100000,
 Model1_ADDR_COMPAT_LAYOUT = 0x0200000,
 Model1_READ_IMPLIES_EXEC = 0x0400000,
 Model1_ADDR_LIMIT_32BIT = 0x0800000,
 Model1_SHORT_INODE = 0x1000000,
 Model1_WHOLE_SECONDS = 0x2000000,
 Model1_STICKY_TIMEOUTS = 0x4000000,
 Model1_ADDR_LIMIT_3GB = 0x8000000,
};

/*
 * Security-relevant compatibility flags that must be
 * cleared upon setuid or setgid exec:
 */





/*
 * Personality types.
 *
 * These go in the low byte.  Avoid using the top bit, it will
 * conflict with error returns.
 */
enum {
 Model1_PER_LINUX = 0x0000,
 Model1_PER_LINUX_32BIT = 0x0000 | Model1_ADDR_LIMIT_32BIT,
 Model1_PER_LINUX_FDPIC = 0x0000 | Model1_FDPIC_FUNCPTRS,
 Model1_PER_SVR4 = 0x0001 | Model1_STICKY_TIMEOUTS | Model1_MMAP_PAGE_ZERO,
 Model1_PER_SVR3 = 0x0002 | Model1_STICKY_TIMEOUTS | Model1_SHORT_INODE,
 Model1_PER_SCOSVR3 = 0x0003 | Model1_STICKY_TIMEOUTS |
      Model1_WHOLE_SECONDS | Model1_SHORT_INODE,
 Model1_PER_OSR5 = 0x0003 | Model1_STICKY_TIMEOUTS | Model1_WHOLE_SECONDS,
 Model1_PER_WYSEV386 = 0x0004 | Model1_STICKY_TIMEOUTS | Model1_SHORT_INODE,
 Model1_PER_ISCR4 = 0x0005 | Model1_STICKY_TIMEOUTS,
 Model1_PER_BSD = 0x0006,
 Model1_PER_SUNOS = 0x0006 | Model1_STICKY_TIMEOUTS,
 Model1_PER_XENIX = 0x0007 | Model1_STICKY_TIMEOUTS | Model1_SHORT_INODE,
 Model1_PER_LINUX32 = 0x0008,
 Model1_PER_LINUX32_3GB = 0x0008 | Model1_ADDR_LIMIT_3GB,
 Model1_PER_IRIX32 = 0x0009 | Model1_STICKY_TIMEOUTS,/* IRIX5 32-bit */
 Model1_PER_IRIXN32 = 0x000a | Model1_STICKY_TIMEOUTS,/* IRIX6 new 32-bit */
 Model1_PER_IRIX64 = 0x000b | Model1_STICKY_TIMEOUTS,/* IRIX6 64-bit */
 Model1_PER_RISCOS = 0x000c,
 Model1_PER_SOLARIS = 0x000d | Model1_STICKY_TIMEOUTS,
 Model1_PER_UW7 = 0x000e | Model1_STICKY_TIMEOUTS | Model1_MMAP_PAGE_ZERO,
 Model1_PER_OSF4 = 0x000f, /* OSF/1 v4 */
 Model1_PER_HPUX = 0x0010,
 Model1_PER_MASK = 0x00ff,
};

/*
 * Return the base personality without flags.
 */


/*
 * Change personality of the currently running process.
 */








/*
 * Copyright (C) 2003 Bernardo Innocenti <bernie@develer.com>
 * Based on former asm-ppc/div64.h and asm-m68knommu/div64.h
 *
 * Optimization for constant divisors on 32-bit machines:
 * Copyright (C) 2006-2015 Nicolas Pitre
 *
 * The semantics of do_div() are:
 *
 * uint32_t do_div(uint64_t *n, uint32_t base)
 * {
 * 	uint32_t remainder = *n % base;
 * 	*n = *n / base;
 * 	return remainder;
 * }
 *
 * NOTE: macro parameter n is evaluated multiple times,
 *       beware of side effects!
 */






/**
 * div_u64_rem - unsigned 64bit divide with 32bit divisor with remainder
 *
 * This is commonly provided by 32bit archs to provide an optimized 64bit
 * divide.
 */
static inline __attribute__((no_instrument_function)) Model1_u64 Model1_div_u64_rem(Model1_u64 Model1_dividend, Model1_u32 Model1_divisor, Model1_u32 *Model1_remainder)
{
 *Model1_remainder = Model1_dividend % Model1_divisor;
 return Model1_dividend / Model1_divisor;
}

/**
 * div_s64_rem - signed 64bit divide with 32bit divisor with remainder
 */
static inline __attribute__((no_instrument_function)) Model1_s64 Model1_div_s64_rem(Model1_s64 Model1_dividend, Model1_s32 Model1_divisor, Model1_s32 *Model1_remainder)
{
 *Model1_remainder = Model1_dividend % Model1_divisor;
 return Model1_dividend / Model1_divisor;
}

/**
 * div64_u64_rem - unsigned 64bit divide with 64bit divisor and remainder
 */
static inline __attribute__((no_instrument_function)) Model1_u64 Model1_div64_u64_rem(Model1_u64 Model1_dividend, Model1_u64 Model1_divisor, Model1_u64 *Model1_remainder)
{
 *Model1_remainder = Model1_dividend % Model1_divisor;
 return Model1_dividend / Model1_divisor;
}

/**
 * div64_u64 - unsigned 64bit divide with 64bit divisor
 */
static inline __attribute__((no_instrument_function)) Model1_u64 Model1_div64_u64(Model1_u64 Model1_dividend, Model1_u64 Model1_divisor)
{
 return Model1_dividend / Model1_divisor;
}

/**
 * div64_s64 - signed 64bit divide with 64bit divisor
 */
static inline __attribute__((no_instrument_function)) Model1_s64 Model1_div64_s64(Model1_s64 Model1_dividend, Model1_s64 Model1_divisor)
{
 return Model1_dividend / Model1_divisor;
}
/**
 * div_u64 - unsigned 64bit divide with 32bit divisor
 *
 * This is the most common 64bit divide and should be used if possible,
 * as many 32bit archs can optimize this variant better than a full 64bit
 * divide.
 */

static inline __attribute__((no_instrument_function)) Model1_u64 Model1_div_u64(Model1_u64 Model1_dividend, Model1_u32 Model1_divisor)
{
 Model1_u32 Model1_remainder;
 return Model1_div_u64_rem(Model1_dividend, Model1_divisor, &Model1_remainder);
}


/**
 * div_s64 - signed 64bit divide with 32bit divisor
 */

static inline __attribute__((no_instrument_function)) Model1_s64 Model1_div_s64(Model1_s64 Model1_dividend, Model1_s32 Model1_divisor)
{
 Model1_s32 Model1_remainder;
 return Model1_div_s64_rem(Model1_dividend, Model1_divisor, &Model1_remainder);
}


Model1_u32 Model1_iter_div_u64_rem(Model1_u64 Model1_dividend, Model1_u32 Model1_divisor, Model1_u64 *Model1_remainder);

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model1_u32
Model1___iter_div_u64_rem(Model1_u64 Model1_dividend, Model1_u32 Model1_divisor, Model1_u64 *Model1_remainder)
{
 Model1_u32 Model1_ret = 0;

 while (Model1_dividend >= Model1_divisor) {
  /* The following asm() prevents the compiler from
		   optimising this loop into a modulo operation.  */
  asm("" : "+rm"(Model1_dividend));

  Model1_dividend -= Model1_divisor;
  Model1_ret++;
 }

 *Model1_remainder = Model1_dividend;

 return Model1_ret;
}




static inline __attribute__((no_instrument_function)) Model1_u64 Model1_mul_u64_u32_shr(Model1_u64 Model1_a, Model1_u32 Model1_mul, unsigned int Model1_shift)
{
 return (Model1_u64)(((unsigned __int128)Model1_a * Model1_mul) >> Model1_shift);
}



static inline __attribute__((no_instrument_function)) Model1_u64 Model1_mul_u64_u64_shr(Model1_u64 Model1_a, Model1_u64 Model1_mul, unsigned int Model1_shift)
{
 return (Model1_u64)(((unsigned __int128)Model1_a * Model1_mul) >> Model1_shift);
}
static inline __attribute__((no_instrument_function)) Model1_u64 Model1_mul_u64_u32_div(Model1_u64 Model1_a, Model1_u32 Model1_mul, Model1_u32 Model1_divisor)
{
 union {
  Model1_u64 Model1_ll;
  struct {



   Model1_u32 Model1_low, Model1_high;

  } Model1_l;
 } Model1_u, Model1_rl, Model1_rh;

 Model1_u.Model1_ll = Model1_a;
 Model1_rl.Model1_ll = (Model1_u64)Model1_u.Model1_l.Model1_low * Model1_mul;
 Model1_rh.Model1_ll = (Model1_u64)Model1_u.Model1_l.Model1_high * Model1_mul + Model1_rl.Model1_l.Model1_high;

 /* Bits 32-63 of the result will be in rh.l.low. */
 Model1_rl.Model1_l.Model1_high = ({ Model1_uint32_t Model1___base = (Model1_divisor); Model1_uint32_t Model1___rem; Model1___rem = ((Model1_uint64_t)(Model1_rh.Model1_ll)) % Model1___base; (Model1_rh.Model1_ll) = ((Model1_uint64_t)(Model1_rh.Model1_ll)) / Model1___base; Model1___rem; });

 /* Bits 0-31 of the result will be in rl.l.low.	*/
 ({ Model1_uint32_t Model1___base = (Model1_divisor); Model1_uint32_t Model1___rem; Model1___rem = ((Model1_uint64_t)(Model1_rl.Model1_ll)) % Model1___base; (Model1_rl.Model1_ll) = ((Model1_uint64_t)(Model1_rl.Model1_ll)) / Model1___base; Model1___rem; });

 Model1_rl.Model1_l.Model1_high = Model1_rh.Model1_l.Model1_low;
 return Model1_rl.Model1_ll;
}







/*
 * Kernel pointers have redundant information, so we can use a
 * scheme where we can return either an error code or a normal
 * pointer with the same return value.
 *
 * This should be a per-architecture thing, to allow different
 * error and pointer decisions.
 */






static inline __attribute__((no_instrument_function)) void * __attribute__((warn_unused_result)) Model1_ERR_PTR(long error)
{
 return (void *) error;
}

static inline __attribute__((no_instrument_function)) long __attribute__((warn_unused_result)) Model1_PTR_ERR( const void *Model1_ptr)
{
 return (long) Model1_ptr;
}

static inline __attribute__((no_instrument_function)) bool __attribute__((warn_unused_result)) Model1_IS_ERR( const void *Model1_ptr)
{
 return __builtin_expect(!!((unsigned long)(void *)((unsigned long)Model1_ptr) >= (unsigned long)-4095), 0);
}

static inline __attribute__((no_instrument_function)) bool __attribute__((warn_unused_result)) Model1_IS_ERR_OR_NULL( const void *Model1_ptr)
{
 return __builtin_expect(!!(!Model1_ptr), 0) || __builtin_expect(!!((unsigned long)(void *)((unsigned long)Model1_ptr) >= (unsigned long)-4095), 0);
}

/**
 * ERR_CAST - Explicitly cast an error-valued pointer to another pointer type
 * @ptr: The pointer to cast.
 *
 * Explicitly cast an error-valued pointer to another pointer type in such a
 * way as to make it clear that's what's going on.
 */
static inline __attribute__((no_instrument_function)) void * __attribute__((warn_unused_result)) Model1_ERR_CAST( const void *Model1_ptr)
{
 /* cast away the const */
 return (void *) Model1_ptr;
}

static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result)) Model1_PTR_ERR_OR_ZERO( const void *Model1_ptr)
{
 if (Model1_IS_ERR(Model1_ptr))
  return Model1_PTR_ERR(Model1_ptr);
 else
  return 0;
}

/* Deprecated */
/*
 * include/linux/irqflags.h
 *
 * IRQ flags tracing: follow the state of the hardirq and softirq flags and
 * provide callbacks for transitions between ON and OFF states.
 *
 * This file gets included from lowlevel asm headers too, to provide
 * wrapped versions of the local_irq_*() APIs, based on the
 * raw_local_irq_*() macros from the lowlevel headers.
 */











/*
 * Interrupt control:
 */

static inline __attribute__((no_instrument_function)) unsigned long Model1_native_save_fl(void)
{
 unsigned long Model1_flags;

 /*
	 * "=rm" is safe here, because "pop" adjusts the stack before
	 * it evaluates its effective address -- this is part of the
	 * documented behavior of the "pop" instruction.
	 */
 asm volatile("# __raw_save_flags\n\t"
       "pushf ; pop %0"
       : "=rm" (Model1_flags)
       : /* no input */
       : "memory");

 return Model1_flags;
}

static inline __attribute__((no_instrument_function)) void Model1_native_restore_fl(unsigned long Model1_flags)
{
 asm volatile("push %0 ; popf"
       : /* no output */
       :"g" (Model1_flags)
       :"memory", "cc");
}

static inline __attribute__((no_instrument_function)) void Model1_native_irq_disable(void)
{
 asm volatile("cli": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model1_native_irq_enable(void)
{
 asm volatile("sti": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model1_native_safe_halt(void)
{
 asm volatile("sti; hlt": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model1_native_halt(void)
{
 asm volatile("hlt": : :"memory");
}
static inline __attribute__((no_instrument_function)) __attribute__((no_instrument_function)) unsigned long Model1_arch_local_save_flags(void)
{
 return Model1_native_save_fl();
}

static inline __attribute__((no_instrument_function)) __attribute__((no_instrument_function)) void Model1_arch_local_irq_restore(unsigned long Model1_flags)
{
 Model1_native_restore_fl(Model1_flags);
}

static inline __attribute__((no_instrument_function)) __attribute__((no_instrument_function)) void Model1_arch_local_irq_disable(void)
{
 Model1_native_irq_disable();
}

static inline __attribute__((no_instrument_function)) __attribute__((no_instrument_function)) void Model1_arch_local_irq_enable(void)
{
 Model1_native_irq_enable();
}

/*
 * Used in the idle loop; sti takes one instruction cycle
 * to complete:
 */
static inline __attribute__((no_instrument_function)) void Model1_arch_safe_halt(void)
{
 Model1_native_safe_halt();
}

/*
 * Used when interrupts are already enabled or to
 * shutdown the processor:
 */
static inline __attribute__((no_instrument_function)) void Model1_halt(void)
{
 Model1_native_halt();
}

/*
 * For spinlocks, etc:
 */
static inline __attribute__((no_instrument_function)) __attribute__((no_instrument_function)) unsigned long Model1_arch_local_irq_save(void)
{
 unsigned long Model1_flags = Model1_arch_local_save_flags();
 Model1_arch_local_irq_disable();
 return Model1_flags;
}
static inline __attribute__((no_instrument_function)) int Model1_arch_irqs_disabled_flags(unsigned long Model1_flags)
{
 return !(Model1_flags & ((1UL) << (9)));
}

static inline __attribute__((no_instrument_function)) int Model1_arch_irqs_disabled(void)
{
 unsigned long Model1_flags = Model1_arch_local_save_flags();

 return Model1_arch_irqs_disabled_flags(Model1_flags);
}
/*
 * Wrap the arch provided IRQ routines to provide appropriate checks.
 */
/*
 * The local_irq_*() APIs are equal to the raw_local_irq*()
 * if !TRACE_IRQFLAGS.
 */
/*
 * Some architectures don't define arch_irqs_disabled(), so even if either
 * definition would be fine we need to use different ones for the time being
 * to avoid build issues.
 */

/*
 * We handle most unaligned accesses in hardware.  On the other hand
 * unaligned DMA can be quite expensive on some Nehalem processors.
 *
 * Based on this we disable the IP header alignment in network drivers.
 */



/*
 * Default implementation of macro that returns current
 * instruction pointer ("program counter").
 */
static inline __attribute__((no_instrument_function)) void *Model1_current_text_addr(void)
{
 void *Model1_pc;

 asm volatile("mov $1f, %0; 1:":"=r" (Model1_pc));

 return Model1_pc;
}

/*
 * These alignment constraints are for performance in the vSMP case,
 * but in the task_struct case we must also meet hardware imposed
 * alignment requirements of the FPU state:
 */
enum Model1_tlb_infos {
 Model1_ENTRIES,
 Model1_NR_INFO
};

extern Model1_u16 __attribute__((__section__(".data..read_mostly"))) Model1_tlb_lli_4k[Model1_NR_INFO];
extern Model1_u16 __attribute__((__section__(".data..read_mostly"))) Model1_tlb_lli_2m[Model1_NR_INFO];
extern Model1_u16 __attribute__((__section__(".data..read_mostly"))) Model1_tlb_lli_4m[Model1_NR_INFO];
extern Model1_u16 __attribute__((__section__(".data..read_mostly"))) Model1_tlb_lld_4k[Model1_NR_INFO];
extern Model1_u16 __attribute__((__section__(".data..read_mostly"))) Model1_tlb_lld_2m[Model1_NR_INFO];
extern Model1_u16 __attribute__((__section__(".data..read_mostly"))) Model1_tlb_lld_4m[Model1_NR_INFO];
extern Model1_u16 __attribute__((__section__(".data..read_mostly"))) Model1_tlb_lld_1g[Model1_NR_INFO];

/*
 *  CPU type and hardware bug flags. Kept separately for each CPU.
 *  Members of this structure are referenced in head.S, so think twice
 *  before touching them. [mj]
 */

struct Model1_cpuinfo_x86 {
 __u8 Model1_x86; /* CPU family */
 __u8 Model1_x86_vendor; /* CPU vendor */
 __u8 Model1_x86_model;
 __u8 Model1_x86_mask;
 /* Number of 4K pages in DTLB/ITLB combined(in pages): */
 int Model1_x86_tlbsize;

 __u8 Model1_x86_virt_bits;
 __u8 Model1_x86_phys_bits;
 /* CPUID returned core id bits: */
 __u8 Model1_x86_coreid_bits;
 /* Max extended CPUID function supported: */
 __u32 Model1_extended_cpuid_level;
 /* Maximum supported CPUID level, -1=no CPUID: */
 int Model1_cpuid_level;
 __u32 Model1_x86_capability[18 + 1];
 char Model1_x86_vendor_id[16];
 char Model1_x86_model_id[64];
 /* in KB - valid for CPUS which support this call: */
 int Model1_x86_cache_size;
 int Model1_x86_cache_alignment; /* In bytes */
 /* Cache QoS architectural values: */
 int Model1_x86_cache_max_rmid; /* max index */
 int Model1_x86_cache_occ_scale; /* scale to bytes */
 int Model1_x86_power;
 unsigned long Model1_loops_per_jiffy;
 /* cpuid returned max cores value: */
 Model1_u16 Model1_x86_max_cores;
 Model1_u16 Model1_apicid;
 Model1_u16 Model1_initial_apicid;
 Model1_u16 Model1_x86_clflush_size;
 /* number of cores as seen by the OS: */
 Model1_u16 Model1_booted_cores;
 /* Physical processor id: */
 Model1_u16 Model1_phys_proc_id;
 /* Logical processor id: */
 Model1_u16 Model1_logical_proc_id;
 /* Core id: */
 Model1_u16 Model1_cpu_core_id;
 /* Index into per_cpu list: */
 Model1_u16 Model1_cpu_index;
 Model1_u32 Model1_microcode;
};
/*
 * capabilities of CPUs
 */
extern struct Model1_cpuinfo_x86 Model1_boot_cpu_data;
extern struct Model1_cpuinfo_x86 Model1_new_cpu_data;

extern struct Model1_tss_struct Model1_doublefault_tss;
extern __u32 Model1_cpu_caps_cleared[18];
extern __u32 Model1_cpu_caps_set[18];


extern __attribute__((section(".data..percpu" "..read_mostly"))) __typeof__(struct Model1_cpuinfo_x86) Model1_cpu_info;






extern const struct Model1_seq_operations Model1_cpuinfo_op;



extern void Model1_cpu_detect(struct Model1_cpuinfo_x86 *Model1_c);

extern void Model1_early_cpu_init(void);
extern void Model1_identify_boot_cpu(void);
extern void Model1_identify_secondary_cpu(struct Model1_cpuinfo_x86 *);
extern void Model1_print_cpu_info(struct Model1_cpuinfo_x86 *);
void Model1_print_cpu_msr(struct Model1_cpuinfo_x86 *);
extern void Model1_init_scattered_cpuid_features(struct Model1_cpuinfo_x86 *Model1_c);
extern unsigned int Model1_init_intel_cacheinfo(struct Model1_cpuinfo_x86 *Model1_c);
extern void Model1_init_amd_cacheinfo(struct Model1_cpuinfo_x86 *Model1_c);

extern void Model1_detect_extended_topology(struct Model1_cpuinfo_x86 *Model1_c);
extern void Model1_detect_ht(struct Model1_cpuinfo_x86 *Model1_c);




static inline __attribute__((no_instrument_function)) int Model1_have_cpuid_p(void)
{
 return 1;
}

static inline __attribute__((no_instrument_function)) void Model1_native_cpuid(unsigned int *Model1_eax, unsigned int *Model1_ebx,
    unsigned int *Model1_ecx, unsigned int *Model1_edx)
{
 /* ecx is often an input as well as an output. */
 asm volatile("cpuid"
     : "=a" (*Model1_eax),
       "=b" (*Model1_ebx),
       "=c" (*Model1_ecx),
       "=d" (*Model1_edx)
     : "0" (*Model1_eax), "2" (*Model1_ecx)
     : "memory");
}

static inline __attribute__((no_instrument_function)) void Model1_load_cr3(Model1_pgd_t *Model1_pgdir)
{
 Model1_write_cr3(Model1___phys_addr_nodebug((unsigned long)(Model1_pgdir)));
}
struct Model1_x86_hw_tss {
 Model1_u32 Model1_reserved1;
 Model1_u64 Model1_sp0;
 Model1_u64 Model1_sp1;
 Model1_u64 Model1_sp2;
 Model1_u64 Model1_reserved2;
 Model1_u64 Model1_ist[7];
 Model1_u32 Model1_reserved3;
 Model1_u32 Model1_reserved4;
 Model1_u16 Model1_reserved5;
 Model1_u16 Model1_io_bitmap_base;

} __attribute__((packed)) __attribute__((__aligned__((1 << (6)))));


/*
 * IO-bitmap sizes:
 */






struct Model1_tss_struct {
 /*
	 * The hardware state:
	 */
 struct Model1_x86_hw_tss Model1_x86_tss;

 /*
	 * The extra 1 is there because the CPU will access an
	 * additional byte beyond the end of the IO permission
	 * bitmap. The extra byte must be all 1 bits, and must
	 * be within the limit.
	 */
 unsigned long Model1_io_bitmap[((65536/8)/sizeof(long)) + 1];
} __attribute__((__aligned__((1 << (6)))));

extern __attribute__((section(".data..percpu" "..shared_aligned"))) __typeof__(struct Model1_tss_struct) Model1_cpu_tss __attribute__((__aligned__((1 << (6)))));





/*
 * Save the original ist values for checking stack pointers during debugging
 */
struct Model1_orig_ist {
 unsigned long Model1_ist[7];
};


extern __attribute__((section(".data..percpu" ""))) __typeof__(struct Model1_orig_ist) Model1_orig_ist;

union Model1_irq_stack_union {
 char Model1_irq_stack[(((1UL) << 12) << (2 + 0))];
 /*
	 * GCC hardcodes the stack canary as %gs:40.  Since the
	 * irq_stack is the object at %gs:0, we reserve the bottom
	 * 48 bytes of the irq stack for the canary.
	 */
 struct {
  char Model1_gs_base[40];
  unsigned long Model1_stack_canary;
 };
};

extern __attribute__((section(".data..percpu" "..first"))) __typeof__(union Model1_irq_stack_union) Model1_irq_stack_union ;
extern typeof(Model1_irq_stack_union) Model1_init_per_cpu__irq_stack_union;

extern __attribute__((section(".data..percpu" ""))) __typeof__(char *) Model1_irq_stack_ptr;
extern __attribute__((section(".data..percpu" ""))) __typeof__(unsigned int) Model1_irq_count;
extern void Model1_ignore_sysret(void);
extern unsigned int Model1_fpu_kernel_xstate_size;
extern unsigned int Model1_fpu_user_xstate_size;

struct Model1_perf_event;

typedef struct {
 unsigned long Model1_seg;
} Model1_mm_segment_t;

struct Model1_thread_struct {
 /* Cached TLS descriptors: */
 struct Model1_desc_struct Model1_tls_array[3];
 unsigned long Model1_sp0;
 unsigned long Model1_sp;



 unsigned short Model1_es;
 unsigned short Model1_ds;
 unsigned short Model1_fsindex;
 unsigned short Model1_gsindex;





 unsigned long Model1_fsbase;
 unsigned long Model1_gsbase;
 /* Save middle states of ptrace breakpoints */
 struct Model1_perf_event *Model1_ptrace_bps[4];
 /* Debug status used for traps, single steps, etc... */
 unsigned long Model1_debugreg6;
 /* Keep track of the exact dr7 value set by the user */
 unsigned long Model1_ptrace_dr7;
 /* Fault info: */
 unsigned long Model1_cr2;
 unsigned long Model1_trap_nr;
 unsigned long Model1_error_code;




 /* IO permissions: */
 unsigned long *Model1_io_bitmap_ptr;
 unsigned long Model1_iopl;
 /* Max allowed port in the bitmap, in bytes: */
 unsigned Model1_io_bitmap_max;

 Model1_mm_segment_t Model1_addr_limit;

 unsigned int Model1_sig_on_uaccess_err:1;
 unsigned int Model1_uaccess_err:1; /* uaccess failed */

 /* Floating point and extended processor state */
 struct Model1_fpu Model1_fpu;
 /*
	 * WARNING: 'fpu' is dynamically-sized.  It *MUST* be at
	 * the end.
	 */
};

/*
 * Set IOPL bits in EFLAGS from given mask
 */
static inline __attribute__((no_instrument_function)) void Model1_native_set_iopl_mask(unsigned Model1_mask)
{
}

static inline __attribute__((no_instrument_function)) void
Model1_native_load_sp0(struct Model1_tss_struct *Model1_tss, struct Model1_thread_struct *thread)
{
 Model1_tss->Model1_x86_tss.Model1_sp0 = thread->Model1_sp0;







}

static inline __attribute__((no_instrument_function)) void Model1_native_swapgs(void)
{

 asm volatile("swapgs" ::: "memory");

}

static inline __attribute__((no_instrument_function)) unsigned long Model1_current_top_of_stack(void)
{

 return ({ typeof(Model1_cpu_tss.Model1_x86_tss.Model1_sp0) Model1_pfo_ret__; switch (sizeof(Model1_cpu_tss.Model1_x86_tss.Model1_sp0)) { case 1: asm("mov" "b ""%%""gs"":" "%" "P1"",%0" : "=q" (Model1_pfo_ret__) : "p" (&(Model1_cpu_tss.Model1_x86_tss.Model1_sp0))); break; case 2: asm("mov" "w ""%%""gs"":" "%" "P1"",%0" : "=r" (Model1_pfo_ret__) : "p" (&(Model1_cpu_tss.Model1_x86_tss.Model1_sp0))); break; case 4: asm("mov" "l ""%%""gs"":" "%" "P1"",%0" : "=r" (Model1_pfo_ret__) : "p" (&(Model1_cpu_tss.Model1_x86_tss.Model1_sp0))); break; case 8: asm("mov" "q ""%%""gs"":" "%" "P1"",%0" : "=r" (Model1_pfo_ret__) : "p" (&(Model1_cpu_tss.Model1_x86_tss.Model1_sp0))); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; });




}






static inline __attribute__((no_instrument_function)) void Model1_load_sp0(struct Model1_tss_struct *Model1_tss,
       struct Model1_thread_struct *thread)
{
 Model1_native_load_sp0(Model1_tss, thread);
}




/* Free all resources held by a thread. */
extern void Model1_release_thread(struct Model1_task_struct *);

unsigned long Model1_get_wchan(struct Model1_task_struct *Model1_p);

/*
 * Generic CPUID function
 * clear %ecx since some cpus (Cyrix MII) do not set or clear %ecx
 * resulting in stale register contents being returned.
 */
static inline __attribute__((no_instrument_function)) void Model1_cpuid(unsigned int Model1_op,
    unsigned int *Model1_eax, unsigned int *Model1_ebx,
    unsigned int *Model1_ecx, unsigned int *Model1_edx)
{
 *Model1_eax = Model1_op;
 *Model1_ecx = 0;
 Model1_native_cpuid(Model1_eax, Model1_ebx, Model1_ecx, Model1_edx);
}

/* Some CPUID calls want 'count' to be placed in ecx */
static inline __attribute__((no_instrument_function)) void Model1_cpuid_count(unsigned int Model1_op, int Model1_count,
          unsigned int *Model1_eax, unsigned int *Model1_ebx,
          unsigned int *Model1_ecx, unsigned int *Model1_edx)
{
 *Model1_eax = Model1_op;
 *Model1_ecx = Model1_count;
 Model1_native_cpuid(Model1_eax, Model1_ebx, Model1_ecx, Model1_edx);
}

/*
 * CPUID functions returning a single datum
 */
static inline __attribute__((no_instrument_function)) unsigned int Model1_cpuid_eax(unsigned int Model1_op)
{
 unsigned int Model1_eax, Model1_ebx, Model1_ecx, Model1_edx;

 Model1_cpuid(Model1_op, &Model1_eax, &Model1_ebx, &Model1_ecx, &Model1_edx);

 return Model1_eax;
}

static inline __attribute__((no_instrument_function)) unsigned int Model1_cpuid_ebx(unsigned int Model1_op)
{
 unsigned int Model1_eax, Model1_ebx, Model1_ecx, Model1_edx;

 Model1_cpuid(Model1_op, &Model1_eax, &Model1_ebx, &Model1_ecx, &Model1_edx);

 return Model1_ebx;
}

static inline __attribute__((no_instrument_function)) unsigned int Model1_cpuid_ecx(unsigned int Model1_op)
{
 unsigned int Model1_eax, Model1_ebx, Model1_ecx, Model1_edx;

 Model1_cpuid(Model1_op, &Model1_eax, &Model1_ebx, &Model1_ecx, &Model1_edx);

 return Model1_ecx;
}

static inline __attribute__((no_instrument_function)) unsigned int Model1_cpuid_edx(unsigned int Model1_op)
{
 unsigned int Model1_eax, Model1_ebx, Model1_ecx, Model1_edx;

 Model1_cpuid(Model1_op, &Model1_eax, &Model1_ebx, &Model1_ecx, &Model1_edx);

 return Model1_edx;
}

/* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_rep_nop(void)
{
 asm volatile("rep; nop" ::: "memory");
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_cpu_relax(void)
{
 Model1_rep_nop();
}



/* Stop speculative execution and prefetching of modified code. */
static inline __attribute__((no_instrument_function)) void Model1_sync_core(void)
{
 int Model1_tmp;
 /*
	 * CPUID is a barrier to speculative execution.
	 * Prefetched instructions are automatically
	 * invalidated when modified.
	 */
 asm volatile("cpuid"
       : "=a" (Model1_tmp)
       : "0" (1)
       : "ebx", "ecx", "edx", "memory");

}

extern void Model1_select_idle_routine(const struct Model1_cpuinfo_x86 *Model1_c);
extern void Model1_init_amd_e400_c1e_mask(void);

extern unsigned long Model1_boot_option_idle_override;
extern bool Model1_amd_e400_c1e_detected;

enum Model1_idle_boot_override {Model1_IDLE_NO_OVERRIDE=0, Model1_IDLE_HALT, Model1_IDLE_NOMWAIT,
    Model1_IDLE_POLL};

extern void Model1_enable_sep_cpu(void);
extern int Model1_sysenter_setup(void);

extern void Model1_early_trap_init(void);
void Model1_early_trap_pf_init(void);

/* Defined in head.S */
extern struct Model1_desc_ptr Model1_early_gdt_descr;

extern void Model1_cpu_set_gdt(int);
extern void Model1_switch_to_new_gdt(int);
extern void Model1_load_percpu_segment(int);
extern void Model1_cpu_init(void);

static inline __attribute__((no_instrument_function)) unsigned long Model1_get_debugctlmsr(void)
{
 unsigned long Model1_debugctlmsr = 0;





 ((Model1_debugctlmsr) = Model1_native_read_msr((0x000001d9)));

 return Model1_debugctlmsr;
}

static inline __attribute__((no_instrument_function)) void Model1_update_debugctlmsr(unsigned long Model1_debugctlmsr)
{




 Model1_wrmsrl(0x000001d9, Model1_debugctlmsr);
}

extern void Model1_set_task_blockstep(struct Model1_task_struct *Model1_task, bool Model1_on);

/* Boot loader type from the setup header: */
extern int Model1_bootloader_type;
extern int Model1_bootloader_version;

extern char Model1_ignore_fpu_irq;
/*
 * Prefetch instructions for Pentium III (+) and AMD Athlon (+)
 *
 * It's not worth to care about 3dnow prefetches for the K6
 * because they are microcoded there and very slow.
 */
static inline __attribute__((no_instrument_function)) void Model1_prefetch(const void *Model1_x)
{
 asm volatile ("661:\n\t" "prefetcht0 %P1" "\n662:\n" ".skip -(((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")) > 0) * " "((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")),0x90\n" "663" ":\n" ".pushsection .altinstructions,\"a\"\n" " .long 661b - .\n" " .long " "664""1""f - .\n" " .word " "( 0*32+25)" "\n" " .byte " "663""b-661b" "\n" " .byte " "665""1""f-""664""1""f" "\n" " .byte " "663""b-662b" "\n" ".popsection\n" ".pushsection .altinstr_replacement, \"ax\"\n" "664""1"":\n\t" "prefetchnta %P1" "\n" "665""1" ":\n\t" ".popsection" : : "i" (0), "m" (*(const char *)Model1_x));


}

/*
 * 3dnow prefetch to get an exclusive cache line.
 * Useful for spinlocks to avoid one state transition in the
 * cache coherency protocol:
 */
static inline __attribute__((no_instrument_function)) void Model1_prefetchw(const void *Model1_x)
{
#if CY_ABSTRACT6
    __builtin_prefetch(Model1_x, 1);
#else
 asm volatile ("661:\n\t" "prefetcht0 %P1" "\n662:\n" ".skip -(((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")) > 0) * " "((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")),0x90\n" "663" ":\n" ".pushsection .altinstructions,\"a\"\n" " .long 661b - .\n" " .long " "664""1""f - .\n" " .word " "( 6*32+ 8)" "\n" " .byte " "663""b-661b" "\n" " .byte " "665""1""f-""664""1""f" "\n" " .byte " "663""b-662b" "\n" ".popsection\n" ".pushsection .altinstr_replacement, \"ax\"\n" "664""1"":\n\t" "prefetchw %P1" "\n" "665""1" ":\n\t" ".popsection" : : "i" (0), "m" (*(const char *)Model1_x));
#endif
}

static inline __attribute__((no_instrument_function)) void Model1_spin_lock_prefetch(const void *Model1_x)
{
 Model1_prefetchw(Model1_x);
}
/*
 * User space process size. 47bits minus one guard page.  The guard
 * page is necessary on Intel CPUs: if a SYSCALL instruction is at
 * the highest possible canonical userspace address, then that
 * syscall will enter the kernel with a non-canonical return
 * address, and SYSRET will explode dangerously.  We avoid this
 * particular problem by preventing anything from being mapped
 * at the maximum canonical address.
 */


/* This decides where the kernel will search for a free chunk of vm
 * space during mmap's.
 */
/*
 * Return saved PC of a blocked thread.
 * What is this good for? it will be always the scheduler or ret_from_fork.
 */



extern unsigned long Model1_KSTK_ESP(struct Model1_task_struct *Model1_task);



extern void Model1_start_thread(struct Model1_pt_regs *Model1_regs, unsigned long Model1_new_ip,
            unsigned long Model1_new_sp);

/*
 * This decides where the kernel will search for a free chunk of vm
 * space during mmap's.
 */




/* Get/set a process' ability to use the timestamp counter instruction */



extern int Model1_get_tsc_mode(unsigned long Model1_adr);
extern int Model1_set_tsc_mode(unsigned int Model1_val);

/* Register/unregister a process' MPX related resource */







static inline __attribute__((no_instrument_function)) int Model1_mpx_enable_management(void)
{
 return -22;
}
static inline __attribute__((no_instrument_function)) int Model1_mpx_disable_management(void)
{
 return -22;
}


extern Model1_u16 Model1_amd_get_nb_id(int Model1_cpu);
extern Model1_u32 Model1_amd_get_nodes_per_socket(void);

static inline __attribute__((no_instrument_function)) Model1_uint32_t Model1_hypervisor_cpuid_base(const char *Model1_sig, Model1_uint32_t Model1_leaves)
{
 Model1_uint32_t Model1_base, Model1_eax, Model1_signature[3];

 for (Model1_base = 0x40000000; Model1_base < 0x40010000; Model1_base += 0x100) {
  Model1_cpuid(Model1_base, &Model1_eax, &Model1_signature[0], &Model1_signature[1], &Model1_signature[2]);

  if (!Model1_memcmp(Model1_sig, Model1_signature, 12) &&
      (Model1_leaves == 0 || ((Model1_eax - Model1_base) >= Model1_leaves)))
   return Model1_base;
 }

 return 0;
}

extern unsigned long Model1_arch_align_stack(unsigned long Model1_sp);
extern void Model1_free_init_pages(char *Model1_what, unsigned long Model1_begin, unsigned long Model1_end);

void Model1_default_idle(void);






void Model1_stop_this_cpu(void *Model1_dummy);
void Model1_df_debug(struct Model1_pt_regs *Model1_regs, long Model1_error_code);






enum Model1_cpuid_leafs
{
 Model1_CPUID_1_EDX = 0,
 Model1_CPUID_8000_0001_EDX,
 Model1_CPUID_8086_0001_EDX,
 Model1_CPUID_LNX_1,
 Model1_CPUID_1_ECX,
 Model1_CPUID_C000_0001_EDX,
 Model1_CPUID_8000_0001_ECX,
 Model1_CPUID_LNX_2,
 Model1_CPUID_LNX_3,
 Model1_CPUID_7_0_EBX,
 Model1_CPUID_D_1_EAX,
 Model1_CPUID_F_0_EDX,
 Model1_CPUID_F_1_EDX,
 Model1_CPUID_8000_0008_EBX,
 Model1_CPUID_6_EAX,
 Model1_CPUID_8000_000A_EDX,
 Model1_CPUID_7_ECX,
 Model1_CPUID_8000_0007_EBX,
};


extern const char * const Model1_x86_cap_flags[18*32];
extern const char * const Model1_x86_power_flags[32];







/*
 * In order to save room, we index into this array by doing
 * X86_BUG_<name> - NCAPINTS*32.
 */
extern const char * const Model1_x86_bug_flags[1*32];




/*
 * There are 32 bits/features in each mask word.  The high bits
 * (selected with (bit>>5) give us the word number and the low 5
 * bits give us the bit/feature number inside the word.
 * (1UL<<((bit)&31) gives us a mask for the feature_bit so we can
 * see if it is set in the mask word.
 */
/*
 * This macro is for detection of features which need kernel
 * infrastructure to be used.  It may *not* directly test the CPU
 * itself.  Use the cpu_has() family if you want true runtime
 * testing of CPU features, like in hypervisor code where you are
 * supporting a possible guest feature where host support for it
 * is not relevant.
 */
/*
 * Fall back to dynamic for gcc versions which don't support asm goto. Should be
 * a minority now anyway.
 */


struct Model1_thread_info {
 struct Model1_task_struct *Model1_task; /* main task structure */
 __u32 Model1_flags; /* low level flags */
 __u32 Model1_status; /* thread synchronous flags */
 __u32 Model1_cpu; /* current CPU */
};
/*
 * thread information flags
 * - these are process state flags that various assembly files
 *   may need to access
 * - pending work-to-be-done flags are in LSW
 * - other flags in MSW
 * Warning: layout of LSW is hardcoded in entry.S
 */
/*
 * work to do in syscall_trace_enter().  Also includes TIF_NOHZ for
 * enter_from_user_mode()
 */





/* work to do on any return to user space */




/* flags to check in __switch_to() */
/*
 * macros/functions for gaining access to the thread information structure
 *
 * preempt_count needs to be 1 initially, until the scheduler is functional.
 */


static inline __attribute__((no_instrument_function)) struct Model1_thread_info *Model1_current_thread_info(void)
{
 return (struct Model1_thread_info *)(Model1_current_top_of_stack() - (((1UL) << 12) << (2 + 0)));
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_current_stack_pointer(void)
{
 unsigned long Model1_sp;

 asm("mov %%rsp,%0" : "=g" (Model1_sp));



 return Model1_sp;
}

/*
 * Walks up the stack frames to make sure that the specified object is
 * entirely contained by a single stack frame.
 *
 * Returns:
 *		 1 if within a frame
 *		-1 if placed across a frame boundary (or outside stack)
 *		 0 unable to determine (no frame pointers, etc)
 */
static inline __attribute__((no_instrument_function)) int Model1_arch_within_stack_frames(const void * const Model1_stack,
        const void * const Model1_stackend,
        const void *Model1_obj, unsigned long Model1_len)
{

 const void *Model1_frame = ((void *)0);
 const void *Model1_oldframe;

 Model1_oldframe = __builtin_frame_address(1);
 if (Model1_oldframe)
  Model1_frame = __builtin_frame_address(2);
 /*
	 * low ----------------------------------------------> high
	 * [saved bp][saved ip][args][local vars][saved bp][saved ip]
	 *                     ^----------------^
	 *               allow copies only within here
	 */
 while (Model1_stack <= Model1_frame && Model1_frame < Model1_stackend) {
  /*
		 * If obj + len extends past the last frame, this
		 * check won't pass and the next frame will be 0,
		 * causing us to bail out and correctly report
		 * the copy as invalid.
		 */
  if (Model1_obj + Model1_len <= Model1_frame)
   return Model1_obj >= Model1_oldframe + 2 * sizeof(void *) ? 1 : -1;
  Model1_oldframe = Model1_frame;
  Model1_frame = *(const void * const *)Model1_frame;
 }
 return -1;



}
/*
 * Thread-synchronous status.
 *
 * This is different from the flags in that nobody else
 * ever touches our thread-synchronous status, so we don't
 * have to worry about atomic accesses.
 */







static inline __attribute__((no_instrument_function)) bool Model1_in_ia32_syscall(void)
{




 if (Model1_current_thread_info()->Model1_status & 0x0002)
  return true;

 return false;
}

/*
 * Force syscall return via IRET by making it look as if there was
 * some work pending. IRET is our most capable (but slowest) syscall
 * return path, which is able to restore modified SS, CS and certain
 * EFLAGS values that other (fast) syscall return instructions
 * are not able to restore properly.
 */


extern void Model1_arch_task_cache_init(void);
extern int Model1_arch_dup_task_struct(struct Model1_task_struct *Model1_dst, struct Model1_task_struct *Model1_src);
extern void Model1_arch_release_task_struct(struct Model1_task_struct *Model1_tsk);
/*
 * flag set/clear/test wrappers
 * - pass TIF_xxxx constants to these functions
 */

static inline __attribute__((no_instrument_function)) void Model1_set_ti_thread_flag(struct Model1_thread_info *Model1_ti, int Model1_flag)
{
 Model1_set_bit(Model1_flag, (unsigned long *)&Model1_ti->Model1_flags);
}

static inline __attribute__((no_instrument_function)) void Model1_clear_ti_thread_flag(struct Model1_thread_info *Model1_ti, int Model1_flag)
{
 Model1_clear_bit(Model1_flag, (unsigned long *)&Model1_ti->Model1_flags);
}

static inline __attribute__((no_instrument_function)) int Model1_test_and_set_ti_thread_flag(struct Model1_thread_info *Model1_ti, int Model1_flag)
{
 return Model1_test_and_set_bit(Model1_flag, (unsigned long *)&Model1_ti->Model1_flags);
}

static inline __attribute__((no_instrument_function)) int Model1_test_and_clear_ti_thread_flag(struct Model1_thread_info *Model1_ti, int Model1_flag)
{
 return Model1_test_and_clear_bit(Model1_flag, (unsigned long *)&Model1_ti->Model1_flags);
}

static inline __attribute__((no_instrument_function)) int Model1_test_ti_thread_flag(struct Model1_thread_info *Model1_ti, int Model1_flag)
{
 return (__builtin_constant_p((Model1_flag)) ? Model1_constant_test_bit((Model1_flag), ((unsigned long *)&Model1_ti->Model1_flags)) : Model1_variable_test_bit((Model1_flag), ((unsigned long *)&Model1_ti->Model1_flags)));
}
static inline __attribute__((no_instrument_function)) void Model1_check_object_size(const void *Model1_ptr, unsigned long Model1_n,
         bool Model1_to_user)
{ }

extern __attribute__((section(".data..percpu" ""))) __typeof__(int) Model1___preempt_count;

/*
 * We use the PREEMPT_NEED_RESCHED bit as an inverted NEED_RESCHED such
 * that a decrement hitting 0 means we can and should reschedule.
 */


/*
 * We mask the PREEMPT_NEED_RESCHED bit so as not to confuse all current users
 * that think a non-zero value indicates we cannot preempt.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_preempt_count(void)
{
 return ({ typeof(Model1___preempt_count) Model1_pfo_ret__; switch (sizeof(Model1___preempt_count)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1___preempt_count)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1___preempt_count)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1___preempt_count)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1___preempt_count)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }) & ~0x80000000;
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_preempt_count_set(int Model1_pc)
{
 do { typedef typeof((Model1___preempt_count)) Model1_pto_T__; if (0) { Model1_pto_T__ Model1_pto_tmp__; Model1_pto_tmp__ = (Model1_pc); (void)Model1_pto_tmp__; } switch (sizeof((Model1___preempt_count))) { case 1: asm("mov" "b %1,""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count)) : "qi" ((Model1_pto_T__)(Model1_pc))); break; case 2: asm("mov" "w %1,""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count)) : "ri" ((Model1_pto_T__)(Model1_pc))); break; case 4: asm("mov" "l %1,""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count)) : "ri" ((Model1_pto_T__)(Model1_pc))); break; case 8: asm("mov" "q %1,""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count)) : "re" ((Model1_pto_T__)(Model1_pc))); break; default: Model1___bad_percpu_size(); } } while (0);
}

/*
 * must be macros to avoid header recursion hell
 */






/*
 * We fold the NEED_RESCHED bit into the preempt count such that
 * preempt_enable() can decrement and test for needing to reschedule with a
 * single instruction.
 *
 * We invert the actual bit, so that when the decrement hits 0 we know we both
 * need to resched (the bit is cleared) and can resched (no preempt count).
 */

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_set_preempt_need_resched(void)
{
 do { typedef typeof((Model1___preempt_count)) Model1_pto_T__; if (0) { Model1_pto_T__ Model1_pto_tmp__; Model1_pto_tmp__ = (~0x80000000); (void)Model1_pto_tmp__; } switch (sizeof((Model1___preempt_count))) { case 1: asm("and" "b %1,""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count)) : "qi" ((Model1_pto_T__)(~0x80000000))); break; case 2: asm("and" "w %1,""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count)) : "ri" ((Model1_pto_T__)(~0x80000000))); break; case 4: asm("and" "l %1,""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count)) : "ri" ((Model1_pto_T__)(~0x80000000))); break; case 8: asm("and" "q %1,""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count)) : "re" ((Model1_pto_T__)(~0x80000000))); break; default: Model1___bad_percpu_size(); } } while (0);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_clear_preempt_need_resched(void)
{
 do { typedef typeof((Model1___preempt_count)) Model1_pto_T__; if (0) { Model1_pto_T__ Model1_pto_tmp__; Model1_pto_tmp__ = (0x80000000); (void)Model1_pto_tmp__; } switch (sizeof((Model1___preempt_count))) { case 1: asm("or" "b %1,""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count)) : "qi" ((Model1_pto_T__)(0x80000000))); break; case 2: asm("or" "w %1,""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count)) : "ri" ((Model1_pto_T__)(0x80000000))); break; case 4: asm("or" "l %1,""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count)) : "ri" ((Model1_pto_T__)(0x80000000))); break; case 8: asm("or" "q %1,""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count)) : "re" ((Model1_pto_T__)(0x80000000))); break; default: Model1___bad_percpu_size(); } } while (0);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model1_test_preempt_need_resched(void)
{
 return !(({ typeof(Model1___preempt_count) Model1_pfo_ret__; switch (sizeof(Model1___preempt_count)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1___preempt_count)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1___preempt_count)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1___preempt_count)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1___preempt_count)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }) & 0x80000000);
}

/*
 * The various preempt_count add/sub methods
 */

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___preempt_count_add(int Model1_val)
{
#if CY_ABSTRACT6
    //We assume only one interleaving would happen, so there's no preempt
#else
 do { typedef typeof((Model1___preempt_count)) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(Model1_val) && ((Model1_val) == 1 || (Model1_val) == -1)) ? (int)(Model1_val) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (Model1_val); (void)Model1_pao_tmp__; } switch (sizeof((Model1___preempt_count))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count)) : "qi" ((Model1_pao_T__)(Model1_val))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count)) : "ri" ((Model1_pao_T__)(Model1_val))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count)) : "ri" ((Model1_pao_T__)(Model1_val))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count)) : "re" ((Model1_pao_T__)(Model1_val))); break; default: Model1___bad_percpu_size(); } } while (0);
#endif
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___preempt_count_sub(int Model1_val)
{
#if CY_ABSTRACT6
    //We assume only one interleaving would happen, so there's no preempt
#else
 do { typedef typeof((Model1___preempt_count)) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(-Model1_val) && ((-Model1_val) == 1 || (-Model1_val) == -1)) ? (int)(-Model1_val) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (-Model1_val); (void)Model1_pao_tmp__; } switch (sizeof((Model1___preempt_count))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count)) : "qi" ((Model1_pao_T__)(-Model1_val))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count)) : "ri" ((Model1_pao_T__)(-Model1_val))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count)) : "ri" ((Model1_pao_T__)(-Model1_val))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model1___preempt_count)) : "re" ((Model1_pao_T__)(-Model1_val))); break; default: Model1___bad_percpu_size(); } } while (0);
#endif
}

/*
 * Because we keep PREEMPT_NEED_RESCHED set when we do _not_ need to reschedule
 * a decrement which hits zero means we have no preempt_count and should
 * reschedule.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model1___preempt_count_dec_and_test(void)
{
 do { bool Model1_c; asm volatile ("decl" " " "%%""gs"":" "%" "0" ";" "\n\tset" "e" " %[_cc_" "e" "]\n" : "+m" (Model1___preempt_count), [_cc_e] "=qm" (Model1_c) : : "memory"); return Model1_c; } while (0);
}

/*
 * Returns true when we need to resched and can (barring IRQ state).
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model1_should_resched(int Model1_preempt_offset)
{
 return __builtin_expect(!!(({ typeof(Model1___preempt_count) Model1_pfo_ret__; switch (sizeof(Model1___preempt_count)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1___preempt_count)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1___preempt_count)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1___preempt_count)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1___preempt_count)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }) == Model1_preempt_offset), 0);
}






/*
 * Are we doing bottom half or hardware interrupt processing?
 * Are we in a softirq context? Interrupt context?
 * in_softirq - Are we currently processing softirq or have bh disabled?
 * in_serving_softirq - Are we currently processing softirq?
 */





/*
 * Are we in NMI context?
 */


/*
 * The preempt_count offset after preempt_disable();
 */






/*
 * The preempt_count offset after spin_lock()
 */


/*
 * The preempt_count offset needed for things like:
 *
 *  spin_lock_bh()
 *
 * Which need to disable both preemption (CONFIG_PREEMPT_COUNT) and
 * softirqs, such that unlock sequences of:
 *
 *  spin_unlock();
 *  local_bh_enable();
 *
 * Work as expected.
 */


/*
 * Are we running in atomic context?  WARNING: this macro cannot
 * always detect atomic context; in particular, it cannot know about
 * held spinlocks in non-preemptible kernels.  Thus it should not be
 * used in the general case to determine whether sleeping is possible.
 * Do not use in_atomic() in driver code.
 */


/*
 * Check whether we were atomic before we did preempt_disable():
 * (used by the scheduler)
 */
/*
 * Even if we don't have any preemption, we need preempt disable/enable
 * to be barriers, so that we don't have things like get_user/put_user
 * that can cause faults and scheduling migrate into our preempt-protected
 * region.
 */














static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___local_bh_disable_ip(unsigned long Model1_ip, unsigned int Model1_cnt)
{
 Model1___preempt_count_add(Model1_cnt);
 __asm__ __volatile__("": : :"memory");
}


static inline __attribute__((no_instrument_function)) void Model1_local_bh_disable(void)
{
 Model1___local_bh_disable_ip(({ __label__ Model1___here; Model1___here: (unsigned long)&&Model1___here; }), (2 * (1UL << (0 + 8))));
}

extern void Model1__local_bh_enable(void);
extern void Model1___local_bh_enable_ip(unsigned long Model1_ip, unsigned int Model1_cnt);

static inline __attribute__((no_instrument_function)) void Model1_local_bh_enable_ip(unsigned long Model1_ip)
{
 Model1___local_bh_enable_ip(Model1_ip, (2 * (1UL << (0 + 8))));
}

static inline __attribute__((no_instrument_function)) void Model1_local_bh_enable(void)
{
 Model1___local_bh_enable_ip(({ __label__ Model1___here; Model1___here: (unsigned long)&&Model1___here; }), (2 * (1UL << (0 + 8))));
}



/*
 * Must define these before including other files, inline functions need them
 */
/*
 * Pull the arch_spinlock_t and arch_rwlock_t definitions:
 */




/*
 * include/linux/spinlock_types.h - generic spinlock type definitions
 *                                  and initializers
 *
 * portions Copyright 2005, Red Hat, Inc., Ingo Molnar
 * Released under the General Public License (GPL).
 */



typedef Model1_u8 Model1___ticket_t;
typedef Model1_u16 Model1___ticketpair_t;
/*
 * Queued spinlock
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * (C) Copyright 2013-2015 Hewlett-Packard Development Company, L.P.
 *
 * Authors: Waiman Long <waiman.long@hp.com>
 */



/*
 * Including atomic.h with PARAVIRT on will cause compilation errors because
 * of recursive header file incluson via paravirt_types.h. So don't include
 * it if PARAVIRT is on.
 */





typedef struct Model1_qspinlock {
 Model1_atomic_t Model1_val;
} Model1_arch_spinlock_t;

/*
 * Initializier
 */


/*
 * Bitfields in the atomic value:
 *
 * When NR_CPUS < 16K
 *  0- 7: locked byte
 *     8: pending
 *  9-15: not used
 * 16-17: tail index
 * 18-31: tail cpu (+1)
 *
 * When NR_CPUS >= 16K
 *  0- 7: locked byte
 *     8: pending
 *  9-10: tail index
 * 11-31: tail cpu (+1)
 */





/*
 * The queue read/write lock data structure
 */

typedef struct Model1_qrwlock {
 Model1_atomic_t Model1_cnts;
 Model1_arch_spinlock_t Model1_wait_lock;
} Model1_arch_rwlock_t;




/*
 * Runtime locking correctness validator
 *
 *  Copyright (C) 2006,2007 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
 *  Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra
 *
 * see Documentation/locking/lockdep-design.txt for more details.
 */



struct Model1_task_struct;
struct Model1_lockdep_map;

/* for sysctl */
extern int Model1_prove_locking;
extern int Model1_lock_stat;
static inline __attribute__((no_instrument_function)) void Model1_lockdep_off(void)
{
}

static inline __attribute__((no_instrument_function)) void Model1_lockdep_on(void)
{
}
/*
 * We don't define lockdep_match_class() and lockdep_match_key() for !LOCKDEP
 * case since the result is not well defined and the caller should rather
 * #ifdef the call himself.
 */





/*
 * The class key takes no space if lockdep is disabled:
 */
struct Model1_lock_class_key { };
struct Model1_pin_cookie { };
static inline __attribute__((no_instrument_function)) void Model1_print_irqtrace_events(struct Model1_task_struct *Model1_curr)
{
}


/*
 * For trivial one-depth nesting of a lock-class, the following
 * global define can be used. (Subsystems with multiple levels
 * of nesting should define their own lock-nesting subclasses.)
 */


/*
 * Map the dependency ops to NOP or to real lockdep ops, depending
 * on the per lock-class debug mode:
 */
static inline __attribute__((no_instrument_function)) void
Model1_lockdep_rcu_suspicious(const char *Model1_file, const int Model1_line, const char *Model1_s)
{
}

typedef struct Model1_raw_spinlock {
 Model1_arch_spinlock_t Model1_raw_lock;
} Model1_raw_spinlock_t;
typedef struct Model1_spinlock {
 union {
  struct Model1_raw_spinlock Model1_rlock;
 };
} Model1_spinlock_t;



/*
 * include/linux/rwlock_types.h - generic rwlock type definitions
 *				  and initializers
 *
 * portions Copyright 2005, Red Hat, Inc., Ingo Molnar
 * Released under the General Public License (GPL).
 */
typedef struct {
 Model1_arch_rwlock_t Model1_raw_lock;
} Model1_rwlock_t;

/*
 * Pull the arch_spin*() functions/declarations (UP-nondebug doesn't need them):
 */












/* Various instructions on x86 need to be replaced for
 * para-virtualization: those hooks are defined here. */
static inline __attribute__((no_instrument_function)) void Model1_paravirt_arch_dup_mmap(struct Model1_mm_struct *Model1_oldmm,
       struct Model1_mm_struct *Model1_mm)
{
}

static inline __attribute__((no_instrument_function)) void Model1_paravirt_arch_exit_mmap(struct Model1_mm_struct *Model1_mm)
{
}


/*
 * Your basic SMP spinlocks, allowing only a single CPU anywhere
 *
 * Simple spin lock operations.  There are two variants, one clears IRQ's
 * on the local processor, one does not.
 *
 * These are fair FIFO ticket locks, which support up to 2^16 CPUs.
 *
 * (the type definitions are in asm/spinlock_types.h)
 */
/* How long a lock should spin before we consider blocking */


extern struct Model1_static_key Model1_paravirt_ticketlocks_enabled;
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model1_static_key_false(struct Model1_static_key *Model1_key);











/**
 * queued_spin_unlock - release a queued spinlock
 * @lock : Pointer to queued spinlock structure
 *
 * A smp_store_release() on the least-significant byte.
 */
static inline __attribute__((no_instrument_function)) void Model1_native_queued_spin_unlock(struct Model1_qspinlock *Model1_lock)
{
 do { do { bool Model1___cond = !((sizeof(*(Model1_u8 *)Model1_lock) == sizeof(char) || sizeof(*(Model1_u8 *)Model1_lock) == sizeof(short) || sizeof(*(Model1_u8 *)Model1_lock) == sizeof(int) || sizeof(*(Model1_u8 *)Model1_lock) == sizeof(long))); extern void Model1___compiletime_assert_17(void) ; if (Model1___cond) Model1___compiletime_assert_17(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*(Model1_u8 *)Model1_lock) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(*(Model1_u8 *)Model1_lock)) (0) }; Model1___write_once_size(&(*(Model1_u8 *)Model1_lock), Model1___u.Model1___c, sizeof(*(Model1_u8 *)Model1_lock)); Model1___u.Model1___val; }); } while (0);
}
static inline __attribute__((no_instrument_function)) void Model1_queued_spin_unlock(struct Model1_qspinlock *Model1_lock)
{
 Model1_native_queued_spin_unlock(Model1_lock);
}
/*
 * Queued spinlock
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * (C) Copyright 2013-2015 Hewlett-Packard Development Company, L.P.
 * (C) Copyright 2015 Hewlett-Packard Enterprise Development LP
 *
 * Authors: Waiman Long <waiman.long@hpe.com>
 */





/**
 * queued_spin_unlock_wait - wait until the _current_ lock holder releases the lock
 * @lock : Pointer to queued spinlock structure
 *
 * There is a very slight possibility of live-lock if the lockers keep coming
 * and the waiter is just unfortunate enough to not see any unlock state.
 */

extern void Model1_queued_spin_unlock_wait(struct Model1_qspinlock *Model1_lock);


/**
 * queued_spin_is_locked - is the spinlock locked?
 * @lock: Pointer to queued spinlock structure
 * Return: 1 if it is locked, 0 otherwise
 */

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_queued_spin_is_locked(struct Model1_qspinlock *Model1_lock)
{
 /*
	 * See queued_spin_unlock_wait().
	 *
	 * Any !0 state indicates it is locked, even if _Q_LOCKED_VAL
	 * isn't immediately observable.
	 */
 return Model1_atomic_read(&Model1_lock->Model1_val);
}


/**
 * queued_spin_value_unlocked - is the spinlock structure unlocked?
 * @lock: queued spinlock structure
 * Return: 1 if it is unlocked, 0 otherwise
 *
 * N.B. Whenever there are tasks waiting for the lock, it is considered
 *      locked wrt the lockref code to avoid lock stealing by the lockref
 *      code and change things underneath the lock. This also allows some
 *      optimizations to be applied without conflict with lockref.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_queued_spin_value_unlocked(struct Model1_qspinlock Model1_lock)
{
 return !Model1_atomic_read(&Model1_lock.Model1_val);
}

/**
 * queued_spin_is_contended - check if the lock is contended
 * @lock : Pointer to queued spinlock structure
 * Return: 1 if lock contended, 0 otherwise
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_queued_spin_is_contended(struct Model1_qspinlock *Model1_lock)
{
 return Model1_atomic_read(&Model1_lock->Model1_val) & ~(((1U << 8) - 1) << 0);
}
/**
 * queued_spin_trylock - try to acquire the queued spinlock
 * @lock : Pointer to queued spinlock structure
 * Return: 1 if lock acquired, 0 if failed
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_queued_spin_trylock(struct Model1_qspinlock *Model1_lock)
{
 if (!Model1_atomic_read(&Model1_lock->Model1_val) &&
    (Model1_atomic_cmpxchg(&Model1_lock->Model1_val, 0, (1U << 0)) == 0))
  return 1;
 return 0;
}

extern void Model1_queued_spin_lock_slowpath(struct Model1_qspinlock *Model1_lock, Model1_u32 Model1_val);

/**
 * queued_spin_lock - acquire a queued spinlock
 * @lock: Pointer to queued spinlock structure
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_queued_spin_lock(struct Model1_qspinlock *Model1_lock)
{
 Model1_u32 Model1_val;

 Model1_val = Model1_atomic_cmpxchg(&Model1_lock->Model1_val, 0, (1U << 0));
 if (__builtin_expect(!!(Model1_val == 0), 1))
  return;
 Model1_queued_spin_lock_slowpath(Model1_lock, Model1_val);
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model1_virt_spin_lock(struct Model1_qspinlock *Model1_lock)
{
 return false;
}


/*
 * Remapping spinlock architecture specific functions to the corresponding
 * queued spinlock functions.
 */
/*
 * Read-write spinlocks, allowing multiple readers
 * but only one writer.
 *
 * NOTE! it is quite common to have readers in interrupts
 * but no interrupt writers. For those circumstances we
 * can "mix" irq-safe locks - any writer needs to get a
 * irq-safe write-lock, but readers can get non-irqsafe
 * read-locks.
 *
 * On x86, we implement read-write locks using the generic qrwlock with
 * x86 specific optimization.
 */






/*
 * Queue read/write lock
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * (C) Copyright 2013-2014 Hewlett-Packard Development Company, L.P.
 *
 * Authors: Waiman Long <waiman.long@hp.com>
 */
/*
 * Writer states & reader shift and bias.
 *
 *       | +0 | +1 | +2 | +3 |
 *   ----+----+----+----+----+
 *    LE | 78 | 56 | 34 | 12 | 0x12345678
 *   ----+----+----+----+----+
 *       | wr |      rd      |
 *       +----+----+----+----+
 *
 *   ----+----+----+----+----+
 *    BE | 12 | 34 | 56 | 78 | 0x12345678
 *   ----+----+----+----+----+
 *       |      rd      | wr |
 *       +----+----+----+----+
 */






/*
 * External function declarations
 */
extern void Model1_queued_read_lock_slowpath(struct Model1_qrwlock *Model1_lock, Model1_u32 Model1_cnts);
extern void Model1_queued_write_lock_slowpath(struct Model1_qrwlock *Model1_lock);

/**
 * queued_read_can_lock- would read_trylock() succeed?
 * @lock: Pointer to queue rwlock structure
 */
static inline __attribute__((no_instrument_function)) int Model1_queued_read_can_lock(struct Model1_qrwlock *Model1_lock)
{
 return !(Model1_atomic_read(&Model1_lock->Model1_cnts) & 0xff);
}

/**
 * queued_write_can_lock- would write_trylock() succeed?
 * @lock: Pointer to queue rwlock structure
 */
static inline __attribute__((no_instrument_function)) int Model1_queued_write_can_lock(struct Model1_qrwlock *Model1_lock)
{
 return !Model1_atomic_read(&Model1_lock->Model1_cnts);
}

/**
 * queued_read_trylock - try to acquire read lock of a queue rwlock
 * @lock : Pointer to queue rwlock structure
 * Return: 1 if lock acquired, 0 if failed
 */
static inline __attribute__((no_instrument_function)) int Model1_queued_read_trylock(struct Model1_qrwlock *Model1_lock)
{
 Model1_u32 Model1_cnts;

 Model1_cnts = Model1_atomic_read(&Model1_lock->Model1_cnts);
 if (__builtin_expect(!!(!(Model1_cnts & 0xff)), 1)) {
  Model1_cnts = (Model1_u32)Model1_atomic_add_return((1U << 8), &Model1_lock->Model1_cnts);
  if (__builtin_expect(!!(!(Model1_cnts & 0xff)), 1))
   return 1;
  Model1_atomic_sub((1U << 8), &Model1_lock->Model1_cnts);
 }
 return 0;
}

/**
 * queued_write_trylock - try to acquire write lock of a queue rwlock
 * @lock : Pointer to queue rwlock structure
 * Return: 1 if lock acquired, 0 if failed
 */
static inline __attribute__((no_instrument_function)) int Model1_queued_write_trylock(struct Model1_qrwlock *Model1_lock)
{
 Model1_u32 Model1_cnts;

 Model1_cnts = Model1_atomic_read(&Model1_lock->Model1_cnts);
 if (__builtin_expect(!!(Model1_cnts), 0))
  return 0;

 return __builtin_expect(!!(Model1_atomic_cmpxchg(&Model1_lock->Model1_cnts, Model1_cnts, Model1_cnts | 0xff) == Model1_cnts), 1);

}
/**
 * queued_read_lock - acquire read lock of a queue rwlock
 * @lock: Pointer to queue rwlock structure
 */
static inline __attribute__((no_instrument_function)) void Model1_queued_read_lock(struct Model1_qrwlock *Model1_lock)
{
 Model1_u32 Model1_cnts;

 Model1_cnts = Model1_atomic_add_return((1U << 8), &Model1_lock->Model1_cnts);
 if (__builtin_expect(!!(!(Model1_cnts & 0xff)), 1))
  return;

 /* The slowpath will decrement the reader count, if necessary. */
 Model1_queued_read_lock_slowpath(Model1_lock, Model1_cnts);
}

/**
 * queued_write_lock - acquire write lock of a queue rwlock
 * @lock : Pointer to queue rwlock structure
 */
static inline __attribute__((no_instrument_function)) void Model1_queued_write_lock(struct Model1_qrwlock *Model1_lock)
{
 /* Optimize for the unfair lock case where the fair flag is 0. */
 if (Model1_atomic_cmpxchg(&Model1_lock->Model1_cnts, 0, 0xff) == 0)
  return;

 Model1_queued_write_lock_slowpath(Model1_lock);
}

/**
 * queued_read_unlock - release read lock of a queue rwlock
 * @lock : Pointer to queue rwlock structure
 */
static inline __attribute__((no_instrument_function)) void Model1_queued_read_unlock(struct Model1_qrwlock *Model1_lock)
{
 /*
	 * Atomically decrement the reader count
	 */
 (void)Model1_atomic_sub_return((1U << 8), &Model1_lock->Model1_cnts);
}

/**
 * __qrwlock_write_byte - retrieve the write byte address of a queue rwlock
 * @lock : Pointer to queue rwlock structure
 * Return: the write byte address of a queue rwlock
 */
static inline __attribute__((no_instrument_function)) Model1_u8 *Model1___qrwlock_write_byte(struct Model1_qrwlock *Model1_lock)
{
 return (Model1_u8 *)Model1_lock + 3 * 0;
}

/**
 * queued_write_unlock - release write lock of a queue rwlock
 * @lock : Pointer to queue rwlock structure
 */
static inline __attribute__((no_instrument_function)) void Model1_queued_write_unlock(struct Model1_qrwlock *Model1_lock)
{
 do { do { bool Model1___cond = !((sizeof(*Model1___qrwlock_write_byte(Model1_lock)) == sizeof(char) || sizeof(*Model1___qrwlock_write_byte(Model1_lock)) == sizeof(short) || sizeof(*Model1___qrwlock_write_byte(Model1_lock)) == sizeof(int) || sizeof(*Model1___qrwlock_write_byte(Model1_lock)) == sizeof(long))); extern void Model1___compiletime_assert_165(void) ; if (Model1___cond) Model1___compiletime_assert_165(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*Model1___qrwlock_write_byte(Model1_lock)) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(*Model1___qrwlock_write_byte(Model1_lock))) (0) }; Model1___write_once_size(&(*Model1___qrwlock_write_byte(Model1_lock)), Model1___u.Model1___c, sizeof(*Model1___qrwlock_write_byte(Model1_lock))); Model1___u.Model1___val; }); } while (0);
}

/*
 * Remapping rwlock architecture specific functions to the corresponding
 * queue rwlock functions.
 */
/*
 * Despite its name it doesn't necessarily has to be a full barrier.
 * It should only guarantee that a STORE before the critical section
 * can not be reordered with LOADs and STOREs inside this section.
 * spin_lock() is the one-way barrier, this LOAD can not escape out
 * of the region. So the default implementation simply ensures that
 * a STORE can not move into the critical section, smp_wmb() should
 * serialize it with another STORE done by spin_lock().
 */




/**
 * raw_spin_unlock_wait - wait until the spinlock gets unlocked
 * @lock: the spinlock in question.
 */
static inline __attribute__((no_instrument_function)) void Model1_do_raw_spin_lock(Model1_raw_spinlock_t *Model1_lock)
{
 (void)0;
 Model1_queued_spin_lock(&Model1_lock->Model1_raw_lock);
}

static inline __attribute__((no_instrument_function)) void
Model1_do_raw_spin_lock_flags(Model1_raw_spinlock_t *Model1_lock, unsigned long *Model1_flags)
{
 (void)0;
 Model1_queued_spin_lock(&Model1_lock->Model1_raw_lock);
}

static inline __attribute__((no_instrument_function)) int Model1_do_raw_spin_trylock(Model1_raw_spinlock_t *Model1_lock)
{
 return Model1_queued_spin_trylock(&(Model1_lock)->Model1_raw_lock);
}

static inline __attribute__((no_instrument_function)) void Model1_do_raw_spin_unlock(Model1_raw_spinlock_t *Model1_lock)
{
 Model1_queued_spin_unlock(&Model1_lock->Model1_raw_lock);
 (void)0;
}


/*
 * Define the various spin_lock methods.  Note we define these
 * regardless of whether CONFIG_SMP or CONFIG_PREEMPT are set. The
 * various methods are defined as nops in the case they are not
 * required.
 */
/*
 * Always evaluate the 'subclass' argument to avoid that the compiler
 * warns about set-but-not-used variables when building with
 * CONFIG_DEBUG_LOCK_ALLOC=n and with W=1.
 */
/**
 * raw_spin_can_lock - would raw_spin_trylock() succeed?
 * @lock: the spinlock in question.
 */


/* Include rwlock functions */








/*
 * rwlock related methods
 *
 * split out from spinlock.h
 *
 * portions Copyright 2005, Red Hat, Inc., Ingo Molnar
 * Released under the General Public License (GPL).
 */
/*
 * Define the various rw_lock methods.  Note we define these
 * regardless of whether CONFIG_SMP or CONFIG_PREEMPT are set. The various
 * methods are defined as nops in the case they are not required.
 */

/*
 * Pull the _spin_*()/_read_*()/_write_*() functions/declarations:
 */









/*
 * include/linux/spinlock_api_smp.h
 *
 * spinlock API declarations on SMP (and debug)
 * (implemented in kernel/spinlock.c)
 *
 * portions Copyright 2005, Red Hat, Inc., Ingo Molnar
 * Released under the General Public License (GPL).
 */

int Model1_in_lock_functions(unsigned long Model1_addr);



void __attribute__((section(".spinlock.text"))) Model1__raw_spin_lock(Model1_raw_spinlock_t *Model1_lock) ;
void __attribute__((section(".spinlock.text"))) Model1__raw_spin_lock_nested(Model1_raw_spinlock_t *Model1_lock, int Model1_subclass)
                        ;
void __attribute__((section(".spinlock.text"))) Model1__raw_spin_lock_bh_nested(Model1_raw_spinlock_t *Model1_lock, int Model1_subclass)
                        ;
void __attribute__((section(".spinlock.text")))
Model1__raw_spin_lock_nest_lock(Model1_raw_spinlock_t *Model1_lock, struct Model1_lockdep_map *Model1_map)
                        ;
void __attribute__((section(".spinlock.text"))) Model1__raw_spin_lock_bh(Model1_raw_spinlock_t *Model1_lock) ;
void __attribute__((section(".spinlock.text"))) Model1__raw_spin_lock_irq(Model1_raw_spinlock_t *Model1_lock)
                        ;

unsigned long __attribute__((section(".spinlock.text"))) Model1__raw_spin_lock_irqsave(Model1_raw_spinlock_t *Model1_lock)
                        ;
unsigned long __attribute__((section(".spinlock.text")))
Model1__raw_spin_lock_irqsave_nested(Model1_raw_spinlock_t *Model1_lock, int Model1_subclass)
                        ;
int __attribute__((section(".spinlock.text"))) Model1__raw_spin_trylock(Model1_raw_spinlock_t *Model1_lock);
int __attribute__((section(".spinlock.text"))) Model1__raw_spin_trylock_bh(Model1_raw_spinlock_t *Model1_lock);
void __attribute__((section(".spinlock.text"))) Model1__raw_spin_unlock(Model1_raw_spinlock_t *Model1_lock) ;
void __attribute__((section(".spinlock.text"))) Model1__raw_spin_unlock_bh(Model1_raw_spinlock_t *Model1_lock) ;
void __attribute__((section(".spinlock.text"))) Model1__raw_spin_unlock_irq(Model1_raw_spinlock_t *Model1_lock) ;
void __attribute__((section(".spinlock.text")))
Model1__raw_spin_unlock_irqrestore(Model1_raw_spinlock_t *Model1_lock, unsigned long Model1_flags)
                        ;
static inline __attribute__((no_instrument_function)) int Model1___raw_spin_trylock(Model1_raw_spinlock_t *Model1_lock)
{
 __asm__ __volatile__("": : :"memory");
 if (Model1_do_raw_spin_trylock(Model1_lock)) {
  do { } while (0);
  return 1;
 }
 __asm__ __volatile__("": : :"memory");
 return 0;
}

/*
 * If lockdep is enabled then we use the non-preemption spin-ops
 * even on CONFIG_PREEMPT, because lockdep assumes that interrupts are
 * not re-enabled during lock-acquire (which the preempt-spin-ops do):
 */


static inline __attribute__((no_instrument_function)) unsigned long Model1___raw_spin_lock_irqsave(Model1_raw_spinlock_t *Model1_lock)
{
 unsigned long Model1_flags;

 do { do { ({ unsigned long Model1___dummy; typeof(Model1_flags) Model1___dummy2; (void)(&Model1___dummy == &Model1___dummy2); 1; }); Model1_flags = Model1_arch_local_irq_save(); } while (0); } while (0);
 __asm__ __volatile__("": : :"memory");
 do { } while (0);
 /*
	 * On lockdep we dont want the hand-coded irq-enable of
	 * do_raw_spin_lock_flags() code, because lockdep assumes
	 * that interrupts are not re-enabled during lock-acquire:
	 */



 Model1_do_raw_spin_lock_flags(Model1_lock, &Model1_flags);

 return Model1_flags;
}

static inline __attribute__((no_instrument_function)) void Model1___raw_spin_lock_irq(Model1_raw_spinlock_t *Model1_lock)
{
 do { Model1_arch_local_irq_disable(); } while (0);
 __asm__ __volatile__("": : :"memory");
 do { } while (0);
 Model1_do_raw_spin_lock(Model1_lock);
}

static inline __attribute__((no_instrument_function)) void Model1___raw_spin_lock_bh(Model1_raw_spinlock_t *Model1_lock)
{
 Model1___local_bh_disable_ip((unsigned long)__builtin_return_address(0), ((2 * (1UL << (0 + 8))) + 0));
 do { } while (0);
 Model1_do_raw_spin_lock(Model1_lock);
}

static inline __attribute__((no_instrument_function)) void Model1___raw_spin_lock(Model1_raw_spinlock_t *Model1_lock)
{
 __asm__ __volatile__("": : :"memory");
 do { } while (0);
 Model1_do_raw_spin_lock(Model1_lock);
}



static inline __attribute__((no_instrument_function)) void Model1___raw_spin_unlock(Model1_raw_spinlock_t *Model1_lock)
{
 do { } while (0);
 Model1_do_raw_spin_unlock(Model1_lock);
 __asm__ __volatile__("": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model1___raw_spin_unlock_irqrestore(Model1_raw_spinlock_t *Model1_lock,
         unsigned long Model1_flags)
{
 do { } while (0);
 Model1_do_raw_spin_unlock(Model1_lock);
 do { do { ({ unsigned long Model1___dummy; typeof(Model1_flags) Model1___dummy2; (void)(&Model1___dummy == &Model1___dummy2); 1; }); Model1_arch_local_irq_restore(Model1_flags); } while (0); } while (0);
 __asm__ __volatile__("": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model1___raw_spin_unlock_irq(Model1_raw_spinlock_t *Model1_lock)
{
 do { } while (0);
 Model1_do_raw_spin_unlock(Model1_lock);
 do { Model1_arch_local_irq_enable(); } while (0);
 __asm__ __volatile__("": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model1___raw_spin_unlock_bh(Model1_raw_spinlock_t *Model1_lock)
{
 do { } while (0);
 Model1_do_raw_spin_unlock(Model1_lock);
 Model1___local_bh_enable_ip((unsigned long)__builtin_return_address(0), ((2 * (1UL << (0 + 8))) + 0));
}

static inline __attribute__((no_instrument_function)) int Model1___raw_spin_trylock_bh(Model1_raw_spinlock_t *Model1_lock)
{
 Model1___local_bh_disable_ip((unsigned long)__builtin_return_address(0), ((2 * (1UL << (0 + 8))) + 0));
 if (Model1_do_raw_spin_trylock(Model1_lock)) {
  do { } while (0);
  return 1;
 }
 Model1___local_bh_enable_ip((unsigned long)__builtin_return_address(0), ((2 * (1UL << (0 + 8))) + 0));
 return 0;
}









/*
 * include/linux/rwlock_api_smp.h
 *
 * spinlock API declarations on SMP (and debug)
 * (implemented in kernel/spinlock.c)
 *
 * portions Copyright 2005, Red Hat, Inc., Ingo Molnar
 * Released under the General Public License (GPL).
 */

void __attribute__((section(".spinlock.text"))) Model1__raw_read_lock(Model1_rwlock_t *Model1_lock) ;
void __attribute__((section(".spinlock.text"))) Model1__raw_write_lock(Model1_rwlock_t *Model1_lock) ;
void __attribute__((section(".spinlock.text"))) Model1__raw_read_lock_bh(Model1_rwlock_t *Model1_lock) ;
void __attribute__((section(".spinlock.text"))) Model1__raw_write_lock_bh(Model1_rwlock_t *Model1_lock) ;
void __attribute__((section(".spinlock.text"))) Model1__raw_read_lock_irq(Model1_rwlock_t *Model1_lock) ;
void __attribute__((section(".spinlock.text"))) Model1__raw_write_lock_irq(Model1_rwlock_t *Model1_lock) ;
unsigned long __attribute__((section(".spinlock.text"))) Model1__raw_read_lock_irqsave(Model1_rwlock_t *Model1_lock)
                       ;
unsigned long __attribute__((section(".spinlock.text"))) Model1__raw_write_lock_irqsave(Model1_rwlock_t *Model1_lock)
                       ;
int __attribute__((section(".spinlock.text"))) Model1__raw_read_trylock(Model1_rwlock_t *Model1_lock);
int __attribute__((section(".spinlock.text"))) Model1__raw_write_trylock(Model1_rwlock_t *Model1_lock);
void __attribute__((section(".spinlock.text"))) Model1__raw_read_unlock(Model1_rwlock_t *Model1_lock) ;
void __attribute__((section(".spinlock.text"))) Model1__raw_write_unlock(Model1_rwlock_t *Model1_lock) ;
void __attribute__((section(".spinlock.text"))) Model1__raw_read_unlock_bh(Model1_rwlock_t *Model1_lock) ;
void __attribute__((section(".spinlock.text"))) Model1__raw_write_unlock_bh(Model1_rwlock_t *Model1_lock) ;
void __attribute__((section(".spinlock.text"))) Model1__raw_read_unlock_irq(Model1_rwlock_t *Model1_lock) ;
void __attribute__((section(".spinlock.text"))) Model1__raw_write_unlock_irq(Model1_rwlock_t *Model1_lock) ;
void __attribute__((section(".spinlock.text")))
Model1__raw_read_unlock_irqrestore(Model1_rwlock_t *Model1_lock, unsigned long Model1_flags)
                       ;
void __attribute__((section(".spinlock.text")))
Model1__raw_write_unlock_irqrestore(Model1_rwlock_t *Model1_lock, unsigned long Model1_flags)
                       ;
static inline __attribute__((no_instrument_function)) int Model1___raw_read_trylock(Model1_rwlock_t *Model1_lock)
{
 __asm__ __volatile__("": : :"memory");
 if (Model1_queued_read_trylock(&(Model1_lock)->Model1_raw_lock)) {
  do { } while (0);
  return 1;
 }
 __asm__ __volatile__("": : :"memory");
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1___raw_write_trylock(Model1_rwlock_t *Model1_lock)
{
 __asm__ __volatile__("": : :"memory");
 if (Model1_queued_write_trylock(&(Model1_lock)->Model1_raw_lock)) {
  do { } while (0);
  return 1;
 }
 __asm__ __volatile__("": : :"memory");
 return 0;
}

/*
 * If lockdep is enabled then we use the non-preemption spin-ops
 * even on CONFIG_PREEMPT, because lockdep assumes that interrupts are
 * not re-enabled during lock-acquire (which the preempt-spin-ops do):
 */


static inline __attribute__((no_instrument_function)) void Model1___raw_read_lock(Model1_rwlock_t *Model1_lock)
{
 __asm__ __volatile__("": : :"memory");
 do { } while (0);
 do {(void)0; Model1_queued_read_lock(&(Model1_lock)->Model1_raw_lock); } while (0);
}

static inline __attribute__((no_instrument_function)) unsigned long Model1___raw_read_lock_irqsave(Model1_rwlock_t *Model1_lock)
{
 unsigned long Model1_flags;

 do { do { ({ unsigned long Model1___dummy; typeof(Model1_flags) Model1___dummy2; (void)(&Model1___dummy == &Model1___dummy2); 1; }); Model1_flags = Model1_arch_local_irq_save(); } while (0); } while (0);
 __asm__ __volatile__("": : :"memory");
 do { } while (0);
 do {(void)0; Model1_queued_read_lock(&((Model1_lock))->Model1_raw_lock); } while (0);

 return Model1_flags;
}

static inline __attribute__((no_instrument_function)) void Model1___raw_read_lock_irq(Model1_rwlock_t *Model1_lock)
{
 do { Model1_arch_local_irq_disable(); } while (0);
 __asm__ __volatile__("": : :"memory");
 do { } while (0);
 do {(void)0; Model1_queued_read_lock(&(Model1_lock)->Model1_raw_lock); } while (0);
}

static inline __attribute__((no_instrument_function)) void Model1___raw_read_lock_bh(Model1_rwlock_t *Model1_lock)
{
 Model1___local_bh_disable_ip((unsigned long)__builtin_return_address(0), ((2 * (1UL << (0 + 8))) + 0));
 do { } while (0);
 do {(void)0; Model1_queued_read_lock(&(Model1_lock)->Model1_raw_lock); } while (0);
}

static inline __attribute__((no_instrument_function)) unsigned long Model1___raw_write_lock_irqsave(Model1_rwlock_t *Model1_lock)
{
 unsigned long Model1_flags;

 do { do { ({ unsigned long Model1___dummy; typeof(Model1_flags) Model1___dummy2; (void)(&Model1___dummy == &Model1___dummy2); 1; }); Model1_flags = Model1_arch_local_irq_save(); } while (0); } while (0);
 __asm__ __volatile__("": : :"memory");
 do { } while (0);
 do {(void)0; Model1_queued_write_lock(&((Model1_lock))->Model1_raw_lock); } while (0);

 return Model1_flags;
}

static inline __attribute__((no_instrument_function)) void Model1___raw_write_lock_irq(Model1_rwlock_t *Model1_lock)
{
 do { Model1_arch_local_irq_disable(); } while (0);
 __asm__ __volatile__("": : :"memory");
 do { } while (0);
 do {(void)0; Model1_queued_write_lock(&(Model1_lock)->Model1_raw_lock); } while (0);
}

static inline __attribute__((no_instrument_function)) void Model1___raw_write_lock_bh(Model1_rwlock_t *Model1_lock)
{
 Model1___local_bh_disable_ip((unsigned long)__builtin_return_address(0), ((2 * (1UL << (0 + 8))) + 0));
 do { } while (0);
 do {(void)0; Model1_queued_write_lock(&(Model1_lock)->Model1_raw_lock); } while (0);
}

static inline __attribute__((no_instrument_function)) void Model1___raw_write_lock(Model1_rwlock_t *Model1_lock)
{
 __asm__ __volatile__("": : :"memory");
 do { } while (0);
 do {(void)0; Model1_queued_write_lock(&(Model1_lock)->Model1_raw_lock); } while (0);
}



static inline __attribute__((no_instrument_function)) void Model1___raw_write_unlock(Model1_rwlock_t *Model1_lock)
{
 do { } while (0);
 do {Model1_queued_write_unlock(&(Model1_lock)->Model1_raw_lock); (void)0; } while (0);
 __asm__ __volatile__("": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model1___raw_read_unlock(Model1_rwlock_t *Model1_lock)
{
 do { } while (0);
 do {Model1_queued_read_unlock(&(Model1_lock)->Model1_raw_lock); (void)0; } while (0);
 __asm__ __volatile__("": : :"memory");
}

static inline __attribute__((no_instrument_function)) void
Model1___raw_read_unlock_irqrestore(Model1_rwlock_t *Model1_lock, unsigned long Model1_flags)
{
 do { } while (0);
 do {Model1_queued_read_unlock(&(Model1_lock)->Model1_raw_lock); (void)0; } while (0);
 do { do { ({ unsigned long Model1___dummy; typeof(Model1_flags) Model1___dummy2; (void)(&Model1___dummy == &Model1___dummy2); 1; }); Model1_arch_local_irq_restore(Model1_flags); } while (0); } while (0);
 __asm__ __volatile__("": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model1___raw_read_unlock_irq(Model1_rwlock_t *Model1_lock)
{
 do { } while (0);
 do {Model1_queued_read_unlock(&(Model1_lock)->Model1_raw_lock); (void)0; } while (0);
 do { Model1_arch_local_irq_enable(); } while (0);
 __asm__ __volatile__("": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model1___raw_read_unlock_bh(Model1_rwlock_t *Model1_lock)
{
 do { } while (0);
 do {Model1_queued_read_unlock(&(Model1_lock)->Model1_raw_lock); (void)0; } while (0);
 Model1___local_bh_enable_ip((unsigned long)__builtin_return_address(0), ((2 * (1UL << (0 + 8))) + 0));
}

static inline __attribute__((no_instrument_function)) void Model1___raw_write_unlock_irqrestore(Model1_rwlock_t *Model1_lock,
          unsigned long Model1_flags)
{
 do { } while (0);
 do {Model1_queued_write_unlock(&(Model1_lock)->Model1_raw_lock); (void)0; } while (0);
 do { do { ({ unsigned long Model1___dummy; typeof(Model1_flags) Model1___dummy2; (void)(&Model1___dummy == &Model1___dummy2); 1; }); Model1_arch_local_irq_restore(Model1_flags); } while (0); } while (0);
 __asm__ __volatile__("": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model1___raw_write_unlock_irq(Model1_rwlock_t *Model1_lock)
{
 do { } while (0);
 do {Model1_queued_write_unlock(&(Model1_lock)->Model1_raw_lock); (void)0; } while (0);
 do { Model1_arch_local_irq_enable(); } while (0);
 __asm__ __volatile__("": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model1___raw_write_unlock_bh(Model1_rwlock_t *Model1_lock)
{
 do { } while (0);
 do {Model1_queued_write_unlock(&(Model1_lock)->Model1_raw_lock); (void)0; } while (0);
 Model1___local_bh_enable_ip((unsigned long)__builtin_return_address(0), ((2 * (1UL << (0 + 8))) + 0));
}




/*
 * Map the spin_lock functions to the raw variants for PREEMPT_RT=n
 */

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model1_raw_spinlock_t *Model1_spinlock_check(Model1_spinlock_t *Model1_lock)
{
 return &Model1_lock->Model1_rlock;
}







static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_spin_lock(Model1_spinlock_t *Model1_lock)
{
 Model1__raw_spin_lock(&Model1_lock->Model1_rlock);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_spin_lock_bh(Model1_spinlock_t *Model1_lock)
{
 Model1__raw_spin_lock_bh(&Model1_lock->Model1_rlock);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_spin_trylock(Model1_spinlock_t *Model1_lock)
{
 return (Model1__raw_spin_trylock(&Model1_lock->Model1_rlock));
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_spin_lock_irq(Model1_spinlock_t *Model1_lock)
{
 Model1__raw_spin_lock_irq(&Model1_lock->Model1_rlock);
}
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_spin_unlock(Model1_spinlock_t *Model1_lock)
{
 Model1___raw_spin_unlock(&Model1_lock->Model1_rlock);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_spin_unlock_bh(Model1_spinlock_t *Model1_lock)
{
 Model1__raw_spin_unlock_bh(&Model1_lock->Model1_rlock);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_spin_unlock_irq(Model1_spinlock_t *Model1_lock)
{
 Model1___raw_spin_unlock_irq(&Model1_lock->Model1_rlock);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_spin_unlock_irqrestore(Model1_spinlock_t *Model1_lock, unsigned long Model1_flags)
{
 do { ({ unsigned long Model1___dummy; typeof(Model1_flags) Model1___dummy2; (void)(&Model1___dummy == &Model1___dummy2); 1; }); Model1__raw_spin_unlock_irqrestore(&Model1_lock->Model1_rlock, Model1_flags); } while (0);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_spin_trylock_bh(Model1_spinlock_t *Model1_lock)
{
 return (Model1__raw_spin_trylock_bh(&Model1_lock->Model1_rlock));
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_spin_trylock_irq(Model1_spinlock_t *Model1_lock)
{
 return ({ do { Model1_arch_local_irq_disable(); } while (0); (Model1__raw_spin_trylock(&Model1_lock->Model1_rlock)) ? 1 : ({ do { Model1_arch_local_irq_enable(); } while (0); 0; }); });
}






static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_spin_unlock_wait(Model1_spinlock_t *Model1_lock)
{
 Model1_queued_spin_unlock_wait(&(&Model1_lock->Model1_rlock)->Model1_raw_lock);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_spin_is_locked(Model1_spinlock_t *Model1_lock)
{
 return Model1_queued_spin_is_locked(&(&Model1_lock->Model1_rlock)->Model1_raw_lock);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_spin_is_contended(Model1_spinlock_t *Model1_lock)
{
 return Model1_queued_spin_is_contended(&(&Model1_lock->Model1_rlock)->Model1_raw_lock);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_spin_can_lock(Model1_spinlock_t *Model1_lock)
{
 return (!Model1_queued_spin_is_locked(&(&Model1_lock->Model1_rlock)->Model1_raw_lock));
}



/*
 * Pull the atomic_t declaration:
 * (asm-mips/atomic.h needs above definitions)
 */

/**
 * atomic_dec_and_lock - lock on reaching reference count zero
 * @atomic: the atomic counter
 * @lock: the spinlock in question
 *
 * Decrements @atomic by 1.  If the result is 0, returns true and locks
 * @lock.  Returns false for all other cases.
 */
extern int Model1__atomic_dec_and_lock(Model1_atomic_t *Model1_atomic, Model1_spinlock_t *Model1_lock);



/*
 * Linux wait queue related types and methods
 */





/* First argument to waitid: */

typedef struct Model1___wait_queue Model1_wait_queue_t;
typedef int (*Model1_wait_queue_func_t)(Model1_wait_queue_t *Model1_wait, unsigned Model1_mode, int Model1_flags, void *Model1_key);
int Model1_default_wake_function(Model1_wait_queue_t *Model1_wait, unsigned Model1_mode, int Model1_flags, void *Model1_key);

/* __wait_queue::flags */



struct Model1___wait_queue {
 unsigned int Model1_flags;
 void *Model1_private;
 Model1_wait_queue_func_t func;
 struct Model1_list_head Model1_task_list;
};

struct Model1_wait_bit_key {
 void *Model1_flags;
 int Model1_bit_nr;

 unsigned long Model1_timeout;
};

struct Model1_wait_bit_queue {
 struct Model1_wait_bit_key Model1_key;
 Model1_wait_queue_t Model1_wait;
};

struct Model1___wait_queue_head {
 Model1_spinlock_t Model1_lock;
 struct Model1_list_head Model1_task_list;
};
typedef struct Model1___wait_queue_head Model1_wait_queue_head_t;

struct Model1_task_struct;

/*
 * Macros for declaration and initialisaton of the datatypes
 */
extern void Model1___init_waitqueue_head(Model1_wait_queue_head_t *Model1_q, const char *Model1_name, struct Model1_lock_class_key *);
static inline __attribute__((no_instrument_function)) void Model1_init_waitqueue_entry(Model1_wait_queue_t *Model1_q, struct Model1_task_struct *Model1_p)
{
 Model1_q->Model1_flags = 0;
 Model1_q->Model1_private = Model1_p;
 Model1_q->func = Model1_default_wake_function;
}

static inline __attribute__((no_instrument_function)) void
Model1_init_waitqueue_func_entry(Model1_wait_queue_t *Model1_q, Model1_wait_queue_func_t func)
{
 Model1_q->Model1_flags = 0;
 Model1_q->Model1_private = ((void *)0);
 Model1_q->func = func;
}

/**
 * waitqueue_active -- locklessly test for waiters on the queue
 * @q: the waitqueue to test for waiters
 *
 * returns true if the wait list is not empty
 *
 * NOTE: this function is lockless and requires care, incorrect usage _will_
 * lead to sporadic and non-obvious failure.
 *
 * Use either while holding wait_queue_head_t::lock or when used for wakeups
 * with an extra smp_mb() like:
 *
 *      CPU0 - waker                    CPU1 - waiter
 *
 *                                      for (;;) {
 *      @cond = true;                     prepare_to_wait(&wq, &wait, state);
 *      smp_mb();                         // smp_mb() from set_current_state()
 *      if (waitqueue_active(wq))         if (@cond)
 *        wake_up(wq);                      break;
 *                                        schedule();
 *                                      }
 *                                      finish_wait(&wq, &wait);
 *
 * Because without the explicit smp_mb() it's possible for the
 * waitqueue_active() load to get hoisted over the @cond store such that we'll
 * observe an empty wait list while the waiter might not observe @cond.
 *
 * Also note that this 'optimization' trades a spin_lock() for an smp_mb(),
 * which (when the lock is uncontended) are of roughly equal cost.
 */
static inline __attribute__((no_instrument_function)) int Model1_waitqueue_active(Model1_wait_queue_head_t *Model1_q)
{
 return !Model1_list_empty(&Model1_q->Model1_task_list);
}

/**
 * wq_has_sleeper - check if there are any waiting processes
 * @wq: wait queue head
 *
 * Returns true if wq has waiting processes
 *
 * Please refer to the comment for waitqueue_active.
 */
static inline __attribute__((no_instrument_function)) bool Model1_wq_has_sleeper(Model1_wait_queue_head_t *Model1_wq)
{
 /*
	 * We need to be sure we are in sync with the
	 * add_wait_queue modifications to the wait queue.
	 *
	 * This memory barrier should be paired with one on the
	 * waiting side.
	 */
 asm volatile("mfence":::"memory");
 return Model1_waitqueue_active(Model1_wq);
}

extern void Model1_add_wait_queue(Model1_wait_queue_head_t *Model1_q, Model1_wait_queue_t *Model1_wait);
extern void Model1_add_wait_queue_exclusive(Model1_wait_queue_head_t *Model1_q, Model1_wait_queue_t *Model1_wait);
extern void Model1_remove_wait_queue(Model1_wait_queue_head_t *Model1_q, Model1_wait_queue_t *Model1_wait);

static inline __attribute__((no_instrument_function)) void Model1___add_wait_queue(Model1_wait_queue_head_t *Model1_head, Model1_wait_queue_t *Model1_new)
{
 Model1_list_add(&Model1_new->Model1_task_list, &Model1_head->Model1_task_list);
}

/*
 * Used for wake-one threads:
 */
static inline __attribute__((no_instrument_function)) void
Model1___add_wait_queue_exclusive(Model1_wait_queue_head_t *Model1_q, Model1_wait_queue_t *Model1_wait)
{
 Model1_wait->Model1_flags |= 0x01;
 Model1___add_wait_queue(Model1_q, Model1_wait);
}

static inline __attribute__((no_instrument_function)) void Model1___add_wait_queue_tail(Model1_wait_queue_head_t *Model1_head,
      Model1_wait_queue_t *Model1_new)
{
 Model1_list_add_tail(&Model1_new->Model1_task_list, &Model1_head->Model1_task_list);
}

static inline __attribute__((no_instrument_function)) void
Model1___add_wait_queue_tail_exclusive(Model1_wait_queue_head_t *Model1_q, Model1_wait_queue_t *Model1_wait)
{
 Model1_wait->Model1_flags |= 0x01;
 Model1___add_wait_queue_tail(Model1_q, Model1_wait);
}

static inline __attribute__((no_instrument_function)) void
Model1___remove_wait_queue(Model1_wait_queue_head_t *Model1_head, Model1_wait_queue_t *old)
{
 Model1_list_del(&old->Model1_task_list);
}

typedef int Model1_wait_bit_action_f(struct Model1_wait_bit_key *, int Model1_mode);
void Model1___wake_up(Model1_wait_queue_head_t *Model1_q, unsigned int Model1_mode, int Model1_nr, void *Model1_key);
void Model1___wake_up_locked_key(Model1_wait_queue_head_t *Model1_q, unsigned int Model1_mode, void *Model1_key);
void Model1___wake_up_sync_key(Model1_wait_queue_head_t *Model1_q, unsigned int Model1_mode, int Model1_nr, void *Model1_key);
void Model1___wake_up_locked(Model1_wait_queue_head_t *Model1_q, unsigned int Model1_mode, int Model1_nr);
void Model1___wake_up_sync(Model1_wait_queue_head_t *Model1_q, unsigned int Model1_mode, int Model1_nr);
void Model1___wake_up_bit(Model1_wait_queue_head_t *, void *, int);
int Model1___wait_on_bit(Model1_wait_queue_head_t *, struct Model1_wait_bit_queue *, Model1_wait_bit_action_f *, unsigned);
int Model1___wait_on_bit_lock(Model1_wait_queue_head_t *, struct Model1_wait_bit_queue *, Model1_wait_bit_action_f *, unsigned);
void Model1_wake_up_bit(void *, int);
void Model1_wake_up_atomic_t(Model1_atomic_t *);
int Model1_out_of_line_wait_on_bit(void *, int, Model1_wait_bit_action_f *, unsigned);
int Model1_out_of_line_wait_on_bit_timeout(void *, int, Model1_wait_bit_action_f *, unsigned, unsigned long);
int Model1_out_of_line_wait_on_bit_lock(void *, int, Model1_wait_bit_action_f *, unsigned);
int Model1_out_of_line_wait_on_atomic_t(Model1_atomic_t *, int (*)(Model1_atomic_t *), unsigned);
Model1_wait_queue_head_t *Model1_bit_waitqueue(void *, int);
/*
 * Wakeup macros to be used to report events to the targets.
 */
/*
 * The below macro ___wait_event() has an explicit shadow of the __ret
 * variable when used from the wait_event_*() macros.
 *
 * This is so that both can use the ___wait_cond_timeout() construct
 * to wrap the condition.
 *
 * The type inconsistency of the wait_event_*() __ret variable is also
 * on purpose; we use long where we can return timeout values and int
 * otherwise.
 */
/**
 * wait_event - sleep until a condition gets true
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 *
 * The process is put to sleep (TASK_UNINTERRUPTIBLE) until the
 * @condition evaluates to true. The @condition is checked each time
 * the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 */
/*
 * io_wait_event() -- like wait_event() but with io_schedule()
 */
/**
 * wait_event_freezable - sleep (or freeze) until a condition gets true
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE -- so as not to contribute
 * to system load) until the @condition evaluates to true. The
 * @condition is checked each time the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 */
/**
 * wait_event_timeout - sleep until a condition gets true or a timeout elapses
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @timeout: timeout, in jiffies
 *
 * The process is put to sleep (TASK_UNINTERRUPTIBLE) until the
 * @condition evaluates to true. The @condition is checked each time
 * the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * Returns:
 * 0 if the @condition evaluated to %false after the @timeout elapsed,
 * 1 if the @condition evaluated to %true after the @timeout elapsed,
 * or the remaining jiffies (at least 1) if the @condition evaluated
 * to %true before the @timeout elapsed.
 */
/*
 * like wait_event_timeout() -- except it uses TASK_INTERRUPTIBLE to avoid
 * increasing load and is freezable.
 */
/*
 * Just like wait_event_cmd(), except it sets exclusive flag
 */
/**
 * wait_event_cmd - sleep until a condition gets true
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @cmd1: the command will be executed before sleep
 * @cmd2: the command will be executed after sleep
 *
 * The process is put to sleep (TASK_UNINTERRUPTIBLE) until the
 * @condition evaluates to true. The @condition is checked each time
 * the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 */
/**
 * wait_event_interruptible - sleep until a condition gets true
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or a signal is received.
 * The @condition is checked each time the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * The function will return -ERESTARTSYS if it was interrupted by a
 * signal and 0 if @condition evaluated to true.
 */
/**
 * wait_event_interruptible_timeout - sleep until a condition gets true or a timeout elapses
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @timeout: timeout, in jiffies
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or a signal is received.
 * The @condition is checked each time the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * Returns:
 * 0 if the @condition evaluated to %false after the @timeout elapsed,
 * 1 if the @condition evaluated to %true after the @timeout elapsed,
 * the remaining jiffies (at least 1) if the @condition evaluated
 * to %true before the @timeout elapsed, or -%ERESTARTSYS if it was
 * interrupted by a signal.
 */
/**
 * wait_event_hrtimeout - sleep until a condition gets true or a timeout elapses
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @timeout: timeout, as a ktime_t
 *
 * The process is put to sleep (TASK_UNINTERRUPTIBLE) until the
 * @condition evaluates to true or a signal is received.
 * The @condition is checked each time the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * The function returns 0 if @condition became true, or -ETIME if the timeout
 * elapsed.
 */
/**
 * wait_event_interruptible_hrtimeout - sleep until a condition gets true or a timeout elapses
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @timeout: timeout, as a ktime_t
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or a signal is received.
 * The @condition is checked each time the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * The function returns 0 if @condition became true, -ERESTARTSYS if it was
 * interrupted by a signal, or -ETIME if the timeout elapsed.
 */
/**
 * wait_event_interruptible_locked - sleep until a condition gets true
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or a signal is received.
 * The @condition is checked each time the waitqueue @wq is woken up.
 *
 * It must be called with wq.lock being held.  This spinlock is
 * unlocked while sleeping but @condition testing is done while lock
 * is held and when this macro exits the lock is held.
 *
 * The lock is locked/unlocked using spin_lock()/spin_unlock()
 * functions which must match the way they are locked/unlocked outside
 * of this macro.
 *
 * wake_up_locked() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * The function will return -ERESTARTSYS if it was interrupted by a
 * signal and 0 if @condition evaluated to true.
 */




/**
 * wait_event_interruptible_locked_irq - sleep until a condition gets true
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or a signal is received.
 * The @condition is checked each time the waitqueue @wq is woken up.
 *
 * It must be called with wq.lock being held.  This spinlock is
 * unlocked while sleeping but @condition testing is done while lock
 * is held and when this macro exits the lock is held.
 *
 * The lock is locked/unlocked using spin_lock_irq()/spin_unlock_irq()
 * functions which must match the way they are locked/unlocked outside
 * of this macro.
 *
 * wake_up_locked() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * The function will return -ERESTARTSYS if it was interrupted by a
 * signal and 0 if @condition evaluated to true.
 */




/**
 * wait_event_interruptible_exclusive_locked - sleep exclusively until a condition gets true
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or a signal is received.
 * The @condition is checked each time the waitqueue @wq is woken up.
 *
 * It must be called with wq.lock being held.  This spinlock is
 * unlocked while sleeping but @condition testing is done while lock
 * is held and when this macro exits the lock is held.
 *
 * The lock is locked/unlocked using spin_lock()/spin_unlock()
 * functions which must match the way they are locked/unlocked outside
 * of this macro.
 *
 * The process is put on the wait queue with an WQ_FLAG_EXCLUSIVE flag
 * set thus when other process waits process on the list if this
 * process is awaken further processes are not considered.
 *
 * wake_up_locked() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * The function will return -ERESTARTSYS if it was interrupted by a
 * signal and 0 if @condition evaluated to true.
 */




/**
 * wait_event_interruptible_exclusive_locked_irq - sleep until a condition gets true
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or a signal is received.
 * The @condition is checked each time the waitqueue @wq is woken up.
 *
 * It must be called with wq.lock being held.  This spinlock is
 * unlocked while sleeping but @condition testing is done while lock
 * is held and when this macro exits the lock is held.
 *
 * The lock is locked/unlocked using spin_lock_irq()/spin_unlock_irq()
 * functions which must match the way they are locked/unlocked outside
 * of this macro.
 *
 * The process is put on the wait queue with an WQ_FLAG_EXCLUSIVE flag
 * set thus when other process waits process on the list if this
 * process is awaken further processes are not considered.
 *
 * wake_up_locked() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * The function will return -ERESTARTSYS if it was interrupted by a
 * signal and 0 if @condition evaluated to true.
 */
/**
 * wait_event_killable - sleep until a condition gets true
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 *
 * The process is put to sleep (TASK_KILLABLE) until the
 * @condition evaluates to true or a signal is received.
 * The @condition is checked each time the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * The function will return -ERESTARTSYS if it was interrupted by a
 * signal and 0 if @condition evaluated to true.
 */
/**
 * wait_event_lock_irq_cmd - sleep until a condition gets true. The
 *			     condition is checked under the lock. This
 *			     is expected to be called with the lock
 *			     taken.
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @lock: a locked spinlock_t, which will be released before cmd
 *	  and schedule() and reacquired afterwards.
 * @cmd: a command which is invoked outside the critical section before
 *	 sleep
 *
 * The process is put to sleep (TASK_UNINTERRUPTIBLE) until the
 * @condition evaluates to true. The @condition is checked each time
 * the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * This is supposed to be called while holding the lock. The lock is
 * dropped before invoking the cmd and going to sleep and is reacquired
 * afterwards.
 */







/**
 * wait_event_lock_irq - sleep until a condition gets true. The
 *			 condition is checked under the lock. This
 *			 is expected to be called with the lock
 *			 taken.
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @lock: a locked spinlock_t, which will be released before schedule()
 *	  and reacquired afterwards.
 *
 * The process is put to sleep (TASK_UNINTERRUPTIBLE) until the
 * @condition evaluates to true. The @condition is checked each time
 * the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * This is supposed to be called while holding the lock. The lock is
 * dropped before going to sleep and is reacquired afterwards.
 */
/**
 * wait_event_interruptible_lock_irq_cmd - sleep until a condition gets true.
 *		The condition is checked under the lock. This is expected to
 *		be called with the lock taken.
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @lock: a locked spinlock_t, which will be released before cmd and
 *	  schedule() and reacquired afterwards.
 * @cmd: a command which is invoked outside the critical section before
 *	 sleep
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or a signal is received. The @condition is
 * checked each time the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * This is supposed to be called while holding the lock. The lock is
 * dropped before invoking the cmd and going to sleep and is reacquired
 * afterwards.
 *
 * The macro will return -ERESTARTSYS if it was interrupted by a signal
 * and 0 if @condition evaluated to true.
 */
/**
 * wait_event_interruptible_lock_irq - sleep until a condition gets true.
 *		The condition is checked under the lock. This is expected
 *		to be called with the lock taken.
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @lock: a locked spinlock_t, which will be released before schedule()
 *	  and reacquired afterwards.
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or signal is received. The @condition is
 * checked each time the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * This is supposed to be called while holding the lock. The lock is
 * dropped before going to sleep and is reacquired afterwards.
 *
 * The macro will return -ERESTARTSYS if it was interrupted by a signal
 * and 0 if @condition evaluated to true.
 */
/**
 * wait_event_interruptible_lock_irq_timeout - sleep until a condition gets
 *		true or a timeout elapses. The condition is checked under
 *		the lock. This is expected to be called with the lock taken.
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @lock: a locked spinlock_t, which will be released before schedule()
 *	  and reacquired afterwards.
 * @timeout: timeout, in jiffies
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or signal is received. The @condition is
 * checked each time the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * This is supposed to be called while holding the lock. The lock is
 * dropped before going to sleep and is reacquired afterwards.
 *
 * The function returns 0 if the @timeout elapsed, -ERESTARTSYS if it
 * was interrupted by a signal, and the remaining jiffies otherwise
 * if the condition evaluated to true before the timeout elapsed.
 */
/*
 * Waitqueues which are removed from the waitqueue_head at wakeup time
 */
void Model1_prepare_to_wait(Model1_wait_queue_head_t *Model1_q, Model1_wait_queue_t *Model1_wait, int Model1_state);
void Model1_prepare_to_wait_exclusive(Model1_wait_queue_head_t *Model1_q, Model1_wait_queue_t *Model1_wait, int Model1_state);
long Model1_prepare_to_wait_event(Model1_wait_queue_head_t *Model1_q, Model1_wait_queue_t *Model1_wait, int Model1_state);
void Model1_finish_wait(Model1_wait_queue_head_t *Model1_q, Model1_wait_queue_t *Model1_wait);
void Model1_abort_exclusive_wait(Model1_wait_queue_head_t *Model1_q, Model1_wait_queue_t *Model1_wait, unsigned int Model1_mode, void *Model1_key);
long Model1_wait_woken(Model1_wait_queue_t *Model1_wait, unsigned Model1_mode, long Model1_timeout);
int Model1_woken_wake_function(Model1_wait_queue_t *Model1_wait, unsigned Model1_mode, int Model1_sync, void *Model1_key);
int Model1_autoremove_wake_function(Model1_wait_queue_t *Model1_wait, unsigned Model1_mode, int Model1_sync, void *Model1_key);
int Model1_wake_bit_function(Model1_wait_queue_t *Model1_wait, unsigned Model1_mode, int Model1_sync, void *Model1_key);
extern int Model1_bit_wait(struct Model1_wait_bit_key *, int);
extern int Model1_bit_wait_io(struct Model1_wait_bit_key *, int);
extern int Model1_bit_wait_timeout(struct Model1_wait_bit_key *, int);
extern int Model1_bit_wait_io_timeout(struct Model1_wait_bit_key *, int);

/**
 * wait_on_bit - wait for a bit to be cleared
 * @word: the word being waited on, a kernel virtual address
 * @bit: the bit of the word being waited on
 * @mode: the task state to sleep in
 *
 * There is a standard hashed waitqueue table for generic use. This
 * is the part of the hashtable's accessor API that waits on a bit.
 * For instance, if one were to have waiters on a bitflag, one would
 * call wait_on_bit() in threads waiting for the bit to clear.
 * One uses wait_on_bit() where one is waiting for the bit to clear,
 * but has no intention of setting it.
 * Returned value will be zero if the bit was cleared, or non-zero
 * if the process received a signal and the mode permitted wakeup
 * on that signal.
 */
static inline __attribute__((no_instrument_function)) int
Model1_wait_on_bit(unsigned long *Model1_word, int Model1_bit, unsigned Model1_mode)
{
 do { Model1__cond_resched(); } while (0);
 if (!(__builtin_constant_p((Model1_bit)) ? Model1_constant_test_bit((Model1_bit), (Model1_word)) : Model1_variable_test_bit((Model1_bit), (Model1_word))))
  return 0;
 return Model1_out_of_line_wait_on_bit(Model1_word, Model1_bit,
           Model1_bit_wait,
           Model1_mode);
}

/**
 * wait_on_bit_io - wait for a bit to be cleared
 * @word: the word being waited on, a kernel virtual address
 * @bit: the bit of the word being waited on
 * @mode: the task state to sleep in
 *
 * Use the standard hashed waitqueue table to wait for a bit
 * to be cleared.  This is similar to wait_on_bit(), but calls
 * io_schedule() instead of schedule() for the actual waiting.
 *
 * Returned value will be zero if the bit was cleared, or non-zero
 * if the process received a signal and the mode permitted wakeup
 * on that signal.
 */
static inline __attribute__((no_instrument_function)) int
Model1_wait_on_bit_io(unsigned long *Model1_word, int Model1_bit, unsigned Model1_mode)
{
 do { Model1__cond_resched(); } while (0);
 if (!(__builtin_constant_p((Model1_bit)) ? Model1_constant_test_bit((Model1_bit), (Model1_word)) : Model1_variable_test_bit((Model1_bit), (Model1_word))))
  return 0;
 return Model1_out_of_line_wait_on_bit(Model1_word, Model1_bit,
           Model1_bit_wait_io,
           Model1_mode);
}

/**
 * wait_on_bit_timeout - wait for a bit to be cleared or a timeout elapses
 * @word: the word being waited on, a kernel virtual address
 * @bit: the bit of the word being waited on
 * @mode: the task state to sleep in
 * @timeout: timeout, in jiffies
 *
 * Use the standard hashed waitqueue table to wait for a bit
 * to be cleared. This is similar to wait_on_bit(), except also takes a
 * timeout parameter.
 *
 * Returned value will be zero if the bit was cleared before the
 * @timeout elapsed, or non-zero if the @timeout elapsed or process
 * received a signal and the mode permitted wakeup on that signal.
 */
static inline __attribute__((no_instrument_function)) int
Model1_wait_on_bit_timeout(unsigned long *Model1_word, int Model1_bit, unsigned Model1_mode,
      unsigned long Model1_timeout)
{
 do { Model1__cond_resched(); } while (0);
 if (!(__builtin_constant_p((Model1_bit)) ? Model1_constant_test_bit((Model1_bit), (Model1_word)) : Model1_variable_test_bit((Model1_bit), (Model1_word))))
  return 0;
 return Model1_out_of_line_wait_on_bit_timeout(Model1_word, Model1_bit,
            Model1_bit_wait_timeout,
            Model1_mode, Model1_timeout);
}

/**
 * wait_on_bit_action - wait for a bit to be cleared
 * @word: the word being waited on, a kernel virtual address
 * @bit: the bit of the word being waited on
 * @action: the function used to sleep, which may take special actions
 * @mode: the task state to sleep in
 *
 * Use the standard hashed waitqueue table to wait for a bit
 * to be cleared, and allow the waiting action to be specified.
 * This is like wait_on_bit() but allows fine control of how the waiting
 * is done.
 *
 * Returned value will be zero if the bit was cleared, or non-zero
 * if the process received a signal and the mode permitted wakeup
 * on that signal.
 */
static inline __attribute__((no_instrument_function)) int
Model1_wait_on_bit_action(unsigned long *Model1_word, int Model1_bit, Model1_wait_bit_action_f *Model1_action,
     unsigned Model1_mode)
{
 do { Model1__cond_resched(); } while (0);
 if (!(__builtin_constant_p((Model1_bit)) ? Model1_constant_test_bit((Model1_bit), (Model1_word)) : Model1_variable_test_bit((Model1_bit), (Model1_word))))
  return 0;
 return Model1_out_of_line_wait_on_bit(Model1_word, Model1_bit, Model1_action, Model1_mode);
}

/**
 * wait_on_bit_lock - wait for a bit to be cleared, when wanting to set it
 * @word: the word being waited on, a kernel virtual address
 * @bit: the bit of the word being waited on
 * @mode: the task state to sleep in
 *
 * There is a standard hashed waitqueue table for generic use. This
 * is the part of the hashtable's accessor API that waits on a bit
 * when one intends to set it, for instance, trying to lock bitflags.
 * For instance, if one were to have waiters trying to set bitflag
 * and waiting for it to clear before setting it, one would call
 * wait_on_bit() in threads waiting to be able to set the bit.
 * One uses wait_on_bit_lock() where one is waiting for the bit to
 * clear with the intention of setting it, and when done, clearing it.
 *
 * Returns zero if the bit was (eventually) found to be clear and was
 * set.  Returns non-zero if a signal was delivered to the process and
 * the @mode allows that signal to wake the process.
 */
static inline __attribute__((no_instrument_function)) int
Model1_wait_on_bit_lock(unsigned long *Model1_word, int Model1_bit, unsigned Model1_mode)
{
 do { Model1__cond_resched(); } while (0);
 if (!Model1_test_and_set_bit(Model1_bit, Model1_word))
  return 0;
 return Model1_out_of_line_wait_on_bit_lock(Model1_word, Model1_bit, Model1_bit_wait, Model1_mode);
}

/**
 * wait_on_bit_lock_io - wait for a bit to be cleared, when wanting to set it
 * @word: the word being waited on, a kernel virtual address
 * @bit: the bit of the word being waited on
 * @mode: the task state to sleep in
 *
 * Use the standard hashed waitqueue table to wait for a bit
 * to be cleared and then to atomically set it.  This is similar
 * to wait_on_bit(), but calls io_schedule() instead of schedule()
 * for the actual waiting.
 *
 * Returns zero if the bit was (eventually) found to be clear and was
 * set.  Returns non-zero if a signal was delivered to the process and
 * the @mode allows that signal to wake the process.
 */
static inline __attribute__((no_instrument_function)) int
Model1_wait_on_bit_lock_io(unsigned long *Model1_word, int Model1_bit, unsigned Model1_mode)
{
 do { Model1__cond_resched(); } while (0);
 if (!Model1_test_and_set_bit(Model1_bit, Model1_word))
  return 0;
 return Model1_out_of_line_wait_on_bit_lock(Model1_word, Model1_bit, Model1_bit_wait_io, Model1_mode);
}

/**
 * wait_on_bit_lock_action - wait for a bit to be cleared, when wanting to set it
 * @word: the word being waited on, a kernel virtual address
 * @bit: the bit of the word being waited on
 * @action: the function used to sleep, which may take special actions
 * @mode: the task state to sleep in
 *
 * Use the standard hashed waitqueue table to wait for a bit
 * to be cleared and then to set it, and allow the waiting action
 * to be specified.
 * This is like wait_on_bit() but allows fine control of how the waiting
 * is done.
 *
 * Returns zero if the bit was (eventually) found to be clear and was
 * set.  Returns non-zero if a signal was delivered to the process and
 * the @mode allows that signal to wake the process.
 */
static inline __attribute__((no_instrument_function)) int
Model1_wait_on_bit_lock_action(unsigned long *Model1_word, int Model1_bit, Model1_wait_bit_action_f *Model1_action,
   unsigned Model1_mode)
{
 do { Model1__cond_resched(); } while (0);
 if (!Model1_test_and_set_bit(Model1_bit, Model1_word))
  return 0;
 return Model1_out_of_line_wait_on_bit_lock(Model1_word, Model1_bit, Model1_action, Model1_mode);
}

/**
 * wait_on_atomic_t - Wait for an atomic_t to become 0
 * @val: The atomic value being waited on, a kernel virtual address
 * @action: the function used to sleep, which may take special actions
 * @mode: the task state to sleep in
 *
 * Wait for an atomic_t to become 0.  We abuse the bit-wait waitqueue table for
 * the purpose of getting a waitqueue, but we set the key to a bit number
 * outside of the target 'word'.
 */
static inline __attribute__((no_instrument_function))
int Model1_wait_on_atomic_t(Model1_atomic_t *Model1_val, int (*Model1_action)(Model1_atomic_t *), unsigned Model1_mode)
{
 do { Model1__cond_resched(); } while (0);
 if (Model1_atomic_read(Model1_val) == 0)
  return 0;
 return Model1_out_of_line_wait_on_atomic_t(Model1_val, Model1_action, Model1_mode);
}






/*
 * Reader/writer consistent mechanism without starving writers. This type of
 * lock for data where the reader wants a consistent set of information
 * and is willing to retry if the information changes. There are two types
 * of readers:
 * 1. Sequence readers which never block a writer but they may have to retry
 *    if a writer is in progress by detecting change in sequence number.
 *    Writers do not wait for a sequence reader.
 * 2. Locking readers which will wait if a writer or another locking reader
 *    is in progress. A locking reader in progress will also block a writer
 *    from going forward. Unlike the regular rwlock, the read lock here is
 *    exclusive so that only one locking reader can get it.
 *
 * This is not as cache friendly as brlock. Also, this may not work well
 * for data that contains pointers, because any writer could
 * invalidate a pointer that a reader was following.
 *
 * Expected non-blocking reader usage:
 * 	do {
 *	    seq = read_seqbegin(&foo);
 * 	...
 *      } while (read_seqretry(&foo, seq));
 *
 *
 * On non-SMP the spin locks disappear but the writer still needs
 * to increment the sequence variables because an interrupt routine could
 * change the state of the data.
 *
 * Based on x86_64 vsyscall gettimeofday 
 * by Keith Owens and Andrea Arcangeli
 */







/*
 * Version using sequence counter only.
 * This can be used when code has its own mutex protecting the
 * updating starting before the write_seqcountbeqin() and ending
 * after the write_seqcount_end().
 */
typedef struct Model1_seqcount {
 unsigned Model1_sequence;



} Model1_seqcount_t;

static inline __attribute__((no_instrument_function)) void Model1___seqcount_init(Model1_seqcount_t *Model1_s, const char *Model1_name,
       struct Model1_lock_class_key *Model1_key)
{
 /*
	 * Make sure we are not reinitializing a held lock:
	 */
 do { (void)(Model1_name); (void)(Model1_key); } while (0);
 Model1_s->Model1_sequence = 0;
}
/**
 * __read_seqcount_begin - begin a seq-read critical section (without barrier)
 * @s: pointer to seqcount_t
 * Returns: count to be passed to read_seqcount_retry
 *
 * __read_seqcount_begin is like read_seqcount_begin, but has no smp_rmb()
 * barrier. Callers should ensure that smp_rmb() or equivalent ordering is
 * provided before actually loading any of the variables that are to be
 * protected in this critical section.
 *
 * Use carefully, only in critical code, and comment how the barrier is
 * provided.
 */
static inline __attribute__((no_instrument_function)) unsigned Model1___read_seqcount_begin(const Model1_seqcount_t *Model1_s)
{
 unsigned Model1_ret;

Model1_repeat:
 Model1_ret = ({ union { typeof(Model1_s->Model1_sequence) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&(Model1_s->Model1_sequence), Model1___u.Model1___c, sizeof(Model1_s->Model1_sequence)); else Model1___read_once_size_nocheck(&(Model1_s->Model1_sequence), Model1___u.Model1___c, sizeof(Model1_s->Model1_sequence)); Model1___u.Model1___val; });
 if (__builtin_expect(!!(Model1_ret & 1), 0)) {
  Model1_cpu_relax();
  goto Model1_repeat;
 }
 return Model1_ret;
}

/**
 * raw_read_seqcount - Read the raw seqcount
 * @s: pointer to seqcount_t
 * Returns: count to be passed to read_seqcount_retry
 *
 * raw_read_seqcount opens a read critical section of the given
 * seqcount without any lockdep checking and without checking or
 * masking the LSB. Calling code is responsible for handling that.
 */
static inline __attribute__((no_instrument_function)) unsigned Model1_raw_read_seqcount(const Model1_seqcount_t *Model1_s)
{
 unsigned Model1_ret = ({ union { typeof(Model1_s->Model1_sequence) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&(Model1_s->Model1_sequence), Model1___u.Model1___c, sizeof(Model1_s->Model1_sequence)); else Model1___read_once_size_nocheck(&(Model1_s->Model1_sequence), Model1___u.Model1___c, sizeof(Model1_s->Model1_sequence)); Model1___u.Model1___val; });
 __asm__ __volatile__("": : :"memory");
 return Model1_ret;
}

/**
 * raw_read_seqcount_begin - start seq-read critical section w/o lockdep
 * @s: pointer to seqcount_t
 * Returns: count to be passed to read_seqcount_retry
 *
 * raw_read_seqcount_begin opens a read critical section of the given
 * seqcount, but without any lockdep checking. Validity of the critical
 * section is tested by checking read_seqcount_retry function.
 */
static inline __attribute__((no_instrument_function)) unsigned Model1_raw_read_seqcount_begin(const Model1_seqcount_t *Model1_s)
{
 unsigned Model1_ret = Model1___read_seqcount_begin(Model1_s);
 __asm__ __volatile__("": : :"memory");
 return Model1_ret;
}

/**
 * read_seqcount_begin - begin a seq-read critical section
 * @s: pointer to seqcount_t
 * Returns: count to be passed to read_seqcount_retry
 *
 * read_seqcount_begin opens a read critical section of the given seqcount.
 * Validity of the critical section is tested by checking read_seqcount_retry
 * function.
 */
static inline __attribute__((no_instrument_function)) unsigned Model1_read_seqcount_begin(const Model1_seqcount_t *Model1_s)
{
                                  ;
 return Model1_raw_read_seqcount_begin(Model1_s);
}

/**
 * raw_seqcount_begin - begin a seq-read critical section
 * @s: pointer to seqcount_t
 * Returns: count to be passed to read_seqcount_retry
 *
 * raw_seqcount_begin opens a read critical section of the given seqcount.
 * Validity of the critical section is tested by checking read_seqcount_retry
 * function.
 *
 * Unlike read_seqcount_begin(), this function will not wait for the count
 * to stabilize. If a writer is active when we begin, we will fail the
 * read_seqcount_retry() instead of stabilizing at the beginning of the
 * critical section.
 */
static inline __attribute__((no_instrument_function)) unsigned Model1_raw_seqcount_begin(const Model1_seqcount_t *Model1_s)
{
 unsigned Model1_ret = ({ union { typeof(Model1_s->Model1_sequence) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&(Model1_s->Model1_sequence), Model1___u.Model1___c, sizeof(Model1_s->Model1_sequence)); else Model1___read_once_size_nocheck(&(Model1_s->Model1_sequence), Model1___u.Model1___c, sizeof(Model1_s->Model1_sequence)); Model1___u.Model1___val; });
 __asm__ __volatile__("": : :"memory");
 return Model1_ret & ~1;
}

/**
 * __read_seqcount_retry - end a seq-read critical section (without barrier)
 * @s: pointer to seqcount_t
 * @start: count, from read_seqcount_begin
 * Returns: 1 if retry is required, else 0
 *
 * __read_seqcount_retry is like read_seqcount_retry, but has no smp_rmb()
 * barrier. Callers should ensure that smp_rmb() or equivalent ordering is
 * provided before actually loading any of the variables that are to be
 * protected in this critical section.
 *
 * Use carefully, only in critical code, and comment how the barrier is
 * provided.
 */
static inline __attribute__((no_instrument_function)) int Model1___read_seqcount_retry(const Model1_seqcount_t *Model1_s, unsigned Model1_start)
{
 return __builtin_expect(!!(Model1_s->Model1_sequence != Model1_start), 0);
}

/**
 * read_seqcount_retry - end a seq-read critical section
 * @s: pointer to seqcount_t
 * @start: count, from read_seqcount_begin
 * Returns: 1 if retry is required, else 0
 *
 * read_seqcount_retry closes a read critical section of the given seqcount.
 * If the critical section was invalid, it must be ignored (and typically
 * retried).
 */
static inline __attribute__((no_instrument_function)) int Model1_read_seqcount_retry(const Model1_seqcount_t *Model1_s, unsigned Model1_start)
{
 __asm__ __volatile__("": : :"memory");
 return Model1___read_seqcount_retry(Model1_s, Model1_start);
}



static inline __attribute__((no_instrument_function)) void Model1_raw_write_seqcount_begin(Model1_seqcount_t *Model1_s)
{
 Model1_s->Model1_sequence++;
 __asm__ __volatile__("": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model1_raw_write_seqcount_end(Model1_seqcount_t *Model1_s)
{
 __asm__ __volatile__("": : :"memory");
 Model1_s->Model1_sequence++;
}

/**
 * raw_write_seqcount_barrier - do a seq write barrier
 * @s: pointer to seqcount_t
 *
 * This can be used to provide an ordering guarantee instead of the
 * usual consistency guarantee. It is one wmb cheaper, because we can
 * collapse the two back-to-back wmb()s.
 *
 *      seqcount_t seq;
 *      bool X = true, Y = false;
 *
 *      void read(void)
 *      {
 *              bool x, y;
 *
 *              do {
 *                      int s = read_seqcount_begin(&seq);
 *
 *                      x = X; y = Y;
 *
 *              } while (read_seqcount_retry(&seq, s));
 *
 *              BUG_ON(!x && !y);
 *      }
 *
 *      void write(void)
 *      {
 *              Y = true;
 *
 *              raw_write_seqcount_barrier(seq);
 *
 *              X = false;
 *      }
 */
static inline __attribute__((no_instrument_function)) void Model1_raw_write_seqcount_barrier(Model1_seqcount_t *Model1_s)
{
 Model1_s->Model1_sequence++;
 __asm__ __volatile__("": : :"memory");
 Model1_s->Model1_sequence++;
}

static inline __attribute__((no_instrument_function)) int Model1_raw_read_seqcount_latch(Model1_seqcount_t *Model1_s)
{
 int Model1_seq = ({ union { typeof(Model1_s->Model1_sequence) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&(Model1_s->Model1_sequence), Model1___u.Model1___c, sizeof(Model1_s->Model1_sequence)); else Model1___read_once_size_nocheck(&(Model1_s->Model1_sequence), Model1___u.Model1___c, sizeof(Model1_s->Model1_sequence)); Model1___u.Model1___val; });
 /* Pairs with the first smp_wmb() in raw_write_seqcount_latch() */
 do { } while (0);
 return Model1_seq;
}

/**
 * raw_write_seqcount_latch - redirect readers to even/odd copy
 * @s: pointer to seqcount_t
 *
 * The latch technique is a multiversion concurrency control method that allows
 * queries during non-atomic modifications. If you can guarantee queries never
 * interrupt the modification -- e.g. the concurrency is strictly between CPUs
 * -- you most likely do not need this.
 *
 * Where the traditional RCU/lockless data structures rely on atomic
 * modifications to ensure queries observe either the old or the new state the
 * latch allows the same for non-atomic updates. The trade-off is doubling the
 * cost of storage; we have to maintain two copies of the entire data
 * structure.
 *
 * Very simply put: we first modify one copy and then the other. This ensures
 * there is always one copy in a stable state, ready to give us an answer.
 *
 * The basic form is a data structure like:
 *
 * struct latch_struct {
 *	seqcount_t		seq;
 *	struct data_struct	data[2];
 * };
 *
 * Where a modification, which is assumed to be externally serialized, does the
 * following:
 *
 * void latch_modify(struct latch_struct *latch, ...)
 * {
 *	smp_wmb();	<- Ensure that the last data[1] update is visible
 *	latch->seq++;
 *	smp_wmb();	<- Ensure that the seqcount update is visible
 *
 *	modify(latch->data[0], ...);
 *
 *	smp_wmb();	<- Ensure that the data[0] update is visible
 *	latch->seq++;
 *	smp_wmb();	<- Ensure that the seqcount update is visible
 *
 *	modify(latch->data[1], ...);
 * }
 *
 * The query will have a form like:
 *
 * struct entry *latch_query(struct latch_struct *latch, ...)
 * {
 *	struct entry *entry;
 *	unsigned seq, idx;
 *
 *	do {
 *		seq = raw_read_seqcount_latch(&latch->seq);
 *
 *		idx = seq & 0x01;
 *		entry = data_query(latch->data[idx], ...);
 *
 *		smp_rmb();
 *	} while (seq != latch->seq);
 *
 *	return entry;
 * }
 *
 * So during the modification, queries are first redirected to data[1]. Then we
 * modify data[0]. When that is complete, we redirect queries back to data[0]
 * and we can modify data[1].
 *
 * NOTE: The non-requirement for atomic modifications does _NOT_ include
 *       the publishing of new entries in the case where data is a dynamic
 *       data structure.
 *
 *       An iteration might start in data[0] and get suspended long enough
 *       to miss an entire modification sequence, once it resumes it might
 *       observe the new entry.
 *
 * NOTE: When data is a dynamic data structure; one should use regular RCU
 *       patterns to manage the lifetimes of the objects within.
 */
static inline __attribute__((no_instrument_function)) void Model1_raw_write_seqcount_latch(Model1_seqcount_t *Model1_s)
{
       __asm__ __volatile__("": : :"memory"); /* prior stores before incrementing "sequence" */
       Model1_s->Model1_sequence++;
       __asm__ __volatile__("": : :"memory"); /* increment "sequence" before following stores */
}

/*
 * Sequence counter only version assumes that callers are using their
 * own mutexing.
 */
static inline __attribute__((no_instrument_function)) void Model1_write_seqcount_begin_nested(Model1_seqcount_t *Model1_s, int Model1_subclass)
{
 Model1_raw_write_seqcount_begin(Model1_s);
 do { } while (0);
}

static inline __attribute__((no_instrument_function)) void Model1_write_seqcount_begin(Model1_seqcount_t *Model1_s)
{
 Model1_write_seqcount_begin_nested(Model1_s, 0);
}

static inline __attribute__((no_instrument_function)) void Model1_write_seqcount_end(Model1_seqcount_t *Model1_s)
{
 do { } while (0);
 Model1_raw_write_seqcount_end(Model1_s);
}

/**
 * write_seqcount_invalidate - invalidate in-progress read-side seq operations
 * @s: pointer to seqcount_t
 *
 * After write_seqcount_invalidate, no read-side seq operations will complete
 * successfully and see data older than this.
 */
static inline __attribute__((no_instrument_function)) void Model1_write_seqcount_invalidate(Model1_seqcount_t *Model1_s)
{
 __asm__ __volatile__("": : :"memory");
 Model1_s->Model1_sequence+=2;
}

typedef struct {
 struct Model1_seqcount Model1_seqcount;
 Model1_spinlock_t Model1_lock;
} Model1_seqlock_t;

/*
 * These macros triggered gcc-3.x compile-time problems.  We think these are
 * OK now.  Be cautious.
 */
/*
 * Read side functions for starting and finalizing a read side section.
 */
static inline __attribute__((no_instrument_function)) unsigned Model1_read_seqbegin(const Model1_seqlock_t *Model1_sl)
{
 return Model1_read_seqcount_begin(&Model1_sl->Model1_seqcount);
}

static inline __attribute__((no_instrument_function)) unsigned Model1_read_seqretry(const Model1_seqlock_t *Model1_sl, unsigned Model1_start)
{
 return Model1_read_seqcount_retry(&Model1_sl->Model1_seqcount, Model1_start);
}

/*
 * Lock out other writers and update the count.
 * Acts like a normal spin_lock/unlock.
 * Don't need preempt_disable() because that is in the spin_lock already.
 */
static inline __attribute__((no_instrument_function)) void Model1_write_seqlock(Model1_seqlock_t *Model1_sl)
{
 Model1_spin_lock(&Model1_sl->Model1_lock);
 Model1_write_seqcount_begin(&Model1_sl->Model1_seqcount);
}

static inline __attribute__((no_instrument_function)) void Model1_write_sequnlock(Model1_seqlock_t *Model1_sl)
{
 Model1_write_seqcount_end(&Model1_sl->Model1_seqcount);
 Model1_spin_unlock(&Model1_sl->Model1_lock);
}

static inline __attribute__((no_instrument_function)) void Model1_write_seqlock_bh(Model1_seqlock_t *Model1_sl)
{
 Model1_spin_lock_bh(&Model1_sl->Model1_lock);
 Model1_write_seqcount_begin(&Model1_sl->Model1_seqcount);
}

static inline __attribute__((no_instrument_function)) void Model1_write_sequnlock_bh(Model1_seqlock_t *Model1_sl)
{
 Model1_write_seqcount_end(&Model1_sl->Model1_seqcount);
 Model1_spin_unlock_bh(&Model1_sl->Model1_lock);
}

static inline __attribute__((no_instrument_function)) void Model1_write_seqlock_irq(Model1_seqlock_t *Model1_sl)
{
 Model1_spin_lock_irq(&Model1_sl->Model1_lock);
 Model1_write_seqcount_begin(&Model1_sl->Model1_seqcount);
}

static inline __attribute__((no_instrument_function)) void Model1_write_sequnlock_irq(Model1_seqlock_t *Model1_sl)
{
 Model1_write_seqcount_end(&Model1_sl->Model1_seqcount);
 Model1_spin_unlock_irq(&Model1_sl->Model1_lock);
}

static inline __attribute__((no_instrument_function)) unsigned long Model1___write_seqlock_irqsave(Model1_seqlock_t *Model1_sl)
{
 unsigned long Model1_flags;

 do { do { ({ unsigned long Model1___dummy; typeof(Model1_flags) Model1___dummy2; (void)(&Model1___dummy == &Model1___dummy2); 1; }); Model1_flags = Model1__raw_spin_lock_irqsave(Model1_spinlock_check(&Model1_sl->Model1_lock)); } while (0); } while (0);
 Model1_write_seqcount_begin(&Model1_sl->Model1_seqcount);
 return Model1_flags;
}




static inline __attribute__((no_instrument_function)) void
Model1_write_sequnlock_irqrestore(Model1_seqlock_t *Model1_sl, unsigned long Model1_flags)
{
 Model1_write_seqcount_end(&Model1_sl->Model1_seqcount);
 Model1_spin_unlock_irqrestore(&Model1_sl->Model1_lock, Model1_flags);
}

/*
 * A locking reader exclusively locks out other writers and locking readers,
 * but doesn't update the sequence number. Acts like a normal spin_lock/unlock.
 * Don't need preempt_disable() because that is in the spin_lock already.
 */
static inline __attribute__((no_instrument_function)) void Model1_read_seqlock_excl(Model1_seqlock_t *Model1_sl)
{
 Model1_spin_lock(&Model1_sl->Model1_lock);
}

static inline __attribute__((no_instrument_function)) void Model1_read_sequnlock_excl(Model1_seqlock_t *Model1_sl)
{
 Model1_spin_unlock(&Model1_sl->Model1_lock);
}

/**
 * read_seqbegin_or_lock - begin a sequence number check or locking block
 * @lock: sequence lock
 * @seq : sequence number to be checked
 *
 * First try it once optimistically without taking the lock. If that fails,
 * take the lock. The sequence number is also used as a marker for deciding
 * whether to be a reader (even) or writer (odd).
 * N.B. seq must be initialized to an even number to begin with.
 */
static inline __attribute__((no_instrument_function)) void Model1_read_seqbegin_or_lock(Model1_seqlock_t *Model1_lock, int *Model1_seq)
{
 if (!(*Model1_seq & 1)) /* Even */
  *Model1_seq = Model1_read_seqbegin(Model1_lock);
 else /* Odd */
  Model1_read_seqlock_excl(Model1_lock);
}

static inline __attribute__((no_instrument_function)) int Model1_need_seqretry(Model1_seqlock_t *Model1_lock, int Model1_seq)
{
 return !(Model1_seq & 1) && Model1_read_seqretry(Model1_lock, Model1_seq);
}

static inline __attribute__((no_instrument_function)) void Model1_done_seqretry(Model1_seqlock_t *Model1_lock, int Model1_seq)
{
 if (Model1_seq & 1)
  Model1_read_sequnlock_excl(Model1_lock);
}

static inline __attribute__((no_instrument_function)) void Model1_read_seqlock_excl_bh(Model1_seqlock_t *Model1_sl)
{
 Model1_spin_lock_bh(&Model1_sl->Model1_lock);
}

static inline __attribute__((no_instrument_function)) void Model1_read_sequnlock_excl_bh(Model1_seqlock_t *Model1_sl)
{
 Model1_spin_unlock_bh(&Model1_sl->Model1_lock);
}

static inline __attribute__((no_instrument_function)) void Model1_read_seqlock_excl_irq(Model1_seqlock_t *Model1_sl)
{
 Model1_spin_lock_irq(&Model1_sl->Model1_lock);
}

static inline __attribute__((no_instrument_function)) void Model1_read_sequnlock_excl_irq(Model1_seqlock_t *Model1_sl)
{
 Model1_spin_unlock_irq(&Model1_sl->Model1_lock);
}

static inline __attribute__((no_instrument_function)) unsigned long Model1___read_seqlock_excl_irqsave(Model1_seqlock_t *Model1_sl)
{
 unsigned long Model1_flags;

 do { do { ({ unsigned long Model1___dummy; typeof(Model1_flags) Model1___dummy2; (void)(&Model1___dummy == &Model1___dummy2); 1; }); Model1_flags = Model1__raw_spin_lock_irqsave(Model1_spinlock_check(&Model1_sl->Model1_lock)); } while (0); } while (0);
 return Model1_flags;
}




static inline __attribute__((no_instrument_function)) void
Model1_read_sequnlock_excl_irqrestore(Model1_seqlock_t *Model1_sl, unsigned long Model1_flags)
{
 Model1_spin_unlock_irqrestore(&Model1_sl->Model1_lock, Model1_flags);
}

static inline __attribute__((no_instrument_function)) unsigned long
Model1_read_seqbegin_or_lock_irqsave(Model1_seqlock_t *Model1_lock, int *Model1_seq)
{
 unsigned long Model1_flags = 0;

 if (!(*Model1_seq & 1)) /* Even */
  *Model1_seq = Model1_read_seqbegin(Model1_lock);
 else /* Odd */
  do { Model1_flags = Model1___read_seqlock_excl_irqsave(Model1_lock); } while (0);

 return Model1_flags;
}

static inline __attribute__((no_instrument_function)) void
Model1_done_seqretry_irqrestore(Model1_seqlock_t *Model1_lock, int Model1_seq, unsigned long Model1_flags)
{
 if (Model1_seq & 1)
  Model1_read_sequnlock_excl_irqrestore(Model1_lock, Model1_flags);
}



/*
 * Nodemasks provide a bitmap suitable for representing the
 * set of Node's in a system, one bit position per Node number.
 *
 * See detailed comments in the file linux/bitmap.h describing the
 * data type on which these nodemasks are based.
 *
 * For details of nodemask_parse_user(), see bitmap_parse_user() in
 * lib/bitmap.c.  For details of nodelist_parse(), see bitmap_parselist(),
 * also in bitmap.c.  For details of node_remap(), see bitmap_bitremap in
 * lib/bitmap.c.  For details of nodes_remap(), see bitmap_remap in
 * lib/bitmap.c.  For details of nodes_onto(), see bitmap_onto in
 * lib/bitmap.c.  For details of nodes_fold(), see bitmap_fold in
 * lib/bitmap.c.
 *
 * The available nodemask operations are:
 *
 * void node_set(node, mask)		turn on bit 'node' in mask
 * void node_clear(node, mask)		turn off bit 'node' in mask
 * void nodes_setall(mask)		set all bits
 * void nodes_clear(mask)		clear all bits
 * int node_isset(node, mask)		true iff bit 'node' set in mask
 * int node_test_and_set(node, mask)	test and set bit 'node' in mask
 *
 * void nodes_and(dst, src1, src2)	dst = src1 & src2  [intersection]
 * void nodes_or(dst, src1, src2)	dst = src1 | src2  [union]
 * void nodes_xor(dst, src1, src2)	dst = src1 ^ src2
 * void nodes_andnot(dst, src1, src2)	dst = src1 & ~src2
 * void nodes_complement(dst, src)	dst = ~src
 *
 * int nodes_equal(mask1, mask2)	Does mask1 == mask2?
 * int nodes_intersects(mask1, mask2)	Do mask1 and mask2 intersect?
 * int nodes_subset(mask1, mask2)	Is mask1 a subset of mask2?
 * int nodes_empty(mask)		Is mask empty (no bits sets)?
 * int nodes_full(mask)			Is mask full (all bits sets)?
 * int nodes_weight(mask)		Hamming weight - number of set bits
 *
 * void nodes_shift_right(dst, src, n)	Shift right
 * void nodes_shift_left(dst, src, n)	Shift left
 *
 * int first_node(mask)			Number lowest set bit, or MAX_NUMNODES
 * int next_node(node, mask)		Next node past 'node', or MAX_NUMNODES
 * int next_node_in(node, mask)		Next node past 'node', or wrap to first,
 *					or MAX_NUMNODES
 * int first_unset_node(mask)		First node not set in mask, or 
 *					MAX_NUMNODES
 *
 * nodemask_t nodemask_of_node(node)	Return nodemask with bit 'node' set
 * NODE_MASK_ALL			Initializer - all bits set
 * NODE_MASK_NONE			Initializer - no bits set
 * unsigned long *nodes_addr(mask)	Array of unsigned long's in mask
 *
 * int nodemask_parse_user(ubuf, ulen, mask)	Parse ascii string as nodemask
 * int nodelist_parse(buf, map)		Parse ascii string as nodelist
 * int node_remap(oldbit, old, new)	newbit = map(old, new)(oldbit)
 * void nodes_remap(dst, src, old, new)	*dst = map(old, new)(src)
 * void nodes_onto(dst, orig, relmap)	*dst = orig relative to relmap
 * void nodes_fold(dst, orig, sz)	dst bits = orig bits mod sz
 *
 * for_each_node_mask(node, mask)	for-loop node over mask
 *
 * int num_online_nodes()		Number of online Nodes
 * int num_possible_nodes()		Number of all possible Nodes
 *
 * int node_random(mask)		Random node with set bit in mask
 *
 * int node_online(node)		Is some node online?
 * int node_possible(node)		Is some node possible?
 *
 * node_set_online(node)		set bit 'node' in node_online_map
 * node_set_offline(node)		clear bit 'node' in node_online_map
 *
 * for_each_node(node)			for-loop node over node_possible_map
 * for_each_online_node(node)		for-loop node over node_online_map
 *
 * Subtlety:
 * 1) The 'type-checked' form of node_isset() causes gcc (3.3.2, anyway)
 *    to generate slightly worse code.  So use a simple one-line #define
 *    for node_isset(), instead of wrapping an inline inside a macro, the
 *    way we do the other calls.
 *
 * NODEMASK_SCRATCH
 * When doing above logical AND, OR, XOR, Remap operations the callers tend to
 * need temporary nodemask_t's on the stack. But if NODES_SHIFT is large,
 * nodemask_t's consume too much stack space.  NODEMASK_SCRATCH is a helper
 * for such situations. See below and CPUMASK_ALLOC also.
 */






typedef struct { unsigned long Model1_bits[((((1 << 6)) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))]; } Model1_nodemask_t;
extern Model1_nodemask_t Model1__unused_nodemask_arg_;

/**
 * nodemask_pr_args - printf args to output a nodemask
 * @maskp: nodemask to be printed
 *
 * Can be used to provide arguments for '%*pb[l]' when printing a nodemask.
 */


/*
 * The inline keyword gives the compiler room to decide to inline, or
 * not inline a function as it sees best.  However, as these functions
 * are called in both __init and non-__init functions, if they are not
 * inlined we will end up with a section mis-match error (of the type of
 * freeable items not being freed).  So we must use __always_inline here
 * to fix the problem.  If other functions in the future also end up in
 * this situation they will also need to be annotated as __always_inline
 */

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___node_set(int Model1_node, volatile Model1_nodemask_t *Model1_dstp)
{
 Model1_set_bit(Model1_node, Model1_dstp->Model1_bits);
}


static inline __attribute__((no_instrument_function)) void Model1___node_clear(int Model1_node, volatile Model1_nodemask_t *Model1_dstp)
{
 Model1_clear_bit(Model1_node, Model1_dstp->Model1_bits);
}


static inline __attribute__((no_instrument_function)) void Model1___nodes_setall(Model1_nodemask_t *Model1_dstp, unsigned int Model1_nbits)
{
 Model1_bitmap_fill(Model1_dstp->Model1_bits, Model1_nbits);
}


static inline __attribute__((no_instrument_function)) void Model1___nodes_clear(Model1_nodemask_t *Model1_dstp, unsigned int Model1_nbits)
{
 Model1_bitmap_zero(Model1_dstp->Model1_bits, Model1_nbits);
}

/* No static inline type checking - see Subtlety (1) above. */




static inline __attribute__((no_instrument_function)) int Model1___node_test_and_set(int Model1_node, Model1_nodemask_t *Model1_addr)
{
 return Model1_test_and_set_bit(Model1_node, Model1_addr->Model1_bits);
}



static inline __attribute__((no_instrument_function)) void Model1___nodes_and(Model1_nodemask_t *Model1_dstp, const Model1_nodemask_t *Model1_src1p,
     const Model1_nodemask_t *Model1_src2p, unsigned int Model1_nbits)
{
 Model1_bitmap_and(Model1_dstp->Model1_bits, Model1_src1p->Model1_bits, Model1_src2p->Model1_bits, Model1_nbits);
}



static inline __attribute__((no_instrument_function)) void Model1___nodes_or(Model1_nodemask_t *Model1_dstp, const Model1_nodemask_t *Model1_src1p,
     const Model1_nodemask_t *Model1_src2p, unsigned int Model1_nbits)
{
 Model1_bitmap_or(Model1_dstp->Model1_bits, Model1_src1p->Model1_bits, Model1_src2p->Model1_bits, Model1_nbits);
}



static inline __attribute__((no_instrument_function)) void Model1___nodes_xor(Model1_nodemask_t *Model1_dstp, const Model1_nodemask_t *Model1_src1p,
     const Model1_nodemask_t *Model1_src2p, unsigned int Model1_nbits)
{
 Model1_bitmap_xor(Model1_dstp->Model1_bits, Model1_src1p->Model1_bits, Model1_src2p->Model1_bits, Model1_nbits);
}



static inline __attribute__((no_instrument_function)) void Model1___nodes_andnot(Model1_nodemask_t *Model1_dstp, const Model1_nodemask_t *Model1_src1p,
     const Model1_nodemask_t *Model1_src2p, unsigned int Model1_nbits)
{
 Model1_bitmap_andnot(Model1_dstp->Model1_bits, Model1_src1p->Model1_bits, Model1_src2p->Model1_bits, Model1_nbits);
}



static inline __attribute__((no_instrument_function)) void Model1___nodes_complement(Model1_nodemask_t *Model1_dstp,
     const Model1_nodemask_t *Model1_srcp, unsigned int Model1_nbits)
{
 Model1_bitmap_complement(Model1_dstp->Model1_bits, Model1_srcp->Model1_bits, Model1_nbits);
}



static inline __attribute__((no_instrument_function)) int Model1___nodes_equal(const Model1_nodemask_t *Model1_src1p,
     const Model1_nodemask_t *Model1_src2p, unsigned int Model1_nbits)
{
 return Model1_bitmap_equal(Model1_src1p->Model1_bits, Model1_src2p->Model1_bits, Model1_nbits);
}



static inline __attribute__((no_instrument_function)) int Model1___nodes_intersects(const Model1_nodemask_t *Model1_src1p,
     const Model1_nodemask_t *Model1_src2p, unsigned int Model1_nbits)
{
 return Model1_bitmap_intersects(Model1_src1p->Model1_bits, Model1_src2p->Model1_bits, Model1_nbits);
}



static inline __attribute__((no_instrument_function)) int Model1___nodes_subset(const Model1_nodemask_t *Model1_src1p,
     const Model1_nodemask_t *Model1_src2p, unsigned int Model1_nbits)
{
 return Model1_bitmap_subset(Model1_src1p->Model1_bits, Model1_src2p->Model1_bits, Model1_nbits);
}


static inline __attribute__((no_instrument_function)) int Model1___nodes_empty(const Model1_nodemask_t *Model1_srcp, unsigned int Model1_nbits)
{
 return Model1_bitmap_empty(Model1_srcp->Model1_bits, Model1_nbits);
}


static inline __attribute__((no_instrument_function)) int Model1___nodes_full(const Model1_nodemask_t *Model1_srcp, unsigned int Model1_nbits)
{
 return Model1_bitmap_full(Model1_srcp->Model1_bits, Model1_nbits);
}


static inline __attribute__((no_instrument_function)) int Model1___nodes_weight(const Model1_nodemask_t *Model1_srcp, unsigned int Model1_nbits)
{
 return Model1_bitmap_weight(Model1_srcp->Model1_bits, Model1_nbits);
}



static inline __attribute__((no_instrument_function)) void Model1___nodes_shift_right(Model1_nodemask_t *Model1_dstp,
     const Model1_nodemask_t *Model1_srcp, int Model1_n, int Model1_nbits)
{
 Model1_bitmap_shift_right(Model1_dstp->Model1_bits, Model1_srcp->Model1_bits, Model1_n, Model1_nbits);
}



static inline __attribute__((no_instrument_function)) void Model1___nodes_shift_left(Model1_nodemask_t *Model1_dstp,
     const Model1_nodemask_t *Model1_srcp, int Model1_n, int Model1_nbits)
{
 Model1_bitmap_shift_left(Model1_dstp->Model1_bits, Model1_srcp->Model1_bits, Model1_n, Model1_nbits);
}

/* FIXME: better would be to fix all architectures to never return
          > MAX_NUMNODES, then the silly min_ts could be dropped. */


static inline __attribute__((no_instrument_function)) int Model1___first_node(const Model1_nodemask_t *Model1_srcp)
{
 return ({ int Model1___min1 = ((1 << 6)); int Model1___min2 = (Model1_find_first_bit(Model1_srcp->Model1_bits, (1 << 6))); Model1___min1 < Model1___min2 ? Model1___min1: Model1___min2; });
}


static inline __attribute__((no_instrument_function)) int Model1___next_node(int Model1_n, const Model1_nodemask_t *Model1_srcp)
{
 return ({ int Model1___min1 = ((1 << 6)); int Model1___min2 = (Model1_find_next_bit(Model1_srcp->Model1_bits, (1 << 6), Model1_n+1)); Model1___min1 < Model1___min2 ? Model1___min1: Model1___min2; });
}

/*
 * Find the next present node in src, starting after node n, wrapping around to
 * the first node in src if needed.  Returns MAX_NUMNODES if src is empty.
 */

int Model1___next_node_in(int Model1_node, const Model1_nodemask_t *Model1_srcp);

static inline __attribute__((no_instrument_function)) void Model1_init_nodemask_of_node(Model1_nodemask_t *Model1_mask, int Model1_node)
{
 Model1___nodes_clear(&(*Model1_mask), (1 << 6));
 Model1___node_set((Model1_node), &(*Model1_mask));
}
static inline __attribute__((no_instrument_function)) int Model1___first_unset_node(const Model1_nodemask_t *Model1_maskp)
{
 return ({ int Model1___min1 = ((1 << 6)); int Model1___min2 = (Model1_find_first_zero_bit(Model1_maskp->Model1_bits, (1 << 6))); Model1___min1 < Model1___min2 ? Model1___min1: Model1___min2; });

}
static inline __attribute__((no_instrument_function)) int Model1___nodemask_parse_user(const char *Model1_buf, int Model1_len,
     Model1_nodemask_t *Model1_dstp, int Model1_nbits)
{
 return Model1_bitmap_parse_user(Model1_buf, Model1_len, Model1_dstp->Model1_bits, Model1_nbits);
}


static inline __attribute__((no_instrument_function)) int Model1___nodelist_parse(const char *Model1_buf, Model1_nodemask_t *Model1_dstp, int Model1_nbits)
{
 return Model1_bitmap_parselist(Model1_buf, Model1_dstp->Model1_bits, Model1_nbits);
}



static inline __attribute__((no_instrument_function)) int Model1___node_remap(int Model1_oldbit,
  const Model1_nodemask_t *Model1_oldp, const Model1_nodemask_t *Model1_newp, int Model1_nbits)
{
 return Model1_bitmap_bitremap(Model1_oldbit, Model1_oldp->Model1_bits, Model1_newp->Model1_bits, Model1_nbits);
}



static inline __attribute__((no_instrument_function)) void Model1___nodes_remap(Model1_nodemask_t *Model1_dstp, const Model1_nodemask_t *Model1_srcp,
  const Model1_nodemask_t *Model1_oldp, const Model1_nodemask_t *Model1_newp, int Model1_nbits)
{
 Model1_bitmap_remap(Model1_dstp->Model1_bits, Model1_srcp->Model1_bits, Model1_oldp->Model1_bits, Model1_newp->Model1_bits, Model1_nbits);
}



static inline __attribute__((no_instrument_function)) void Model1___nodes_onto(Model1_nodemask_t *Model1_dstp, const Model1_nodemask_t *Model1_origp,
  const Model1_nodemask_t *Model1_relmapp, int Model1_nbits)
{
 Model1_bitmap_onto(Model1_dstp->Model1_bits, Model1_origp->Model1_bits, Model1_relmapp->Model1_bits, Model1_nbits);
}



static inline __attribute__((no_instrument_function)) void Model1___nodes_fold(Model1_nodemask_t *Model1_dstp, const Model1_nodemask_t *Model1_origp,
  int Model1_sz, int Model1_nbits)
{
 Model1_bitmap_fold(Model1_dstp->Model1_bits, Model1_origp->Model1_bits, Model1_sz, Model1_nbits);
}
/*
 * Bitmasks that are kept for all the nodes.
 */
enum Model1_node_states {
 Model1_N_POSSIBLE, /* The node could become online at some point */
 Model1_N_ONLINE, /* The node is online */
 Model1_N_NORMAL_MEMORY, /* The node has regular memory */



 Model1_N_HIGH_MEMORY = Model1_N_NORMAL_MEMORY,




 Model1_N_MEMORY = Model1_N_HIGH_MEMORY,

 Model1_N_CPU, /* The node has one or more cpus */
 Model1_NR_NODE_STATES
};

/*
 * The following particular system nodemasks and operations
 * on them manage all possible and online nodes.
 */

extern Model1_nodemask_t Model1_node_states[Model1_NR_NODE_STATES];


static inline __attribute__((no_instrument_function)) int Model1_node_state(int Model1_node, enum Model1_node_states Model1_state)
{
 return (__builtin_constant_p(((Model1_node))) ? Model1_constant_test_bit(((Model1_node)), ((Model1_node_states[Model1_state]).Model1_bits)) : Model1_variable_test_bit(((Model1_node)), ((Model1_node_states[Model1_state]).Model1_bits)));
}

static inline __attribute__((no_instrument_function)) void Model1_node_set_state(int Model1_node, enum Model1_node_states Model1_state)
{
 Model1___node_set(Model1_node, &Model1_node_states[Model1_state]);
}

static inline __attribute__((no_instrument_function)) void Model1_node_clear_state(int Model1_node, enum Model1_node_states Model1_state)
{
 Model1___node_clear(Model1_node, &Model1_node_states[Model1_state]);
}

static inline __attribute__((no_instrument_function)) int Model1_num_node_state(enum Model1_node_states Model1_state)
{
 return Model1___nodes_weight(&(Model1_node_states[Model1_state]), (1 << 6));
}






static inline __attribute__((no_instrument_function)) int Model1_next_online_node(int Model1_nid)
{
 return Model1___next_node((Model1_nid), &(Model1_node_states[Model1_N_ONLINE]));
}
static inline __attribute__((no_instrument_function)) int Model1_next_memory_node(int Model1_nid)
{
 return Model1___next_node((Model1_nid), &(Model1_node_states[Model1_N_MEMORY]));
}

extern int Model1_nr_node_ids;
extern int Model1_nr_online_nodes;

static inline __attribute__((no_instrument_function)) void Model1_node_set_online(int Model1_nid)
{
 Model1_node_set_state(Model1_nid, Model1_N_ONLINE);
 Model1_nr_online_nodes = Model1_num_node_state(Model1_N_ONLINE);
}

static inline __attribute__((no_instrument_function)) void Model1_node_set_offline(int Model1_nid)
{
 Model1_node_clear_state(Model1_nid, Model1_N_ONLINE);
 Model1_nr_online_nodes = Model1_num_node_state(Model1_N_ONLINE);
}
extern int Model1_node_random(const Model1_nodemask_t *Model1_maskp);
/*
 * For nodemask scrach area.
 * NODEMASK_ALLOC(type, name) allocates an object with a specified type and
 * name.
 */
/* A example struture for using NODEMASK_ALLOC, used in mempolicy. */
struct Model1_nodemask_scratch {
 Model1_nodemask_t Model1_mask1;
 Model1_nodemask_t Model1_mask2;
};
/*
 * Macros for manipulating and testing flags related to a
 * pageblock_nr_pages number of pages.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation version 2 of the License
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
 *
 * Copyright (C) IBM Corporation, 2006
 *
 * Original author, Mel Gorman
 * Major cleanups and reduction of bit operations, Andy Whitcroft
 */





/* Bit indices that affect a whole block of pages */
enum Model1_pageblock_bits {
 Model1_PB_migrate,
 Model1_PB_migrate_end = Model1_PB_migrate + 3 - 1,
   /* 3 bits required for migrate types */
 Model1_PB_migrate_skip,/* If set the block is skipped by compaction */

 /*
	 * Assume the bits will always align on a word. If this assumption
	 * changes then get/set pageblock needs updating.
	 */
 Model1_NR_PAGEBLOCK_BITS
};
/* Huge pages are a constant size */
/* Forward declaration */
struct Model1_page;

unsigned long Model1_get_pfnblock_flags_mask(struct Model1_page *Model1_page,
    unsigned long Model1_pfn,
    unsigned long Model1_end_bitidx,
    unsigned long Model1_mask);

void Model1_set_pfnblock_flags_mask(struct Model1_page *Model1_page,
    unsigned long Model1_flags,
    unsigned long Model1_pfn,
    unsigned long Model1_end_bitidx,
    unsigned long Model1_mask);

/* Declarations for getting and setting flags. See mm/page_alloc.c */






/*
 * DO NOT MODIFY.
 *
 * This file was generated by Kbuild
 */

/*
 * When a memory allocation must conform to specific limitations (such
 * as being suitable for DMA) the caller will pass in hints to the
 * allocator in the gfp_mask, in the zone modifier bits.  These bits
 * are used to select a priority ordered list of memory zones which
 * match the requested limits. See gfp_zone() in include/linux/gfp.h
 */
/* SECTION_SHIFT	#bits space required to store a section # */




/*
 * page->flags layout:
 *
 * There are five possibilities for how page->flags get laid out.  The first
 * pair is for the normal case without sparsemem. The second pair is for
 * sparsemem when there is plenty of space for node and section information.
 * The last is when there is insufficient space in page->flags and a separate
 * lookup is necessary.
 *
 * No sparsemem or sparsemem vmemmap: |       NODE     | ZONE |             ... | FLAGS |
 *      " plus space for last_cpupid: |       NODE     | ZONE | LAST_CPUPID ... | FLAGS |
 * classic sparse with space for node:| SECTION | NODE | ZONE |             ... | FLAGS |
 *      " plus space for last_cpupid: | SECTION | NODE | ZONE | LAST_CPUPID ... | FLAGS |
 * classic sparse no space for node:  | SECTION |     ZONE    | ... | FLAGS |
 */
/*
 * We are going to use the flags for the page to node mapping if its in
 * there.  This includes the case where there is no node, so it is implicit.
 */



/* Free memory management - zoned buddy allocator.  */







/*
 * PAGE_ALLOC_COSTLY_ORDER is the order at which allocations are deemed
 * costly to service.  That is between allocation orders which should
 * coalesce naturally under reasonable reclaim pressure and those which
 * will not.
 */


enum {
 Model1_MIGRATE_UNMOVABLE,
 Model1_MIGRATE_MOVABLE,
 Model1_MIGRATE_RECLAIMABLE,
 Model1_MIGRATE_PCPTYPES, /* the number of types on the pcp lists */
 Model1_MIGRATE_HIGHATOMIC = Model1_MIGRATE_PCPTYPES,
 Model1_MIGRATE_TYPES
};

/* In mm/page_alloc.c; keep in sync also with show_migration_types() there */
extern char * const Model1_migratetype_names[Model1_MIGRATE_TYPES];
extern int Model1_page_group_by_mobility_disabled;
struct Model1_free_area {
 struct Model1_list_head Model1_free_list[Model1_MIGRATE_TYPES];
 unsigned long Model1_nr_free;
};

struct Model1_pglist_data;

/*
 * zone->lock and the zone lru_lock are two of the hottest locks in the kernel.
 * So add a wild amount of padding here to ensure that they fall into separate
 * cachelines.  There are very few zone structures in the machine, so space
 * consumption is not a concern here.
 */

struct Model1_zone_padding {
 char Model1_x[0];
} __attribute__((__aligned__(1 << (6))));





enum Model1_zone_stat_item {
 /* First 128 byte cacheline (assuming 64 bit words) */
 Model1_NR_FREE_PAGES,
 Model1_NR_ZONE_LRU_BASE, /* Used only for compaction and reclaim retry */
 Model1_NR_ZONE_INACTIVE_ANON = Model1_NR_ZONE_LRU_BASE,
 Model1_NR_ZONE_ACTIVE_ANON,
 Model1_NR_ZONE_INACTIVE_FILE,
 Model1_NR_ZONE_ACTIVE_FILE,
 Model1_NR_ZONE_UNEVICTABLE,
 Model1_NR_ZONE_WRITE_PENDING, /* Count of dirty, writeback and unstable pages */
 Model1_NR_MLOCK, /* mlock()ed pages found and moved off LRU */
 Model1_NR_SLAB_RECLAIMABLE,
 Model1_NR_SLAB_UNRECLAIMABLE,
 Model1_NR_PAGETABLE, /* used for pagetables */
 Model1_NR_KERNEL_STACK_KB, /* measured in KiB */
 /* Second 128 byte cacheline */
 Model1_NR_BOUNCE,




 Model1_NUMA_HIT, /* allocated in intended node */
 Model1_NUMA_MISS, /* allocated in non intended node */
 Model1_NUMA_FOREIGN, /* was intended here, hit elsewhere */
 Model1_NUMA_INTERLEAVE_HIT, /* interleaver preferred this zone */
 Model1_NUMA_LOCAL, /* allocation from local node */
 Model1_NUMA_OTHER, /* allocation from other node */

 Model1_NR_FREE_CMA_PAGES,
 Model1_NR_VM_ZONE_STAT_ITEMS };

enum Model1_node_stat_item {
 Model1_NR_LRU_BASE,
 Model1_NR_INACTIVE_ANON = Model1_NR_LRU_BASE, /* must match order of LRU_[IN]ACTIVE */
 Model1_NR_ACTIVE_ANON, /*  "     "     "   "       "         */
 Model1_NR_INACTIVE_FILE, /*  "     "     "   "       "         */
 Model1_NR_ACTIVE_FILE, /*  "     "     "   "       "         */
 Model1_NR_UNEVICTABLE, /*  "     "     "   "       "         */
 Model1_NR_ISOLATED_ANON, /* Temporary isolated pages from anon lru */
 Model1_NR_ISOLATED_FILE, /* Temporary isolated pages from file lru */
 Model1_NR_PAGES_SCANNED, /* pages scanned since last reclaim */
 Model1_WORKINGSET_REFAULT,
 Model1_WORKINGSET_ACTIVATE,
 Model1_WORKINGSET_NODERECLAIM,
 Model1_NR_ANON_MAPPED, /* Mapped anonymous pages */
 Model1_NR_FILE_MAPPED, /* pagecache pages mapped into pagetables.
			   only modified from process context */
 Model1_NR_FILE_PAGES,
 Model1_NR_FILE_DIRTY,
 Model1_NR_WRITEBACK,
 Model1_NR_WRITEBACK_TEMP, /* Writeback using temporary buffers */
 Model1_NR_SHMEM, /* shmem pages (included tmpfs/GEM pages) */
 Model1_NR_SHMEM_THPS,
 Model1_NR_SHMEM_PMDMAPPED,
 Model1_NR_ANON_THPS,
 Model1_NR_UNSTABLE_NFS, /* NFS unstable pages */
 Model1_NR_VMSCAN_WRITE,
 Model1_NR_VMSCAN_IMMEDIATE, /* Prioritise for reclaim when writeback ends */
 Model1_NR_DIRTIED, /* page dirtyings since bootup */
 Model1_NR_WRITTEN, /* page writings since bootup */
 Model1_NR_VM_NODE_STAT_ITEMS
};

/*
 * We do arithmetic on the LRU lists in various places in the code,
 * so it is important to keep the active lists LRU_ACTIVE higher in
 * the array than the corresponding inactive lists, and to keep
 * the *_FILE lists LRU_FILE higher than the corresponding _ANON lists.
 *
 * This has to be kept in sync with the statistics in zone_stat_item
 * above and the descriptions in vmstat_text in mm/vmstat.c
 */




enum Model1_lru_list {
 Model1_LRU_INACTIVE_ANON = 0,
 Model1_LRU_ACTIVE_ANON = 0 + 1,
 Model1_LRU_INACTIVE_FILE = 0 + 2,
 Model1_LRU_ACTIVE_FILE = 0 + 2 + 1,
 Model1_LRU_UNEVICTABLE,
 Model1_NR_LRU_LISTS
};





static inline __attribute__((no_instrument_function)) int Model1_is_file_lru(enum Model1_lru_list Model1_lru)
{
 return (Model1_lru == Model1_LRU_INACTIVE_FILE || Model1_lru == Model1_LRU_ACTIVE_FILE);
}

static inline __attribute__((no_instrument_function)) int Model1_is_active_lru(enum Model1_lru_list Model1_lru)
{
 return (Model1_lru == Model1_LRU_ACTIVE_ANON || Model1_lru == Model1_LRU_ACTIVE_FILE);
}

struct Model1_zone_reclaim_stat {
 /*
	 * The pageout code in vmscan.c keeps track of how many of the
	 * mem/swap backed and file backed pages are referenced.
	 * The higher the rotated/scanned ratio, the more valuable
	 * that cache is.
	 *
	 * The anon LRU stats live in [0], file LRU stats in [1]
	 */
 unsigned long Model1_recent_rotated[2];
 unsigned long Model1_recent_scanned[2];
};

struct Model1_lruvec {
 struct Model1_list_head Model1_lists[Model1_NR_LRU_LISTS];
 struct Model1_zone_reclaim_stat Model1_reclaim_stat;
 /* Evictions & activations on the inactive file list */
 Model1_atomic_long_t Model1_inactive_age;



};

/* Mask used at gathering information at once (see memcontrol.c) */




/* Isolate clean file */

/* Isolate unmapped file */

/* Isolate for asynchronous migration */

/* Isolate unevictable pages */


/* LRU Isolation modes. */
typedef unsigned Model1_isolate_mode_t;

enum Model1_zone_watermarks {
 Model1_WMARK_MIN,
 Model1_WMARK_LOW,
 Model1_WMARK_HIGH,
 Model1_NR_WMARK
};





struct Model1_per_cpu_pages {
 int Model1_count; /* number of pages in the list */
 int Model1_high; /* high watermark, emptying needed */
 int Model1_batch; /* chunk size for buddy add/remove */

 /* Lists of pages, one per migrate type stored on the pcp-lists */
 struct Model1_list_head Model1_lists[Model1_MIGRATE_PCPTYPES];
};

struct Model1_per_cpu_pageset {
 struct Model1_per_cpu_pages Model1_pcp;

 Model1_s8 Model1_expire;


 Model1_s8 Model1_stat_threshold;
 Model1_s8 Model1_vm_stat_diff[Model1_NR_VM_ZONE_STAT_ITEMS];

};

struct Model1_per_cpu_nodestat {
 Model1_s8 Model1_stat_threshold;
 Model1_s8 Model1_vm_node_stat_diff[Model1_NR_VM_NODE_STAT_ITEMS];
};



enum Model1_zone_type {

 /*
	 * ZONE_DMA is used when there are devices that are not able
	 * to do DMA to all of addressable memory (ZONE_NORMAL). Then we
	 * carve out the portion of memory that is needed for these devices.
	 * The range is arch specific.
	 *
	 * Some examples
	 *
	 * Architecture		Limit
	 * ---------------------------
	 * parisc, ia64, sparc	<4G
	 * s390			<2G
	 * arm			Various
	 * alpha		Unlimited or 0-16MB.
	 *
	 * i386, x86_64 and multiple other arches
	 * 			<16M.
	 */
 Model1_ZONE_DMA,


 /*
	 * x86_64 needs two ZONE_DMAs because it supports devices that are
	 * only able to do DMA to the lower 16M but also 32 bit devices that
	 * can only do DMA areas below 4G.
	 */
 Model1_ZONE_DMA32,

 /*
	 * Normal addressable memory is in ZONE_NORMAL. DMA operations can be
	 * performed on pages in ZONE_NORMAL if the DMA devices support
	 * transfers to all addressable memory.
	 */
 Model1_ZONE_NORMAL,
 Model1_ZONE_MOVABLE,



 Model1___MAX_NR_ZONES

};



struct Model1_zone {
 /* Read-mostly fields */

 /* zone watermarks, access with *_wmark_pages(zone) macros */
 unsigned long Model1_watermark[Model1_NR_WMARK];

 unsigned long Model1_nr_reserved_highatomic;

 /*
	 * We don't know if the memory that we're going to allocate will be
	 * freeable or/and it will be released eventually, so to avoid totally
	 * wasting several GB of ram we must reserve some of the lower zone
	 * memory (otherwise we risk to run OOM on the lower zones despite
	 * there being tons of freeable ram on the higher zones).  This array is
	 * recalculated at runtime if the sysctl_lowmem_reserve_ratio sysctl
	 * changes.
	 */
 long Model1_lowmem_reserve[4];


 int Model1_node;

 struct Model1_pglist_data *Model1_zone_pgdat;
 struct Model1_per_cpu_pageset *Model1_pageset;
 /* zone_start_pfn == zone_start_paddr >> PAGE_SHIFT */
 unsigned long Model1_zone_start_pfn;

 /*
	 * spanned_pages is the total pages spanned by the zone, including
	 * holes, which is calculated as:
	 * 	spanned_pages = zone_end_pfn - zone_start_pfn;
	 *
	 * present_pages is physical pages existing within the zone, which
	 * is calculated as:
	 *	present_pages = spanned_pages - absent_pages(pages in holes);
	 *
	 * managed_pages is present pages managed by the buddy system, which
	 * is calculated as (reserved_pages includes pages allocated by the
	 * bootmem allocator):
	 *	managed_pages = present_pages - reserved_pages;
	 *
	 * So present_pages may be used by memory hotplug or memory power
	 * management logic to figure out unmanaged pages by checking
	 * (present_pages - managed_pages). And managed_pages should be used
	 * by page allocator and vm scanner to calculate all kinds of watermarks
	 * and thresholds.
	 *
	 * Locking rules:
	 *
	 * zone_start_pfn and spanned_pages are protected by span_seqlock.
	 * It is a seqlock because it has to be read outside of zone->lock,
	 * and it is done in the main allocator path.  But, it is written
	 * quite infrequently.
	 *
	 * The span_seq lock is declared along with zone->lock because it is
	 * frequently read in proximity to zone->lock.  It's good to
	 * give them a chance of being in the same cacheline.
	 *
	 * Write access to present_pages at runtime should be protected by
	 * mem_hotplug_begin/end(). Any reader who can't tolerant drift of
	 * present_pages should get_online_mems() to get a stable value.
	 *
	 * Read access to managed_pages should be safe because it's unsigned
	 * long. Write access to zone->managed_pages and totalram_pages are
	 * protected by managed_page_count_lock at runtime. Idealy only
	 * adjust_managed_page_count() should be used instead of directly
	 * touching zone->managed_pages and totalram_pages.
	 */
 unsigned long Model1_managed_pages;
 unsigned long Model1_spanned_pages;
 unsigned long Model1_present_pages;

 const char *Model1_name;
 /*
	 * wait_table		-- the array holding the hash table
	 * wait_table_hash_nr_entries	-- the size of the hash table array
	 * wait_table_bits	-- wait_table_size == (1 << wait_table_bits)
	 *
	 * The purpose of all these is to keep track of the people
	 * waiting for a page to become available and make them
	 * runnable again when possible. The trouble is that this
	 * consumes a lot of space, especially when so few things
	 * wait on pages at a given time. So instead of using
	 * per-page waitqueues, we use a waitqueue hash table.
	 *
	 * The bucket discipline is to sleep on the same queue when
	 * colliding and wake all in that wait queue when removing.
	 * When something wakes, it must check to be sure its page is
	 * truly available, a la thundering herd. The cost of a
	 * collision is great, but given the expected load of the
	 * table, they should be so rare as to be outweighed by the
	 * benefits from the saved space.
	 *
	 * __wait_on_page_locked() and unlock_page() in mm/filemap.c, are the
	 * primary users of these fields, and in mm/page_alloc.c
	 * free_area_init_core() performs the initialization of them.
	 */
 Model1_wait_queue_head_t *Model1_wait_table;
 unsigned long Model1_wait_table_hash_nr_entries;
 unsigned long Model1_wait_table_bits;

 /* Write-intensive fields used from the page allocator */
 struct Model1_zone_padding Model1__pad1_;

 /* free areas of different sizes */
 struct Model1_free_area Model1_free_area[11];

 /* zone flags, see below */
 unsigned long Model1_flags;

 /* Primarily protects free_area */
 Model1_spinlock_t Model1_lock;

 /* Write-intensive fields used by compaction and vmstats. */
 struct Model1_zone_padding Model1__pad2_;

 /*
	 * When free pages are below this point, additional steps are taken
	 * when reading the number of free pages to avoid per-cpu counter
	 * drift allowing watermarks to be breached
	 */
 unsigned long Model1_percpu_drift_mark;


 /* pfn where compaction free scanner should start */
 unsigned long Model1_compact_cached_free_pfn;
 /* pfn where async and sync compaction migration scanner should start */
 unsigned long Model1_compact_cached_migrate_pfn[2];



 /*
	 * On compaction failure, 1<<compact_defer_shift compactions
	 * are skipped before trying again. The number attempted since
	 * last failure is tracked with compact_considered.
	 */
 unsigned int Model1_compact_considered;
 unsigned int Model1_compact_defer_shift;
 int Model1_compact_order_failed;



 /* Set to true when the PG_migrate_skip bits should be cleared */
 bool Model1_compact_blockskip_flush;


 bool Model1_contiguous;

 struct Model1_zone_padding Model1__pad3_;
 /* Zone statistics */
 Model1_atomic_long_t Model1_vm_stat[Model1_NR_VM_ZONE_STAT_ITEMS];
} __attribute__((__aligned__(1 << (6))));

enum Model1_pgdat_flags {
 Model1_PGDAT_CONGESTED, /* pgdat has many dirty pages backed by
					 * a congested BDI
					 */
 Model1_PGDAT_DIRTY, /* reclaim scanning has recently found
					 * many dirty file pages at the tail
					 * of the LRU.
					 */
 Model1_PGDAT_WRITEBACK, /* reclaim scanning has recently found
					 * many pages under writeback
					 */
 Model1_PGDAT_RECLAIM_LOCKED, /* prevents concurrent reclaim */
};

static inline __attribute__((no_instrument_function)) unsigned long Model1_zone_end_pfn(const struct Model1_zone *Model1_zone)
{
 return Model1_zone->Model1_zone_start_pfn + Model1_zone->Model1_spanned_pages;
}

static inline __attribute__((no_instrument_function)) bool Model1_zone_spans_pfn(const struct Model1_zone *Model1_zone, unsigned long Model1_pfn)
{
 return Model1_zone->Model1_zone_start_pfn <= Model1_pfn && Model1_pfn < Model1_zone_end_pfn(Model1_zone);
}

static inline __attribute__((no_instrument_function)) bool Model1_zone_is_initialized(struct Model1_zone *Model1_zone)
{
 return !!Model1_zone->Model1_wait_table;
}

static inline __attribute__((no_instrument_function)) bool Model1_zone_is_empty(struct Model1_zone *Model1_zone)
{
 return Model1_zone->Model1_spanned_pages == 0;
}

/*
 * The "priority" of VM scanning is how much of the queues we will scan in one
 * go. A value of 12 for DEF_PRIORITY implies that we will scan 1/4096th of the
 * queues ("queue_length >> 12") during an aging round.
 */


/* Maximum number of zones on a zonelist */


enum {
 Model1_ZONELIST_FALLBACK, /* zonelist with fallback */

 /*
	 * The NUMA zonelists are doubled because we need zonelists that
	 * restrict the allocations to a single node for __GFP_THISNODE.
	 */
 Model1_ZONELIST_NOFALLBACK, /* zonelist without fallback (__GFP_THISNODE) */

 Model1_MAX_ZONELISTS
};

/*
 * This struct contains information about a zone in a zonelist. It is stored
 * here to avoid dereferences into large structures and lookups of tables
 */
struct Model1_zoneref {
 struct Model1_zone *Model1_zone; /* Pointer to actual zone */
 int Model1_zone_idx; /* zone_idx(zoneref->zone) */
};

/*
 * One allocation request operates on a zonelist. A zonelist
 * is a list of zones, the first one is the 'goal' of the
 * allocation, the other zones are fallback zones, in decreasing
 * priority.
 *
 * To speed the reading of the zonelist, the zonerefs contain the zone index
 * of the entry being read. Helper functions to access information given
 * a struct zoneref are
 *
 * zonelist_zone()	- Return the struct zone * for an entry in _zonerefs
 * zonelist_zone_idx()	- Return the index of the zone for an entry
 * zonelist_node_idx()	- Return the index of the node for an entry
 */
struct Model1_zonelist {
 struct Model1_zoneref Model1__zonerefs[((1 << 6) * 4) + 1];
};


/* The array of struct pages - for discontigmem use pgdat->lmem_map */
extern struct Model1_page *Model1_mem_map;


/*
 * The pg_data_t structure is used in machines with CONFIG_DISCONTIGMEM
 * (mostly NUMA machines?) to denote a higher-level memory zone than the
 * zone denotes.
 *
 * On NUMA machines, each NUMA node would have a pg_data_t to describe
 * it's memory layout.
 *
 * Memory statistics and page replacement data structures are maintained on a
 * per-zone basis.
 */
struct Model1_bootmem_data;
typedef struct Model1_pglist_data {
 struct Model1_zone Model1_node_zones[4];
 struct Model1_zonelist Model1_node_zonelists[Model1_MAX_ZONELISTS];
 int Model1_nr_zones;
 unsigned long Model1_node_start_pfn;
 unsigned long Model1_node_present_pages; /* total number of physical pages */
 unsigned long Model1_node_spanned_pages; /* total size of physical page
					     range, including holes */
 int Model1_node_id;
 Model1_wait_queue_head_t Model1_kswapd_wait;
 Model1_wait_queue_head_t Model1_pfmemalloc_wait;
 struct Model1_task_struct *Model1_kswapd; /* Protected by
					   mem_hotplug_begin/end() */
 int Model1_kswapd_order;
 enum Model1_zone_type Model1_kswapd_classzone_idx;


 int Model1_kcompactd_max_order;
 enum Model1_zone_type Model1_kcompactd_classzone_idx;
 Model1_wait_queue_head_t Model1_kcompactd_wait;
 struct Model1_task_struct *Model1_kcompactd;
 /*
	 * This is a per-node reserve of pages that are not available
	 * to userspace allocations.
	 */
 unsigned long Model1_totalreserve_pages;


 /*
	 * zone reclaim becomes active if more unmapped pages exist.
	 */
 unsigned long Model1_min_unmapped_pages;
 unsigned long Model1_min_slab_pages;


 /* Write-intensive fields used by page reclaim */
 struct Model1_zone_padding Model1__pad1_;
 Model1_spinlock_t Model1_lru_lock;
 /* Fields commonly accessed by the page reclaim scanner */
 struct Model1_lruvec Model1_lruvec;

 /*
	 * The target ratio of ACTIVE_ANON to INACTIVE_ANON pages on
	 * this node's LRU.  Maintained by the pageout code.
	 */
 unsigned int Model1_inactive_ratio;

 unsigned long Model1_flags;

 struct Model1_zone_padding Model1__pad2_;

 /* Per-node vmstats */
 struct Model1_per_cpu_nodestat *Model1_per_cpu_nodestats;
 Model1_atomic_long_t Model1_vm_stat[Model1_NR_VM_NODE_STAT_ITEMS];
} Model1_pg_data_t;
static inline __attribute__((no_instrument_function)) Model1_spinlock_t *Model1_zone_lru_lock(struct Model1_zone *Model1_zone)
{
 return &Model1_zone->Model1_zone_pgdat->Model1_lru_lock;
}

static inline __attribute__((no_instrument_function)) struct Model1_lruvec *Model1_node_lruvec(struct Model1_pglist_data *Model1_pgdat)
{
 return &Model1_pgdat->Model1_lruvec;
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_pgdat_end_pfn(Model1_pg_data_t *Model1_pgdat)
{
 return Model1_pgdat->Model1_node_start_pfn + Model1_pgdat->Model1_node_spanned_pages;
}

static inline __attribute__((no_instrument_function)) bool Model1_pgdat_is_empty(Model1_pg_data_t *Model1_pgdat)
{
 return !Model1_pgdat->Model1_node_start_pfn && !Model1_pgdat->Model1_node_spanned_pages;
}

static inline __attribute__((no_instrument_function)) int Model1_zone_id(const struct Model1_zone *Model1_zone)
{
 struct Model1_pglist_data *Model1_pgdat = Model1_zone->Model1_zone_pgdat;

 return Model1_zone - Model1_pgdat->Model1_node_zones;
}







static inline __attribute__((no_instrument_function)) bool Model1_is_dev_zone(const struct Model1_zone *Model1_zone)
{
 return false;
}







/*
 *	Routines to manage notifier chains for passing status changes to any
 *	interested routines. We need this instead of hard coded call lists so
 *	that modules can poke their nose into the innards. The network devices
 *	needed them so here they are for the rest of you.
 *
 *				Alan Cox <Alan.Cox@linux.org>
 */





/*
 * Mutexes: blocking mutual exclusion locks
 *
 * started by Ingo Molnar:
 *
 *  Copyright (C) 2004, 2005, 2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
 *
 * This file contains the main data structure and API definitions.
 */



/*
 * An MCS like lock especially tailored for optimistic spinning for sleeping
 * lock implementations (mutex, rwsem, etc).
 */
struct Model1_optimistic_spin_node {
 struct Model1_optimistic_spin_node *Model1_next, *Model1_prev;
 int Model1_locked; /* 1 if lock acquired */
 int Model1_cpu; /* encoded CPU # + 1 value */
};

struct Model1_optimistic_spin_queue {
 /*
	 * Stores an encoded value of the CPU # of the tail node in the queue.
	 * If the queue is empty, then it's set to OSQ_UNLOCKED_VAL.
	 */
 Model1_atomic_t Model1_tail;
};



/* Init macro and function. */


static inline __attribute__((no_instrument_function)) void Model1_osq_lock_init(struct Model1_optimistic_spin_queue *Model1_lock)
{
 Model1_atomic_set(&Model1_lock->Model1_tail, (0));
}

extern bool Model1_osq_lock(struct Model1_optimistic_spin_queue *Model1_lock);
extern void Model1_osq_unlock(struct Model1_optimistic_spin_queue *Model1_lock);

static inline __attribute__((no_instrument_function)) bool Model1_osq_is_locked(struct Model1_optimistic_spin_queue *Model1_lock)
{
 return Model1_atomic_read(&Model1_lock->Model1_tail) != (0);
}

/*
 * Simple, straightforward mutexes with strict semantics:
 *
 * - only one task can hold the mutex at a time
 * - only the owner can unlock the mutex
 * - multiple unlocks are not permitted
 * - recursive locking is not permitted
 * - a mutex object must be initialized via the API
 * - a mutex object must not be initialized via memset or copying
 * - task may not exit with mutex held
 * - memory areas where held locks reside must not be freed
 * - held mutexes must not be reinitialized
 * - mutexes may not be used in hardware or software interrupt
 *   contexts such as tasklets and timers
 *
 * These semantics are fully enforced when DEBUG_MUTEXES is
 * enabled. Furthermore, besides enforcing the above rules, the mutex
 * debugging code also implements a number of additional features
 * that make lock debugging easier and faster:
 *
 * - uses symbolic names of mutexes, whenever they are printed in debug output
 * - point-of-acquire tracking, symbolic lookup of function names
 * - list of all locks held in the system, printout of them
 * - owner tracking
 * - detects self-recursing locks and prints out all relevant info
 * - detects multi-task circular deadlocks and prints out all affected
 *   locks and tasks (and only those tasks)
 */
struct Model1_mutex {
 /* 1: unlocked, 0: locked, negative: locked, possible waiters */
 Model1_atomic_t Model1_count;
 Model1_spinlock_t Model1_wait_lock;
 struct Model1_list_head Model1_wait_list;

 struct Model1_task_struct *Model1_owner;


 struct Model1_optimistic_spin_queue Model1_osq; /* Spinner MCS lock */







};

/*
 * This is the control structure for tasks blocked on mutex,
 * which resides on the blocked task's kernel stack:
 */
struct Model1_mutex_waiter {
 struct Model1_list_head Model1_list;
 struct Model1_task_struct *Model1_task;



};





/**
 * mutex_init - initialize the mutex
 * @mutex: the mutex to be initialized
 *
 * Initialize the mutex to unlocked state.
 *
 * It is not allowed to initialize an already locked mutex.
 */






static inline __attribute__((no_instrument_function)) void Model1_mutex_destroy(struct Model1_mutex *Model1_lock) {}
extern void Model1___mutex_init(struct Model1_mutex *Model1_lock, const char *Model1_name,
    struct Model1_lock_class_key *Model1_key);

/**
 * mutex_is_locked - is the mutex locked
 * @lock: the mutex to be queried
 *
 * Returns 1 if the mutex is locked, 0 if unlocked.
 */
static inline __attribute__((no_instrument_function)) int Model1_mutex_is_locked(struct Model1_mutex *Model1_lock)
{
 return Model1_atomic_read(&Model1_lock->Model1_count) != 1;
}

/*
 * See kernel/locking/mutex.c for detailed documentation of these APIs.
 * Also see Documentation/locking/mutex-design.txt.
 */
extern void Model1_mutex_lock(struct Model1_mutex *Model1_lock);
extern int __attribute__((warn_unused_result)) Model1_mutex_lock_interruptible(struct Model1_mutex *Model1_lock);
extern int __attribute__((warn_unused_result)) Model1_mutex_lock_killable(struct Model1_mutex *Model1_lock);







/*
 * NOTE: mutex_trylock() follows the spin_trylock() convention,
 *       not the down_trylock() convention!
 *
 * Returns 1 if the mutex has been acquired successfully, and 0 on contention.
 */
extern int Model1_mutex_trylock(struct Model1_mutex *Model1_lock);
extern void Model1_mutex_unlock(struct Model1_mutex *Model1_lock);

extern int Model1_atomic_dec_and_mutex_lock(Model1_atomic_t *Model1_cnt, struct Model1_mutex *Model1_lock);
/* rwsem.h: R/W semaphores, public interface
 *
 * Written by David Howells (dhowells@redhat.com).
 * Derived from asm-i386/semaphore.h
 */
struct Model1_rw_semaphore;





/* All arch specific implementations share the same struct */
struct Model1_rw_semaphore {
 Model1_atomic_long_t Model1_count;
 struct Model1_list_head Model1_wait_list;
 Model1_raw_spinlock_t Model1_wait_lock;

 struct Model1_optimistic_spin_queue Model1_osq; /* spinner MCS lock */
 /*
	 * Write owner. Used as a speculative check to see
	 * if the owner is running on the cpu.
	 */
 struct Model1_task_struct *Model1_owner;




};

extern struct Model1_rw_semaphore *Model1_rwsem_down_read_failed(struct Model1_rw_semaphore *Model1_sem);
extern struct Model1_rw_semaphore *Model1_rwsem_down_write_failed(struct Model1_rw_semaphore *Model1_sem);
extern struct Model1_rw_semaphore *Model1_rwsem_down_write_failed_killable(struct Model1_rw_semaphore *Model1_sem);
extern struct Model1_rw_semaphore *Model1_rwsem_wake(struct Model1_rw_semaphore *);
extern struct Model1_rw_semaphore *Model1_rwsem_downgrade_wake(struct Model1_rw_semaphore *Model1_sem);

/* Include the arch specific part */

/* rwsem.h: R/W semaphores implemented using XADD/CMPXCHG for i486+
 *
 * Written by David Howells (dhowells@redhat.com).
 *
 * Derived from asm-x86/semaphore.h
 *
 *
 * The MSW of the count is the negated number of active writers and waiting
 * lockers, and the LSW is the total number of active locks
 *
 * The lock count is initialized to 0 (no active and no waiting lockers).
 *
 * When a writer subtracts WRITE_BIAS, it'll get 0xffff0001 for the case of an
 * uncontended lock. This can be determined because XADD returns the old value.
 * Readers increment by 1 and see a positive value when uncontended, negative
 * if there are writers (and maybe) readers waiting (in which case it goes to
 * sleep).
 *
 * The value of WAITING_BIAS supports up to 32766 waiting processes. This can
 * be extended to 65534 by manually checking the whole MSW rather than relying
 * on the S flag.
 *
 * The value of ACTIVE_BIAS supports up to 65535 active processes.
 *
 * This should be totally fair - if anything is waiting, a process that wants a
 * lock will go to the back of the queue. When the currently active lock is
 * released, if there's a writer at the front of the queue, then that and only
 * that will be woken up; if there's a bunch of consecutive readers at the
 * front, then they'll all be woken up, but no other readers will be.
 */
/*
 * The bias values and the counter type limits the number of
 * potential readers/writers to 32767 for 32 bits and 2147483647
 * for 64 bits.
 */
/*
 * lock for reading
 */
static inline __attribute__((no_instrument_function)) void Model1___down_read(struct Model1_rw_semaphore *Model1_sem)
{
 asm volatile("# beginning down_read\n\t"
       ".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " " " "incq" " " "(%1)\n\t"
       /* adds 0x00000001 */
       "  jns        1f\n"
       "  call call_rwsem_down_read_failed\n"
       "1:\n\t"
       "# ending down_read\n\t"
       : "+m" (Model1_sem->Model1_count)
       : "a" (Model1_sem)
       : "memory", "cc");
}

/*
 * trylock for reading -- returns 1 if successful, 0 if contention
 */
static inline __attribute__((no_instrument_function)) bool Model1___down_read_trylock(struct Model1_rw_semaphore *Model1_sem)
{
 long Model1_result, Model1_tmp;
 asm volatile("# beginning __down_read_trylock\n\t"
       "  mov          %0,%1\n\t"
       "1:\n\t"
       "  mov          %1,%2\n\t"
       "  add          %3,%2\n\t"
       "  jle	     2f\n\t"
       ".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "  cmpxchg  %2,%0\n\t"
       "  jnz	     1b\n\t"
       "2:\n\t"
       "# ending __down_read_trylock\n\t"
       : "+m" (Model1_sem->Model1_count), "=&a" (Model1_result), "=&r" (Model1_tmp)
       : "i" (0x00000001L)
       : "memory", "cc");
 return Model1_result >= 0;
}

/*
 * lock for writing
 */
static inline __attribute__((no_instrument_function)) void Model1___down_write(struct Model1_rw_semaphore *Model1_sem)
{
 ({ long Model1_tmp; struct Model1_rw_semaphore* Model1_ret; asm volatile("# beginning down_write\n\t" ".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "  xadd      %1,(%3)\n\t" "  test " " " "%k1" " " "," " " "%k1" " " "\n\t" "  jz        1f\n" "  call " "call_rwsem_down_write_failed" "\n" "1:\n" "# ending down_write" : "+m" (Model1_sem->Model1_count), "=d" (Model1_tmp), "=a" (Model1_ret) : "a" (Model1_sem), "1" (((-0xffffffffL -1) + 0x00000001L)) : "memory", "cc"); Model1_ret; });
}

static inline __attribute__((no_instrument_function)) int Model1___down_write_killable(struct Model1_rw_semaphore *Model1_sem)
{
 if (Model1_IS_ERR(({ long Model1_tmp; struct Model1_rw_semaphore* Model1_ret; asm volatile("# beginning down_write\n\t" ".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "  xadd      %1,(%3)\n\t" "  test " " " "%k1" " " "," " " "%k1" " " "\n\t" "  jz        1f\n" "  call " "call_rwsem_down_write_failed_killable" "\n" "1:\n" "# ending down_write" : "+m" (Model1_sem->Model1_count), "=d" (Model1_tmp), "=a" (Model1_ret) : "a" (Model1_sem), "1" (((-0xffffffffL -1) + 0x00000001L)) : "memory", "cc"); Model1_ret; })))
  return -4;

 return 0;
}

/*
 * trylock for writing -- returns 1 if successful, 0 if contention
 */
static inline __attribute__((no_instrument_function)) bool Model1___down_write_trylock(struct Model1_rw_semaphore *Model1_sem)
{
 bool Model1_result;
 long Model1_tmp0, Model1_tmp1;
 asm volatile("# beginning __down_write_trylock\n\t"
       "  mov          %0,%1\n\t"
       "1:\n\t"
       "  test " " " "%k1" " " "," " " "%k1" " " "\n\t"
       /* was the active mask 0 before? */
       "  jnz          2f\n\t"
       "  mov          %1,%2\n\t"
       "  add          %4,%2\n\t"
       ".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "  cmpxchg  %2,%0\n\t"
       "  jnz	     1b\n\t"
       "2:\n\t"
       "\n\tset" "e" " %[_cc_" "e" "]\n"
       "# ending __down_write_trylock\n\t"
       : "+m" (Model1_sem->Model1_count), "=&a" (Model1_tmp0), "=&r" (Model1_tmp1),
         [_cc_e] "=qm" (Model1_result)
       : "er" (((-0xffffffffL -1) + 0x00000001L))
       : "memory", "cc");
 return Model1_result;
}

/*
 * unlock after reading
 */
static inline __attribute__((no_instrument_function)) void Model1___up_read(struct Model1_rw_semaphore *Model1_sem)
{
 long Model1_tmp;
 asm volatile("# beginning __up_read\n\t"
       ".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "  xadd      %1,(%2)\n\t"
       /* subtracts 1, returns the old value */
       "  jns        1f\n\t"
       "  call call_rwsem_wake\n" /* expects old value in %edx */
       "1:\n"
       "# ending __up_read\n"
       : "+m" (Model1_sem->Model1_count), "=d" (Model1_tmp)
       : "a" (Model1_sem), "1" (-0x00000001L)
       : "memory", "cc");
}

/*
 * unlock after writing
 */
static inline __attribute__((no_instrument_function)) void Model1___up_write(struct Model1_rw_semaphore *Model1_sem)
{
 long Model1_tmp;
 asm volatile("# beginning __up_write\n\t"
       ".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " "  xadd      %1,(%2)\n\t"
       /* subtracts 0xffff0001, returns the old value */
       "  jns        1f\n\t"
       "  call call_rwsem_wake\n" /* expects old value in %edx */
       "1:\n\t"
       "# ending __up_write\n"
       : "+m" (Model1_sem->Model1_count), "=d" (Model1_tmp)
       : "a" (Model1_sem), "1" (-((-0xffffffffL -1) + 0x00000001L))
       : "memory", "cc");
}

/*
 * downgrade write lock to read lock
 */
static inline __attribute__((no_instrument_function)) void Model1___downgrade_write(struct Model1_rw_semaphore *Model1_sem)
{
 asm volatile("# beginning __downgrade_write\n\t"
       ".pushsection .smp_locks,\"a\"\n" ".balign 4\n" ".long 671f - .\n" ".popsection\n" "671:" "\n\tlock; " " " "addq" " " "%2,(%1)\n\t"
       /*
		      * transitions 0xZZZZ0001 -> 0xYYYY0001 (i386)
		      *     0xZZZZZZZZ00000001 -> 0xYYYYYYYY00000001 (x86_64)
		      */
       "  jns       1f\n\t"
       "  call call_rwsem_downgrade_wake\n"
       "1:\n\t"
       "# ending __downgrade_write\n"
       : "+m" (Model1_sem->Model1_count)
       : "a" (Model1_sem), "er" (-(-0xffffffffL -1))
       : "memory", "cc");
}

/* In all implementations count != 0 means locked */
static inline __attribute__((no_instrument_function)) int Model1_rwsem_is_locked(struct Model1_rw_semaphore *Model1_sem)
{
 return Model1_atomic_long_read(&Model1_sem->Model1_count) != 0;
}




/* Common initializer macros and functions */
extern void Model1___init_rwsem(struct Model1_rw_semaphore *Model1_sem, const char *Model1_name,
    struct Model1_lock_class_key *Model1_key);
/*
 * This is the same regardless of which rwsem implementation that is being used.
 * It is just a heuristic meant to be called by somebody alreadying holding the
 * rwsem to see if somebody from an incompatible type is wanting access to the
 * lock.
 */
static inline __attribute__((no_instrument_function)) int Model1_rwsem_is_contended(struct Model1_rw_semaphore *Model1_sem)
{
 return !Model1_list_empty(&Model1_sem->Model1_wait_list);
}

/*
 * lock for reading
 */
extern void Model1_down_read(struct Model1_rw_semaphore *Model1_sem);

/*
 * trylock for reading -- returns 1 if successful, 0 if contention
 */
extern int Model1_down_read_trylock(struct Model1_rw_semaphore *Model1_sem);

/*
 * lock for writing
 */
extern void Model1_down_write(struct Model1_rw_semaphore *Model1_sem);
extern int __attribute__((warn_unused_result)) Model1_down_write_killable(struct Model1_rw_semaphore *Model1_sem);

/*
 * trylock for writing -- returns 1 if successful, 0 if contention
 */
extern int Model1_down_write_trylock(struct Model1_rw_semaphore *Model1_sem);

/*
 * release a read lock
 */
extern void Model1_up_read(struct Model1_rw_semaphore *Model1_sem);

/*
 * release a write lock
 */
extern void Model1_up_write(struct Model1_rw_semaphore *Model1_sem);

/*
 * downgrade write lock to read lock
 */
extern void Model1_downgrade_write(struct Model1_rw_semaphore *Model1_sem);
/*
 * Sleepable Read-Copy Update mechanism for mutual exclusion
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, you can access it online at
 * http://www.gnu.org/licenses/gpl-2.0.html.
 *
 * Copyright (C) IBM Corporation, 2006
 * Copyright (C) Fujitsu, 2012
 *
 * Author: Paul McKenney <paulmck@us.ibm.com>
 *	   Lai Jiangshan <laijs@cn.fujitsu.com>
 *
 * For detailed explanation of Read-Copy Update mechanism see -
 * 		Documentation/RCU/ *.txt
 *
 */






/*
 * Read-Copy Update mechanism for mutual exclusion
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, you can access it online at
 * http://www.gnu.org/licenses/gpl-2.0.html.
 *
 * Copyright IBM Corporation, 2001
 *
 * Author: Dipankar Sarma <dipankar@in.ibm.com>
 *
 * Based on the original work by Paul McKenney <paulmck@us.ibm.com>
 * and inputs from Rusty Russell, Andrea Arcangeli and Andi Kleen.
 * Papers:
 * http://www.rdrop.com/users/paulmck/paper/rclockpdcsproof.pdf
 * http://lse.sourceforge.net/locking/rclock_OLS.2001.05.01c.sc.pdf (OLS2001)
 *
 * For detailed explanation of Read-Copy Update mechanism see -
 *		http://lse.sourceforge.net/locking/rcupdate.html
 *
 */



/*
 * (C) Copyright 2001 Linus Torvalds
 *
 * Atomic wait-for-completion handler data structures.
 * See kernel/sched/completion.c for details.
 */



/*
 * struct completion - structure used to maintain state for a "completion"
 *
 * This is the opaque structure used to maintain the state for a "completion".
 * Completions currently use a FIFO to queue threads that have to wait for
 * the "completion" event.
 *
 * See also:  complete(), wait_for_completion() (and friends _timeout,
 * _interruptible, _interruptible_timeout, and _killable), init_completion(),
 * reinit_completion(), and macros DECLARE_COMPLETION(),
 * DECLARE_COMPLETION_ONSTACK().
 */
struct Model1_completion {
 unsigned int Model1_done;
 Model1_wait_queue_head_t Model1_wait;
};







/**
 * DECLARE_COMPLETION - declare and initialize a completion structure
 * @work:  identifier for the completion structure
 *
 * This macro declares and initializes a completion structure. Generally used
 * for static declarations. You should use the _ONSTACK variant for automatic
 * variables.
 */



/*
 * Lockdep needs to run a non-constant initializer for on-stack
 * completions - so we use the _ONSTACK() variant for those that
 * are on the kernel stack:
 */
/**
 * DECLARE_COMPLETION_ONSTACK - declare and initialize a completion structure
 * @work:  identifier for the completion structure
 *
 * This macro declares and initializes a completion structure on the kernel
 * stack.
 */







/**
 * init_completion - Initialize a dynamically allocated completion
 * @x:  pointer to completion structure that is to be initialized
 *
 * This inline function will initialize a dynamically created completion
 * structure.
 */
static inline __attribute__((no_instrument_function)) void Model1_init_completion(struct Model1_completion *Model1_x)
{
 Model1_x->Model1_done = 0;
 do { static struct Model1_lock_class_key Model1___key; Model1___init_waitqueue_head((&Model1_x->Model1_wait), "&x->wait", &Model1___key); } while (0);
}

/**
 * reinit_completion - reinitialize a completion structure
 * @x:  pointer to completion structure that is to be reinitialized
 *
 * This inline function should be used to reinitialize a completion structure so it can
 * be reused. This is especially important after complete_all() is used.
 */
static inline __attribute__((no_instrument_function)) void Model1_reinit_completion(struct Model1_completion *Model1_x)
{
 Model1_x->Model1_done = 0;
}

extern void Model1_wait_for_completion(struct Model1_completion *);
extern void Model1_wait_for_completion_io(struct Model1_completion *);
extern int Model1_wait_for_completion_interruptible(struct Model1_completion *Model1_x);
extern int Model1_wait_for_completion_killable(struct Model1_completion *Model1_x);
extern unsigned long Model1_wait_for_completion_timeout(struct Model1_completion *Model1_x,
         unsigned long Model1_timeout);
extern unsigned long Model1_wait_for_completion_io_timeout(struct Model1_completion *Model1_x,
          unsigned long Model1_timeout);
extern long Model1_wait_for_completion_interruptible_timeout(
 struct Model1_completion *Model1_x, unsigned long Model1_timeout);
extern long Model1_wait_for_completion_killable_timeout(
 struct Model1_completion *Model1_x, unsigned long Model1_timeout);
extern bool Model1_try_wait_for_completion(struct Model1_completion *Model1_x);
extern bool Model1_completion_done(struct Model1_completion *Model1_x);

extern void Model1_complete(struct Model1_completion *);
extern void Model1_complete_all(struct Model1_completion *);






enum Model1_debug_obj_state {
 Model1_ODEBUG_STATE_NONE,
 Model1_ODEBUG_STATE_INIT,
 Model1_ODEBUG_STATE_INACTIVE,
 Model1_ODEBUG_STATE_ACTIVE,
 Model1_ODEBUG_STATE_DESTROYED,
 Model1_ODEBUG_STATE_NOTAVAILABLE,
 Model1_ODEBUG_STATE_MAX,
};

struct Model1_debug_obj_descr;

/**
 * struct debug_obj - representaion of an tracked object
 * @node:	hlist node to link the object into the tracker list
 * @state:	tracked object state
 * @astate:	current active state
 * @object:	pointer to the real object
 * @descr:	pointer to an object type specific debug description structure
 */
struct Model1_debug_obj {
 struct Model1_hlist_node Model1_node;
 enum Model1_debug_obj_state Model1_state;
 unsigned int Model1_astate;
 void *Model1_object;
 struct Model1_debug_obj_descr *Model1_descr;
};

/**
 * struct debug_obj_descr - object type specific debug description structure
 *
 * @name:		name of the object typee
 * @debug_hint:		function returning address, which have associated
 *			kernel symbol, to allow identify the object
 * @is_static_object:	return true if the obj is static, otherwise return false
 * @fixup_init:		fixup function, which is called when the init check
 *			fails. All fixup functions must return true if fixup
 *			was successful, otherwise return false
 * @fixup_activate:	fixup function, which is called when the activate check
 *			fails
 * @fixup_destroy:	fixup function, which is called when the destroy check
 *			fails
 * @fixup_free:		fixup function, which is called when the free check
 *			fails
 * @fixup_assert_init:  fixup function, which is called when the assert_init
 *			check fails
 */
struct Model1_debug_obj_descr {
 const char *Model1_name;
 void *(*Model1_debug_hint)(void *Model1_addr);
 bool (*Model1_is_static_object)(void *Model1_addr);
 bool (*Model1_fixup_init)(void *Model1_addr, enum Model1_debug_obj_state Model1_state);
 bool (*Model1_fixup_activate)(void *Model1_addr, enum Model1_debug_obj_state Model1_state);
 bool (*Model1_fixup_destroy)(void *Model1_addr, enum Model1_debug_obj_state Model1_state);
 bool (*Model1_fixup_free)(void *Model1_addr, enum Model1_debug_obj_state Model1_state);
 bool (*Model1_fixup_assert_init)(void *Model1_addr, enum Model1_debug_obj_state Model1_state);
};
static inline __attribute__((no_instrument_function)) void
Model1_debug_object_init (void *Model1_addr, struct Model1_debug_obj_descr *Model1_descr) { }
static inline __attribute__((no_instrument_function)) void
Model1_debug_object_init_on_stack(void *Model1_addr, struct Model1_debug_obj_descr *Model1_descr) { }
static inline __attribute__((no_instrument_function)) int
Model1_debug_object_activate (void *Model1_addr, struct Model1_debug_obj_descr *Model1_descr) { return 0; }
static inline __attribute__((no_instrument_function)) void
Model1_debug_object_deactivate(void *Model1_addr, struct Model1_debug_obj_descr *Model1_descr) { }
static inline __attribute__((no_instrument_function)) void
Model1_debug_object_destroy (void *Model1_addr, struct Model1_debug_obj_descr *Model1_descr) { }
static inline __attribute__((no_instrument_function)) void
Model1_debug_object_free (void *Model1_addr, struct Model1_debug_obj_descr *Model1_descr) { }
static inline __attribute__((no_instrument_function)) void
Model1_debug_object_assert_init(void *Model1_addr, struct Model1_debug_obj_descr *Model1_descr) { }

static inline __attribute__((no_instrument_function)) void Model1_debug_objects_early_init(void) { }
static inline __attribute__((no_instrument_function)) void Model1_debug_objects_mem_init(void) { }





static inline __attribute__((no_instrument_function)) void
Model1_debug_check_no_obj_freed(const void *Model1_address, unsigned long Model1_size) { }


/*
 *  include/linux/ktime.h
 *
 *  ktime_t - nanosecond-resolution time format.
 *
 *   Copyright(C) 2005, Thomas Gleixner <tglx@linutronix.de>
 *   Copyright(C) 2005, Red Hat, Inc., Ingo Molnar
 *
 *  data type definitions, declarations, prototypes and macros.
 *
 *  Started by: Thomas Gleixner and Ingo Molnar
 *
 *  Credits:
 *
 *  	Roman Zippel provided the ideas and primary code snippets of
 *  	the ktime_t union and further simplifications of the original
 *  	code.
 *
 *  For licencing details see kernel-base/COPYING
 */





















struct Model1_timespec {
 Model1___kernel_time_t Model1_tv_sec; /* seconds */
 long Model1_tv_nsec; /* nanoseconds */
};


struct Model1_timeval {
 Model1___kernel_time_t Model1_tv_sec; /* seconds */
 Model1___kernel_suseconds_t Model1_tv_usec; /* microseconds */
};

struct Model1_timezone {
 int Model1_tz_minuteswest; /* minutes west of Greenwich */
 int Model1_tz_dsttime; /* type of dst correction */
};


/*
 * Names of the interval timers, and structure
 * defining a timer setting:
 */




struct Model1_itimerspec {
 struct Model1_timespec Model1_it_interval; /* timer period */
 struct Model1_timespec Model1_it_value; /* timer expiration */
};

struct Model1_itimerval {
 struct Model1_timeval Model1_it_interval; /* timer interval */
 struct Model1_timeval Model1_it_value; /* current value */
};

/*
 * The IDs of the various system clocks (for POSIX.1b interval timers):
 */
/*
 * The various flags for setting POSIX.1b interval timers:
 */


typedef Model1___s64 Model1_time64_t;

/*
 * This wants to go into uapi/linux/time.h once we agreed about the
 * userspace interfaces.
 */
/* Parameters used to convert the timespec values: */
/* Located here for timespec[64]_valid_strict */






static inline __attribute__((no_instrument_function)) struct Model1_timespec Model1_timespec64_to_timespec(const struct Model1_timespec Model1_ts64)
{
 return Model1_ts64;
}

static inline __attribute__((no_instrument_function)) struct Model1_timespec Model1_timespec_to_timespec64(const struct Model1_timespec Model1_ts)
{
 return Model1_ts;
}

static inline __attribute__((no_instrument_function)) struct Model1_itimerspec Model1_itimerspec64_to_itimerspec(struct Model1_itimerspec *Model1_its64)
{
 return *Model1_its64;
}

static inline __attribute__((no_instrument_function)) struct Model1_itimerspec Model1_itimerspec_to_itimerspec64(struct Model1_itimerspec *Model1_its)
{
 return *Model1_its;
}
/*
 * timespec64_add_safe assumes both values are positive and checks for
 * overflow. It will return TIME64_MAX in case of overflow.
 */
extern struct Model1_timespec Model1_timespec64_add_safe(const struct Model1_timespec Model1_lhs,
      const struct Model1_timespec Model1_rhs);

extern struct Model1_timezone Model1_sys_tz;



static inline __attribute__((no_instrument_function)) int Model1_timespec_equal(const struct Model1_timespec *Model1_a,
                                 const struct Model1_timespec *Model1_b)
{
 return (Model1_a->Model1_tv_sec == Model1_b->Model1_tv_sec) && (Model1_a->Model1_tv_nsec == Model1_b->Model1_tv_nsec);
}

/*
 * lhs < rhs:  return <0
 * lhs == rhs: return 0
 * lhs > rhs:  return >0
 */
static inline __attribute__((no_instrument_function)) int Model1_timespec_compare(const struct Model1_timespec *Model1_lhs, const struct Model1_timespec *Model1_rhs)
{
 if (Model1_lhs->Model1_tv_sec < Model1_rhs->Model1_tv_sec)
  return -1;
 if (Model1_lhs->Model1_tv_sec > Model1_rhs->Model1_tv_sec)
  return 1;
 return Model1_lhs->Model1_tv_nsec - Model1_rhs->Model1_tv_nsec;
}

static inline __attribute__((no_instrument_function)) int Model1_timeval_compare(const struct Model1_timeval *Model1_lhs, const struct Model1_timeval *Model1_rhs)
{
 if (Model1_lhs->Model1_tv_sec < Model1_rhs->Model1_tv_sec)
  return -1;
 if (Model1_lhs->Model1_tv_sec > Model1_rhs->Model1_tv_sec)
  return 1;
 return Model1_lhs->Model1_tv_usec - Model1_rhs->Model1_tv_usec;
}

extern Model1_time64_t Model1_mktime64(const unsigned int Model1_year, const unsigned int Model1_mon,
   const unsigned int Model1_day, const unsigned int Model1_hour,
   const unsigned int Model1_min, const unsigned int Model1_sec);

/**
 * Deprecated. Use mktime64().
 */
static inline __attribute__((no_instrument_function)) unsigned long Model1_mktime(const unsigned int Model1_year,
   const unsigned int Model1_mon, const unsigned int Model1_day,
   const unsigned int Model1_hour, const unsigned int Model1_min,
   const unsigned int Model1_sec)
{
 return Model1_mktime64(Model1_year, Model1_mon, Model1_day, Model1_hour, Model1_min, Model1_sec);
}

extern void Model1_set_normalized_timespec(struct Model1_timespec *Model1_ts, Model1_time_t Model1_sec, Model1_s64 Model1_nsec);

/*
 * timespec_add_safe assumes both values are positive and checks
 * for overflow. It will return TIME_T_MAX if the reutrn would be
 * smaller then either of the arguments.
 */
extern struct Model1_timespec Model1_timespec_add_safe(const struct Model1_timespec Model1_lhs,
      const struct Model1_timespec Model1_rhs);


static inline __attribute__((no_instrument_function)) struct Model1_timespec Model1_timespec_add(struct Model1_timespec Model1_lhs,
      struct Model1_timespec Model1_rhs)
{
 struct Model1_timespec Model1_ts_delta;
 Model1_set_normalized_timespec(&Model1_ts_delta, Model1_lhs.Model1_tv_sec + Model1_rhs.Model1_tv_sec,
    Model1_lhs.Model1_tv_nsec + Model1_rhs.Model1_tv_nsec);
 return Model1_ts_delta;
}

/*
 * sub = lhs - rhs, in normalized form
 */
static inline __attribute__((no_instrument_function)) struct Model1_timespec Model1_timespec_sub(struct Model1_timespec Model1_lhs,
      struct Model1_timespec Model1_rhs)
{
 struct Model1_timespec Model1_ts_delta;
 Model1_set_normalized_timespec(&Model1_ts_delta, Model1_lhs.Model1_tv_sec - Model1_rhs.Model1_tv_sec,
    Model1_lhs.Model1_tv_nsec - Model1_rhs.Model1_tv_nsec);
 return Model1_ts_delta;
}

/*
 * Returns true if the timespec is norm, false if denorm:
 */
static inline __attribute__((no_instrument_function)) bool Model1_timespec_valid(const struct Model1_timespec *Model1_ts)
{
 /* Dates before 1970 are bogus */
 if (Model1_ts->Model1_tv_sec < 0)
  return false;
 /* Can't have more nanoseconds then a second */
 if ((unsigned long)Model1_ts->Model1_tv_nsec >= 1000000000L)
  return false;
 return true;
}

static inline __attribute__((no_instrument_function)) bool Model1_timespec_valid_strict(const struct Model1_timespec *Model1_ts)
{
 if (!Model1_timespec_valid(Model1_ts))
  return false;
 /* Disallow values that could overflow ktime_t */
 if ((unsigned long long)Model1_ts->Model1_tv_sec >= (((Model1_s64)~((Model1_u64)1 << 63)) / 1000000000L))
  return false;
 return true;
}

static inline __attribute__((no_instrument_function)) bool Model1_timeval_valid(const struct Model1_timeval *Model1_tv)
{
 /* Dates before 1970 are bogus */
 if (Model1_tv->Model1_tv_sec < 0)
  return false;

 /* Can't have more microseconds then a second */
 if (Model1_tv->Model1_tv_usec < 0 || Model1_tv->Model1_tv_usec >= 1000000L)
  return false;

 return true;
}

extern struct Model1_timespec Model1_timespec_trunc(struct Model1_timespec Model1_t, unsigned Model1_gran);

/*
 * Validates if a timespec/timeval used to inject a time offset is valid.
 * Offsets can be postive or negative. The value of the timeval/timespec
 * is the sum of its fields, but *NOTE*: the field tv_usec/tv_nsec must
 * always be non-negative.
 */
static inline __attribute__((no_instrument_function)) bool Model1_timeval_inject_offset_valid(const struct Model1_timeval *Model1_tv)
{
 /* We don't check the tv_sec as it can be positive or negative */

 /* Can't have more microseconds then a second */
 if (Model1_tv->Model1_tv_usec < 0 || Model1_tv->Model1_tv_usec >= 1000000L)
  return false;
 return true;
}

static inline __attribute__((no_instrument_function)) bool Model1_timespec_inject_offset_valid(const struct Model1_timespec *Model1_ts)
{
 /* We don't check the tv_sec as it can be positive or negative */

 /* Can't have more nanoseconds then a second */
 if (Model1_ts->Model1_tv_nsec < 0 || Model1_ts->Model1_tv_nsec >= 1000000000L)
  return false;
 return true;
}




/* Some architectures do not supply their own clocksource.
 * This is mainly the case in architectures that get their
 * inter-tick times by reading the counter on their interval
 * timer. Since these timers wrap every tick, they're not really
 * useful as clocksources. Wrapping them to act like one is possible
 * but not very efficient. So we provide a callout these arches
 * can implement for use with the jiffies clocksource to provide
 * finer then tick granular time.
 */




struct Model1_itimerval;
extern int Model1_do_setitimer(int Model1_which, struct Model1_itimerval *Model1_value,
   struct Model1_itimerval *Model1_ovalue);
extern int Model1_do_getitimer(int Model1_which, struct Model1_itimerval *Model1_value);

extern unsigned int Model1_alarm_setitimer(unsigned int Model1_seconds);

extern long Model1_do_utimes(int Model1_dfd, const char *Model1_filename, struct Model1_timespec *Model1_times, int Model1_flags);

struct Model1_tms;
extern void Model1_do_sys_times(struct Model1_tms *);

/*
 * Similar to the struct tm in userspace <time.h>, but it needs to be here so
 * that the kernel source is self contained.
 */
struct Model1_tm {
 /*
	 * the number of seconds after the minute, normally in the range
	 * 0 to 59, but can be up to 60 to allow for leap seconds
	 */
 int Model1_tm_sec;
 /* the number of minutes after the hour, in the range 0 to 59*/
 int Model1_tm_min;
 /* the number of hours past midnight, in the range 0 to 23 */
 int Model1_tm_hour;
 /* the day of the month, in the range 1 to 31 */
 int Model1_tm_mday;
 /* the number of months since January, in the range 0 to 11 */
 int Model1_tm_mon;
 /* the number of years since 1900 */
 long Model1_tm_year;
 /* the number of days since Sunday, in the range 0 to 6 */
 int Model1_tm_wday;
 /* the number of days since January 1, in the range 0 to 365 */
 int Model1_tm_yday;
};

void Model1_time64_to_tm(Model1_time64_t Model1_totalsecs, int Model1_offset, struct Model1_tm *Model1_result);

/**
 * time_to_tm - converts the calendar time to local broken-down time
 *
 * @totalsecs	the number of seconds elapsed since 00:00:00 on January 1, 1970,
 *		Coordinated Universal Time (UTC).
 * @offset	offset seconds adding to totalsecs.
 * @result	pointer to struct tm variable to receive broken-down time
 */
static inline __attribute__((no_instrument_function)) void Model1_time_to_tm(Model1_time_t Model1_totalsecs, int Model1_offset, struct Model1_tm *Model1_result)
{
 Model1_time64_to_tm(Model1_totalsecs, Model1_offset, Model1_result);
}

/**
 * timespec_to_ns - Convert timespec to nanoseconds
 * @ts:		pointer to the timespec variable to be converted
 *
 * Returns the scalar nanosecond representation of the timespec
 * parameter.
 */
static inline __attribute__((no_instrument_function)) Model1_s64 Model1_timespec_to_ns(const struct Model1_timespec *Model1_ts)
{
 return ((Model1_s64) Model1_ts->Model1_tv_sec * 1000000000L) + Model1_ts->Model1_tv_nsec;
}

/**
 * timeval_to_ns - Convert timeval to nanoseconds
 * @ts:		pointer to the timeval variable to be converted
 *
 * Returns the scalar nanosecond representation of the timeval
 * parameter.
 */
static inline __attribute__((no_instrument_function)) Model1_s64 Model1_timeval_to_ns(const struct Model1_timeval *Model1_tv)
{
 return ((Model1_s64) Model1_tv->Model1_tv_sec * 1000000000L) +
  Model1_tv->Model1_tv_usec * 1000L;
}

/**
 * ns_to_timespec - Convert nanoseconds to timespec
 * @nsec:	the nanoseconds value to be converted
 *
 * Returns the timespec representation of the nsec parameter.
 */
extern struct Model1_timespec Model1_ns_to_timespec(const Model1_s64 Model1_nsec);

/**
 * ns_to_timeval - Convert nanoseconds to timeval
 * @nsec:	the nanoseconds value to be converted
 *
 * Returns the timeval representation of the nsec parameter.
 */
extern struct Model1_timeval Model1_ns_to_timeval(const Model1_s64 Model1_nsec);

/**
 * timespec_add_ns - Adds nanoseconds to a timespec
 * @a:		pointer to timespec to be incremented
 * @ns:		unsigned nanoseconds value to be added
 *
 * This must always be inlined because its used from the x86-64 vdso,
 * which cannot call other kernel functions.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_timespec_add_ns(struct Model1_timespec *Model1_a, Model1_u64 Model1_ns)
{
 Model1_a->Model1_tv_sec += Model1___iter_div_u64_rem(Model1_a->Model1_tv_nsec + Model1_ns, 1000000000L, &Model1_ns);
 Model1_a->Model1_tv_nsec = Model1_ns;
}







/*****************************************************************************
 *                                                                           *
 * Copyright (c) David L. Mills 1993                                         *
 *                                                                           *
 * Permission to use, copy, modify, and distribute this software and its     *
 * documentation for any purpose and without fee is hereby granted, provided *
 * that the above copyright notice appears in all copies and that both the   *
 * copyright notice and this permission notice appear in supporting          *
 * documentation, and that the name University of Delaware not be used in    *
 * advertising or publicity pertaining to distribution of the software       *
 * without specific, written prior permission.  The University of Delaware   *
 * makes no representations about the suitability this software for any      *
 * purpose.  It is provided "as is" without express or implied warranty.     *
 *                                                                           *
 *****************************************************************************/

/*
 * Modification history timex.h
 *
 * 29 Dec 97	Russell King
 *	Moved CLOCK_TICK_RATE, CLOCK_TICK_FACTOR and FINETUNE to asm/timex.h
 *	for ARM machines
 *
 *  9 Jan 97    Adrian Sun
 *      Shifted LATCH define to allow access to alpha machines.
 *
 * 26 Sep 94	David L. Mills
 *	Added defines for hybrid phase/frequency-lock loop.
 *
 * 19 Mar 94	David L. Mills
 *	Moved defines from kernel routines to header file and added new
 *	defines for PPS phase-lock loop.
 *
 * 20 Feb 94	David L. Mills
 *	Revised status codes and structures for external clock and PPS
 *	signal discipline.
 *
 * 28 Nov 93	David L. Mills
 *	Adjusted parameters to improve stability and increase poll
 *	interval.
 *
 * 17 Sep 93    David L. Mills
 *      Created file $NTP/include/sys/timex.h
 * 07 Oct 93    Torsten Duwe
 *      Derived linux/timex.h
 * 1995-08-13    Torsten Duwe
 *      kernel PLL updated to 1994-12-13 specs (rfc-1589)
 * 1997-08-30    Ulrich Windl
 *      Added new constant NTP_PHASE_LIMIT
 * 2004-08-12    Christoph Lameter
 *      Reworked time interpolation logic
 */




/*****************************************************************************
 *                                                                           *
 * Copyright (c) David L. Mills 1993                                         *
 *                                                                           *
 * Permission to use, copy, modify, and distribute this software and its     *
 * documentation for any purpose and without fee is hereby granted, provided *
 * that the above copyright notice appears in all copies and that both the   *
 * copyright notice and this permission notice appear in supporting          *
 * documentation, and that the name University of Delaware not be used in    *
 * advertising or publicity pertaining to distribution of the software       *
 * without specific, written prior permission.  The University of Delaware   *
 * makes no representations about the suitability this software for any      *
 * purpose.  It is provided "as is" without express or implied warranty.     *
 *                                                                           *
 *****************************************************************************/

/*
 * Modification history timex.h
 *
 * 29 Dec 97	Russell King
 *	Moved CLOCK_TICK_RATE, CLOCK_TICK_FACTOR and FINETUNE to asm/timex.h
 *	for ARM machines
 *
 *  9 Jan 97    Adrian Sun
 *      Shifted LATCH define to allow access to alpha machines.
 *
 * 26 Sep 94	David L. Mills
 *	Added defines for hybrid phase/frequency-lock loop.
 *
 * 19 Mar 94	David L. Mills
 *	Moved defines from kernel routines to header file and added new
 *	defines for PPS phase-lock loop.
 *
 * 20 Feb 94	David L. Mills
 *	Revised status codes and structures for external clock and PPS
 *	signal discipline.
 *
 * 28 Nov 93	David L. Mills
 *	Adjusted parameters to improve stability and increase poll
 *	interval.
 *
 * 17 Sep 93    David L. Mills
 *      Created file $NTP/include/sys/timex.h
 * 07 Oct 93    Torsten Duwe
 *      Derived linux/timex.h
 * 1995-08-13    Torsten Duwe
 *      kernel PLL updated to 1994-12-13 specs (rfc-1589)
 * 1997-08-30    Ulrich Windl
 *      Added new constant NTP_PHASE_LIMIT
 * 2004-08-12    Christoph Lameter
 *      Reworked time interpolation logic
 */







/*
 * syscall interface - used (mainly by NTP daemon)
 * to discipline kernel clock oscillator
 */
struct Model1_timex {
 unsigned int Model1_modes; /* mode selector */
 Model1___kernel_long_t Model1_offset; /* time offset (usec) */
 Model1___kernel_long_t Model1_freq; /* frequency offset (scaled ppm) */
 Model1___kernel_long_t Model1_maxerror;/* maximum error (usec) */
 Model1___kernel_long_t Model1_esterror;/* estimated error (usec) */
 int Model1_status; /* clock command/status */
 Model1___kernel_long_t Model1_constant;/* pll time constant */
 Model1___kernel_long_t Model1_precision;/* clock precision (usec) (read only) */
 Model1___kernel_long_t Model1_tolerance;/* clock frequency tolerance (ppm)
				   * (read only)
				   */
 struct Model1_timeval Model1_time; /* (read only, except for ADJ_SETOFFSET) */
 Model1___kernel_long_t Model1_tick; /* (modified) usecs between clock ticks */

 Model1___kernel_long_t Model1_ppsfreq;/* pps frequency (scaled ppm) (ro) */
 Model1___kernel_long_t Model1_jitter; /* pps jitter (us) (ro) */
 int Model1_shift; /* interval duration (s) (shift) (ro) */
 Model1___kernel_long_t Model1_stabil; /* pps stability (scaled ppm) (ro) */
 Model1___kernel_long_t Model1_jitcnt; /* jitter limit exceeded (ro) */
 Model1___kernel_long_t Model1_calcnt; /* calibration intervals (ro) */
 Model1___kernel_long_t Model1_errcnt; /* calibration errors (ro) */
 Model1___kernel_long_t Model1_stbcnt; /* stability limit exceeded (ro) */

 int Model1_tai; /* TAI offset (ro) */

 int :32; int :32; int :32; int :32;
 int :32; int :32; int :32; int :32;
 int :32; int :32; int :32;
};

/*
 * Mode codes (timex.mode)
 */
/* NTP userland likes the MOD_ prefix better */
/*
 * Status codes (timex.status)
 */
/* read-only bits */



/*
 * Clock states (time_state)
 */

















/*
 * x86 TSC related functions
 */
/*
 * Standard way to access the cycle counter.
 */
typedef unsigned long long Model1_cycles_t;

extern unsigned int Model1_cpu_khz;
extern unsigned int Model1_tsc_khz;

extern void Model1_disable_TSC(void);

static inline __attribute__((no_instrument_function)) Model1_cycles_t Model1_get_cycles(void)
{





 return Model1_rdtsc();
}

extern struct Model1_system_counterval_t Model1_convert_art_to_tsc(Model1_cycle_t Model1_art);

extern void Model1_tsc_init(void);
extern void Model1_mark_tsc_unstable(char *Model1_reason);
extern int Model1_unsynchronized_tsc(void);
extern int Model1_check_tsc_unstable(void);
extern unsigned long Model1_native_calibrate_cpu(void);
extern unsigned long Model1_native_calibrate_tsc(void);
extern unsigned long long Model1_native_sched_clock_from_tsc(Model1_u64 Model1_tsc);

extern int Model1_tsc_clocksource_reliable;

/*
 * Boot-time check whether the TSCs are synchronized across
 * all CPUs/cores:
 */
extern void Model1_check_tsc_sync_source(int Model1_cpu);
extern void Model1_check_tsc_sync_target(void);

extern int Model1_notsc_setup(char *);
extern void Model1_tsc_save_sched_clock_state(void);
extern void Model1_tsc_restore_sched_clock_state(void);

unsigned long Model1_cpu_khz_from_msr(void);

/* Assume we use the PIT time source for the clock tick */


/*
 * The random_get_entropy() function is used by the /dev/random driver
 * in order to extract entropy via the relative unpredictability of
 * when an interrupt takes places versus a high speed, fine-grained
 * timing source or cycle counter.  Since it will be occurred on every
 * single interrupt, it must have a very low cost/overhead.
 *
 * By default we use get_cycles() for this purpose, but individual
 * architectures may override this in their asm/timex.h header file.
 */



/*
 * SHIFT_PLL is used as a dampening factor to define how much we
 * adjust the frequency correction for a given offset in PLL mode.
 * It also used in dampening the offset correction, to define how
 * much of the current value in time_offset we correct for each
 * second. Changing this value changes the stiffness of the ntp
 * adjustment code. A lower value makes it more flexible, reducing
 * NTP convergence time. A higher value makes it stiffer, increasing
 * convergence time, but making the clock more stable.
 *
 * In David Mills' nanokernel reference implementation SHIFT_PLL is 4.
 * However this seems to increase convergence time much too long.
 *
 * https://lists.ntp.org/pipermail/hackers/2008-January/003487.html
 *
 * In the above mailing list discussion, it seems the value of 4
 * was appropriate for other Unix systems with HZ=100, and that
 * SHIFT_PLL should be decreased as HZ increases. However, Linux's
 * clock steering implementation is HZ independent.
 *
 * Through experimentation, a SHIFT_PLL value of 2 was found to allow
 * for fast convergence (very similar to the NTPv3 code used prior to
 * v2.6.19), with good clock stability.
 *
 *
 * SHIFT_FLL is used as a dampening factor to define how much we
 * adjust the frequency correction for a given offset in FLL mode.
 * In David Mills' nanokernel reference implementation SHIFT_FLL is 2.
 *
 * MAXTC establishes the maximum time constant of the PLL.
 */




/*
 * SHIFT_USEC defines the scaling (shift) of the time_freq and
 * time_tolerance variables, which represent the current frequency
 * offset and maximum frequency tolerance.
 */
/*
 * kernel variables
 * Note: maximum error = NTP synch distance = dispersion + delay / 2;
 * estimated error = NTP dispersion.
 */
extern unsigned long Model1_tick_usec; /* USER_HZ period (usec) */
extern unsigned long Model1_tick_nsec; /* SHIFTED_HZ period (nsec) */

/* Required to safely shift negative values */
extern int Model1_do_adjtimex(struct Model1_timex *);
extern void Model1_hardpps(const struct Model1_timespec *, const struct Model1_timespec *);

int Model1_read_current_timer(unsigned long *Model1_timer_val);
void Model1_ntp_notify_cmos_timer(void);

/* The clock frequency of the i8253/i8254 PIT */
/* Automatically generated by kernel/time/timeconst.bc */
/* Time conversion constants for HZ == 1000 */

/*
 * The following defines establish the engineering parameters of the PLL
 * model. The HZ variable establishes the timer interrupt frequency, 100 Hz
 * for the SunOS kernel, 256 Hz for the Ultrix kernel and 1024 Hz for the
 * OSF/1 kernel. The SHIFT_HZ define expresses the same value as the
 * nearest power of two in order to avoid hardware multiply operations.
 */
/* Suppose we want to divide two numbers NOM and DEN: NOM/DEN, then we can
 * improve accuracy by shifting LSH bits, hence calculating:
 *     (NOM << LSH) / DEN
 * This however means trouble for large NOM, because (NOM << LSH) may no
 * longer fit in 32 bits. The following way of calculating this gives us
 * some slack, under the following conditions:
 *   - (NOM / DEN) fits in (32 - LSH) bits.
 *   - (NOM % DEN) fits in (32 - LSH) bits.
 */



/* LATCH is used in the interval timer and ftape setup. */


extern int Model1_register_refined_jiffies(long Model1_clock_tick_rate);

/* TICK_NSEC is the time between ticks in nsec assuming SHIFTED_HZ */


/* TICK_USEC is the time between ticks in usec assuming fake USER_HZ */


/* some arch's have a small-data section that can be accessed register-relative
 * but that can only take up to, say, 4-byte variables. jiffies being part of
 * an 8-byte variable may not be correctly accessed unless we force the issue
 */


/*
 * The 64-bit value is not atomic - you MUST NOT read it
 * without sampling the sequence number in jiffies_lock.
 * get_jiffies_64() will do this for you as appropriate.
 */
extern Model1_u64 __attribute__((section(".data"))) Model1_jiffies_64;
extern unsigned long volatile __attribute__((section(".data"))) Model1_jiffies;




static inline __attribute__((no_instrument_function)) Model1_u64 Model1_get_jiffies_64(void)
{
 return (Model1_u64)Model1_jiffies;
}


/*
 *	These inlines deal with timer wrapping correctly. You are 
 *	strongly encouraged to use them
 *	1. Because people otherwise forget
 *	2. Because if the timer wrap changes in future you won't have to
 *	   alter your driver code.
 *
 * time_after(a,b) returns true if the time a is after time b.
 *
 * Do this with "<0" and ">=0" to only test the sign of the result. A
 * good compiler would generate better code (and a really good compiler
 * wouldn't care). Gcc is currently neither.
 */
/*
 * Calculate whether a is in the range of [b, c].
 */




/*
 * Calculate whether a is in the range of [b, c).
 */




/* Same as above, but does so with platform independent 64bit types.
 * These must be used when utilizing jiffies_64 (i.e. return value of
 * get_jiffies_64() */
/*
 * These four macros compare jiffies and 'a' for convenience.
 */

/* time_is_before_jiffies(a) return true if a is before jiffies */


/* time_is_after_jiffies(a) return true if a is after jiffies */


/* time_is_before_eq_jiffies(a) return true if a is before or equal to jiffies*/


/* time_is_after_eq_jiffies(a) return true if a is after or equal to jiffies*/


/*
 * Have the 32 bit jiffies value wrap 5 minutes after boot
 * so jiffies wrap bugs show up earlier.
 */


/*
 * Change timeval to jiffies, trying to avoid the
 * most obvious overflows..
 *
 * And some not so obvious.
 *
 * Note that we don't want to return LONG_MAX, because
 * for various timeout reasons we often end up having
 * to wait "jiffies+1" in order to guarantee that we wait
 * at _least_ "jiffies" - so "jiffies+1" had better still
 * be positive.
 */


extern unsigned long Model1_preset_lpj;

/*
 * We want to do realistic conversions of time so we need to use the same
 * values the update wall clock code uses as the jiffies size.  This value
 * is: TICK_NSEC (which is defined in timex.h).  This
 * is a constant and is in nanoseconds.  We will use scaled math
 * with a set of scales defined here as SEC_JIFFIE_SC,  USEC_JIFFIE_SC and
 * NSEC_JIFFIE_SC.  Note that these defines contain nothing but
 * constants and so are computed at compile time.  SHIFT_HZ (computed in
 * timex.h) adjusts the scaling for different HZ values.

 * Scaled math???  What is that?
 *
 * Scaled math is a way to do integer math on values that would,
 * otherwise, either overflow, underflow, or cause undesired div
 * instructions to appear in the execution path.  In short, we "scale"
 * up the operands so they take more bits (more precision, less
 * underflow), do the desired operation and then "scale" the result back
 * by the same amount.  If we do the scaling by shifting we avoid the
 * costly mpy and the dastardly div instructions.

 * Suppose, for example, we want to convert from seconds to jiffies
 * where jiffies is defined in nanoseconds as NSEC_PER_JIFFIE.  The
 * simple math is: jiff = (sec * NSEC_PER_SEC) / NSEC_PER_JIFFIE; We
 * observe that (NSEC_PER_SEC / NSEC_PER_JIFFIE) is a constant which we
 * might calculate at compile time, however, the result will only have
 * about 3-4 bits of precision (less for smaller values of HZ).
 *
 * So, we scale as follows:
 * jiff = (sec) * (NSEC_PER_SEC / NSEC_PER_JIFFIE);
 * jiff = ((sec) * ((NSEC_PER_SEC * SCALE)/ NSEC_PER_JIFFIE)) / SCALE;
 * Then we make SCALE a power of two so:
 * jiff = ((sec) * ((NSEC_PER_SEC << SCALE)/ NSEC_PER_JIFFIE)) >> SCALE;
 * Now we define:
 * #define SEC_CONV = ((NSEC_PER_SEC << SCALE)/ NSEC_PER_JIFFIE))
 * jiff = (sec * SEC_CONV) >> SCALE;
 *
 * Often the math we use will expand beyond 32-bits so we tell C how to
 * do this and pass the 64-bit result of the mpy through the ">> SCALE"
 * which should take the result back to 32-bits.  We want this expansion
 * to capture as much precision as possible.  At the same time we don't
 * want to overflow so we pick the SCALE to avoid this.  In this file,
 * that means using a different scale for each range of HZ values (as
 * defined in timex.h).
 *
 * For those who want to know, gcc will give a 64-bit result from a "*"
 * operator if the result is a long long AND at least one of the
 * operands is cast to long long (usually just prior to the "*" so as
 * not to confuse it into thinking it really has a 64-bit operand,
 * which, buy the way, it can do, but it takes more code and at least 2
 * mpys).

 * We also need to be aware that one second in nanoseconds is only a
 * couple of bits away from overflowing a 32-bit word, so we MUST use
 * 64-bits to get the full range time in nanoseconds.

 */

/*
 * Here are the scales we will use.  One for seconds, nanoseconds and
 * microseconds.
 *
 * Within the limits of cpp we do a rough cut at the SEC_JIFFIE_SC and
 * check if the sign bit is set.  If not, we bump the shift count by 1.
 * (Gets an extra bit of precision where we can use it.)
 * We know it is set for HZ = 1024 and HZ = 100 not for 1000.
 * Haven't tested others.

 * Limits of cpp (for #if expressions) only long (no long long), but
 * then we only need the most signicant bit.
 */
/*
 * The maximum jiffie value is (MAX_INT >> 1).  Here we translate that
 * into seconds.  The 64-bit case will overflow if we are not careful,
 * so use the messy SH_DIV macro to do it.  Still all constants.
 */
/*
 * Convert various time units to each other:
 */
extern unsigned int Model1_jiffies_to_msecs(const unsigned long Model1_j);
extern unsigned int Model1_jiffies_to_usecs(const unsigned long Model1_j);

static inline __attribute__((no_instrument_function)) Model1_u64 Model1_jiffies_to_nsecs(const unsigned long Model1_j)
{
 return (Model1_u64)Model1_jiffies_to_usecs(Model1_j) * 1000L;
}

extern unsigned long Model1___msecs_to_jiffies(const unsigned int Model1_m);

/*
 * HZ is equal to or smaller than 1000, and 1000 is a nice round
 * multiple of HZ, divide with the factor between them, but round
 * upwards:
 */
static inline __attribute__((no_instrument_function)) unsigned long Model1__msecs_to_jiffies(const unsigned int Model1_m)
{
 return (Model1_m + (1000L / 1000) - 1) / (1000L / 1000);
}
/**
 * msecs_to_jiffies: - convert milliseconds to jiffies
 * @m:	time in milliseconds
 *
 * conversion is done as follows:
 *
 * - negative values mean 'infinite timeout' (MAX_JIFFY_OFFSET)
 *
 * - 'too large' values [that would result in larger than
 *   MAX_JIFFY_OFFSET values] mean 'infinite timeout' too.
 *
 * - all other values are converted to jiffies by either multiplying
 *   the input value by a factor or dividing it with a factor and
 *   handling any 32-bit overflows.
 *   for the details see __msecs_to_jiffies()
 *
 * msecs_to_jiffies() checks for the passed in value being a constant
 * via __builtin_constant_p() allowing gcc to eliminate most of the
 * code, __msecs_to_jiffies() is called if the value passed does not
 * allow constant folding and the actual conversion must be done at
 * runtime.
 * the HZ range specific helpers _msecs_to_jiffies() are called both
 * directly here and from __msecs_to_jiffies() in the case where
 * constant folding is not possible.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) unsigned long Model1_msecs_to_jiffies(const unsigned int Model1_m)
{
 if (__builtin_constant_p(Model1_m)) {
  if ((int)Model1_m < 0)
   return ((((long)(~0UL>>1)) >> 1)-1);
  return Model1__msecs_to_jiffies(Model1_m);
 } else {
  return Model1___msecs_to_jiffies(Model1_m);
 }
}

extern unsigned long Model1___usecs_to_jiffies(const unsigned int Model1_u);

static inline __attribute__((no_instrument_function)) unsigned long Model1__usecs_to_jiffies(const unsigned int Model1_u)
{
 return (Model1_u + (1000000L / 1000) - 1) / (1000000L / 1000);
}
/**
 * usecs_to_jiffies: - convert microseconds to jiffies
 * @u:	time in microseconds
 *
 * conversion is done as follows:
 *
 * - 'too large' values [that would result in larger than
 *   MAX_JIFFY_OFFSET values] mean 'infinite timeout' too.
 *
 * - all other values are converted to jiffies by either multiplying
 *   the input value by a factor or dividing it with a factor and
 *   handling any 32-bit overflows as for msecs_to_jiffies.
 *
 * usecs_to_jiffies() checks for the passed in value being a constant
 * via __builtin_constant_p() allowing gcc to eliminate most of the
 * code, __usecs_to_jiffies() is called if the value passed does not
 * allow constant folding and the actual conversion must be done at
 * runtime.
 * the HZ range specific helpers _usecs_to_jiffies() are called both
 * directly here and from __msecs_to_jiffies() in the case where
 * constant folding is not possible.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) unsigned long Model1_usecs_to_jiffies(const unsigned int Model1_u)
{
 if (__builtin_constant_p(Model1_u)) {
  if (Model1_u > Model1_jiffies_to_usecs(((((long)(~0UL>>1)) >> 1)-1)))
   return ((((long)(~0UL>>1)) >> 1)-1);
  return Model1__usecs_to_jiffies(Model1_u);
 } else {
  return Model1___usecs_to_jiffies(Model1_u);
 }
}

extern unsigned long Model1_timespec64_to_jiffies(const struct Model1_timespec *Model1_value);
extern void Model1_jiffies_to_timespec64(const unsigned long Model1_jiffies,
      struct Model1_timespec *Model1_value);
static inline __attribute__((no_instrument_function)) unsigned long Model1_timespec_to_jiffies(const struct Model1_timespec *Model1_value)
{
 struct Model1_timespec Model1_ts = Model1_timespec_to_timespec64(*Model1_value);

 return Model1_timespec64_to_jiffies(&Model1_ts);
}

static inline __attribute__((no_instrument_function)) void Model1_jiffies_to_timespec(const unsigned long Model1_jiffies,
           struct Model1_timespec *Model1_value)
{
 struct Model1_timespec Model1_ts;

 Model1_jiffies_to_timespec64(Model1_jiffies, &Model1_ts);
 *Model1_value = Model1_timespec64_to_timespec(Model1_ts);
}

extern unsigned long Model1_timeval_to_jiffies(const struct Model1_timeval *Model1_value);
extern void Model1_jiffies_to_timeval(const unsigned long Model1_jiffies,
          struct Model1_timeval *Model1_value);

extern Model1_clock_t Model1_jiffies_to_clock_t(unsigned long Model1_x);
static inline __attribute__((no_instrument_function)) Model1_clock_t Model1_jiffies_delta_to_clock_t(long Model1_delta)
{
 return Model1_jiffies_to_clock_t(({ typeof(0L) Model1__max1 = (0L); typeof(Model1_delta) Model1__max2 = (Model1_delta); (void) (&Model1__max1 == &Model1__max2); Model1__max1 > Model1__max2 ? Model1__max1 : Model1__max2; }));
}

extern unsigned long Model1_clock_t_to_jiffies(unsigned long Model1_x);
extern Model1_u64 Model1_jiffies_64_to_clock_t(Model1_u64 Model1_x);
extern Model1_u64 Model1_nsec_to_clock_t(Model1_u64 Model1_x);
extern Model1_u64 Model1_nsecs_to_jiffies64(Model1_u64 Model1_n);
extern unsigned long Model1_nsecs_to_jiffies(Model1_u64 Model1_n);

/*
 * ktime_t:
 *
 * A single 64-bit variable is used to store the hrtimers
 * internal representation of time values in scalar nanoseconds. The
 * design plays out best on 64-bit CPUs, where most conversions are
 * NOPs and most arithmetic ktime_t operations are plain arithmetic
 * operations.
 *
 */
union Model1_ktime {
 Model1_s64 Model1_tv64;
};

typedef union Model1_ktime Model1_ktime_t; /* Kill this */

/**
 * ktime_set - Set a ktime_t variable from a seconds/nanoseconds value
 * @secs:	seconds to set
 * @nsecs:	nanoseconds to set
 *
 * Return: The ktime_t representation of the value.
 */
static inline __attribute__((no_instrument_function)) Model1_ktime_t Model1_ktime_set(const Model1_s64 Model1_secs, const unsigned long Model1_nsecs)
{
 if (__builtin_expect(!!(Model1_secs >= (((Model1_s64)~((Model1_u64)1 << 63)) / 1000000000L)), 0))
  return (Model1_ktime_t){ .Model1_tv64 = ((Model1_s64)~((Model1_u64)1 << 63)) };

 return (Model1_ktime_t) { .Model1_tv64 = Model1_secs * 1000000000L + (Model1_s64)Model1_nsecs };
}

/* Subtract two ktime_t variables. rem = lhs -rhs: */



/* Add two ktime_t variables. res = lhs + rhs: */



/*
 * Add a ktime_t variable and a scalar nanosecond value.
 * res = kt + nsval:
 */



/*
 * Subtract a scalar nanosecod from a ktime_t variable
 * res = kt - nsval:
 */



/* convert a timespec to ktime_t format: */
static inline __attribute__((no_instrument_function)) Model1_ktime_t Model1_timespec_to_ktime(struct Model1_timespec Model1_ts)
{
 return Model1_ktime_set(Model1_ts.Model1_tv_sec, Model1_ts.Model1_tv_nsec);
}

/* convert a timespec64 to ktime_t format: */
static inline __attribute__((no_instrument_function)) Model1_ktime_t Model1_timespec64_to_ktime(struct Model1_timespec Model1_ts)
{
 return Model1_ktime_set(Model1_ts.Model1_tv_sec, Model1_ts.Model1_tv_nsec);
}

/* convert a timeval to ktime_t format: */
static inline __attribute__((no_instrument_function)) Model1_ktime_t Model1_timeval_to_ktime(struct Model1_timeval Model1_tv)
{
 return Model1_ktime_set(Model1_tv.Model1_tv_sec, Model1_tv.Model1_tv_usec * 1000L);
}

/* Map the ktime_t to timespec conversion to ns_to_timespec function */


/* Map the ktime_t to timespec conversion to ns_to_timespec function */


/* Map the ktime_t to timeval conversion to ns_to_timeval function */


/* Convert ktime_t to nanoseconds - NOP in the scalar storage format: */



/**
 * ktime_equal - Compares two ktime_t variables to see if they are equal
 * @cmp1:	comparable1
 * @cmp2:	comparable2
 *
 * Compare two ktime_t variables.
 *
 * Return: 1 if equal.
 */
static inline __attribute__((no_instrument_function)) int Model1_ktime_equal(const Model1_ktime_t Model1_cmp1, const Model1_ktime_t Model1_cmp2)
{
 return Model1_cmp1.Model1_tv64 == Model1_cmp2.Model1_tv64;
}

/**
 * ktime_compare - Compares two ktime_t variables for less, greater or equal
 * @cmp1:	comparable1
 * @cmp2:	comparable2
 *
 * Return: ...
 *   cmp1  < cmp2: return <0
 *   cmp1 == cmp2: return 0
 *   cmp1  > cmp2: return >0
 */
static inline __attribute__((no_instrument_function)) int Model1_ktime_compare(const Model1_ktime_t Model1_cmp1, const Model1_ktime_t Model1_cmp2)
{
 if (Model1_cmp1.Model1_tv64 < Model1_cmp2.Model1_tv64)
  return -1;
 if (Model1_cmp1.Model1_tv64 > Model1_cmp2.Model1_tv64)
  return 1;
 return 0;
}

/**
 * ktime_after - Compare if a ktime_t value is bigger than another one.
 * @cmp1:	comparable1
 * @cmp2:	comparable2
 *
 * Return: true if cmp1 happened after cmp2.
 */
static inline __attribute__((no_instrument_function)) bool Model1_ktime_after(const Model1_ktime_t Model1_cmp1, const Model1_ktime_t Model1_cmp2)
{
 return Model1_ktime_compare(Model1_cmp1, Model1_cmp2) > 0;
}

/**
 * ktime_before - Compare if a ktime_t value is smaller than another one.
 * @cmp1:	comparable1
 * @cmp2:	comparable2
 *
 * Return: true if cmp1 happened before cmp2.
 */
static inline __attribute__((no_instrument_function)) bool Model1_ktime_before(const Model1_ktime_t Model1_cmp1, const Model1_ktime_t Model1_cmp2)
{
 return Model1_ktime_compare(Model1_cmp1, Model1_cmp2) < 0;
}
static inline __attribute__((no_instrument_function)) Model1_s64 Model1_ktime_divns(const Model1_ktime_t Model1_kt, Model1_s64 Model1_div)
{
 /*
	 * 32-bit implementation cannot handle negative divisors,
	 * so catch them on 64bit as well.
	 */
 ({ int Model1___ret_warn_on = !!(Model1_div < 0); if (__builtin_expect(!!(Model1___ret_warn_on), 0)) Model1_warn_slowpath_null("./include/linux/ktime.h", 194); __builtin_expect(!!(Model1___ret_warn_on), 0); });
 return Model1_kt.Model1_tv64 / Model1_div;
}


static inline __attribute__((no_instrument_function)) Model1_s64 Model1_ktime_to_us(const Model1_ktime_t Model1_kt)
{
 return Model1_ktime_divns(Model1_kt, 1000L);
}

static inline __attribute__((no_instrument_function)) Model1_s64 Model1_ktime_to_ms(const Model1_ktime_t Model1_kt)
{
 return Model1_ktime_divns(Model1_kt, 1000000L);
}

static inline __attribute__((no_instrument_function)) Model1_s64 Model1_ktime_us_delta(const Model1_ktime_t Model1_later, const Model1_ktime_t Model1_earlier)
{
       return Model1_ktime_to_us(({ (Model1_ktime_t){ .Model1_tv64 = (Model1_later).Model1_tv64 - (Model1_earlier).Model1_tv64 }; }));
}

static inline __attribute__((no_instrument_function)) Model1_s64 Model1_ktime_ms_delta(const Model1_ktime_t Model1_later, const Model1_ktime_t Model1_earlier)
{
 return Model1_ktime_to_ms(({ (Model1_ktime_t){ .Model1_tv64 = (Model1_later).Model1_tv64 - (Model1_earlier).Model1_tv64 }; }));
}

static inline __attribute__((no_instrument_function)) Model1_ktime_t Model1_ktime_add_us(const Model1_ktime_t Model1_kt, const Model1_u64 Model1_usec)
{
 return ({ (Model1_ktime_t){ .Model1_tv64 = (Model1_kt).Model1_tv64 + (Model1_usec * 1000L) }; });
}

static inline __attribute__((no_instrument_function)) Model1_ktime_t Model1_ktime_add_ms(const Model1_ktime_t Model1_kt, const Model1_u64 Model1_msec)
{
 return ({ (Model1_ktime_t){ .Model1_tv64 = (Model1_kt).Model1_tv64 + (Model1_msec * 1000000L) }; });
}

static inline __attribute__((no_instrument_function)) Model1_ktime_t Model1_ktime_sub_us(const Model1_ktime_t Model1_kt, const Model1_u64 Model1_usec)
{
 return ({ (Model1_ktime_t){ .Model1_tv64 = (Model1_kt).Model1_tv64 - (Model1_usec * 1000L) }; });
}

extern Model1_ktime_t Model1_ktime_add_safe(const Model1_ktime_t Model1_lhs, const Model1_ktime_t Model1_rhs);

/**
 * ktime_to_timespec_cond - convert a ktime_t variable to timespec
 *			    format only if the variable contains data
 * @kt:		the ktime_t variable to convert
 * @ts:		the timespec variable to store the result in
 *
 * Return: %true if there was a successful conversion, %false if kt was 0.
 */
static inline __attribute__((no_instrument_function)) __attribute__((warn_unused_result)) bool Model1_ktime_to_timespec_cond(const Model1_ktime_t Model1_kt,
             struct Model1_timespec *Model1_ts)
{
 if (Model1_kt.Model1_tv64) {
  *Model1_ts = Model1_ns_to_timespec((Model1_kt).Model1_tv64);
  return true;
 } else {
  return false;
 }
}

/**
 * ktime_to_timespec64_cond - convert a ktime_t variable to timespec64
 *			    format only if the variable contains data
 * @kt:		the ktime_t variable to convert
 * @ts:		the timespec variable to store the result in
 *
 * Return: %true if there was a successful conversion, %false if kt was 0.
 */
static inline __attribute__((no_instrument_function)) __attribute__((warn_unused_result)) bool Model1_ktime_to_timespec64_cond(const Model1_ktime_t Model1_kt,
             struct Model1_timespec *Model1_ts)
{
 if (Model1_kt.Model1_tv64) {
  *Model1_ts = Model1_ns_to_timespec((Model1_kt).Model1_tv64);
  return true;
 } else {
  return false;
 }
}

/*
 * The resolution of the clocks. The resolution value is returned in
 * the clock_getres() system call to give application programmers an
 * idea of the (in)accuracy of timers. Timer values are rounded up to
 * this resolution values.
 */



static inline __attribute__((no_instrument_function)) Model1_ktime_t Model1_ns_to_ktime(Model1_u64 Model1_ns)
{
 static const Model1_ktime_t Model1_ktime_zero = { .Model1_tv64 = 0 };

 return ({ (Model1_ktime_t){ .Model1_tv64 = (Model1_ktime_zero).Model1_tv64 + (Model1_ns) }; });
}

static inline __attribute__((no_instrument_function)) Model1_ktime_t Model1_ms_to_ktime(Model1_u64 Model1_ms)
{
 static const Model1_ktime_t Model1_ktime_zero = { .Model1_tv64 = 0 };

 return Model1_ktime_add_ms(Model1_ktime_zero, Model1_ms);
}







/* Included from linux/ktime.h */

void Model1_timekeeping_init(void);
extern int Model1_timekeeping_suspended;

/*
 * Get and set timeofday
 */
extern void Model1_do_gettimeofday(struct Model1_timeval *Model1_tv);
extern int Model1_do_settimeofday64(const struct Model1_timespec *Model1_ts);
extern int Model1_do_sys_settimeofday64(const struct Model1_timespec *Model1_tv,
     const struct Model1_timezone *Model1_tz);
static inline __attribute__((no_instrument_function)) int Model1_do_sys_settimeofday(const struct Model1_timespec *Model1_tv,
          const struct Model1_timezone *Model1_tz)
{
 struct Model1_timespec Model1_ts64;

 if (!Model1_tv)
  return Model1_do_sys_settimeofday64(((void *)0), Model1_tz);

 if (!Model1_timespec_valid(Model1_tv))
  return -22;

 Model1_ts64 = Model1_timespec_to_timespec64(*Model1_tv);
 return Model1_do_sys_settimeofday64(&Model1_ts64, Model1_tz);
}

/*
 * Kernel time accessors
 */
unsigned long Model1_get_seconds(void);
struct Model1_timespec Model1_current_kernel_time64(void);
/* does not take xtime_lock */
struct Model1_timespec Model1___current_kernel_time(void);

static inline __attribute__((no_instrument_function)) struct Model1_timespec Model1_current_kernel_time(void)
{
 struct Model1_timespec Model1_now = Model1_current_kernel_time64();

 return Model1_timespec64_to_timespec(Model1_now);
}

/*
 * timespec based interfaces
 */
struct Model1_timespec Model1_get_monotonic_coarse64(void);
extern void Model1_getrawmonotonic64(struct Model1_timespec *Model1_ts);
extern void Model1_ktime_get_ts64(struct Model1_timespec *Model1_ts);
extern Model1_time64_t Model1_ktime_get_seconds(void);
extern Model1_time64_t Model1_ktime_get_real_seconds(void);

extern int Model1___getnstimeofday64(struct Model1_timespec *Model1_tv);
extern void Model1_getnstimeofday64(struct Model1_timespec *Model1_tv);
extern void Model1_getboottime64(struct Model1_timespec *Model1_ts);


/**
 * Deprecated. Use do_settimeofday64().
 */
static inline __attribute__((no_instrument_function)) int Model1_do_settimeofday(const struct Model1_timespec *Model1_ts)
{
 return Model1_do_settimeofday64(Model1_ts);
}

static inline __attribute__((no_instrument_function)) int Model1___getnstimeofday(struct Model1_timespec *Model1_ts)
{
 return Model1___getnstimeofday64(Model1_ts);
}

static inline __attribute__((no_instrument_function)) void Model1_getnstimeofday(struct Model1_timespec *Model1_ts)
{
 Model1_getnstimeofday64(Model1_ts);
}

static inline __attribute__((no_instrument_function)) void Model1_ktime_get_ts(struct Model1_timespec *Model1_ts)
{
 Model1_ktime_get_ts64(Model1_ts);
}

static inline __attribute__((no_instrument_function)) void Model1_ktime_get_real_ts(struct Model1_timespec *Model1_ts)
{
 Model1_getnstimeofday64(Model1_ts);
}

static inline __attribute__((no_instrument_function)) void Model1_getrawmonotonic(struct Model1_timespec *Model1_ts)
{
 Model1_getrawmonotonic64(Model1_ts);
}

static inline __attribute__((no_instrument_function)) struct Model1_timespec Model1_get_monotonic_coarse(void)
{
 return Model1_get_monotonic_coarse64();
}

static inline __attribute__((no_instrument_function)) void Model1_getboottime(struct Model1_timespec *Model1_ts)
{
 return Model1_getboottime64(Model1_ts);
}
/*
 * ktime_t based interfaces
 */

enum Model1_tk_offsets {
 Model1_TK_OFFS_REAL,
 Model1_TK_OFFS_BOOT,
 Model1_TK_OFFS_TAI,
 Model1_TK_OFFS_MAX,
};

extern Model1_ktime_t Model1_ktime_get(void);
extern Model1_ktime_t Model1_ktime_get_with_offset(enum Model1_tk_offsets Model1_offs);
extern Model1_ktime_t Model1_ktime_mono_to_any(Model1_ktime_t Model1_tmono, enum Model1_tk_offsets Model1_offs);
extern Model1_ktime_t Model1_ktime_get_raw(void);
extern Model1_u32 Model1_ktime_get_resolution_ns(void);

/**
 * ktime_get_real - get the real (wall-) time in ktime_t format
 */
static inline __attribute__((no_instrument_function)) Model1_ktime_t Model1_ktime_get_real(void)
{
 return Model1_ktime_get_with_offset(Model1_TK_OFFS_REAL);
}

/**
 * ktime_get_boottime - Returns monotonic time since boot in ktime_t format
 *
 * This is similar to CLOCK_MONTONIC/ktime_get, but also includes the
 * time spent in suspend.
 */
static inline __attribute__((no_instrument_function)) Model1_ktime_t Model1_ktime_get_boottime(void)
{
 return Model1_ktime_get_with_offset(Model1_TK_OFFS_BOOT);
}

/**
 * ktime_get_clocktai - Returns the TAI time of day in ktime_t format
 */
static inline __attribute__((no_instrument_function)) Model1_ktime_t Model1_ktime_get_clocktai(void)
{
 return Model1_ktime_get_with_offset(Model1_TK_OFFS_TAI);
}

/**
 * ktime_mono_to_real - Convert monotonic time to clock realtime
 */
static inline __attribute__((no_instrument_function)) Model1_ktime_t Model1_ktime_mono_to_real(Model1_ktime_t Model1_mono)
{
 return Model1_ktime_mono_to_any(Model1_mono, Model1_TK_OFFS_REAL);
}

static inline __attribute__((no_instrument_function)) Model1_u64 Model1_ktime_get_ns(void)
{
 return ((Model1_ktime_get()).Model1_tv64);
}

static inline __attribute__((no_instrument_function)) Model1_u64 Model1_ktime_get_real_ns(void)
{
 return ((Model1_ktime_get_real()).Model1_tv64);
}

static inline __attribute__((no_instrument_function)) Model1_u64 Model1_ktime_get_boot_ns(void)
{
 return ((Model1_ktime_get_boottime()).Model1_tv64);
}

static inline __attribute__((no_instrument_function)) Model1_u64 Model1_ktime_get_tai_ns(void)
{
 return ((Model1_ktime_get_clocktai()).Model1_tv64);
}

static inline __attribute__((no_instrument_function)) Model1_u64 Model1_ktime_get_raw_ns(void)
{
 return ((Model1_ktime_get_raw()).Model1_tv64);
}

extern Model1_u64 Model1_ktime_get_mono_fast_ns(void);
extern Model1_u64 Model1_ktime_get_raw_fast_ns(void);

/*
 * Timespec interfaces utilizing the ktime based ones
 */
static inline __attribute__((no_instrument_function)) void Model1_get_monotonic_boottime(struct Model1_timespec *Model1_ts)
{
 *Model1_ts = Model1_ns_to_timespec((Model1_ktime_get_boottime()).Model1_tv64);
}

static inline __attribute__((no_instrument_function)) void Model1_get_monotonic_boottime64(struct Model1_timespec *Model1_ts)
{
 *Model1_ts = Model1_ns_to_timespec((Model1_ktime_get_boottime()).Model1_tv64);
}

static inline __attribute__((no_instrument_function)) void Model1_timekeeping_clocktai(struct Model1_timespec *Model1_ts)
{
 *Model1_ts = Model1_ns_to_timespec((Model1_ktime_get_clocktai()).Model1_tv64);
}

/*
 * RTC specific
 */
extern bool Model1_timekeeping_rtc_skipsuspend(void);
extern bool Model1_timekeeping_rtc_skipresume(void);

extern void Model1_timekeeping_inject_sleeptime64(struct Model1_timespec *Model1_delta);

/*
 * PPS accessor
 */
extern void Model1_ktime_get_raw_and_real_ts64(struct Model1_timespec *Model1_ts_raw,
            struct Model1_timespec *Model1_ts_real);

/*
 * struct system_time_snapshot - simultaneous raw/real time capture with
 *	counter value
 * @cycles:	Clocksource counter value to produce the system times
 * @real:	Realtime system time
 * @raw:	Monotonic raw system time
 * @clock_was_set_seq:	The sequence number of clock was set events
 * @cs_was_changed_seq:	The sequence number of clocksource change events
 */
struct Model1_system_time_snapshot {
 Model1_cycle_t Model1_cycles;
 Model1_ktime_t Model1_real;
 Model1_ktime_t Model1_raw;
 unsigned int Model1_clock_was_set_seq;
 Model1_u8 Model1_cs_was_changed_seq;
};

/*
 * struct system_device_crosststamp - system/device cross-timestamp
 *	(syncronized capture)
 * @device:		Device time
 * @sys_realtime:	Realtime simultaneous with device time
 * @sys_monoraw:	Monotonic raw simultaneous with device time
 */
struct Model1_system_device_crosststamp {
 Model1_ktime_t Model1_device;
 Model1_ktime_t Model1_sys_realtime;
 Model1_ktime_t Model1_sys_monoraw;
};

/*
 * struct system_counterval_t - system counter value with the pointer to the
 *	corresponding clocksource
 * @cycles:	System counter value
 * @cs:		Clocksource corresponding to system counter value. Used by
 *	timekeeping code to verify comparibility of two cycle values
 */
struct Model1_system_counterval_t {
 Model1_cycle_t Model1_cycles;
 struct Model1_clocksource *Model1_cs;
};

/*
 * Get cross timestamp between system clock and device clock
 */
extern int Model1_get_device_system_crosststamp(
   int (*Model1_get_time_fn)(Model1_ktime_t *Model1_device_time,
    struct Model1_system_counterval_t *Model1_system_counterval,
    void *Model1_ctx),
   void *Model1_ctx,
   struct Model1_system_time_snapshot *Model1_history,
   struct Model1_system_device_crosststamp *Model1_xtstamp);

/*
 * Simultaneously snapshot realtime and monotonic raw clocks
 */
extern void Model1_ktime_get_snapshot(struct Model1_system_time_snapshot *Model1_systime_snapshot);

/*
 * Persistent clock related interfaces
 */
extern int Model1_persistent_clock_is_local;

extern void Model1_read_persistent_clock(struct Model1_timespec *Model1_ts);
extern void Model1_read_persistent_clock64(struct Model1_timespec *Model1_ts);
extern void Model1_read_boot_clock64(struct Model1_timespec *Model1_ts);
extern int Model1_update_persistent_clock(struct Model1_timespec Model1_now);
extern int Model1_update_persistent_clock64(struct Model1_timespec Model1_now);





extern int Model1_rcu_expedited; /* for sysctl */
extern int Model1_rcu_normal; /* also for sysctl */
bool Model1_rcu_gp_is_normal(void); /* Internal RCU use. */
bool Model1_rcu_gp_is_expedited(void); /* Internal RCU use. */
void Model1_rcu_expedite_gp(void);
void Model1_rcu_unexpedite_gp(void);


enum Model1_rcutorture_type {
 Model1_RCU_FLAVOR,
 Model1_RCU_BH_FLAVOR,
 Model1_RCU_SCHED_FLAVOR,
 Model1_RCU_TASKS_FLAVOR,
 Model1_SRCU_FLAVOR,
 Model1_INVALID_RCU_FLAVOR
};


void Model1_rcutorture_get_gp_data(enum Model1_rcutorture_type Model1_test_type, int *Model1_flags,
       unsigned long *Model1_gpnum, unsigned long *Model1_completed);
void Model1_rcutorture_record_test_transition(void);
void Model1_rcutorture_record_progress(unsigned long Model1_vernum);
void Model1_do_trace_rcu_torture_read(const char *Model1_rcutorturename,
          struct Model1_callback_head *Model1_rhp,
          unsigned long Model1_secs,
          unsigned long Model1_c_old,
          unsigned long Model1_c);
/* Exported common interfaces */
/* In classic RCU, call_rcu() is just call_rcu_sched(). */




/**
 * call_rcu_bh() - Queue an RCU for invocation after a quicker grace period.
 * @head: structure to be used for queueing the RCU updates.
 * @func: actual callback function to be invoked after the grace period
 *
 * The callback function will be invoked some time after a full grace
 * period elapses, in other words after all currently executing RCU
 * read-side critical sections have completed. call_rcu_bh() assumes
 * that the read-side critical sections end on completion of a softirq
 * handler. This means that read-side critical sections in process
 * context must not be interrupted by softirqs. This interface is to be
 * used when most of the read-side critical sections are in softirq context.
 * RCU read-side critical sections are delimited by :
 *  - rcu_read_lock() and  rcu_read_unlock(), if in interrupt context.
 *  OR
 *  - rcu_read_lock_bh() and rcu_read_unlock_bh(), if in process context.
 *  These may be nested.
 *
 * See the description of call_rcu() for more detailed information on
 * memory ordering guarantees.
 */
void Model1_call_rcu_bh(struct Model1_callback_head *Model1_head,
   Model1_rcu_callback_t func);

/**
 * call_rcu_sched() - Queue an RCU for invocation after sched grace period.
 * @head: structure to be used for queueing the RCU updates.
 * @func: actual callback function to be invoked after the grace period
 *
 * The callback function will be invoked some time after a full grace
 * period elapses, in other words after all currently executing RCU
 * read-side critical sections have completed. call_rcu_sched() assumes
 * that the read-side critical sections end on enabling of preemption
 * or on voluntary preemption.
 * RCU read-side critical sections are delimited by :
 *  - rcu_read_lock_sched() and  rcu_read_unlock_sched(),
 *  OR
 *  anything that disables preemption.
 *  These may be nested.
 *
 * See the description of call_rcu() for more detailed information on
 * memory ordering guarantees.
 */
void Model1_call_rcu_sched(struct Model1_callback_head *Model1_head,
      Model1_rcu_callback_t func);

void Model1_synchronize_sched(void);

/*
 * Structure allowing asynchronous waiting on RCU.
 */
struct Model1_rcu_synchronize {
 struct Model1_callback_head Model1_head;
 struct Model1_completion Model1_completion;
};
void Model1_wakeme_after_rcu(struct Model1_callback_head *Model1_head);

void Model1___wait_rcu_gp(bool Model1_checktiny, int Model1_n, Model1_call_rcu_func_t *Model1_crcu_array,
     struct Model1_rcu_synchronize *Model1_rs_array);
/**
 * synchronize_rcu_mult - Wait concurrently for multiple grace periods
 * @...: List of call_rcu() functions for the flavors to wait on.
 *
 * This macro waits concurrently for multiple flavors of RCU grace periods.
 * For example, synchronize_rcu_mult(call_rcu, call_rcu_bh) would wait
 * on concurrent RCU and RCU-bh grace periods.  Waiting on a give SRCU
 * domain requires you to write a wrapper function for that SRCU domain's
 * call_srcu() function, supplying the corresponding srcu_struct.
 *
 * If Tiny RCU, tell _wait_rcu_gp() not to bother waiting for RCU
 * or RCU-bh, given that anywhere synchronize_rcu_mult() can be called
 * is automatically a grace period.
 */



/**
 * call_rcu_tasks() - Queue an RCU for invocation task-based grace period
 * @head: structure to be used for queueing the RCU updates.
 * @func: actual callback function to be invoked after the grace period
 *
 * The callback function will be invoked some time after a full grace
 * period elapses, in other words after all currently executing RCU
 * read-side critical sections have completed. call_rcu_tasks() assumes
 * that the read-side critical sections end at a voluntary context
 * switch (not a preemption!), entry into idle, or transition to usermode
 * execution.  As such, there are no read-side primitives analogous to
 * rcu_read_lock() and rcu_read_unlock() because this primitive is intended
 * to determine that all tasks have passed through a safe state, not so
 * much for data-strcuture synchronization.
 *
 * See the description of call_rcu() for more detailed information on
 * memory ordering guarantees.
 */
void Model1_call_rcu_tasks(struct Model1_callback_head *Model1_head, Model1_rcu_callback_t func);
void Model1_synchronize_rcu_tasks(void);
void Model1_rcu_barrier_tasks(void);
static inline __attribute__((no_instrument_function)) void Model1___rcu_read_lock(void)
{
 if (0)
  __asm__ __volatile__("": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model1___rcu_read_unlock(void)
{
 if (0)
  __asm__ __volatile__("": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model1_synchronize_rcu(void)
{
 Model1_synchronize_sched();
}

static inline __attribute__((no_instrument_function)) int Model1_rcu_preempt_depth(void)
{
 return 0;
}



/* Internal to kernel */
void Model1_rcu_init(void);
void Model1_rcu_sched_qs(void);
void Model1_rcu_bh_qs(void);
void Model1_rcu_check_callbacks(int Model1_user);
void Model1_rcu_report_dead(unsigned int Model1_cpu);


void Model1_rcu_end_inkernel_boot(void);





void Model1_rcu_sysrq_start(void);
void Model1_rcu_sysrq_end(void);
static inline __attribute__((no_instrument_function)) void Model1_rcu_user_enter(void) { }
static inline __attribute__((no_instrument_function)) void Model1_rcu_user_exit(void) { }





static inline __attribute__((no_instrument_function)) void Model1_rcu_init_nohz(void)
{
}


/**
 * RCU_NONIDLE - Indicate idle-loop code that needs RCU readers
 * @a: Code that RCU needs to pay attention to.
 *
 * RCU, RCU-bh, and RCU-sched read-side critical sections are forbidden
 * in the inner idle loop, that is, between the rcu_idle_enter() and
 * the rcu_idle_exit() -- RCU will happily ignore any such read-side
 * critical sections.  However, things like powertop need tracepoints
 * in the inner idle loop.
 *
 * This macro provides the way out:  RCU_NONIDLE(do_something_with_RCU())
 * will tell RCU that it needs to pay attention, invoke its argument
 * (in this example, calling the do_something_with_RCU() function),
 * and then tell RCU to go back to ignoring this CPU.  It is permissible
 * to nest RCU_NONIDLE() wrappers, but not indefinitely (but the limit is
 * on the order of a million or so, even on 32-bit systems).  It is
 * not legal to block within RCU_NONIDLE(), nor is it permissible to
 * transfer control either into or out of RCU_NONIDLE()'s statement.
 */







/*
 * Note a voluntary context switch for RCU-tasks benefit.  This is a
 * macro rather than an inline function to avoid #include hell.
 */
/**
 * cond_resched_rcu_qs - Report potential quiescent states to RCU
 *
 * This macro resembles cond_resched(), except that it is defined to
 * report potential quiescent states to RCU-tasks even if the cond_resched()
 * machinery were to be shut off, as some advocate for PREEMPT kernels.
 */







bool Model1___rcu_is_watching(void);


/*
 * Infrastructure to implement the synchronize_() primitives in
 * TREE_RCU and rcu_barrier_() primitives in TINY_RCU.
 */



/*
 * Read-Copy Update mechanism for mutual exclusion (tree-based version)
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, you can access it online at
 * http://www.gnu.org/licenses/gpl-2.0.html.
 *
 * Copyright IBM Corporation, 2008
 *
 * Author: Dipankar Sarma <dipankar@in.ibm.com>
 *	   Paul E. McKenney <paulmck@linux.vnet.ibm.com> Hierarchical algorithm
 *
 * Based on the original work by Paul McKenney <paulmck@us.ibm.com>
 * and inputs from Rusty Russell, Andrea Arcangeli and Andi Kleen.
 *
 * For detailed explanation of Read-Copy Update mechanism see -
 *	Documentation/RCU
 */




void Model1_rcu_note_context_switch(void);
int Model1_rcu_needs_cpu(Model1_u64 Model1_basem, Model1_u64 *Model1_nextevt);
void Model1_rcu_cpu_stall_reset(void);

/*
 * Note a virtualization-based context switch.  This is simply a
 * wrapper around rcu_note_context_switch(), which allows TINY_RCU
 * to save a few bytes. The caller must have disabled interrupts.
 */
static inline __attribute__((no_instrument_function)) void Model1_rcu_virt_note_context_switch(int Model1_cpu)
{
 Model1_rcu_note_context_switch();
}

void Model1_synchronize_rcu_bh(void);
void Model1_synchronize_sched_expedited(void);
void Model1_synchronize_rcu_expedited(void);

void Model1_kfree_call_rcu(struct Model1_callback_head *Model1_head, Model1_rcu_callback_t func);

/**
 * synchronize_rcu_bh_expedited - Brute-force RCU-bh grace period
 *
 * Wait for an RCU-bh grace period to elapse, but use a "big hammer"
 * approach to force the grace period to end quickly.  This consumes
 * significant time on all CPUs and is unfriendly to real-time workloads,
 * so is thus not recommended for any sort of common-case code.  In fact,
 * if you are using synchronize_rcu_bh_expedited() in a loop, please
 * restructure your code to batch your updates, and then use a single
 * synchronize_rcu_bh() instead.
 *
 * Note that it is illegal to call this function while holding any lock
 * that is acquired by a CPU-hotplug notifier.  And yes, it is also illegal
 * to call this function from a CPU-hotplug notifier.  Failing to observe
 * these restriction will result in deadlock.
 */
static inline __attribute__((no_instrument_function)) void Model1_synchronize_rcu_bh_expedited(void)
{
 Model1_synchronize_sched_expedited();
}

void Model1_rcu_barrier(void);
void Model1_rcu_barrier_bh(void);
void Model1_rcu_barrier_sched(void);
unsigned long Model1_get_state_synchronize_rcu(void);
void Model1_cond_synchronize_rcu(unsigned long Model1_oldstate);
unsigned long Model1_get_state_synchronize_sched(void);
void Model1_cond_synchronize_sched(unsigned long Model1_oldstate);

extern unsigned long Model1_rcutorture_testseq;
extern unsigned long Model1_rcutorture_vernum;
unsigned long Model1_rcu_batches_started(void);
unsigned long Model1_rcu_batches_started_bh(void);
unsigned long Model1_rcu_batches_started_sched(void);
unsigned long Model1_rcu_batches_completed(void);
unsigned long Model1_rcu_batches_completed_bh(void);
unsigned long Model1_rcu_batches_completed_sched(void);
unsigned long Model1_rcu_exp_batches_completed(void);
unsigned long Model1_rcu_exp_batches_completed_sched(void);
void Model1_show_rcu_gp_kthreads(void);

void Model1_rcu_force_quiescent_state(void);
void Model1_rcu_bh_force_quiescent_state(void);
void Model1_rcu_sched_force_quiescent_state(void);

void Model1_rcu_idle_enter(void);
void Model1_rcu_idle_exit(void);
void Model1_rcu_irq_enter(void);
void Model1_rcu_irq_exit(void);
void Model1_rcu_irq_enter_irqson(void);
void Model1_rcu_irq_exit_irqson(void);

void Model1_exit_rcu(void);

void Model1_rcu_scheduler_starting(void);
extern int Model1_rcu_scheduler_active __attribute__((__section__(".data..read_mostly")));

bool Model1_rcu_is_watching(void);

void Model1_rcu_all_qs(void);

/* RCUtree hotplug events */
int Model1_rcutree_prepare_cpu(unsigned int Model1_cpu);
int Model1_rcutree_online_cpu(unsigned int Model1_cpu);
int Model1_rcutree_offline_cpu(unsigned int Model1_cpu);
int Model1_rcutree_dead_cpu(unsigned int Model1_cpu);
int Model1_rcutree_dying_cpu(unsigned int Model1_cpu);






/*
 * init_rcu_head_on_stack()/destroy_rcu_head_on_stack() are needed for dynamic
 * initialization and destruction of rcu_head on the stack. rcu_head structures
 * allocated dynamically in the heap or defined statically don't need any
 * initialization.
 */






static inline __attribute__((no_instrument_function)) void Model1_init_rcu_head(struct Model1_callback_head *Model1_head)
{
}

static inline __attribute__((no_instrument_function)) void Model1_destroy_rcu_head(struct Model1_callback_head *Model1_head)
{
}

static inline __attribute__((no_instrument_function)) void Model1_init_rcu_head_on_stack(struct Model1_callback_head *Model1_head)
{
}

static inline __attribute__((no_instrument_function)) void Model1_destroy_rcu_head_on_stack(struct Model1_callback_head *Model1_head)
{
}





static inline __attribute__((no_instrument_function)) bool Model1_rcu_lockdep_current_cpu_online(void)
{
 return true;
}
static inline __attribute__((no_instrument_function)) int Model1_rcu_read_lock_held(void)
{
 return 1;
}

static inline __attribute__((no_instrument_function)) int Model1_rcu_read_lock_bh_held(void)
{
 return 1;
}

static inline __attribute__((no_instrument_function)) int Model1_rcu_read_lock_sched_held(void)
{
 return !0;
}
/*
 * Helper functions for rcu_dereference_check(), rcu_dereference_protected()
 * and rcu_assign_pointer().  Some of these could be folded into their
 * callers, but they are left separate in order to ease introduction of
 * multiple flavors of pointers to match the multiple flavors of RCU
 * (e.g., __rcu_bh, * __rcu_sched, and __srcu), should this make sense in
 * the future.
 */
/**
 * RCU_INITIALIZER() - statically initialize an RCU-protected global variable
 * @v: The value to statically initialize with.
 */


/**
 * rcu_assign_pointer() - assign to RCU-protected pointer
 * @p: pointer to assign to
 * @v: value to assign (publish)
 *
 * Assigns the specified value to the specified RCU-protected
 * pointer, ensuring that any concurrent RCU readers will see
 * any prior initialization.
 *
 * Inserts memory barriers on architectures that require them
 * (which is most of them), and also prevents the compiler from
 * reordering the code that initializes the structure after the pointer
 * assignment.  More importantly, this call documents which pointers
 * will be dereferenced by RCU read-side code.
 *
 * In some special cases, you may use RCU_INIT_POINTER() instead
 * of rcu_assign_pointer().  RCU_INIT_POINTER() is a bit faster due
 * to the fact that it does not constrain either the CPU or the compiler.
 * That said, using RCU_INIT_POINTER() when you should have used
 * rcu_assign_pointer() is a very bad thing that results in
 * impossible-to-diagnose memory corruption.  So please be careful.
 * See the RCU_INIT_POINTER() comment header for details.
 *
 * Note that rcu_assign_pointer() evaluates each of its arguments only
 * once, appearances notwithstanding.  One of the "extra" evaluations
 * is in typeof() and the other visible only to sparse (__CHECKER__),
 * neither of which actually execute the argument.  As with most cpp
 * macros, this execute-arguments-only-once property is important, so
 * please be careful when making changes to rcu_assign_pointer() and the
 * other macros that it invokes.
 */
/**
 * rcu_access_pointer() - fetch RCU pointer with no dereferencing
 * @p: The pointer to read
 *
 * Return the value of the specified RCU-protected pointer, but omit the
 * smp_read_barrier_depends() and keep the READ_ONCE().  This is useful
 * when the value of this pointer is accessed, but the pointer is not
 * dereferenced, for example, when testing an RCU-protected pointer against
 * NULL.  Although rcu_access_pointer() may also be used in cases where
 * update-side locks prevent the value of the pointer from changing, you
 * should instead use rcu_dereference_protected() for this use case.
 *
 * It is also permissible to use rcu_access_pointer() when read-side
 * access to the pointer was removed at least one grace period ago, as
 * is the case in the context of the RCU callback that is freeing up
 * the data, or after a synchronize_rcu() returns.  This can be useful
 * when tearing down multi-linked structures after a grace period
 * has elapsed.
 */


/**
 * rcu_dereference_check() - rcu_dereference with debug checking
 * @p: The pointer to read, prior to dereferencing
 * @c: The conditions under which the dereference will take place
 *
 * Do an rcu_dereference(), but check that the conditions under which the
 * dereference will take place are correct.  Typically the conditions
 * indicate the various locking conditions that should be held at that
 * point.  The check should return true if the conditions are satisfied.
 * An implicit check for being in an RCU read-side critical section
 * (rcu_read_lock()) is included.
 *
 * For example:
 *
 *	bar = rcu_dereference_check(foo->bar, lockdep_is_held(&foo->lock));
 *
 * could be used to indicate to lockdep that foo->bar may only be dereferenced
 * if either rcu_read_lock() is held, or that the lock required to replace
 * the bar struct at foo->bar is held.
 *
 * Note that the list of conditions may also include indications of when a lock
 * need not be held, for example during initialisation or destruction of the
 * target struct:
 *
 *	bar = rcu_dereference_check(foo->bar, lockdep_is_held(&foo->lock) ||
 *					      atomic_read(&foo->usage) == 0);
 *
 * Inserts memory barriers on architectures that require them
 * (currently only the Alpha), prevents the compiler from refetching
 * (and from merging fetches), and, more importantly, documents exactly
 * which pointers are protected by RCU and checks that the pointer is
 * annotated as __rcu.
 */



/**
 * rcu_dereference_bh_check() - rcu_dereference_bh with debug checking
 * @p: The pointer to read, prior to dereferencing
 * @c: The conditions under which the dereference will take place
 *
 * This is the RCU-bh counterpart to rcu_dereference_check().
 */



/**
 * rcu_dereference_sched_check() - rcu_dereference_sched with debug checking
 * @p: The pointer to read, prior to dereferencing
 * @c: The conditions under which the dereference will take place
 *
 * This is the RCU-sched counterpart to rcu_dereference_check().
 */




/*
 * The tracing infrastructure traces RCU (we want that), but unfortunately
 * some of the RCU checks causes tracing to lock up the system.
 *
 * The no-tracing version of rcu_dereference_raw() must not call
 * rcu_read_lock_held().
 */


/**
 * rcu_dereference_protected() - fetch RCU pointer when updates prevented
 * @p: The pointer to read, prior to dereferencing
 * @c: The conditions under which the dereference will take place
 *
 * Return the value of the specified RCU-protected pointer, but omit
 * both the smp_read_barrier_depends() and the READ_ONCE().  This
 * is useful in cases where update-side locks prevent the value of the
 * pointer from changing.  Please note that this primitive does -not-
 * prevent the compiler from repeating this reference or combining it
 * with other references, so it should not be used without protection
 * of appropriate locks.
 *
 * This function is only for update-side use.  Using this function
 * when protected only by rcu_read_lock() will result in infrequent
 * but very ugly failures.
 */




/**
 * rcu_dereference() - fetch RCU-protected pointer for dereferencing
 * @p: The pointer to read, prior to dereferencing
 *
 * This is a simple wrapper around rcu_dereference_check().
 */


/**
 * rcu_dereference_bh() - fetch an RCU-bh-protected pointer for dereferencing
 * @p: The pointer to read, prior to dereferencing
 *
 * Makes rcu_dereference_check() do the dirty work.
 */


/**
 * rcu_dereference_sched() - fetch RCU-sched-protected pointer for dereferencing
 * @p: The pointer to read, prior to dereferencing
 *
 * Makes rcu_dereference_check() do the dirty work.
 */


/**
 * rcu_pointer_handoff() - Hand off a pointer from RCU to other mechanism
 * @p: The pointer to hand off
 *
 * This is simply an identity function, but it documents where a pointer
 * is handed off from RCU to some other synchronization mechanism, for
 * example, reference counting or locking.  In C11, it would map to
 * kill_dependency().  It could be used as follows:
 *
 *	rcu_read_lock();
 *	p = rcu_dereference(gp);
 *	long_lived = is_long_lived(p);
 *	if (long_lived) {
 *		if (!atomic_inc_not_zero(p->refcnt))
 *			long_lived = false;
 *		else
 *			p = rcu_pointer_handoff(p);
 *	}
 *	rcu_read_unlock();
 */


/**
 * rcu_read_lock() - mark the beginning of an RCU read-side critical section
 *
 * When synchronize_rcu() is invoked on one CPU while other CPUs
 * are within RCU read-side critical sections, then the
 * synchronize_rcu() is guaranteed to block until after all the other
 * CPUs exit their critical sections.  Similarly, if call_rcu() is invoked
 * on one CPU while other CPUs are within RCU read-side critical
 * sections, invocation of the corresponding RCU callback is deferred
 * until after the all the other CPUs exit their critical sections.
 *
 * Note, however, that RCU callbacks are permitted to run concurrently
 * with new RCU read-side critical sections.  One way that this can happen
 * is via the following sequence of events: (1) CPU 0 enters an RCU
 * read-side critical section, (2) CPU 1 invokes call_rcu() to register
 * an RCU callback, (3) CPU 0 exits the RCU read-side critical section,
 * (4) CPU 2 enters a RCU read-side critical section, (5) the RCU
 * callback is invoked.  This is legal, because the RCU read-side critical
 * section that was running concurrently with the call_rcu() (and which
 * therefore might be referencing something that the corresponding RCU
 * callback would free up) has completed before the corresponding
 * RCU callback is invoked.
 *
 * RCU read-side critical sections may be nested.  Any deferred actions
 * will be deferred until the outermost RCU read-side critical section
 * completes.
 *
 * You can avoid reading and understanding the next paragraph by
 * following this rule: don't put anything in an rcu_read_lock() RCU
 * read-side critical section that would block in a !PREEMPT kernel.
 * But if you want the full story, read on!
 *
 * In non-preemptible RCU implementations (TREE_RCU and TINY_RCU),
 * it is illegal to block while in an RCU read-side critical section.
 * In preemptible RCU implementations (PREEMPT_RCU) in CONFIG_PREEMPT
 * kernel builds, RCU read-side critical sections may be preempted,
 * but explicit blocking is illegal.  Finally, in preemptible RCU
 * implementations in real-time (with -rt patchset) kernel builds, RCU
 * read-side critical sections may be preempted and they may also block, but
 * only when acquiring spinlocks that are subject to priority inheritance.
 */
static inline __attribute__((no_instrument_function)) void Model1_rcu_read_lock(void)
{
 Model1___rcu_read_lock();
 (void)0;
 do { } while (0);
 do { } while (0);

}

/*
 * So where is rcu_write_lock()?  It does not exist, as there is no
 * way for writers to lock out RCU readers.  This is a feature, not
 * a bug -- this property is what provides RCU's performance benefits.
 * Of course, writers must coordinate with each other.  The normal
 * spinlock primitives work well for this, but any other technique may be
 * used as well.  RCU does not care how the writers keep out of each
 * others' way, as long as they do so.
 */

/**
 * rcu_read_unlock() - marks the end of an RCU read-side critical section.
 *
 * In most situations, rcu_read_unlock() is immune from deadlock.
 * However, in kernels built with CONFIG_RCU_BOOST, rcu_read_unlock()
 * is responsible for deboosting, which it does via rt_mutex_unlock().
 * Unfortunately, this function acquires the scheduler's runqueue and
 * priority-inheritance spinlocks.  This means that deadlock could result
 * if the caller of rcu_read_unlock() already holds one of these locks or
 * any lock that is ever acquired while holding them; or any lock which
 * can be taken from interrupt context because rcu_boost()->rt_mutex_lock()
 * does not disable irqs while taking ->wait_lock.
 *
 * That said, RCU readers are never priority boosted unless they were
 * preempted.  Therefore, one way to avoid deadlock is to make sure
 * that preemption never happens within any RCU read-side critical
 * section whose outermost rcu_read_unlock() is called with one of
 * rt_mutex_unlock()'s locks held.  Such preemption can be avoided in
 * a number of ways, for example, by invoking preempt_disable() before
 * critical section's outermost rcu_read_lock().
 *
 * Given that the set of locks acquired by rt_mutex_unlock() might change
 * at any time, a somewhat more future-proofed approach is to make sure
 * that that preemption never happens within any RCU read-side critical
 * section whose outermost rcu_read_unlock() is called with irqs disabled.
 * This approach relies on the fact that rt_mutex_unlock() currently only
 * acquires irq-disabled locks.
 *
 * The second of these two approaches is best in most situations,
 * however, the first approach can also be useful, at least to those
 * developers willing to keep abreast of the set of locks acquired by
 * rt_mutex_unlock().
 *
 * See rcu_read_lock() for more information.
 */
static inline __attribute__((no_instrument_function)) void Model1_rcu_read_unlock(void)
{
 do { } while (0);

 (void)0;
 Model1___rcu_read_unlock();
 do { } while (0); /* Keep acq info for rls diags. */
}

/**
 * rcu_read_lock_bh() - mark the beginning of an RCU-bh critical section
 *
 * This is equivalent of rcu_read_lock(), but to be used when updates
 * are being done using call_rcu_bh() or synchronize_rcu_bh(). Since
 * both call_rcu_bh() and synchronize_rcu_bh() consider completion of a
 * softirq handler to be a quiescent state, a process in RCU read-side
 * critical section must be protected by disabling softirqs. Read-side
 * critical sections in interrupt context can use just rcu_read_lock(),
 * though this should at least be commented to avoid confusing people
 * reading the code.
 *
 * Note that rcu_read_lock_bh() and the matching rcu_read_unlock_bh()
 * must occur in the same context, for example, it is illegal to invoke
 * rcu_read_unlock_bh() from one task if the matching rcu_read_lock_bh()
 * was invoked from some other task.
 */
static inline __attribute__((no_instrument_function)) void Model1_rcu_read_lock_bh(void)
{
 Model1_local_bh_disable();
 (void)0;
 do { } while (0);
 do { } while (0);

}

/*
 * rcu_read_unlock_bh - marks the end of a softirq-only RCU critical section
 *
 * See rcu_read_lock_bh() for more information.
 */
static inline __attribute__((no_instrument_function)) void Model1_rcu_read_unlock_bh(void)
{
 do { } while (0);

 do { } while (0);
 (void)0;
 Model1_local_bh_enable();
}

/**
 * rcu_read_lock_sched() - mark the beginning of a RCU-sched critical section
 *
 * This is equivalent of rcu_read_lock(), but to be used when updates
 * are being done using call_rcu_sched() or synchronize_rcu_sched().
 * Read-side critical sections can also be introduced by anything that
 * disables preemption, including local_irq_disable() and friends.
 *
 * Note that rcu_read_lock_sched() and the matching rcu_read_unlock_sched()
 * must occur in the same context, for example, it is illegal to invoke
 * rcu_read_unlock_sched() from process context if the matching
 * rcu_read_lock_sched() was invoked from an NMI handler.
 */
static inline __attribute__((no_instrument_function)) void Model1_rcu_read_lock_sched(void)
{
 __asm__ __volatile__("": : :"memory");
 (void)0;
 do { } while (0);
 do { } while (0);

}

/* Used by lockdep and tracing: cannot be traced, cannot call lockdep. */
static inline __attribute__((no_instrument_function)) __attribute__((no_instrument_function)) void Model1_rcu_read_lock_sched_notrace(void)
{
 __asm__ __volatile__("": : :"memory");
 (void)0;
}

/*
 * rcu_read_unlock_sched - marks the end of a RCU-classic critical section
 *
 * See rcu_read_lock_sched for more information.
 */
static inline __attribute__((no_instrument_function)) void Model1_rcu_read_unlock_sched(void)
{
 do { } while (0);

 do { } while (0);
 (void)0;
 __asm__ __volatile__("": : :"memory");
}

/* Used by lockdep and tracing: cannot be traced, cannot call lockdep. */
static inline __attribute__((no_instrument_function)) __attribute__((no_instrument_function)) void Model1_rcu_read_unlock_sched_notrace(void)
{
 (void)0;
 __asm__ __volatile__("": : :"memory");
}

/**
 * RCU_INIT_POINTER() - initialize an RCU protected pointer
 *
 * Initialize an RCU-protected pointer in special cases where readers
 * do not need ordering constraints on the CPU or the compiler.  These
 * special cases are:
 *
 * 1.	This use of RCU_INIT_POINTER() is NULLing out the pointer -or-
 * 2.	The caller has taken whatever steps are required to prevent
 *	RCU readers from concurrently accessing this pointer -or-
 * 3.	The referenced data structure has already been exposed to
 *	readers either at compile time or via rcu_assign_pointer() -and-
 *	a.	You have not made -any- reader-visible changes to
 *		this structure since then -or-
 *	b.	It is OK for readers accessing this structure from its
 *		new location to see the old state of the structure.  (For
 *		example, the changes were to statistical counters or to
 *		other state where exact synchronization is not required.)
 *
 * Failure to follow these rules governing use of RCU_INIT_POINTER() will
 * result in impossible-to-diagnose memory corruption.  As in the structures
 * will look OK in crash dumps, but any concurrent RCU readers might
 * see pre-initialized values of the referenced data structure.  So
 * please be very careful how you use RCU_INIT_POINTER()!!!
 *
 * If you are creating an RCU-protected linked structure that is accessed
 * by a single external-to-structure RCU-protected pointer, then you may
 * use RCU_INIT_POINTER() to initialize the internal RCU-protected
 * pointers, but you must use rcu_assign_pointer() to initialize the
 * external-to-structure pointer -after- you have completely initialized
 * the reader-accessible portions of the linked structure.
 *
 * Note that unlike rcu_assign_pointer(), RCU_INIT_POINTER() provides no
 * ordering guarantees for either the CPU or the compiler.
 */






/**
 * RCU_POINTER_INITIALIZER() - statically initialize an RCU protected pointer
 *
 * GCC-style initialization for an RCU-protected pointer in a structure field.
 */



/*
 * Does the specified offset indicate that the corresponding rcu_head
 * structure can be handled by kfree_rcu()?
 */


/*
 * Helper macro for kfree_rcu() to prevent argument-expansion eyestrain.
 */






/**
 * kfree_rcu() - kfree an object after a grace period.
 * @ptr:	pointer to kfree
 * @rcu_head:	the name of the struct rcu_head within the type of @ptr.
 *
 * Many rcu callbacks functions just call kfree() on the base structure.
 * These functions are trivial, but their size adds up, and furthermore
 * when they are used in a kernel module, that module must invoke the
 * high-latency rcu_barrier() function at module-unload time.
 *
 * The kfree_rcu() function handles this issue.  Rather than encoding a
 * function address in the embedded rcu_head structure, kfree_rcu() instead
 * encodes the offset of the rcu_head structure within the base structure.
 * Because the functions are not allowed in the low-order 4096 bytes of
 * kernel virtual memory, offsets up to 4095 bytes can be accommodated.
 * If the offset is larger than 4095 bytes, a compile-time error will
 * be generated in __kfree_rcu().  If this error is triggered, you can
 * either fall back to use of call_rcu() or rearrange the structure to
 * position the rcu_head structure into the first 4096 bytes.
 *
 * Note that the allowable offset might decrease in the future, for example,
 * to allow something like kmem_cache_free_rcu().
 *
 * The BUILD_BUG_ON check must not involve any function calls, hence the
 * checks are done in macros here.
 */
static inline __attribute__((no_instrument_function)) bool Model1_rcu_is_nocb_cpu(int Model1_cpu) { return false; }



/* Only for use by adaptive-ticks code. */





static inline __attribute__((no_instrument_function)) bool Model1_rcu_sys_is_idle(void)
{
 return false;
}

static inline __attribute__((no_instrument_function)) void Model1_rcu_sysidle_force_exit(void)
{
}




/*
 * Dump the ftrace buffer, but only one time per callsite per boot.
 */
/*
 * workqueue.h --- work queue handling for Linux.
 */





struct Model1_tvec_base;

struct Model1_timer_list {
 /*
	 * All fields that change during normal runtime grouped to the
	 * same cacheline
	 */
 struct Model1_hlist_node Model1_entry;
 unsigned long Model1_expires;
 void (*Model1_function)(unsigned long);
 unsigned long Model1_data;
 Model1_u32 Model1_flags;


 int Model1_start_pid;
 void *Model1_start_site;
 char Model1_start_comm[16];




};
/*
 * A deferrable timer will work normally when the system is busy, but
 * will not cause a CPU to come out of idle just to service it; instead,
 * the timer will be serviced when the CPU eventually wakes up with a
 * subsequent non-deferrable timer.
 *
 * An irqsafe timer is executed with IRQ disabled and it's safe to wait for
 * the completion of the running instance from IRQ handlers, for example,
 * by calling del_timer_sync().
 *
 * Note: The irq disabled callback execution is a special case for
 * workqueue locking issues. It's not meant for executing random crap
 * with interrupts disabled. Abuse is monitored!
 */
void Model1_init_timer_key(struct Model1_timer_list *Model1_timer, unsigned int Model1_flags,
      const char *Model1_name, struct Model1_lock_class_key *Model1_key);







static inline __attribute__((no_instrument_function)) void Model1_destroy_timer_on_stack(struct Model1_timer_list *Model1_timer) { }
static inline __attribute__((no_instrument_function)) void Model1_init_timer_on_stack_key(struct Model1_timer_list *Model1_timer,
        unsigned int Model1_flags, const char *Model1_name,
        struct Model1_lock_class_key *Model1_key)
{
 Model1_init_timer_key(Model1_timer, Model1_flags, Model1_name, Model1_key);
}
/**
 * timer_pending - is a timer pending?
 * @timer: the timer in question
 *
 * timer_pending will tell whether a given timer is currently pending,
 * or not. Callers must ensure serialization wrt. other operations done
 * to this timer, eg. interrupt contexts, or other CPUs on SMP.
 *
 * return value: 1 if the timer is pending, 0 if not.
 */
static inline __attribute__((no_instrument_function)) int Model1_timer_pending(const struct Model1_timer_list * Model1_timer)
{
 return Model1_timer->Model1_entry.Model1_pprev != ((void *)0);
}

extern void Model1_add_timer_on(struct Model1_timer_list *Model1_timer, int Model1_cpu);
extern int Model1_del_timer(struct Model1_timer_list * Model1_timer);
extern int Model1_mod_timer(struct Model1_timer_list *Model1_timer, unsigned long Model1_expires);
extern int Model1_mod_timer_pending(struct Model1_timer_list *Model1_timer, unsigned long Model1_expires);

/*
 * The jiffies value which is added to now, when there is no timer
 * in the timer wheel:
 */


/*
 * Timer-statistics info:
 */


extern int Model1_timer_stats_active;

extern void Model1_init_timer_stats(void);

extern void Model1_timer_stats_update_stats(void *Model1_timer, Model1_pid_t Model1_pid, void *Model1_startf,
         void *Model1_timerf, char *Model1_comm, Model1_u32 Model1_flags);

extern void Model1___timer_stats_timer_set_start_info(struct Model1_timer_list *Model1_timer,
            void *Model1_addr);

static inline __attribute__((no_instrument_function)) void Model1_timer_stats_timer_set_start_info(struct Model1_timer_list *Model1_timer)
{
 if (__builtin_expect(!!(!Model1_timer_stats_active), 1))
  return;
 Model1___timer_stats_timer_set_start_info(Model1_timer, __builtin_return_address(0));
}

static inline __attribute__((no_instrument_function)) void Model1_timer_stats_timer_clear_start_info(struct Model1_timer_list *Model1_timer)
{
 Model1_timer->Model1_start_site = ((void *)0);
}
extern void Model1_add_timer(struct Model1_timer_list *Model1_timer);

extern int Model1_try_to_del_timer_sync(struct Model1_timer_list *Model1_timer);


  extern int Model1_del_timer_sync(struct Model1_timer_list *Model1_timer);






extern void Model1_init_timers(void);
extern void Model1_run_local_timers(void);
struct Model1_hrtimer;
extern enum Model1_hrtimer_restart Model1_it_real_fn(struct Model1_hrtimer *);



/*
 * sysctl.h: General linux system control interface
 *
 * Begun 24 March 1995, Stephen Tweedie
 *
 ****************************************************************
 ****************************************************************
 **
 **  WARNING:
 **  The values in this file are exported to user space via 
 **  the sysctl() binary interface.  Do *NOT* change the
 **  numbering of any existing values here, and do not change
 **  any numbers within any one set of values.  If you have to
 **  redefine an existing interface, use a new number for it.
 **  The kernel will then return -ENOTDIR to any application using
 **  the old binary interface.
 **
 ****************************************************************
 ****************************************************************
 */







/*
  Red Black Trees
  (C) 1999  Andrea Arcangeli <andrea@suse.de>
  
  This program is free software; you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation; either version 2 of the License, or
  (at your option) any later version.

  This program is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with this program; if not, write to the Free Software
  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA

  linux/include/linux/rbtree.h

  To use rbtrees you'll have to implement your own insert and search cores.
  This will avoid us to use callbacks and to drop drammatically performances.
  I know it's not the cleaner way,  but in C (not in C++) to get
  performances and genericity...

  See Documentation/rbtree.txt for documentation and samples.
*/
struct Model1_rb_node {
 unsigned long Model1___rb_parent_color;
 struct Model1_rb_node *Model1_rb_right;
 struct Model1_rb_node *Model1_rb_left;
} __attribute__((aligned(sizeof(long))));
    /* The alignment might seem pointless, but allegedly CRIS needs it */

struct Model1_rb_root {
 struct Model1_rb_node *Model1_rb_node;
};
/* 'empty' nodes are nodes that are known not to be inserted in an rbtree */






extern void Model1_rb_insert_color(struct Model1_rb_node *, struct Model1_rb_root *);
extern void Model1_rb_erase(struct Model1_rb_node *, struct Model1_rb_root *);


/* Find logical next and previous nodes in a tree */
extern struct Model1_rb_node *Model1_rb_next(const struct Model1_rb_node *);
extern struct Model1_rb_node *Model1_rb_prev(const struct Model1_rb_node *);
extern struct Model1_rb_node *Model1_rb_first(const struct Model1_rb_root *);
extern struct Model1_rb_node *Model1_rb_last(const struct Model1_rb_root *);

/* Postorder iteration - always visit the parent after its children */
extern struct Model1_rb_node *Model1_rb_first_postorder(const struct Model1_rb_root *);
extern struct Model1_rb_node *Model1_rb_next_postorder(const struct Model1_rb_node *);

/* Fast replacement of a single node without remove/rebalance/add/rebalance */
extern void Model1_rb_replace_node(struct Model1_rb_node *Model1_victim, struct Model1_rb_node *Model1_new,
       struct Model1_rb_root *Model1_root);
extern void Model1_rb_replace_node_rcu(struct Model1_rb_node *Model1_victim, struct Model1_rb_node *Model1_new,
    struct Model1_rb_root *Model1_root);

static inline __attribute__((no_instrument_function)) void Model1_rb_link_node(struct Model1_rb_node *Model1_node, struct Model1_rb_node *Model1_parent,
    struct Model1_rb_node **Model1_rb_link)
{
 Model1_node->Model1___rb_parent_color = (unsigned long)Model1_parent;
 Model1_node->Model1_rb_left = Model1_node->Model1_rb_right = ((void *)0);

 *Model1_rb_link = Model1_node;
}

static inline __attribute__((no_instrument_function)) void Model1_rb_link_node_rcu(struct Model1_rb_node *Model1_node, struct Model1_rb_node *Model1_parent,
        struct Model1_rb_node **Model1_rb_link)
{
 Model1_node->Model1___rb_parent_color = (unsigned long)Model1_parent;
 Model1_node->Model1_rb_left = Model1_node->Model1_rb_right = ((void *)0);

 ({ Model1_uintptr_t Model1__r_a_p__v = (Model1_uintptr_t)(Model1_node); if (__builtin_constant_p(Model1_node) && (Model1__r_a_p__v) == (Model1_uintptr_t)((void *)0)) ({ union { typeof((*Model1_rb_link)) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof((*Model1_rb_link))) ((typeof(*Model1_rb_link))(Model1__r_a_p__v)) }; Model1___write_once_size(&((*Model1_rb_link)), Model1___u.Model1___c, sizeof((*Model1_rb_link))); Model1___u.Model1___val; }); else do { do { bool Model1___cond = !((sizeof(*&*Model1_rb_link) == sizeof(char) || sizeof(*&*Model1_rb_link) == sizeof(short) || sizeof(*&*Model1_rb_link) == sizeof(int) || sizeof(*&*Model1_rb_link) == sizeof(long))); extern void Model1___compiletime_assert_97(void) ; if (Model1___cond) Model1___compiletime_assert_97(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&*Model1_rb_link) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(*&*Model1_rb_link)) ((typeof(*((typeof(*Model1_rb_link))Model1__r_a_p__v)) *)((typeof(*Model1_rb_link))Model1__r_a_p__v)) }; Model1___write_once_size(&(*&*Model1_rb_link), Model1___u.Model1___c, sizeof(*&*Model1_rb_link)); Model1___u.Model1___val; }); } while (0); Model1__r_a_p__v; });
}






/**
 * rbtree_postorder_for_each_entry_safe - iterate in post-order over rb_root of
 * given type allowing the backing memory of @pos to be invalidated
 *
 * @pos:	the 'type *' to use as a loop cursor.
 * @n:		another 'type *' to use as temporary storage
 * @root:	'rb_root *' of the rbtree.
 * @field:	the name of the rb_node field within 'type'.
 *
 * rbtree_postorder_for_each_entry_safe() provides a similar guarantee as
 * list_for_each_entry_safe() and allows the iteration to continue independent
 * of changes to @pos by the body of the loop.
 *
 * Note, however, that it cannot handle other modifications that re-order the
 * rbtree it is iterating over. This includes calling rb_erase() on @pos, as
 * rb_erase() may rebalance the tree, causing us to miss some nodes.
 */
/*
 * sysctl.h: General linux system control interface
 *
 * Begun 24 March 1995, Stephen Tweedie
 *
 ****************************************************************
 ****************************************************************
 **
 **  WARNING:
 **  The values in this file are exported to user space via 
 **  the sysctl() binary interface.  Do *NOT* change the
 **  numbering of any existing values here, and do not change
 **  any numbers within any one set of values.  If you have to
 **  redefine an existing interface, use a new number for it.
 **  The kernel will then return -ENOTDIR to any application using
 **  the old binary interface.
 **
 ****************************************************************
 ****************************************************************
 */
struct Model1___sysctl_args {
 int *Model1_name;
 int Model1_nlen;
 void *Model1_oldval;
 Model1_size_t *Model1_oldlenp;
 void *Model1_newval;
 Model1_size_t Model1_newlen;
 unsigned long Model1___unused[4];
};

/* Define sysctl names first */

/* Top-level names: */

enum
{
 Model1_CTL_KERN=1, /* General kernel info and control */
 Model1_CTL_VM=2, /* VM management */
 Model1_CTL_NET=3, /* Networking */
 Model1_CTL_PROC=4, /* removal breaks strace(1) compilation */
 Model1_CTL_FS=5, /* Filesystems */
 Model1_CTL_DEBUG=6, /* Debugging */
 Model1_CTL_DEV=7, /* Devices */
 Model1_CTL_BUS=8, /* Busses */
 Model1_CTL_ABI=9, /* Binary emulation */
 Model1_CTL_CPU=10, /* CPU stuff (speed scaling, etc) */
 Model1_CTL_ARLAN=254, /* arlan wireless driver */
 Model1_CTL_S390DBF=5677, /* s390 debug */
 Model1_CTL_SUNRPC=7249, /* sunrpc debug */
 Model1_CTL_PM=9899, /* frv power management */
 Model1_CTL_FRV=9898, /* frv specific sysctls */
};

/* CTL_BUS names: */
enum
{
 Model1_CTL_BUS_ISA=1 /* ISA */
};

/* /proc/sys/fs/inotify/ */
enum
{
 Model1_INOTIFY_MAX_USER_INSTANCES=1, /* max instances per user */
 Model1_INOTIFY_MAX_USER_WATCHES=2, /* max watches per user */
 Model1_INOTIFY_MAX_QUEUED_EVENTS=3 /* max queued events per instance */
};

/* CTL_KERN names: */
enum
{
 Model1_KERN_OSTYPE=1, /* string: system version */
 Model1_KERN_OSRELEASE=2, /* string: system release */
 Model1_KERN_OSREV=3, /* int: system revision */
 Model1_KERN_VERSION=4, /* string: compile time info */
 Model1_KERN_SECUREMASK=5, /* struct: maximum rights mask */
 Model1_KERN_PROF=6, /* table: profiling information */
 Model1_KERN_NODENAME=7, /* string: hostname */
 Model1_KERN_DOMAINNAME=8, /* string: domainname */

 Model1_KERN_PANIC=15, /* int: panic timeout */
 Model1_KERN_REALROOTDEV=16, /* real root device to mount after initrd */

 Model1_KERN_SPARC_REBOOT=21, /* reboot command on Sparc */
 Model1_KERN_CTLALTDEL=22, /* int: allow ctl-alt-del to reboot */
 Model1_KERN_PRINTK=23, /* struct: control printk logging parameters */
 Model1_KERN_NAMETRANS=24, /* Name translation */
 Model1_KERN_PPC_HTABRECLAIM=25, /* turn htab reclaimation on/off on PPC */
 Model1_KERN_PPC_ZEROPAGED=26, /* turn idle page zeroing on/off on PPC */
 Model1_KERN_PPC_POWERSAVE_NAP=27, /* use nap mode for power saving */
 Model1_KERN_MODPROBE=28, /* string: modprobe path */
 Model1_KERN_SG_BIG_BUFF=29, /* int: sg driver reserved buffer size */
 Model1_KERN_ACCT=30, /* BSD process accounting parameters */
 Model1_KERN_PPC_L2CR=31, /* l2cr register on PPC */

 Model1_KERN_RTSIGNR=32, /* Number of rt sigs queued */
 Model1_KERN_RTSIGMAX=33, /* Max queuable */

 Model1_KERN_SHMMAX=34, /* long: Maximum shared memory segment */
 Model1_KERN_MSGMAX=35, /* int: Maximum size of a messege */
 Model1_KERN_MSGMNB=36, /* int: Maximum message queue size */
 Model1_KERN_MSGPOOL=37, /* int: Maximum system message pool size */
 Model1_KERN_SYSRQ=38, /* int: Sysreq enable */
 Model1_KERN_MAX_THREADS=39, /* int: Maximum nr of threads in the system */
  Model1_KERN_RANDOM=40, /* Random driver */
  Model1_KERN_SHMALL=41, /* int: Maximum size of shared memory */
  Model1_KERN_MSGMNI=42, /* int: msg queue identifiers */
  Model1_KERN_SEM=43, /* struct: sysv semaphore limits */
  Model1_KERN_SPARC_STOP_A=44, /* int: Sparc Stop-A enable */
  Model1_KERN_SHMMNI=45, /* int: shm array identifiers */
 Model1_KERN_OVERFLOWUID=46, /* int: overflow UID */
 Model1_KERN_OVERFLOWGID=47, /* int: overflow GID */
 Model1_KERN_SHMPATH=48, /* string: path to shm fs */
 Model1_KERN_HOTPLUG=49, /* string: path to uevent helper (deprecated) */
 Model1_KERN_IEEE_EMULATION_WARNINGS=50, /* int: unimplemented ieee instructions */
 Model1_KERN_S390_USER_DEBUG_LOGGING=51, /* int: dumps of user faults */
 Model1_KERN_CORE_USES_PID=52, /* int: use core or core.%pid */
 Model1_KERN_TAINTED=53, /* int: various kernel tainted flags */
 Model1_KERN_CADPID=54, /* int: PID of the process to notify on CAD */
 Model1_KERN_PIDMAX=55, /* int: PID # limit */
   Model1_KERN_CORE_PATTERN=56, /* string: pattern for core-file names */
 Model1_KERN_PANIC_ON_OOPS=57, /* int: whether we will panic on an oops */
 Model1_KERN_HPPA_PWRSW=58, /* int: hppa soft-power enable */
 Model1_KERN_HPPA_UNALIGNED=59, /* int: hppa unaligned-trap enable */
 Model1_KERN_PRINTK_RATELIMIT=60, /* int: tune printk ratelimiting */
 Model1_KERN_PRINTK_RATELIMIT_BURST=61, /* int: tune printk ratelimiting */
 Model1_KERN_PTY=62, /* dir: pty driver */
 Model1_KERN_NGROUPS_MAX=63, /* int: NGROUPS_MAX */
 Model1_KERN_SPARC_SCONS_PWROFF=64, /* int: serial console power-off halt */
 Model1_KERN_HZ_TIMER=65, /* int: hz timer on or off */
 Model1_KERN_UNKNOWN_NMI_PANIC=66, /* int: unknown nmi panic flag */
 Model1_KERN_BOOTLOADER_TYPE=67, /* int: boot loader type */
 Model1_KERN_RANDOMIZE=68, /* int: randomize virtual address space */
 Model1_KERN_SETUID_DUMPABLE=69, /* int: behaviour of dumps for setuid core */
 Model1_KERN_SPIN_RETRY=70, /* int: number of spinlock retries */
 Model1_KERN_ACPI_VIDEO_FLAGS=71, /* int: flags for setting up video after ACPI sleep */
 Model1_KERN_IA64_UNALIGNED=72, /* int: ia64 unaligned userland trap enable */
 Model1_KERN_COMPAT_LOG=73, /* int: print compat layer  messages */
 Model1_KERN_MAX_LOCK_DEPTH=74, /* int: rtmutex's maximum lock depth */
 Model1_KERN_NMI_WATCHDOG=75, /* int: enable/disable nmi watchdog */
 Model1_KERN_PANIC_ON_NMI=76, /* int: whether we will panic on an unrecovered */
 Model1_KERN_PANIC_ON_WARN=77, /* int: call panic() in WARN() functions */
};



/* CTL_VM names: */
enum
{
 Model1_VM_UNUSED1=1, /* was: struct: Set vm swapping control */
 Model1_VM_UNUSED2=2, /* was; int: Linear or sqrt() swapout for hogs */
 Model1_VM_UNUSED3=3, /* was: struct: Set free page thresholds */
 Model1_VM_UNUSED4=4, /* Spare */
 Model1_VM_OVERCOMMIT_MEMORY=5, /* Turn off the virtual memory safety limit */
 Model1_VM_UNUSED5=6, /* was: struct: Set buffer memory thresholds */
 Model1_VM_UNUSED7=7, /* was: struct: Set cache memory thresholds */
 Model1_VM_UNUSED8=8, /* was: struct: Control kswapd behaviour */
 Model1_VM_UNUSED9=9, /* was: struct: Set page table cache parameters */
 Model1_VM_PAGE_CLUSTER=10, /* int: set number of pages to swap together */
 Model1_VM_DIRTY_BACKGROUND=11, /* dirty_background_ratio */
 Model1_VM_DIRTY_RATIO=12, /* dirty_ratio */
 Model1_VM_DIRTY_WB_CS=13, /* dirty_writeback_centisecs */
 Model1_VM_DIRTY_EXPIRE_CS=14, /* dirty_expire_centisecs */
 Model1_VM_NR_PDFLUSH_THREADS=15, /* nr_pdflush_threads */
 Model1_VM_OVERCOMMIT_RATIO=16, /* percent of RAM to allow overcommit in */
 Model1_VM_PAGEBUF=17, /* struct: Control pagebuf parameters */
 Model1_VM_HUGETLB_PAGES=18, /* int: Number of available Huge Pages */
 Model1_VM_SWAPPINESS=19, /* Tendency to steal mapped memory */
 Model1_VM_LOWMEM_RESERVE_RATIO=20,/* reservation ratio for lower memory zones */
 Model1_VM_MIN_FREE_KBYTES=21, /* Minimum free kilobytes to maintain */
 Model1_VM_MAX_MAP_COUNT=22, /* int: Maximum number of mmaps/address-space */
 Model1_VM_LAPTOP_MODE=23, /* vm laptop mode */
 Model1_VM_BLOCK_DUMP=24, /* block dump mode */
 Model1_VM_HUGETLB_GROUP=25, /* permitted hugetlb group */
 Model1_VM_VFS_CACHE_PRESSURE=26, /* dcache/icache reclaim pressure */
 Model1_VM_LEGACY_VA_LAYOUT=27, /* legacy/compatibility virtual address space layout */
 Model1_VM_SWAP_TOKEN_TIMEOUT=28, /* default time for token time out */
 Model1_VM_DROP_PAGECACHE=29, /* int: nuke lots of pagecache */
 Model1_VM_PERCPU_PAGELIST_FRACTION=30,/* int: fraction of pages in each percpu_pagelist */
 Model1_VM_ZONE_RECLAIM_MODE=31, /* reclaim local zone memory before going off node */
 Model1_VM_MIN_UNMAPPED=32, /* Set min percent of unmapped pages */
 Model1_VM_PANIC_ON_OOM=33, /* panic at out-of-memory */
 Model1_VM_VDSO_ENABLED=34, /* map VDSO into new processes? */
 Model1_VM_MIN_SLAB=35, /* Percent pages ignored by zone reclaim */
};


/* CTL_NET names: */
enum
{
 Model1_NET_CORE=1,
 Model1_NET_ETHER=2,
 Model1_NET_802=3,
 Model1_NET_UNIX=4,
 Model1_NET_IPV4=5,
 Model1_NET_IPX=6,
 Model1_NET_ATALK=7,
 Model1_NET_NETROM=8,
 Model1_NET_AX25=9,
 Model1_NET_BRIDGE=10,
 Model1_NET_ROSE=11,
 Model1_NET_IPV6=12,
 Model1_NET_X25=13,
 Model1_NET_TR=14,
 Model1_NET_DECNET=15,
 Model1_NET_ECONET=16,
 Model1_NET_SCTP=17,
 Model1_NET_LLC=18,
 Model1_NET_NETFILTER=19,
 Model1_NET_DCCP=20,
 Model1_NET_IRDA=412,
};

/* /proc/sys/kernel/random */
enum
{
 Model1_RANDOM_POOLSIZE=1,
 Model1_RANDOM_ENTROPY_COUNT=2,
 Model1_RANDOM_READ_THRESH=3,
 Model1_RANDOM_WRITE_THRESH=4,
 Model1_RANDOM_BOOT_ID=5,
 Model1_RANDOM_UUID=6
};

/* /proc/sys/kernel/pty */
enum
{
 Model1_PTY_MAX=1,
 Model1_PTY_NR=2
};

/* /proc/sys/bus/isa */
enum
{
 Model1_BUS_ISA_MEM_BASE=1,
 Model1_BUS_ISA_PORT_BASE=2,
 Model1_BUS_ISA_PORT_SHIFT=3
};

/* /proc/sys/net/core */
enum
{
 Model1_NET_CORE_WMEM_MAX=1,
 Model1_NET_CORE_RMEM_MAX=2,
 Model1_NET_CORE_WMEM_DEFAULT=3,
 Model1_NET_CORE_RMEM_DEFAULT=4,
/* was	NET_CORE_DESTROY_DELAY */
 Model1_NET_CORE_MAX_BACKLOG=6,
 Model1_NET_CORE_FASTROUTE=7,
 Model1_NET_CORE_MSG_COST=8,
 Model1_NET_CORE_MSG_BURST=9,
 Model1_NET_CORE_OPTMEM_MAX=10,
 Model1_NET_CORE_HOT_LIST_LENGTH=11,
 Model1_NET_CORE_DIVERT_VERSION=12,
 Model1_NET_CORE_NO_CONG_THRESH=13,
 Model1_NET_CORE_NO_CONG=14,
 Model1_NET_CORE_LO_CONG=15,
 Model1_NET_CORE_MOD_CONG=16,
 Model1_NET_CORE_DEV_WEIGHT=17,
 Model1_NET_CORE_SOMAXCONN=18,
 Model1_NET_CORE_BUDGET=19,
 Model1_NET_CORE_AEVENT_ETIME=20,
 Model1_NET_CORE_AEVENT_RSEQTH=21,
 Model1_NET_CORE_WARNINGS=22,
};

/* /proc/sys/net/ethernet */

/* /proc/sys/net/802 */

/* /proc/sys/net/unix */

enum
{
 Model1_NET_UNIX_DESTROY_DELAY=1,
 Model1_NET_UNIX_DELETE_DELAY=2,
 Model1_NET_UNIX_MAX_DGRAM_QLEN=3,
};

/* /proc/sys/net/netfilter */
enum
{
 Model1_NET_NF_CONNTRACK_MAX=1,
 Model1_NET_NF_CONNTRACK_TCP_TIMEOUT_SYN_SENT=2,
 Model1_NET_NF_CONNTRACK_TCP_TIMEOUT_SYN_RECV=3,
 Model1_NET_NF_CONNTRACK_TCP_TIMEOUT_ESTABLISHED=4,
 Model1_NET_NF_CONNTRACK_TCP_TIMEOUT_FIN_WAIT=5,
 Model1_NET_NF_CONNTRACK_TCP_TIMEOUT_CLOSE_WAIT=6,
 Model1_NET_NF_CONNTRACK_TCP_TIMEOUT_LAST_ACK=7,
 Model1_NET_NF_CONNTRACK_TCP_TIMEOUT_TIME_WAIT=8,
 Model1_NET_NF_CONNTRACK_TCP_TIMEOUT_CLOSE=9,
 Model1_NET_NF_CONNTRACK_UDP_TIMEOUT=10,
 Model1_NET_NF_CONNTRACK_UDP_TIMEOUT_STREAM=11,
 Model1_NET_NF_CONNTRACK_ICMP_TIMEOUT=12,
 Model1_NET_NF_CONNTRACK_GENERIC_TIMEOUT=13,
 Model1_NET_NF_CONNTRACK_BUCKETS=14,
 Model1_NET_NF_CONNTRACK_LOG_INVALID=15,
 Model1_NET_NF_CONNTRACK_TCP_TIMEOUT_MAX_RETRANS=16,
 Model1_NET_NF_CONNTRACK_TCP_LOOSE=17,
 Model1_NET_NF_CONNTRACK_TCP_BE_LIBERAL=18,
 Model1_NET_NF_CONNTRACK_TCP_MAX_RETRANS=19,
 Model1_NET_NF_CONNTRACK_SCTP_TIMEOUT_CLOSED=20,
 Model1_NET_NF_CONNTRACK_SCTP_TIMEOUT_COOKIE_WAIT=21,
 Model1_NET_NF_CONNTRACK_SCTP_TIMEOUT_COOKIE_ECHOED=22,
 Model1_NET_NF_CONNTRACK_SCTP_TIMEOUT_ESTABLISHED=23,
 Model1_NET_NF_CONNTRACK_SCTP_TIMEOUT_SHUTDOWN_SENT=24,
 Model1_NET_NF_CONNTRACK_SCTP_TIMEOUT_SHUTDOWN_RECD=25,
 Model1_NET_NF_CONNTRACK_SCTP_TIMEOUT_SHUTDOWN_ACK_SENT=26,
 Model1_NET_NF_CONNTRACK_COUNT=27,
 Model1_NET_NF_CONNTRACK_ICMPV6_TIMEOUT=28,
 Model1_NET_NF_CONNTRACK_FRAG6_TIMEOUT=29,
 Model1_NET_NF_CONNTRACK_FRAG6_LOW_THRESH=30,
 Model1_NET_NF_CONNTRACK_FRAG6_HIGH_THRESH=31,
 Model1_NET_NF_CONNTRACK_CHECKSUM=32,
};

/* /proc/sys/net/ipv4 */
enum
{
 /* v2.0 compatibile variables */
 Model1_NET_IPV4_FORWARD=8,
 Model1_NET_IPV4_DYNADDR=9,

 Model1_NET_IPV4_CONF=16,
 Model1_NET_IPV4_NEIGH=17,
 Model1_NET_IPV4_ROUTE=18,
 Model1_NET_IPV4_FIB_HASH=19,
 Model1_NET_IPV4_NETFILTER=20,

 Model1_NET_IPV4_TCP_TIMESTAMPS=33,
 Model1_NET_IPV4_TCP_WINDOW_SCALING=34,
 Model1_NET_IPV4_TCP_SACK=35,
 Model1_NET_IPV4_TCP_RETRANS_COLLAPSE=36,
 Model1_NET_IPV4_DEFAULT_TTL=37,
 Model1_NET_IPV4_AUTOCONFIG=38,
 Model1_NET_IPV4_NO_PMTU_DISC=39,
 Model1_NET_IPV4_TCP_SYN_RETRIES=40,
 Model1_NET_IPV4_IPFRAG_HIGH_THRESH=41,
 Model1_NET_IPV4_IPFRAG_LOW_THRESH=42,
 Model1_NET_IPV4_IPFRAG_TIME=43,
 Model1_NET_IPV4_TCP_MAX_KA_PROBES=44,
 Model1_NET_IPV4_TCP_KEEPALIVE_TIME=45,
 Model1_NET_IPV4_TCP_KEEPALIVE_PROBES=46,
 Model1_NET_IPV4_TCP_RETRIES1=47,
 Model1_NET_IPV4_TCP_RETRIES2=48,
 Model1_NET_IPV4_TCP_FIN_TIMEOUT=49,
 Model1_NET_IPV4_IP_MASQ_DEBUG=50,
 Model1_NET_TCP_SYNCOOKIES=51,
 Model1_NET_TCP_STDURG=52,
 Model1_NET_TCP_RFC1337=53,
 Model1_NET_TCP_SYN_TAILDROP=54,
 Model1_NET_TCP_MAX_SYN_BACKLOG=55,
 Model1_NET_IPV4_LOCAL_PORT_RANGE=56,
 Model1_NET_IPV4_ICMP_ECHO_IGNORE_ALL=57,
 Model1_NET_IPV4_ICMP_ECHO_IGNORE_BROADCASTS=58,
 Model1_NET_IPV4_ICMP_SOURCEQUENCH_RATE=59,
 Model1_NET_IPV4_ICMP_DESTUNREACH_RATE=60,
 Model1_NET_IPV4_ICMP_TIMEEXCEED_RATE=61,
 Model1_NET_IPV4_ICMP_PARAMPROB_RATE=62,
 Model1_NET_IPV4_ICMP_ECHOREPLY_RATE=63,
 Model1_NET_IPV4_ICMP_IGNORE_BOGUS_ERROR_RESPONSES=64,
 Model1_NET_IPV4_IGMP_MAX_MEMBERSHIPS=65,
 Model1_NET_TCP_TW_RECYCLE=66,
 Model1_NET_IPV4_ALWAYS_DEFRAG=67,
 Model1_NET_IPV4_TCP_KEEPALIVE_INTVL=68,
 Model1_NET_IPV4_INET_PEER_THRESHOLD=69,
 Model1_NET_IPV4_INET_PEER_MINTTL=70,
 Model1_NET_IPV4_INET_PEER_MAXTTL=71,
 Model1_NET_IPV4_INET_PEER_GC_MINTIME=72,
 Model1_NET_IPV4_INET_PEER_GC_MAXTIME=73,
 Model1_NET_TCP_ORPHAN_RETRIES=74,
 Model1_NET_TCP_ABORT_ON_OVERFLOW=75,
 Model1_NET_TCP_SYNACK_RETRIES=76,
 Model1_NET_TCP_MAX_ORPHANS=77,
 Model1_NET_TCP_MAX_TW_BUCKETS=78,
 Model1_NET_TCP_FACK=79,
 Model1_NET_TCP_REORDERING=80,
 Model1_NET_TCP_ECN=81,
 Model1_NET_TCP_DSACK=82,
 Model1_NET_TCP_MEM=83,
 Model1_NET_TCP_WMEM=84,
 Model1_NET_TCP_RMEM=85,
 Model1_NET_TCP_APP_WIN=86,
 Model1_NET_TCP_ADV_WIN_SCALE=87,
 Model1_NET_IPV4_NONLOCAL_BIND=88,
 Model1_NET_IPV4_ICMP_RATELIMIT=89,
 Model1_NET_IPV4_ICMP_RATEMASK=90,
 Model1_NET_TCP_TW_REUSE=91,
 Model1_NET_TCP_FRTO=92,
 Model1_NET_TCP_LOW_LATENCY=93,
 Model1_NET_IPV4_IPFRAG_SECRET_INTERVAL=94,
 Model1_NET_IPV4_IGMP_MAX_MSF=96,
 Model1_NET_TCP_NO_METRICS_SAVE=97,
 Model1_NET_TCP_DEFAULT_WIN_SCALE=105,
 Model1_NET_TCP_MODERATE_RCVBUF=106,
 Model1_NET_TCP_TSO_WIN_DIVISOR=107,
 Model1_NET_TCP_BIC_BETA=108,
 Model1_NET_IPV4_ICMP_ERRORS_USE_INBOUND_IFADDR=109,
 Model1_NET_TCP_CONG_CONTROL=110,
 Model1_NET_TCP_ABC=111,
 Model1_NET_IPV4_IPFRAG_MAX_DIST=112,
  Model1_NET_TCP_MTU_PROBING=113,
 Model1_NET_TCP_BASE_MSS=114,
 Model1_NET_IPV4_TCP_WORKAROUND_SIGNED_WINDOWS=115,
 Model1_NET_TCP_DMA_COPYBREAK=116,
 Model1_NET_TCP_SLOW_START_AFTER_IDLE=117,
 Model1_NET_CIPSOV4_CACHE_ENABLE=118,
 Model1_NET_CIPSOV4_CACHE_BUCKET_SIZE=119,
 Model1_NET_CIPSOV4_RBM_OPTFMT=120,
 Model1_NET_CIPSOV4_RBM_STRICTVALID=121,
 Model1_NET_TCP_AVAIL_CONG_CONTROL=122,
 Model1_NET_TCP_ALLOWED_CONG_CONTROL=123,
 Model1_NET_TCP_MAX_SSTHRESH=124,
 Model1_NET_TCP_FRTO_RESPONSE=125,
};

enum {
 Model1_NET_IPV4_ROUTE_FLUSH=1,
 Model1_NET_IPV4_ROUTE_MIN_DELAY=2, /* obsolete since 2.6.25 */
 Model1_NET_IPV4_ROUTE_MAX_DELAY=3, /* obsolete since 2.6.25 */
 Model1_NET_IPV4_ROUTE_GC_THRESH=4,
 Model1_NET_IPV4_ROUTE_MAX_SIZE=5,
 Model1_NET_IPV4_ROUTE_GC_MIN_INTERVAL=6,
 Model1_NET_IPV4_ROUTE_GC_TIMEOUT=7,
 Model1_NET_IPV4_ROUTE_GC_INTERVAL=8, /* obsolete since 2.6.38 */
 Model1_NET_IPV4_ROUTE_REDIRECT_LOAD=9,
 Model1_NET_IPV4_ROUTE_REDIRECT_NUMBER=10,
 Model1_NET_IPV4_ROUTE_REDIRECT_SILENCE=11,
 Model1_NET_IPV4_ROUTE_ERROR_COST=12,
 Model1_NET_IPV4_ROUTE_ERROR_BURST=13,
 Model1_NET_IPV4_ROUTE_GC_ELASTICITY=14,
 Model1_NET_IPV4_ROUTE_MTU_EXPIRES=15,
 Model1_NET_IPV4_ROUTE_MIN_PMTU=16,
 Model1_NET_IPV4_ROUTE_MIN_ADVMSS=17,
 Model1_NET_IPV4_ROUTE_SECRET_INTERVAL=18,
 Model1_NET_IPV4_ROUTE_GC_MIN_INTERVAL_MS=19,
};

enum
{
 Model1_NET_PROTO_CONF_ALL=-2,
 Model1_NET_PROTO_CONF_DEFAULT=-3

 /* And device ifindices ... */
};

enum
{
 Model1_NET_IPV4_CONF_FORWARDING=1,
 Model1_NET_IPV4_CONF_MC_FORWARDING=2,
 Model1_NET_IPV4_CONF_PROXY_ARP=3,
 Model1_NET_IPV4_CONF_ACCEPT_REDIRECTS=4,
 Model1_NET_IPV4_CONF_SECURE_REDIRECTS=5,
 Model1_NET_IPV4_CONF_SEND_REDIRECTS=6,
 Model1_NET_IPV4_CONF_SHARED_MEDIA=7,
 Model1_NET_IPV4_CONF_RP_FILTER=8,
 Model1_NET_IPV4_CONF_ACCEPT_SOURCE_ROUTE=9,
 Model1_NET_IPV4_CONF_BOOTP_RELAY=10,
 Model1_NET_IPV4_CONF_LOG_MARTIANS=11,
 Model1_NET_IPV4_CONF_TAG=12,
 Model1_NET_IPV4_CONF_ARPFILTER=13,
 Model1_NET_IPV4_CONF_MEDIUM_ID=14,
 Model1_NET_IPV4_CONF_NOXFRM=15,
 Model1_NET_IPV4_CONF_NOPOLICY=16,
 Model1_NET_IPV4_CONF_FORCE_IGMP_VERSION=17,
 Model1_NET_IPV4_CONF_ARP_ANNOUNCE=18,
 Model1_NET_IPV4_CONF_ARP_IGNORE=19,
 Model1_NET_IPV4_CONF_PROMOTE_SECONDARIES=20,
 Model1_NET_IPV4_CONF_ARP_ACCEPT=21,
 Model1_NET_IPV4_CONF_ARP_NOTIFY=22,
};

/* /proc/sys/net/ipv4/netfilter */
enum
{
 Model1_NET_IPV4_NF_CONNTRACK_MAX=1,
 Model1_NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_SYN_SENT=2,
 Model1_NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_SYN_RECV=3,
 Model1_NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_ESTABLISHED=4,
 Model1_NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_FIN_WAIT=5,
 Model1_NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_CLOSE_WAIT=6,
 Model1_NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_LAST_ACK=7,
 Model1_NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_TIME_WAIT=8,
 Model1_NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_CLOSE=9,
 Model1_NET_IPV4_NF_CONNTRACK_UDP_TIMEOUT=10,
 Model1_NET_IPV4_NF_CONNTRACK_UDP_TIMEOUT_STREAM=11,
 Model1_NET_IPV4_NF_CONNTRACK_ICMP_TIMEOUT=12,
 Model1_NET_IPV4_NF_CONNTRACK_GENERIC_TIMEOUT=13,
 Model1_NET_IPV4_NF_CONNTRACK_BUCKETS=14,
 Model1_NET_IPV4_NF_CONNTRACK_LOG_INVALID=15,
 Model1_NET_IPV4_NF_CONNTRACK_TCP_TIMEOUT_MAX_RETRANS=16,
 Model1_NET_IPV4_NF_CONNTRACK_TCP_LOOSE=17,
 Model1_NET_IPV4_NF_CONNTRACK_TCP_BE_LIBERAL=18,
 Model1_NET_IPV4_NF_CONNTRACK_TCP_MAX_RETRANS=19,
  Model1_NET_IPV4_NF_CONNTRACK_SCTP_TIMEOUT_CLOSED=20,
  Model1_NET_IPV4_NF_CONNTRACK_SCTP_TIMEOUT_COOKIE_WAIT=21,
  Model1_NET_IPV4_NF_CONNTRACK_SCTP_TIMEOUT_COOKIE_ECHOED=22,
  Model1_NET_IPV4_NF_CONNTRACK_SCTP_TIMEOUT_ESTABLISHED=23,
  Model1_NET_IPV4_NF_CONNTRACK_SCTP_TIMEOUT_SHUTDOWN_SENT=24,
  Model1_NET_IPV4_NF_CONNTRACK_SCTP_TIMEOUT_SHUTDOWN_RECD=25,
  Model1_NET_IPV4_NF_CONNTRACK_SCTP_TIMEOUT_SHUTDOWN_ACK_SENT=26,
 Model1_NET_IPV4_NF_CONNTRACK_COUNT=27,
 Model1_NET_IPV4_NF_CONNTRACK_CHECKSUM=28,
};

/* /proc/sys/net/ipv6 */
enum {
 Model1_NET_IPV6_CONF=16,
 Model1_NET_IPV6_NEIGH=17,
 Model1_NET_IPV6_ROUTE=18,
 Model1_NET_IPV6_ICMP=19,
 Model1_NET_IPV6_BINDV6ONLY=20,
 Model1_NET_IPV6_IP6FRAG_HIGH_THRESH=21,
 Model1_NET_IPV6_IP6FRAG_LOW_THRESH=22,
 Model1_NET_IPV6_IP6FRAG_TIME=23,
 Model1_NET_IPV6_IP6FRAG_SECRET_INTERVAL=24,
 Model1_NET_IPV6_MLD_MAX_MSF=25,
};

enum {
 Model1_NET_IPV6_ROUTE_FLUSH=1,
 Model1_NET_IPV6_ROUTE_GC_THRESH=2,
 Model1_NET_IPV6_ROUTE_MAX_SIZE=3,
 Model1_NET_IPV6_ROUTE_GC_MIN_INTERVAL=4,
 Model1_NET_IPV6_ROUTE_GC_TIMEOUT=5,
 Model1_NET_IPV6_ROUTE_GC_INTERVAL=6,
 Model1_NET_IPV6_ROUTE_GC_ELASTICITY=7,
 Model1_NET_IPV6_ROUTE_MTU_EXPIRES=8,
 Model1_NET_IPV6_ROUTE_MIN_ADVMSS=9,
 Model1_NET_IPV6_ROUTE_GC_MIN_INTERVAL_MS=10
};

enum {
 Model1_NET_IPV6_FORWARDING=1,
 Model1_NET_IPV6_HOP_LIMIT=2,
 Model1_NET_IPV6_MTU=3,
 Model1_NET_IPV6_ACCEPT_RA=4,
 Model1_NET_IPV6_ACCEPT_REDIRECTS=5,
 Model1_NET_IPV6_AUTOCONF=6,
 Model1_NET_IPV6_DAD_TRANSMITS=7,
 Model1_NET_IPV6_RTR_SOLICITS=8,
 Model1_NET_IPV6_RTR_SOLICIT_INTERVAL=9,
 Model1_NET_IPV6_RTR_SOLICIT_DELAY=10,
 Model1_NET_IPV6_USE_TEMPADDR=11,
 Model1_NET_IPV6_TEMP_VALID_LFT=12,
 Model1_NET_IPV6_TEMP_PREFERED_LFT=13,
 Model1_NET_IPV6_REGEN_MAX_RETRY=14,
 Model1_NET_IPV6_MAX_DESYNC_FACTOR=15,
 Model1_NET_IPV6_MAX_ADDRESSES=16,
 Model1_NET_IPV6_FORCE_MLD_VERSION=17,
 Model1_NET_IPV6_ACCEPT_RA_DEFRTR=18,
 Model1_NET_IPV6_ACCEPT_RA_PINFO=19,
 Model1_NET_IPV6_ACCEPT_RA_RTR_PREF=20,
 Model1_NET_IPV6_RTR_PROBE_INTERVAL=21,
 Model1_NET_IPV6_ACCEPT_RA_RT_INFO_MAX_PLEN=22,
 Model1_NET_IPV6_PROXY_NDP=23,
 Model1_NET_IPV6_ACCEPT_SOURCE_ROUTE=25,
 Model1_NET_IPV6_ACCEPT_RA_FROM_LOCAL=26,
 Model1___NET_IPV6_MAX
};

/* /proc/sys/net/ipv6/icmp */
enum {
 Model1_NET_IPV6_ICMP_RATELIMIT=1
};

/* /proc/sys/net/<protocol>/neigh/<dev> */
enum {
 Model1_NET_NEIGH_MCAST_SOLICIT=1,
 Model1_NET_NEIGH_UCAST_SOLICIT=2,
 Model1_NET_NEIGH_APP_SOLICIT=3,
 Model1_NET_NEIGH_RETRANS_TIME=4,
 Model1_NET_NEIGH_REACHABLE_TIME=5,
 Model1_NET_NEIGH_DELAY_PROBE_TIME=6,
 Model1_NET_NEIGH_GC_STALE_TIME=7,
 Model1_NET_NEIGH_UNRES_QLEN=8,
 Model1_NET_NEIGH_PROXY_QLEN=9,
 Model1_NET_NEIGH_ANYCAST_DELAY=10,
 Model1_NET_NEIGH_PROXY_DELAY=11,
 Model1_NET_NEIGH_LOCKTIME=12,
 Model1_NET_NEIGH_GC_INTERVAL=13,
 Model1_NET_NEIGH_GC_THRESH1=14,
 Model1_NET_NEIGH_GC_THRESH2=15,
 Model1_NET_NEIGH_GC_THRESH3=16,
 Model1_NET_NEIGH_RETRANS_TIME_MS=17,
 Model1_NET_NEIGH_REACHABLE_TIME_MS=18,
};

/* /proc/sys/net/dccp */
enum {
 Model1_NET_DCCP_DEFAULT=1,
};

/* /proc/sys/net/ipx */
enum {
 Model1_NET_IPX_PPROP_BROADCASTING=1,
 Model1_NET_IPX_FORWARDING=2
};

/* /proc/sys/net/llc */
enum {
 Model1_NET_LLC2=1,
 Model1_NET_LLC_STATION=2,
};

/* /proc/sys/net/llc/llc2 */
enum {
 Model1_NET_LLC2_TIMEOUT=1,
};

/* /proc/sys/net/llc/station */
enum {
 Model1_NET_LLC_STATION_ACK_TIMEOUT=1,
};

/* /proc/sys/net/llc/llc2/timeout */
enum {
 Model1_NET_LLC2_ACK_TIMEOUT=1,
 Model1_NET_LLC2_P_TIMEOUT=2,
 Model1_NET_LLC2_REJ_TIMEOUT=3,
 Model1_NET_LLC2_BUSY_TIMEOUT=4,
};

/* /proc/sys/net/appletalk */
enum {
 Model1_NET_ATALK_AARP_EXPIRY_TIME=1,
 Model1_NET_ATALK_AARP_TICK_TIME=2,
 Model1_NET_ATALK_AARP_RETRANSMIT_LIMIT=3,
 Model1_NET_ATALK_AARP_RESOLVE_TIME=4
};


/* /proc/sys/net/netrom */
enum {
 Model1_NET_NETROM_DEFAULT_PATH_QUALITY=1,
 Model1_NET_NETROM_OBSOLESCENCE_COUNT_INITIALISER=2,
 Model1_NET_NETROM_NETWORK_TTL_INITIALISER=3,
 Model1_NET_NETROM_TRANSPORT_TIMEOUT=4,
 Model1_NET_NETROM_TRANSPORT_MAXIMUM_TRIES=5,
 Model1_NET_NETROM_TRANSPORT_ACKNOWLEDGE_DELAY=6,
 Model1_NET_NETROM_TRANSPORT_BUSY_DELAY=7,
 Model1_NET_NETROM_TRANSPORT_REQUESTED_WINDOW_SIZE=8,
 Model1_NET_NETROM_TRANSPORT_NO_ACTIVITY_TIMEOUT=9,
 Model1_NET_NETROM_ROUTING_CONTROL=10,
 Model1_NET_NETROM_LINK_FAILS_COUNT=11,
 Model1_NET_NETROM_RESET=12
};

/* /proc/sys/net/ax25 */
enum {
 Model1_NET_AX25_IP_DEFAULT_MODE=1,
 Model1_NET_AX25_DEFAULT_MODE=2,
 Model1_NET_AX25_BACKOFF_TYPE=3,
 Model1_NET_AX25_CONNECT_MODE=4,
 Model1_NET_AX25_STANDARD_WINDOW=5,
 Model1_NET_AX25_EXTENDED_WINDOW=6,
 Model1_NET_AX25_T1_TIMEOUT=7,
 Model1_NET_AX25_T2_TIMEOUT=8,
 Model1_NET_AX25_T3_TIMEOUT=9,
 Model1_NET_AX25_IDLE_TIMEOUT=10,
 Model1_NET_AX25_N2=11,
 Model1_NET_AX25_PACLEN=12,
 Model1_NET_AX25_PROTOCOL=13,
 Model1_NET_AX25_DAMA_SLAVE_TIMEOUT=14
};

/* /proc/sys/net/rose */
enum {
 Model1_NET_ROSE_RESTART_REQUEST_TIMEOUT=1,
 Model1_NET_ROSE_CALL_REQUEST_TIMEOUT=2,
 Model1_NET_ROSE_RESET_REQUEST_TIMEOUT=3,
 Model1_NET_ROSE_CLEAR_REQUEST_TIMEOUT=4,
 Model1_NET_ROSE_ACK_HOLD_BACK_TIMEOUT=5,
 Model1_NET_ROSE_ROUTING_CONTROL=6,
 Model1_NET_ROSE_LINK_FAIL_TIMEOUT=7,
 Model1_NET_ROSE_MAX_VCS=8,
 Model1_NET_ROSE_WINDOW_SIZE=9,
 Model1_NET_ROSE_NO_ACTIVITY_TIMEOUT=10
};

/* /proc/sys/net/x25 */
enum {
 Model1_NET_X25_RESTART_REQUEST_TIMEOUT=1,
 Model1_NET_X25_CALL_REQUEST_TIMEOUT=2,
 Model1_NET_X25_RESET_REQUEST_TIMEOUT=3,
 Model1_NET_X25_CLEAR_REQUEST_TIMEOUT=4,
 Model1_NET_X25_ACK_HOLD_BACK_TIMEOUT=5,
 Model1_NET_X25_FORWARD=6
};

/* /proc/sys/net/token-ring */
enum
{
 Model1_NET_TR_RIF_TIMEOUT=1
};

/* /proc/sys/net/decnet/ */
enum {
 Model1_NET_DECNET_NODE_TYPE = 1,
 Model1_NET_DECNET_NODE_ADDRESS = 2,
 Model1_NET_DECNET_NODE_NAME = 3,
 Model1_NET_DECNET_DEFAULT_DEVICE = 4,
 Model1_NET_DECNET_TIME_WAIT = 5,
 Model1_NET_DECNET_DN_COUNT = 6,
 Model1_NET_DECNET_DI_COUNT = 7,
 Model1_NET_DECNET_DR_COUNT = 8,
 Model1_NET_DECNET_DST_GC_INTERVAL = 9,
 Model1_NET_DECNET_CONF = 10,
 Model1_NET_DECNET_NO_FC_MAX_CWND = 11,
 Model1_NET_DECNET_MEM = 12,
 Model1_NET_DECNET_RMEM = 13,
 Model1_NET_DECNET_WMEM = 14,
 Model1_NET_DECNET_DEBUG_LEVEL = 255
};

/* /proc/sys/net/decnet/conf/<dev> */
enum {
 Model1_NET_DECNET_CONF_LOOPBACK = -2,
 Model1_NET_DECNET_CONF_DDCMP = -3,
 Model1_NET_DECNET_CONF_PPP = -4,
 Model1_NET_DECNET_CONF_X25 = -5,
 Model1_NET_DECNET_CONF_GRE = -6,
 Model1_NET_DECNET_CONF_ETHER = -7

 /* ... and ifindex of devices */
};

/* /proc/sys/net/decnet/conf/<dev>/ */
enum {
 Model1_NET_DECNET_CONF_DEV_PRIORITY = 1,
 Model1_NET_DECNET_CONF_DEV_T1 = 2,
 Model1_NET_DECNET_CONF_DEV_T2 = 3,
 Model1_NET_DECNET_CONF_DEV_T3 = 4,
 Model1_NET_DECNET_CONF_DEV_FORWARDING = 5,
 Model1_NET_DECNET_CONF_DEV_BLKSIZE = 6,
 Model1_NET_DECNET_CONF_DEV_STATE = 7
};

/* /proc/sys/net/sctp */
enum {
 Model1_NET_SCTP_RTO_INITIAL = 1,
 Model1_NET_SCTP_RTO_MIN = 2,
 Model1_NET_SCTP_RTO_MAX = 3,
 Model1_NET_SCTP_RTO_ALPHA = 4,
 Model1_NET_SCTP_RTO_BETA = 5,
 Model1_NET_SCTP_VALID_COOKIE_LIFE = 6,
 Model1_NET_SCTP_ASSOCIATION_MAX_RETRANS = 7,
 Model1_NET_SCTP_PATH_MAX_RETRANS = 8,
 Model1_NET_SCTP_MAX_INIT_RETRANSMITS = 9,
 Model1_NET_SCTP_HB_INTERVAL = 10,
 Model1_NET_SCTP_PRESERVE_ENABLE = 11,
 Model1_NET_SCTP_MAX_BURST = 12,
 Model1_NET_SCTP_ADDIP_ENABLE = 13,
 Model1_NET_SCTP_PRSCTP_ENABLE = 14,
 Model1_NET_SCTP_SNDBUF_POLICY = 15,
 Model1_NET_SCTP_SACK_TIMEOUT = 16,
 Model1_NET_SCTP_RCVBUF_POLICY = 17,
};

/* /proc/sys/net/bridge */
enum {
 Model1_NET_BRIDGE_NF_CALL_ARPTABLES = 1,
 Model1_NET_BRIDGE_NF_CALL_IPTABLES = 2,
 Model1_NET_BRIDGE_NF_CALL_IP6TABLES = 3,
 Model1_NET_BRIDGE_NF_FILTER_VLAN_TAGGED = 4,
 Model1_NET_BRIDGE_NF_FILTER_PPPOE_TAGGED = 5,
};

/* proc/sys/net/irda */
enum {
 Model1_NET_IRDA_DISCOVERY=1,
 Model1_NET_IRDA_DEVNAME=2,
 Model1_NET_IRDA_DEBUG=3,
 Model1_NET_IRDA_FAST_POLL=4,
 Model1_NET_IRDA_DISCOVERY_SLOTS=5,
 Model1_NET_IRDA_DISCOVERY_TIMEOUT=6,
 Model1_NET_IRDA_SLOT_TIMEOUT=7,
 Model1_NET_IRDA_MAX_BAUD_RATE=8,
 Model1_NET_IRDA_MIN_TX_TURN_TIME=9,
 Model1_NET_IRDA_MAX_TX_DATA_SIZE=10,
 Model1_NET_IRDA_MAX_TX_WINDOW=11,
 Model1_NET_IRDA_MAX_NOREPLY_TIME=12,
 Model1_NET_IRDA_WARN_NOREPLY_TIME=13,
 Model1_NET_IRDA_LAP_KEEPALIVE_TIME=14,
};


/* CTL_FS names: */
enum
{
 Model1_FS_NRINODE=1, /* int:current number of allocated inodes */
 Model1_FS_STATINODE=2,
 Model1_FS_MAXINODE=3, /* int:maximum number of inodes that can be allocated */
 Model1_FS_NRDQUOT=4, /* int:current number of allocated dquots */
 Model1_FS_MAXDQUOT=5, /* int:maximum number of dquots that can be allocated */
 Model1_FS_NRFILE=6, /* int:current number of allocated filedescriptors */
 Model1_FS_MAXFILE=7, /* int:maximum number of filedescriptors that can be allocated */
 Model1_FS_DENTRY=8,
 Model1_FS_NRSUPER=9, /* int:current number of allocated super_blocks */
 Model1_FS_MAXSUPER=10, /* int:maximum number of super_blocks that can be allocated */
 Model1_FS_OVERFLOWUID=11, /* int: overflow UID */
 Model1_FS_OVERFLOWGID=12, /* int: overflow GID */
 Model1_FS_LEASES=13, /* int: leases enabled */
 Model1_FS_DIR_NOTIFY=14, /* int: directory notification enabled */
 Model1_FS_LEASE_TIME=15, /* int: maximum time to wait for a lease break */
 Model1_FS_DQSTATS=16, /* disc quota usage statistics and control */
 Model1_FS_XFS=17, /* struct: control xfs parameters */
 Model1_FS_AIO_NR=18, /* current system-wide number of aio requests */
 Model1_FS_AIO_MAX_NR=19, /* system-wide maximum number of aio requests */
 Model1_FS_INOTIFY=20, /* inotify submenu */
 Model1_FS_OCFS2=988, /* ocfs2 */
};

/* /proc/sys/fs/quota/ */
enum {
 Model1_FS_DQ_LOOKUPS = 1,
 Model1_FS_DQ_DROPS = 2,
 Model1_FS_DQ_READS = 3,
 Model1_FS_DQ_WRITES = 4,
 Model1_FS_DQ_CACHE_HITS = 5,
 Model1_FS_DQ_ALLOCATED = 6,
 Model1_FS_DQ_FREE = 7,
 Model1_FS_DQ_SYNCS = 8,
 Model1_FS_DQ_WARNINGS = 9,
};

/* CTL_DEBUG names: */

/* CTL_DEV names: */
enum {
 Model1_DEV_CDROM=1,
 Model1_DEV_HWMON=2,
 Model1_DEV_PARPORT=3,
 Model1_DEV_RAID=4,
 Model1_DEV_MAC_HID=5,
 Model1_DEV_SCSI=6,
 Model1_DEV_IPMI=7,
};

/* /proc/sys/dev/cdrom */
enum {
 Model1_DEV_CDROM_INFO=1,
 Model1_DEV_CDROM_AUTOCLOSE=2,
 Model1_DEV_CDROM_AUTOEJECT=3,
 Model1_DEV_CDROM_DEBUG=4,
 Model1_DEV_CDROM_LOCK=5,
 Model1_DEV_CDROM_CHECK_MEDIA=6
};

/* /proc/sys/dev/parport */
enum {
 Model1_DEV_PARPORT_DEFAULT=-3
};

/* /proc/sys/dev/raid */
enum {
 Model1_DEV_RAID_SPEED_LIMIT_MIN=1,
 Model1_DEV_RAID_SPEED_LIMIT_MAX=2
};

/* /proc/sys/dev/parport/default */
enum {
 Model1_DEV_PARPORT_DEFAULT_TIMESLICE=1,
 Model1_DEV_PARPORT_DEFAULT_SPINTIME=2
};

/* /proc/sys/dev/parport/parport n */
enum {
 Model1_DEV_PARPORT_SPINTIME=1,
 Model1_DEV_PARPORT_BASE_ADDR=2,
 Model1_DEV_PARPORT_IRQ=3,
 Model1_DEV_PARPORT_DMA=4,
 Model1_DEV_PARPORT_MODES=5,
 Model1_DEV_PARPORT_DEVICES=6,
 Model1_DEV_PARPORT_AUTOPROBE=16
};

/* /proc/sys/dev/parport/parport n/devices/ */
enum {
 Model1_DEV_PARPORT_DEVICES_ACTIVE=-3,
};

/* /proc/sys/dev/parport/parport n/devices/device n */
enum {
 Model1_DEV_PARPORT_DEVICE_TIMESLICE=1,
};

/* /proc/sys/dev/mac_hid */
enum {
 Model1_DEV_MAC_HID_KEYBOARD_SENDS_LINUX_KEYCODES=1,
 Model1_DEV_MAC_HID_KEYBOARD_LOCK_KEYCODES=2,
 Model1_DEV_MAC_HID_MOUSE_BUTTON_EMULATION=3,
 Model1_DEV_MAC_HID_MOUSE_BUTTON2_KEYCODE=4,
 Model1_DEV_MAC_HID_MOUSE_BUTTON3_KEYCODE=5,
 Model1_DEV_MAC_HID_ADB_MOUSE_SENDS_KEYCODES=6
};

/* /proc/sys/dev/scsi */
enum {
 Model1_DEV_SCSI_LOGGING_LEVEL=1,
};

/* /proc/sys/dev/ipmi */
enum {
 Model1_DEV_IPMI_POWEROFF_POWERCYCLE=1,
};

/* /proc/sys/abi */
enum
{
 Model1_ABI_DEFHANDLER_COFF=1, /* default handler for coff binaries */
 Model1_ABI_DEFHANDLER_ELF=2, /* default handler for ELF binaries */
 Model1_ABI_DEFHANDLER_LCALL7=3,/* default handler for procs using lcall7 */
 Model1_ABI_DEFHANDLER_LIBCSO=4,/* default handler for an libc.so ELF interp */
 Model1_ABI_TRACE=5, /* tracing flags */
 Model1_ABI_FAKE_UTSNAME=6, /* fake target utsname information */
};

/* For the /proc/sys support */
struct Model1_completion;
struct Model1_ctl_table;
struct Model1_nsproxy;
struct Model1_ctl_table_root;
struct Model1_ctl_table_header;
struct Model1_ctl_dir;

typedef int Model1_proc_handler (struct Model1_ctl_table *Model1_ctl, int Model1_write,
     void *Model1_buffer, Model1_size_t *Model1_lenp, Model1_loff_t *Model1_ppos);

extern int Model1_proc_dostring(struct Model1_ctl_table *, int,
    void *, Model1_size_t *, Model1_loff_t *);
extern int Model1_proc_dointvec(struct Model1_ctl_table *, int,
    void *, Model1_size_t *, Model1_loff_t *);
extern int Model1_proc_douintvec(struct Model1_ctl_table *, int,
    void *, Model1_size_t *, Model1_loff_t *);
extern int Model1_proc_dointvec_minmax(struct Model1_ctl_table *, int,
    void *, Model1_size_t *, Model1_loff_t *);
extern int Model1_proc_dointvec_jiffies(struct Model1_ctl_table *, int,
     void *, Model1_size_t *, Model1_loff_t *);
extern int Model1_proc_dointvec_userhz_jiffies(struct Model1_ctl_table *, int,
     void *, Model1_size_t *, Model1_loff_t *);
extern int Model1_proc_dointvec_ms_jiffies(struct Model1_ctl_table *, int,
        void *, Model1_size_t *, Model1_loff_t *);
extern int Model1_proc_doulongvec_minmax(struct Model1_ctl_table *, int,
      void *, Model1_size_t *, Model1_loff_t *);
extern int Model1_proc_doulongvec_ms_jiffies_minmax(struct Model1_ctl_table *Model1_table, int,
          void *, Model1_size_t *, Model1_loff_t *);
extern int Model1_proc_do_large_bitmap(struct Model1_ctl_table *, int,
    void *, Model1_size_t *, Model1_loff_t *);

/*
 * Register a set of sysctl names by calling register_sysctl_table
 * with an initialised array of struct ctl_table's.  An entry with 
 * NULL procname terminates the table.  table->de will be
 * set up by the registration and need not be initialised in advance.
 *
 * sysctl names can be mirrored automatically under /proc/sys.  The
 * procname supplied controls /proc naming.
 *
 * The table's mode will be honoured both for sys_sysctl(2) and
 * proc-fs access.
 *
 * Leaf nodes in the sysctl tree will be represented by a single file
 * under /proc; non-leaf nodes will be represented by directories.  A
 * null procname disables /proc mirroring at this node.
 *
 * sysctl(2) can automatically manage read and write requests through
 * the sysctl table.  The data and maxlen fields of the ctl_table
 * struct enable minimal validation of the values being written to be
 * performed, and the mode field allows minimal authentication.
 * 
 * There must be a proc_handler routine for any terminal nodes
 * mirrored under /proc/sys (non-terminals are handled by a built-in
 * directory handler).  Several default handlers are available to
 * cover common cases.
 */

/* Support for userspace poll() to watch for changes */
struct Model1_ctl_table_poll {
 Model1_atomic_t Model1_event;
 Model1_wait_queue_head_t Model1_wait;
};

static inline __attribute__((no_instrument_function)) void *Model1_proc_sys_poll_event(struct Model1_ctl_table_poll *Model1_poll)
{
 return (void *)(unsigned long)Model1_atomic_read(&Model1_poll->Model1_event);
}
/* A sysctl table is an array of struct ctl_table: */
struct Model1_ctl_table
{
 const char *Model1_procname; /* Text ID for /proc/sys, or zero */
 void *Model1_data;
 int Model1_maxlen;
 Model1_umode_t Model1_mode;
 struct Model1_ctl_table *Model1_child; /* Deprecated */
 Model1_proc_handler *Model1_proc_handler; /* Callback for text formatting */
 struct Model1_ctl_table_poll *Model1_poll;
 void *Model1_extra1;
 void *Model1_extra2;
};

struct Model1_ctl_node {
 struct Model1_rb_node Model1_node;
 struct Model1_ctl_table_header *Model1_header;
};

/* struct ctl_table_header is used to maintain dynamic lists of
   struct ctl_table trees. */
struct Model1_ctl_table_header
{
 union {
  struct {
   struct Model1_ctl_table *Model1_ctl_table;
   int Model1_used;
   int Model1_count;
   int Model1_nreg;
  };
  struct Model1_callback_head Model1_rcu;
 };
 struct Model1_completion *Model1_unregistering;
 struct Model1_ctl_table *Model1_ctl_table_arg;
 struct Model1_ctl_table_root *Model1_root;
 struct Model1_ctl_table_set *Model1_set;
 struct Model1_ctl_dir *Model1_parent;
 struct Model1_ctl_node *Model1_node;
};

struct Model1_ctl_dir {
 /* Header must be at the start of ctl_dir */
 struct Model1_ctl_table_header Model1_header;
 struct Model1_rb_root Model1_root;
};

struct Model1_ctl_table_set {
 int (*Model1_is_seen)(struct Model1_ctl_table_set *);
 struct Model1_ctl_dir Model1_dir;
};

struct Model1_ctl_table_root {
 struct Model1_ctl_table_set Model1_default_set;
 struct Model1_ctl_table_set *(*Model1_lookup)(struct Model1_ctl_table_root *Model1_root,
        struct Model1_nsproxy *Model1_namespaces);
 int (*Model1_permissions)(struct Model1_ctl_table_header *Model1_head, struct Model1_ctl_table *Model1_table);
};

/* struct ctl_path describes where in the hierarchy a table is added */
struct Model1_ctl_path {
 const char *Model1_procname;
};



void Model1_proc_sys_poll_notify(struct Model1_ctl_table_poll *Model1_poll);

extern void Model1_setup_sysctl_set(struct Model1_ctl_table_set *Model1_p,
 struct Model1_ctl_table_root *Model1_root,
 int (*Model1_is_seen)(struct Model1_ctl_table_set *));
extern void Model1_retire_sysctl_set(struct Model1_ctl_table_set *Model1_set);

void Model1_register_sysctl_root(struct Model1_ctl_table_root *Model1_root);
struct Model1_ctl_table_header *Model1___register_sysctl_table(
 struct Model1_ctl_table_set *Model1_set,
 const char *Model1_path, struct Model1_ctl_table *Model1_table);
struct Model1_ctl_table_header *Model1___register_sysctl_paths(
 struct Model1_ctl_table_set *Model1_set,
 const struct Model1_ctl_path *Model1_path, struct Model1_ctl_table *Model1_table);
struct Model1_ctl_table_header *Model1_register_sysctl(const char *Model1_path, struct Model1_ctl_table *Model1_table);
struct Model1_ctl_table_header *Model1_register_sysctl_table(struct Model1_ctl_table * Model1_table);
struct Model1_ctl_table_header *Model1_register_sysctl_paths(const struct Model1_ctl_path *Model1_path,
      struct Model1_ctl_table *Model1_table);

void Model1_unregister_sysctl_table(struct Model1_ctl_table_header * Model1_table);

extern int Model1_sysctl_init(void);

extern struct Model1_ctl_table Model1_sysctl_mount_point[];
int Model1_sysctl_max_threads(struct Model1_ctl_table *Model1_table, int Model1_write,
         void *Model1_buffer, Model1_size_t *Model1_lenp, Model1_loff_t *Model1_ppos);

extern unsigned int Model1_sysctl_timer_migration;
int Model1_timer_migration_handler(struct Model1_ctl_table *Model1_table, int Model1_write,
       void *Model1_buffer, Model1_size_t *Model1_lenp,
       Model1_loff_t *Model1_ppos);


unsigned long Model1___round_jiffies(unsigned long Model1_j, int Model1_cpu);
unsigned long Model1___round_jiffies_relative(unsigned long Model1_j, int Model1_cpu);
unsigned long Model1_round_jiffies(unsigned long Model1_j);
unsigned long Model1_round_jiffies_relative(unsigned long Model1_j);

unsigned long Model1___round_jiffies_up(unsigned long Model1_j, int Model1_cpu);
unsigned long Model1___round_jiffies_up_relative(unsigned long Model1_j, int Model1_cpu);
unsigned long Model1_round_jiffies_up(unsigned long Model1_j);
unsigned long Model1_round_jiffies_up_relative(unsigned long Model1_j);


int Model1_timers_dead_cpu(unsigned int Model1_cpu);







struct Model1_workqueue_struct;

struct Model1_work_struct;
typedef void (*Model1_work_func_t)(struct Model1_work_struct *Model1_work);
void Model1_delayed_work_timer_fn(unsigned long Model1___data);

/*
 * The first word is the work queue pointer and the flags rolled into
 * one
 */


enum {
 Model1_WORK_STRUCT_PENDING_BIT = 0, /* work item is pending execution */
 Model1_WORK_STRUCT_DELAYED_BIT = 1, /* work item is delayed */
 Model1_WORK_STRUCT_PWQ_BIT = 2, /* data points to pwq */
 Model1_WORK_STRUCT_LINKED_BIT = 3, /* next work is linked to this one */




 Model1_WORK_STRUCT_COLOR_SHIFT = 4, /* color for workqueue flushing */


 Model1_WORK_STRUCT_COLOR_BITS = 4,

 Model1_WORK_STRUCT_PENDING = 1 << Model1_WORK_STRUCT_PENDING_BIT,
 Model1_WORK_STRUCT_DELAYED = 1 << Model1_WORK_STRUCT_DELAYED_BIT,
 Model1_WORK_STRUCT_PWQ = 1 << Model1_WORK_STRUCT_PWQ_BIT,
 Model1_WORK_STRUCT_LINKED = 1 << Model1_WORK_STRUCT_LINKED_BIT,



 Model1_WORK_STRUCT_STATIC = 0,


 /*
	 * The last color is no color used for works which don't
	 * participate in workqueue flushing.
	 */
 Model1_WORK_NR_COLORS = (1 << Model1_WORK_STRUCT_COLOR_BITS) - 1,
 Model1_WORK_NO_COLOR = Model1_WORK_NR_COLORS,

 /* not bound to any CPU, prefer the local CPU */
 Model1_WORK_CPU_UNBOUND = 64,

 /*
	 * Reserve 7 bits off of pwq pointer w/ debugobjects turned off.
	 * This makes pwqs aligned to 256 bytes and allows 15 workqueue
	 * flush colors.
	 */
 Model1_WORK_STRUCT_FLAG_BITS = Model1_WORK_STRUCT_COLOR_SHIFT +
      Model1_WORK_STRUCT_COLOR_BITS,

 /* data contains off-queue information when !WORK_STRUCT_PWQ */
 Model1_WORK_OFFQ_FLAG_BASE = Model1_WORK_STRUCT_COLOR_SHIFT,

 Model1___WORK_OFFQ_CANCELING = Model1_WORK_OFFQ_FLAG_BASE,
 Model1_WORK_OFFQ_CANCELING = (1 << Model1___WORK_OFFQ_CANCELING),

 /*
	 * When a work item is off queue, its high bits point to the last
	 * pool it was on.  Cap at 31 bits and use the highest number to
	 * indicate that no pool is associated.
	 */
 Model1_WORK_OFFQ_FLAG_BITS = 1,
 Model1_WORK_OFFQ_POOL_SHIFT = Model1_WORK_OFFQ_FLAG_BASE + Model1_WORK_OFFQ_FLAG_BITS,
 Model1_WORK_OFFQ_LEFT = 64 - Model1_WORK_OFFQ_POOL_SHIFT,
 Model1_WORK_OFFQ_POOL_BITS = Model1_WORK_OFFQ_LEFT <= 31 ? Model1_WORK_OFFQ_LEFT : 31,
 Model1_WORK_OFFQ_POOL_NONE = (1LU << Model1_WORK_OFFQ_POOL_BITS) - 1,

 /* convenience constants */
 Model1_WORK_STRUCT_FLAG_MASK = (1UL << Model1_WORK_STRUCT_FLAG_BITS) - 1,
 Model1_WORK_STRUCT_WQ_DATA_MASK = ~Model1_WORK_STRUCT_FLAG_MASK,
 Model1_WORK_STRUCT_NO_POOL = (unsigned long)Model1_WORK_OFFQ_POOL_NONE << Model1_WORK_OFFQ_POOL_SHIFT,

 /* bit mask for work_busy() return values */
 Model1_WORK_BUSY_PENDING = 1 << 0,
 Model1_WORK_BUSY_RUNNING = 1 << 1,

 /* maximum string length for set_worker_desc() */
 Model1_WORKER_DESC_LEN = 24,
};

struct Model1_work_struct {
 Model1_atomic_long_t Model1_data;
 struct Model1_list_head Model1_entry;
 Model1_work_func_t func;



};





struct Model1_delayed_work {
 struct Model1_work_struct Model1_work;
 struct Model1_timer_list Model1_timer;

 /* target workqueue and CPU ->timer uses to queue ->work */
 struct Model1_workqueue_struct *Model1_wq;
 int Model1_cpu;
};

/*
 * A struct for workqueue attributes.  This can be used to change
 * attributes of an unbound workqueue.
 *
 * Unlike other fields, ->no_numa isn't a property of a worker_pool.  It
 * only modifies how apply_workqueue_attrs() select pools and thus doesn't
 * participate in pool hash calculations or equality comparisons.
 */
struct Model1_workqueue_attrs {
 int Model1_nice; /* nice level */
 Model1_cpumask_var_t Model1_cpumask; /* allowed CPUs */
 bool Model1_no_numa; /* disable NUMA affinity */
};

static inline __attribute__((no_instrument_function)) struct Model1_delayed_work *Model1_to_delayed_work(struct Model1_work_struct *Model1_work)
{
 return ({ const typeof( ((struct Model1_delayed_work *)0)->Model1_work ) *Model1___mptr = (Model1_work); (struct Model1_delayed_work *)( (char *)Model1___mptr - __builtin_offsetof(struct Model1_delayed_work, Model1_work) );});
}

struct Model1_execute_work {
 struct Model1_work_struct Model1_work;
};
static inline __attribute__((no_instrument_function)) void Model1___init_work(struct Model1_work_struct *Model1_work, int Model1_onstack) { }
static inline __attribute__((no_instrument_function)) void Model1_destroy_work_on_stack(struct Model1_work_struct *Model1_work) { }
static inline __attribute__((no_instrument_function)) void Model1_destroy_delayed_work_on_stack(struct Model1_delayed_work *Model1_work) { }
static inline __attribute__((no_instrument_function)) unsigned int Model1_work_static(struct Model1_work_struct *Model1_work) { return 0; }


/*
 * initialize all of a work item in one go
 *
 * NOTE! No point in using "atomic_long_set()": using a direct
 * assignment of the work data initializer allows the compiler
 * to generate better code.
 */
/**
 * work_pending - Find out whether a work item is currently pending
 * @work: The work item in question
 */



/**
 * delayed_work_pending - Find out whether a delayable work item is currently
 * pending
 * @w: The work item in question
 */



/*
 * Workqueue flags and constants.  For details, please refer to
 * Documentation/workqueue.txt.
 */
enum {
 Model1_WQ_UNBOUND = 1 << 1, /* not bound to any cpu */
 Model1_WQ_FREEZABLE = 1 << 2, /* freeze during suspend */
 Model1_WQ_MEM_RECLAIM = 1 << 3, /* may be used for memory reclaim */
 Model1_WQ_HIGHPRI = 1 << 4, /* high priority */
 Model1_WQ_CPU_INTENSIVE = 1 << 5, /* cpu intensive workqueue */
 Model1_WQ_SYSFS = 1 << 6, /* visible in sysfs, see wq_sysfs_register() */

 /*
	 * Per-cpu workqueues are generally preferred because they tend to
	 * show better performance thanks to cache locality.  Per-cpu
	 * workqueues exclude the scheduler from choosing the CPU to
	 * execute the worker threads, which has an unfortunate side effect
	 * of increasing power consumption.
	 *
	 * The scheduler considers a CPU idle if it doesn't have any task
	 * to execute and tries to keep idle cores idle to conserve power;
	 * however, for example, a per-cpu work item scheduled from an
	 * interrupt handler on an idle CPU will force the scheduler to
	 * excute the work item on that CPU breaking the idleness, which in
	 * turn may lead to more scheduling choices which are sub-optimal
	 * in terms of power consumption.
	 *
	 * Workqueues marked with WQ_POWER_EFFICIENT are per-cpu by default
	 * but become unbound if workqueue.power_efficient kernel param is
	 * specified.  Per-cpu workqueues which are identified to
	 * contribute significantly to power-consumption are identified and
	 * marked with this flag and enabling the power_efficient mode
	 * leads to noticeable power saving at the cost of small
	 * performance disadvantage.
	 *
	 * http://thread.gmane.org/gmane.linux.kernel/1480396
	 */
 Model1_WQ_POWER_EFFICIENT = 1 << 7,

 Model1___WQ_DRAINING = 1 << 16, /* internal: workqueue is draining */
 Model1___WQ_ORDERED = 1 << 17, /* internal: workqueue is ordered */
 Model1___WQ_LEGACY = 1 << 18, /* internal: create*_workqueue() */

 Model1_WQ_MAX_ACTIVE = 512, /* I like 512, better ideas? */
 Model1_WQ_MAX_UNBOUND_PER_CPU = 4, /* 4 * #cpus for unbound wq */
 Model1_WQ_DFL_ACTIVE = Model1_WQ_MAX_ACTIVE / 2,
};

/* unbound wq's aren't per-cpu, scale max_active according to #cpus */



/*
 * System-wide workqueues which are always present.
 *
 * system_wq is the one used by schedule[_delayed]_work[_on]().
 * Multi-CPU multi-threaded.  There are users which expect relatively
 * short queue flush time.  Don't queue works which can run for too
 * long.
 *
 * system_highpri_wq is similar to system_wq but for work items which
 * require WQ_HIGHPRI.
 *
 * system_long_wq is similar to system_wq but may host long running
 * works.  Queue flushing might take relatively long.
 *
 * system_unbound_wq is unbound workqueue.  Workers are not bound to
 * any specific CPU, not concurrency managed, and all queued works are
 * executed immediately as long as max_active limit is not reached and
 * resources are available.
 *
 * system_freezable_wq is equivalent to system_wq except that it's
 * freezable.
 *
 * *_power_efficient_wq are inclined towards saving power and converted
 * into WQ_UNBOUND variants if 'wq_power_efficient' is enabled; otherwise,
 * they are same as their non-power-efficient counterparts - e.g.
 * system_power_efficient_wq is identical to system_wq if
 * 'wq_power_efficient' is disabled.  See WQ_POWER_EFFICIENT for more info.
 */
extern struct Model1_workqueue_struct *Model1_system_wq;
extern struct Model1_workqueue_struct *Model1_system_highpri_wq;
extern struct Model1_workqueue_struct *Model1_system_long_wq;
extern struct Model1_workqueue_struct *Model1_system_unbound_wq;
extern struct Model1_workqueue_struct *Model1_system_freezable_wq;
extern struct Model1_workqueue_struct *Model1_system_power_efficient_wq;
extern struct Model1_workqueue_struct *Model1_system_freezable_power_efficient_wq;

extern struct Model1_workqueue_struct *
Model1___alloc_workqueue_key(const char *Model1_fmt, unsigned int Model1_flags, int Model1_max_active,
 struct Model1_lock_class_key *Model1_key, const char *Model1_lock_name, ...) __attribute__((format(printf, 1, 6)));

/**
 * alloc_workqueue - allocate a workqueue
 * @fmt: printf format for the name of the workqueue
 * @flags: WQ_* flags
 * @max_active: max in-flight work items, 0 for default
 * @args...: args for @fmt
 *
 * Allocate a workqueue with the specified parameters.  For detailed
 * information on WQ_* flags, please refer to Documentation/workqueue.txt.
 *
 * The __lock_name macro dance is to guarantee that single lock_class_key
 * doesn't end up with different namesm, which isn't allowed by lockdep.
 *
 * RETURNS:
 * Pointer to the allocated workqueue on success, %NULL on failure.
 */
/**
 * alloc_ordered_workqueue - allocate an ordered workqueue
 * @fmt: printf format for the name of the workqueue
 * @flags: WQ_* flags (only WQ_FREEZABLE and WQ_MEM_RECLAIM are meaningful)
 * @args...: args for @fmt
 *
 * Allocate an ordered workqueue.  An ordered workqueue executes at
 * most one work item at any given time in the queued order.  They are
 * implemented as unbound workqueues with @max_active of one.
 *
 * RETURNS:
 * Pointer to the allocated workqueue on success, %NULL on failure.
 */
extern void Model1_destroy_workqueue(struct Model1_workqueue_struct *Model1_wq);

struct Model1_workqueue_attrs *Model1_alloc_workqueue_attrs(Model1_gfp_t Model1_gfp_mask);
void Model1_free_workqueue_attrs(struct Model1_workqueue_attrs *Model1_attrs);
int Model1_apply_workqueue_attrs(struct Model1_workqueue_struct *Model1_wq,
     const struct Model1_workqueue_attrs *Model1_attrs);
int Model1_workqueue_set_unbound_cpumask(Model1_cpumask_var_t Model1_cpumask);

extern bool Model1_queue_work_on(int Model1_cpu, struct Model1_workqueue_struct *Model1_wq,
   struct Model1_work_struct *Model1_work);
extern bool Model1_queue_delayed_work_on(int Model1_cpu, struct Model1_workqueue_struct *Model1_wq,
   struct Model1_delayed_work *Model1_work, unsigned long Model1_delay);
extern bool Model1_mod_delayed_work_on(int Model1_cpu, struct Model1_workqueue_struct *Model1_wq,
   struct Model1_delayed_work *Model1_dwork, unsigned long Model1_delay);

extern void Model1_flush_workqueue(struct Model1_workqueue_struct *Model1_wq);
extern void Model1_drain_workqueue(struct Model1_workqueue_struct *Model1_wq);

extern int Model1_schedule_on_each_cpu(Model1_work_func_t func);

int Model1_execute_in_process_context(Model1_work_func_t Model1_fn, struct Model1_execute_work *);

extern bool Model1_flush_work(struct Model1_work_struct *Model1_work);
extern bool Model1_cancel_work_sync(struct Model1_work_struct *Model1_work);

extern bool Model1_flush_delayed_work(struct Model1_delayed_work *Model1_dwork);
extern bool Model1_cancel_delayed_work(struct Model1_delayed_work *Model1_dwork);
extern bool Model1_cancel_delayed_work_sync(struct Model1_delayed_work *Model1_dwork);

extern void Model1_workqueue_set_max_active(struct Model1_workqueue_struct *Model1_wq,
         int Model1_max_active);
extern bool Model1_current_is_workqueue_rescuer(void);
extern bool Model1_workqueue_congested(int Model1_cpu, struct Model1_workqueue_struct *Model1_wq);
extern unsigned int Model1_work_busy(struct Model1_work_struct *Model1_work);
extern __attribute__((format(printf, 1, 2))) void Model1_set_worker_desc(const char *Model1_fmt, ...);
extern void Model1_print_worker_info(const char *Model1_log_lvl, struct Model1_task_struct *Model1_task);
extern void Model1_show_workqueue_state(void);

/**
 * queue_work - queue work on a workqueue
 * @wq: workqueue to use
 * @work: work to queue
 *
 * Returns %false if @work was already on a queue, %true otherwise.
 *
 * We queue the work to the CPU on which it was submitted, but if the CPU dies
 * it can be processed by another CPU.
 */
static inline __attribute__((no_instrument_function)) bool Model1_queue_work(struct Model1_workqueue_struct *Model1_wq,
         struct Model1_work_struct *Model1_work)
{
 return Model1_queue_work_on(Model1_WORK_CPU_UNBOUND, Model1_wq, Model1_work);
}

/**
 * queue_delayed_work - queue work on a workqueue after delay
 * @wq: workqueue to use
 * @dwork: delayable work to queue
 * @delay: number of jiffies to wait before queueing
 *
 * Equivalent to queue_delayed_work_on() but tries to use the local CPU.
 */
static inline __attribute__((no_instrument_function)) bool Model1_queue_delayed_work(struct Model1_workqueue_struct *Model1_wq,
          struct Model1_delayed_work *Model1_dwork,
          unsigned long Model1_delay)
{
 return Model1_queue_delayed_work_on(Model1_WORK_CPU_UNBOUND, Model1_wq, Model1_dwork, Model1_delay);
}

/**
 * mod_delayed_work - modify delay of or queue a delayed work
 * @wq: workqueue to use
 * @dwork: work to queue
 * @delay: number of jiffies to wait before queueing
 *
 * mod_delayed_work_on() on local CPU.
 */
static inline __attribute__((no_instrument_function)) bool Model1_mod_delayed_work(struct Model1_workqueue_struct *Model1_wq,
        struct Model1_delayed_work *Model1_dwork,
        unsigned long Model1_delay)
{
 return Model1_mod_delayed_work_on(Model1_WORK_CPU_UNBOUND, Model1_wq, Model1_dwork, Model1_delay);
}

/**
 * schedule_work_on - put work task on a specific cpu
 * @cpu: cpu to put the work task on
 * @work: job to be done
 *
 * This puts a job on a specific cpu
 */
static inline __attribute__((no_instrument_function)) bool Model1_schedule_work_on(int Model1_cpu, struct Model1_work_struct *Model1_work)
{
 return Model1_queue_work_on(Model1_cpu, Model1_system_wq, Model1_work);
}

/**
 * schedule_work - put work task in global workqueue
 * @work: job to be done
 *
 * Returns %false if @work was already on the kernel-global workqueue and
 * %true otherwise.
 *
 * This puts a job in the kernel-global workqueue if it was not already
 * queued and leaves it in the same position on the kernel-global
 * workqueue otherwise.
 */
static inline __attribute__((no_instrument_function)) bool Model1_schedule_work(struct Model1_work_struct *Model1_work)
{
 return Model1_queue_work(Model1_system_wq, Model1_work);
}

/**
 * flush_scheduled_work - ensure that any scheduled work has run to completion.
 *
 * Forces execution of the kernel-global workqueue and blocks until its
 * completion.
 *
 * Think twice before calling this function!  It's very easy to get into
 * trouble if you don't take great care.  Either of the following situations
 * will lead to deadlock:
 *
 *	One of the work items currently on the workqueue needs to acquire
 *	a lock held by your code or its caller.
 *
 *	Your code is running in the context of a work routine.
 *
 * They will be detected by lockdep when they occur, but the first might not
 * occur very often.  It depends on what work items are on the workqueue and
 * what locks they need, which you have no control over.
 *
 * In most situations flushing the entire workqueue is overkill; you merely
 * need to know that a particular work item isn't queued and isn't running.
 * In such cases you should use cancel_delayed_work_sync() or
 * cancel_work_sync() instead.
 */
static inline __attribute__((no_instrument_function)) void Model1_flush_scheduled_work(void)
{
 Model1_flush_workqueue(Model1_system_wq);
}

/**
 * schedule_delayed_work_on - queue work in global workqueue on CPU after delay
 * @cpu: cpu to use
 * @dwork: job to be done
 * @delay: number of jiffies to wait
 *
 * After waiting for a given time this puts a job in the kernel-global
 * workqueue on the specified CPU.
 */
static inline __attribute__((no_instrument_function)) bool Model1_schedule_delayed_work_on(int Model1_cpu, struct Model1_delayed_work *Model1_dwork,
         unsigned long Model1_delay)
{
 return Model1_queue_delayed_work_on(Model1_cpu, Model1_system_wq, Model1_dwork, Model1_delay);
}

/**
 * schedule_delayed_work - put work task in global workqueue after delay
 * @dwork: job to be done
 * @delay: number of jiffies to wait or 0 for immediate execution
 *
 * After waiting for a given time this puts a job in the kernel-global
 * workqueue.
 */
static inline __attribute__((no_instrument_function)) bool Model1_schedule_delayed_work(struct Model1_delayed_work *Model1_dwork,
      unsigned long Model1_delay)
{
 return Model1_queue_delayed_work(Model1_system_wq, Model1_dwork, Model1_delay);
}

/**
 * keventd_up - is workqueue initialized yet?
 */
static inline __attribute__((no_instrument_function)) bool Model1_keventd_up(void)
{
 return Model1_system_wq != ((void *)0);
}







long Model1_work_on_cpu(int Model1_cpu, long (*Model1_fn)(void *), void *Model1_arg);



extern void Model1_freeze_workqueues_begin(void);
extern bool Model1_freeze_workqueues_busy(void);
extern void Model1_thaw_workqueues(void);



int Model1_workqueue_sysfs_register(struct Model1_workqueue_struct *Model1_wq);
static inline __attribute__((no_instrument_function)) void Model1_wq_watchdog_touch(int Model1_cpu) { }



int Model1_workqueue_prepare_cpu(unsigned int Model1_cpu);
int Model1_workqueue_online_cpu(unsigned int Model1_cpu);
int Model1_workqueue_offline_cpu(unsigned int Model1_cpu);

struct Model1_srcu_struct_array {
 unsigned long Model1_c[2];
 unsigned long Model1_seq[2];
};

struct Model1_rcu_batch {
 struct Model1_callback_head *Model1_head, **Model1_tail;
};



struct Model1_srcu_struct {
 unsigned long Model1_completed;
 struct Model1_srcu_struct_array *Model1_per_cpu_ref;
 Model1_spinlock_t Model1_queue_lock; /* protect ->batch_queue, ->running */
 bool Model1_running;
 /* callbacks just queued */
 struct Model1_rcu_batch Model1_batch_queue;
 /* callbacks try to do the first check_zero */
 struct Model1_rcu_batch Model1_batch_check0;
 /* callbacks done with the first check_zero and the flip */
 struct Model1_rcu_batch Model1_batch_check1;
 struct Model1_rcu_batch Model1_batch_done;
 struct Model1_delayed_work Model1_work;



};
int Model1_init_srcu_struct(struct Model1_srcu_struct *Model1_sp);




void Model1_process_srcu(struct Model1_work_struct *Model1_work);
/*
 * Define and initialize a srcu struct at build time.
 * Do -not- call init_srcu_struct() nor cleanup_srcu_struct() on it.
 *
 * Note that although DEFINE_STATIC_SRCU() hides the name from other
 * files, the per-CPU variable rules nevertheless require that the
 * chosen name be globally unique.  These rules also prohibit use of
 * DEFINE_STATIC_SRCU() within a function.  If these rules are too
 * restrictive, declare the srcu_struct manually.  For example, in
 * each file:
 *
 *	static struct srcu_struct my_srcu;
 *
 * Then, before the first use of each my_srcu, manually initialize it:
 *
 *	init_srcu_struct(&my_srcu);
 *
 * See include/linux/percpu-defs.h for the rules on per-CPU variables.
 */






/**
 * call_srcu() - Queue a callback for invocation after an SRCU grace period
 * @sp: srcu_struct in queue the callback
 * @head: structure to be used for queueing the SRCU callback.
 * @func: function to be invoked after the SRCU grace period
 *
 * The callback function will be invoked some time after a full SRCU
 * grace period elapses, in other words after all pre-existing SRCU
 * read-side critical sections have completed.  However, the callback
 * function might well execute concurrently with other SRCU read-side
 * critical sections that started after call_srcu() was invoked.  SRCU
 * read-side critical sections are delimited by srcu_read_lock() and
 * srcu_read_unlock(), and may be nested.
 *
 * The callback will be invoked from process context, but must nevertheless
 * be fast and must not block.
 */
void Model1_call_srcu(struct Model1_srcu_struct *Model1_sp, struct Model1_callback_head *Model1_head,
  void (*func)(struct Model1_callback_head *Model1_head));

void Model1_cleanup_srcu_struct(struct Model1_srcu_struct *Model1_sp);
int Model1___srcu_read_lock(struct Model1_srcu_struct *Model1_sp) ;
void Model1___srcu_read_unlock(struct Model1_srcu_struct *Model1_sp, int Model1_idx) ;
void Model1_synchronize_srcu(struct Model1_srcu_struct *Model1_sp);
void Model1_synchronize_srcu_expedited(struct Model1_srcu_struct *Model1_sp);
unsigned long Model1_srcu_batches_completed(struct Model1_srcu_struct *Model1_sp);
void Model1_srcu_barrier(struct Model1_srcu_struct *Model1_sp);
static inline __attribute__((no_instrument_function)) int Model1_srcu_read_lock_held(struct Model1_srcu_struct *Model1_sp)
{
 return 1;
}



/**
 * srcu_dereference_check - fetch SRCU-protected pointer for later dereferencing
 * @p: the pointer to fetch and protect for later dereferencing
 * @sp: pointer to the srcu_struct, which is used to check that we
 *	really are in an SRCU read-side critical section.
 * @c: condition to check for update-side use
 *
 * If PROVE_RCU is enabled, invoking this outside of an RCU read-side
 * critical section will result in an RCU-lockdep splat, unless @c evaluates
 * to 1.  The @c argument will normally be a logical expression containing
 * lockdep_is_held() calls.
 */



/**
 * srcu_dereference - fetch SRCU-protected pointer for later dereferencing
 * @p: the pointer to fetch and protect for later dereferencing
 * @sp: pointer to the srcu_struct, which is used to check that we
 *	really are in an SRCU read-side critical section.
 *
 * Makes rcu_dereference_check() do the dirty work.  If PROVE_RCU
 * is enabled, invoking this outside of an RCU read-side critical
 * section will result in an RCU-lockdep splat.
 */


/**
 * srcu_read_lock - register a new reader for an SRCU-protected structure.
 * @sp: srcu_struct in which to register the new reader.
 *
 * Enter an SRCU read-side critical section.  Note that SRCU read-side
 * critical sections may be nested.  However, it is illegal to
 * call anything that waits on an SRCU grace period for the same
 * srcu_struct, whether directly or indirectly.  Please note that
 * one way to indirectly wait on an SRCU grace period is to acquire
 * a mutex that is held elsewhere while calling synchronize_srcu() or
 * synchronize_srcu_expedited().
 *
 * Note that srcu_read_lock() and the matching srcu_read_unlock() must
 * occur in the same context, for example, it is illegal to invoke
 * srcu_read_unlock() in an irq handler if the matching srcu_read_lock()
 * was invoked in process context.
 */
static inline __attribute__((no_instrument_function)) int Model1_srcu_read_lock(struct Model1_srcu_struct *Model1_sp)
{
 int Model1_retval;

 __asm__ __volatile__("": : :"memory");
 Model1_retval = Model1___srcu_read_lock(Model1_sp);
 __asm__ __volatile__("": : :"memory");
 do { } while (0);
 return Model1_retval;
}

/**
 * srcu_read_unlock - unregister a old reader from an SRCU-protected structure.
 * @sp: srcu_struct in which to unregister the old reader.
 * @idx: return value from corresponding srcu_read_lock().
 *
 * Exit an SRCU read-side critical section.
 */
static inline __attribute__((no_instrument_function)) void Model1_srcu_read_unlock(struct Model1_srcu_struct *Model1_sp, int Model1_idx)

{
 do { } while (0);
 Model1___srcu_read_unlock(Model1_sp, Model1_idx);
}

/**
 * smp_mb__after_srcu_read_unlock - ensure full ordering after srcu_read_unlock
 *
 * Converts the preceding srcu_read_unlock into a two-way memory barrier.
 *
 * Call this after srcu_read_unlock, to guarantee that all memory operations
 * that occur after smp_mb__after_srcu_read_unlock will appear to happen after
 * the preceding srcu_read_unlock.
 */
static inline __attribute__((no_instrument_function)) void Model1_smp_mb__after_srcu_read_unlock(void)
{
 /* __srcu_read_unlock has smp_mb() internally so nothing to do here. */
}

/*
 * Notifier chains are of four types:
 *
 *	Atomic notifier chains: Chain callbacks run in interrupt/atomic
 *		context. Callouts are not allowed to block.
 *	Blocking notifier chains: Chain callbacks run in process context.
 *		Callouts are allowed to block.
 *	Raw notifier chains: There are no restrictions on callbacks,
 *		registration, or unregistration.  All locking and protection
 *		must be provided by the caller.
 *	SRCU notifier chains: A variant of blocking notifier chains, with
 *		the same restrictions.
 *
 * atomic_notifier_chain_register() may be called from an atomic context,
 * but blocking_notifier_chain_register() and srcu_notifier_chain_register()
 * must be called from a process context.  Ditto for the corresponding
 * _unregister() routines.
 *
 * atomic_notifier_chain_unregister(), blocking_notifier_chain_unregister(),
 * and srcu_notifier_chain_unregister() _must not_ be called from within
 * the call chain.
 *
 * SRCU notifier chains are an alternative form of blocking notifier chains.
 * They use SRCU (Sleepable Read-Copy Update) instead of rw-semaphores for
 * protection of the chain links.  This means there is _very_ low overhead
 * in srcu_notifier_call_chain(): no cache bounces and no memory barriers.
 * As compensation, srcu_notifier_chain_unregister() is rather expensive.
 * SRCU notifier chains should be used when the chain will be called very
 * often but notifier_blocks will seldom be removed.  Also, SRCU notifier
 * chains are slightly more difficult to use because they require special
 * runtime initialization.
 */

struct Model1_notifier_block;

typedef int (*Model1_notifier_fn_t)(struct Model1_notifier_block *Model1_nb,
   unsigned long Model1_action, void *Model1_data);

struct Model1_notifier_block {
 Model1_notifier_fn_t Model1_notifier_call;
 struct Model1_notifier_block *Model1_next;
 int Model1_priority;
};

struct Model1_atomic_notifier_head {
 Model1_spinlock_t Model1_lock;
 struct Model1_notifier_block *Model1_head;
};

struct Model1_blocking_notifier_head {
 struct Model1_rw_semaphore Model1_rwsem;
 struct Model1_notifier_block *Model1_head;
};

struct Model1_raw_notifier_head {
 struct Model1_notifier_block *Model1_head;
};

struct Model1_srcu_notifier_head {
 struct Model1_mutex Model1_mutex;
 struct Model1_srcu_struct Model1_srcu;
 struct Model1_notifier_block *Model1_head;
};
/* srcu_notifier_heads must be initialized and cleaned up dynamically */
extern void Model1_srcu_init_notifier_head(struct Model1_srcu_notifier_head *Model1_nh);
/* srcu_notifier_heads cannot be initialized statically */
extern int Model1_atomic_notifier_chain_register(struct Model1_atomic_notifier_head *Model1_nh,
  struct Model1_notifier_block *Model1_nb);
extern int Model1_blocking_notifier_chain_register(struct Model1_blocking_notifier_head *Model1_nh,
  struct Model1_notifier_block *Model1_nb);
extern int Model1_raw_notifier_chain_register(struct Model1_raw_notifier_head *Model1_nh,
  struct Model1_notifier_block *Model1_nb);
extern int Model1_srcu_notifier_chain_register(struct Model1_srcu_notifier_head *Model1_nh,
  struct Model1_notifier_block *Model1_nb);

extern int Model1_blocking_notifier_chain_cond_register(
  struct Model1_blocking_notifier_head *Model1_nh,
  struct Model1_notifier_block *Model1_nb);

extern int Model1_atomic_notifier_chain_unregister(struct Model1_atomic_notifier_head *Model1_nh,
  struct Model1_notifier_block *Model1_nb);
extern int Model1_blocking_notifier_chain_unregister(struct Model1_blocking_notifier_head *Model1_nh,
  struct Model1_notifier_block *Model1_nb);
extern int Model1_raw_notifier_chain_unregister(struct Model1_raw_notifier_head *Model1_nh,
  struct Model1_notifier_block *Model1_nb);
extern int Model1_srcu_notifier_chain_unregister(struct Model1_srcu_notifier_head *Model1_nh,
  struct Model1_notifier_block *Model1_nb);

extern int Model1_atomic_notifier_call_chain(struct Model1_atomic_notifier_head *Model1_nh,
  unsigned long Model1_val, void *Model1_v);
extern int Model1___atomic_notifier_call_chain(struct Model1_atomic_notifier_head *Model1_nh,
 unsigned long Model1_val, void *Model1_v, int Model1_nr_to_call, int *Model1_nr_calls);
extern int Model1_blocking_notifier_call_chain(struct Model1_blocking_notifier_head *Model1_nh,
  unsigned long Model1_val, void *Model1_v);
extern int Model1___blocking_notifier_call_chain(struct Model1_blocking_notifier_head *Model1_nh,
 unsigned long Model1_val, void *Model1_v, int Model1_nr_to_call, int *Model1_nr_calls);
extern int Model1_raw_notifier_call_chain(struct Model1_raw_notifier_head *Model1_nh,
  unsigned long Model1_val, void *Model1_v);
extern int Model1___raw_notifier_call_chain(struct Model1_raw_notifier_head *Model1_nh,
 unsigned long Model1_val, void *Model1_v, int Model1_nr_to_call, int *Model1_nr_calls);
extern int Model1_srcu_notifier_call_chain(struct Model1_srcu_notifier_head *Model1_nh,
  unsigned long Model1_val, void *Model1_v);
extern int Model1___srcu_notifier_call_chain(struct Model1_srcu_notifier_head *Model1_nh,
 unsigned long Model1_val, void *Model1_v, int Model1_nr_to_call, int *Model1_nr_calls);





      /* Bad/Veto action */
/*
 * Clean way to return from the notifier and stop further calls.
 */


/* Encapsulate (negative) errno value (in particular, NOTIFY_BAD <=> EPERM). */
static inline __attribute__((no_instrument_function)) int Model1_notifier_from_errno(int err)
{
 if (err)
  return 0x8000 | (0x0001 - err);

 return 0x0001;
}

/* Restore (negative) errno value from notify return value. */
static inline __attribute__((no_instrument_function)) int Model1_notifier_to_errno(int Model1_ret)
{
 Model1_ret &= ~0x8000;
 return Model1_ret > 0x0001 ? 0x0001 - Model1_ret : 0;
}

/*
 *	Declared notifiers so far. I can imagine quite a few more chains
 *	over time (eg laptop power reset chains, reboot chain (to clean 
 *	device units up), device [un]mount chain, module load/unload chain,
 *	low memory chain, screenblank chain (for plug in modular screenblankers) 
 *	VC switch chains (for loadable kernel svgalib VC switch helpers) etc...
 */

/* CPU notfiers are defined in include/linux/cpu.h. */

/* netdevice notifiers are defined in include/linux/netdevice.h */

/* reboot notifiers are defined in include/linux/reboot.h. */

/* Hibernation and suspend events are defined in include/linux/suspend.h. */

/* Virtual Terminal events are defined in include/linux/vt.h. */



/* Console keyboard events.
 * Note: KBD_KEYCODE is always sent before KBD_UNBOUND_KEYCODE, KBD_UNICODE and
 * KBD_KEYSYM. */






extern struct Model1_blocking_notifier_head Model1_reboot_notifier_list;


struct Model1_page;
struct Model1_zone;
struct Model1_pglist_data;
struct Model1_mem_section;
struct Model1_memory_block;
struct Model1_resource;
/*
 * Stub functions for when hotplug is off
 */
static inline __attribute__((no_instrument_function)) void Model1_pgdat_resize_lock(struct Model1_pglist_data *Model1_p, unsigned long *Model1_f) {}
static inline __attribute__((no_instrument_function)) void Model1_pgdat_resize_unlock(struct Model1_pglist_data *Model1_p, unsigned long *Model1_f) {}
static inline __attribute__((no_instrument_function)) void Model1_pgdat_resize_init(struct Model1_pglist_data *Model1_pgdat) {}

static inline __attribute__((no_instrument_function)) unsigned Model1_zone_span_seqbegin(struct Model1_zone *Model1_zone)
{
 return 0;
}
static inline __attribute__((no_instrument_function)) int Model1_zone_span_seqretry(struct Model1_zone *Model1_zone, unsigned Model1_iv)
{
 return 0;
}
static inline __attribute__((no_instrument_function)) void Model1_zone_span_writelock(struct Model1_zone *Model1_zone) {}
static inline __attribute__((no_instrument_function)) void Model1_zone_span_writeunlock(struct Model1_zone *Model1_zone) {}
static inline __attribute__((no_instrument_function)) void Model1_zone_seqlock_init(struct Model1_zone *Model1_zone) {}

static inline __attribute__((no_instrument_function)) int Model1_mhp_notimplemented(const char *func)
{
 Model1_printk("\001" "4" "%s() called, with CONFIG_MEMORY_HOTPLUG disabled\n", func);
 Model1_dump_stack();
 return -38;
}

static inline __attribute__((no_instrument_function)) void Model1_register_page_bootmem_info_node(struct Model1_pglist_data *Model1_pgdat)
{
}

static inline __attribute__((no_instrument_function)) int Model1_try_online_node(int Model1_nid)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model1_get_online_mems(void) {}
static inline __attribute__((no_instrument_function)) void Model1_put_online_mems(void) {}

static inline __attribute__((no_instrument_function)) void Model1_mem_hotplug_begin(void) {}
static inline __attribute__((no_instrument_function)) void Model1_mem_hotplug_done(void) {}
static inline __attribute__((no_instrument_function)) bool Model1_is_mem_section_removable(unsigned long Model1_pfn,
     unsigned long Model1_nr_pages)
{
 return false;
}

static inline __attribute__((no_instrument_function)) void Model1_try_offline_node(int Model1_nid) {}

static inline __attribute__((no_instrument_function)) int Model1_offline_pages(unsigned long Model1_start_pfn, unsigned long Model1_nr_pages)
{
 return -22;
}

static inline __attribute__((no_instrument_function)) void Model1_remove_memory(int Model1_nid, Model1_u64 Model1_start, Model1_u64 Model1_size) {}


extern int Model1_walk_memory_range(unsigned long Model1_start_pfn, unsigned long Model1_end_pfn,
  void *Model1_arg, int (*func)(struct Model1_memory_block *, void *));
extern int Model1_add_memory(int Model1_nid, Model1_u64 Model1_start, Model1_u64 Model1_size);
extern int Model1_add_memory_resource(int Model1_nid, struct Model1_resource *Model1_resource, bool Model1_online);
extern int Model1_zone_for_memory(int Model1_nid, Model1_u64 Model1_start, Model1_u64 Model1_size, int Model1_zone_default,
  bool Model1_for_device);
extern int Model1_arch_add_memory(int Model1_nid, Model1_u64 Model1_start, Model1_u64 Model1_size, bool Model1_for_device);
extern int Model1_offline_pages(unsigned long Model1_start_pfn, unsigned long Model1_nr_pages);
extern bool Model1_is_memblock_offlined(struct Model1_memory_block *Model1_mem);
extern void Model1_remove_memory(int Model1_nid, Model1_u64 Model1_start, Model1_u64 Model1_size);
extern int Model1_sparse_add_one_section(struct Model1_zone *Model1_zone, unsigned long Model1_start_pfn);
extern void Model1_sparse_remove_one_section(struct Model1_zone *Model1_zone, struct Model1_mem_section *Model1_ms,
  unsigned long Model1_map_offset);
extern struct Model1_page *Model1_sparse_decode_mem_map(unsigned long Model1_coded_mem_map,
       unsigned long Model1_pnum);
extern int Model1_zone_can_shift(unsigned long Model1_pfn, unsigned long Model1_nr_pages,
     enum Model1_zone_type Model1_target);

extern struct Model1_mutex Model1_zonelists_mutex;
void Model1_build_all_zonelists(Model1_pg_data_t *Model1_pgdat, struct Model1_zone *Model1_zone);
void Model1_wakeup_kswapd(struct Model1_zone *Model1_zone, int Model1_order, enum Model1_zone_type Model1_classzone_idx);
bool Model1___zone_watermark_ok(struct Model1_zone *Model1_z, unsigned int Model1_order, unsigned long Model1_mark,
    int Model1_classzone_idx, unsigned int Model1_alloc_flags,
    long Model1_free_pages);
bool Model1_zone_watermark_ok(struct Model1_zone *Model1_z, unsigned int Model1_order,
  unsigned long Model1_mark, int Model1_classzone_idx,
  unsigned int Model1_alloc_flags);
bool Model1_zone_watermark_ok_safe(struct Model1_zone *Model1_z, unsigned int Model1_order,
  unsigned long Model1_mark, int Model1_classzone_idx);
enum Model1_memmap_context {
 Model1_MEMMAP_EARLY,
 Model1_MEMMAP_HOTPLUG,
};
extern int Model1_init_currently_empty_zone(struct Model1_zone *Model1_zone, unsigned long Model1_start_pfn,
         unsigned long Model1_size);

extern void Model1_lruvec_init(struct Model1_lruvec *Model1_lruvec);

static inline __attribute__((no_instrument_function)) struct Model1_pglist_data *Model1_lruvec_pgdat(struct Model1_lruvec *Model1_lruvec)
{



 return ({ const typeof( ((struct Model1_pglist_data *)0)->Model1_lruvec ) *Model1___mptr = (Model1_lruvec); (struct Model1_pglist_data *)( (char *)Model1___mptr - __builtin_offsetof(struct Model1_pglist_data, Model1_lruvec) );});

}

extern unsigned long Model1_lruvec_lru_size(struct Model1_lruvec *Model1_lruvec, enum Model1_lru_list Model1_lru);


void Model1_memory_present(int Model1_nid, unsigned long Model1_start, unsigned long Model1_end);







static inline __attribute__((no_instrument_function)) int Model1_local_memory_node(int Model1_node_id) { return Model1_node_id; };






/*
 * zone_idx() returns 0 for the ZONE_DMA zone, 1 for the ZONE_NORMAL zone, etc.
 */


/*
 * Returns true if a zone has pages managed by the buddy allocator.
 * All the reclaim decisions have to use this function rather than
 * populated_zone(). If the whole zone is reserved then we can easily
 * end up with populated_zone() && !managed_zone().
 */
static inline __attribute__((no_instrument_function)) bool Model1_managed_zone(struct Model1_zone *Model1_zone)
{
 return Model1_zone->Model1_managed_pages;
}

/* Returns true if a zone has memory */
static inline __attribute__((no_instrument_function)) bool Model1_populated_zone(struct Model1_zone *Model1_zone)
{
 return Model1_zone->Model1_present_pages;
}

extern int Model1_movable_zone;
static inline __attribute__((no_instrument_function)) int Model1_is_highmem_idx(enum Model1_zone_type Model1_idx)
{




 return 0;

}

/**
 * is_highmem - helper function to quickly check if a struct zone is a 
 *              highmem zone or not.  This is an attempt to keep references
 *              to ZONE_{DMA/NORMAL/HIGHMEM/etc} in general code to a minimum.
 * @zone - pointer to struct zone variable
 */
static inline __attribute__((no_instrument_function)) int Model1_is_highmem(struct Model1_zone *Model1_zone)
{



 return 0;

}

/* These two functions are used to setup the per zone pages min values */
struct Model1_ctl_table;
int Model1_min_free_kbytes_sysctl_handler(struct Model1_ctl_table *, int,
     void *, Model1_size_t *, Model1_loff_t *);
int Model1_watermark_scale_factor_sysctl_handler(struct Model1_ctl_table *, int,
     void *, Model1_size_t *, Model1_loff_t *);
extern int Model1_sysctl_lowmem_reserve_ratio[4 -1];
int Model1_lowmem_reserve_ratio_sysctl_handler(struct Model1_ctl_table *, int,
     void *, Model1_size_t *, Model1_loff_t *);
int Model1_percpu_pagelist_fraction_sysctl_handler(struct Model1_ctl_table *, int,
     void *, Model1_size_t *, Model1_loff_t *);
int Model1_sysctl_min_unmapped_ratio_sysctl_handler(struct Model1_ctl_table *, int,
   void *, Model1_size_t *, Model1_loff_t *);
int Model1_sysctl_min_slab_ratio_sysctl_handler(struct Model1_ctl_table *, int,
   void *, Model1_size_t *, Model1_loff_t *);

extern int Model1_numa_zonelist_order_handler(struct Model1_ctl_table *, int,
   void *, Model1_size_t *, Model1_loff_t *);
extern char Model1_numa_zonelist_order[];



/* K8 NUMA support */
/* Copyright 2002,2003 by Andi Kleen, SuSE Labs */
/* 2.5 Version loosely based on the NUMAQ Code by Pat Gaughen. */













/*
 * We need the APIC definitions automatically as part of 'smp.h'
 */









/*
 * Structure definitions for SMP machines following the
 * Intel Multiprocessing Specification 1.1 and 1.4.
 */

/*
 * This tag identifies where the SMP configuration
 * information is.
 */







/* Intel MP Floating Pointer Structure */
struct Model1_mpf_intel {
 char Model1_signature[4]; /* "_MP_"			*/
 unsigned int Model1_physptr; /* Configuration table address	*/
 unsigned char Model1_length; /* Our length (paragraphs)	*/
 unsigned char Model1_specification; /* Specification version	*/
 unsigned char Model1_checksum; /* Checksum (makes sum 0)	*/
 unsigned char Model1_feature1; /* Standard or configuration ?	*/
 unsigned char Model1_feature2; /* Bit7 set for IMCR|PIC	*/
 unsigned char Model1_feature3; /* Unused (0)			*/
 unsigned char Model1_feature4; /* Unused (0)			*/
 unsigned char Model1_feature5; /* Unused (0)			*/
};



struct Model1_mpc_table {
 char Model1_signature[4];
 unsigned short Model1_length; /* Size of table */
 char Model1_spec; /* 0x01 */
 char Model1_checksum;
 char Model1_oem[8];
 char Model1_productid[12];
 unsigned int Model1_oemptr; /* 0 if not present */
 unsigned short Model1_oemsize; /* 0 if not present */
 unsigned short Model1_oemcount;
 unsigned int Model1_lapic; /* APIC address */
 unsigned int Model1_reserved;
};

/* Followed by entries */






/* Used by IBM NUMA-Q to describe node locality */
struct Model1_mpc_cpu {
 unsigned char Model1_type;
 unsigned char Model1_apicid; /* Local APIC number */
 unsigned char Model1_apicver; /* Its versions */
 unsigned char Model1_cpuflag;
 unsigned int Model1_cpufeature;
 unsigned int Model1_featureflag; /* CPUID feature value */
 unsigned int Model1_reserved[2];
};

struct Model1_mpc_bus {
 unsigned char Model1_type;
 unsigned char Model1_busid;
 unsigned char Model1_bustype[6];
};

/* List of Bus Type string values, Intel MP Spec. */
struct Model1_mpc_ioapic {
 unsigned char Model1_type;
 unsigned char Model1_apicid;
 unsigned char Model1_apicver;
 unsigned char Model1_flags;
 unsigned int Model1_apicaddr;
};

struct Model1_mpc_intsrc {
 unsigned char Model1_type;
 unsigned char Model1_irqtype;
 unsigned short Model1_irqflag;
 unsigned char Model1_srcbus;
 unsigned char Model1_srcbusirq;
 unsigned char Model1_dstapic;
 unsigned char Model1_dstirq;
};

enum Model1_mp_irq_source_types {
 Model1_mp_INT = 0,
 Model1_mp_NMI = 1,
 Model1_mp_SMI = 2,
 Model1_mp_ExtINT = 3
};







struct Model1_mpc_lintsrc {
 unsigned char Model1_type;
 unsigned char Model1_irqtype;
 unsigned short Model1_irqflag;
 unsigned char Model1_srcbusid;
 unsigned char Model1_srcbusirq;
 unsigned char Model1_destapic;
 unsigned char Model1_destapiclint;
};



struct Model1_mpc_oemtable {
 char Model1_signature[4];
 unsigned short Model1_length; /* Size of table */
 char Model1_rev; /* 0x01 */
 char Model1_checksum;
 char Model1_mpc[8];
};

/*
 *	Default configurations
 *
 *	1	2 CPU ISA 82489DX
 *	2	2 CPU EISA 82489DX neither IRQ 0 timer nor IRQ 13 DMA chaining
 *	3	2 CPU EISA 82489DX
 *	4	2 CPU MCA 82489DX
 *	5	2 CPU ISA+PCI
 *	6	2 CPU EISA+PCI
 *	7	2 CPU MCA+PCI
 */

enum Model1_mp_bustype {
 Model1_MP_BUS_ISA = 1,
 Model1_MP_BUS_EISA,
 Model1_MP_BUS_PCI,
};






/* setup_data types */






/* ram_size flags */




/* loadflags */






/* xloadflags */








/*
 * These are set up by the setup-routine at boot-time:
 */

struct Model1_screen_info {
 __u8 Model1_orig_x; /* 0x00 */
 __u8 Model1_orig_y; /* 0x01 */
 Model1___u16 Model1_ext_mem_k; /* 0x02 */
 Model1___u16 Model1_orig_video_page; /* 0x04 */
 __u8 Model1_orig_video_mode; /* 0x06 */
 __u8 Model1_orig_video_cols; /* 0x07 */
 __u8 Model1_flags; /* 0x08 */
 __u8 Model1_unused2; /* 0x09 */
 Model1___u16 Model1_orig_video_ega_bx;/* 0x0a */
 Model1___u16 Model1_unused3; /* 0x0c */
 __u8 Model1_orig_video_lines; /* 0x0e */
 __u8 Model1_orig_video_isVGA; /* 0x0f */
 Model1___u16 Model1_orig_video_points;/* 0x10 */

 /* VESA graphic mode -- linear frame buffer */
 Model1___u16 Model1_lfb_width; /* 0x12 */
 Model1___u16 Model1_lfb_height; /* 0x14 */
 Model1___u16 Model1_lfb_depth; /* 0x16 */
 __u32 Model1_lfb_base; /* 0x18 */
 __u32 Model1_lfb_size; /* 0x1c */
 Model1___u16 Model1_cl_magic, Model1_cl_offset; /* 0x20 */
 Model1___u16 Model1_lfb_linelength; /* 0x24 */
 __u8 Model1_red_size; /* 0x26 */
 __u8 Model1_red_pos; /* 0x27 */
 __u8 Model1_green_size; /* 0x28 */
 __u8 Model1_green_pos; /* 0x29 */
 __u8 Model1_blue_size; /* 0x2a */
 __u8 Model1_blue_pos; /* 0x2b */
 __u8 Model1_rsvd_size; /* 0x2c */
 __u8 Model1_rsvd_pos; /* 0x2d */
 Model1___u16 Model1_vesapm_seg; /* 0x2e */
 Model1___u16 Model1_vesapm_off; /* 0x30 */
 Model1___u16 Model1_pages; /* 0x32 */
 Model1___u16 Model1_vesa_attributes; /* 0x34 */
 __u32 Model1_capabilities; /* 0x36 */
 __u32 Model1_ext_lfb_base; /* 0x3a */
 __u8 Model1__reserved[2]; /* 0x3e */
} __attribute__((packed));

extern struct Model1_screen_info Model1_screen_info;
/*
 * Include file for the interface to an APM BIOS
 * Copyright 1994-2001 Stephen Rothwell (sfr@canb.auug.org.au)
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the
 * Free Software Foundation; either version 2, or (at your option) any
 * later version.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 * General Public License for more details.
 */




/*
 * Include file for the interface to an APM BIOS
 * Copyright 1994-2001 Stephen Rothwell (sfr@canb.auug.org.au)
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the
 * Free Software Foundation; either version 2, or (at your option) any
 * later version.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 * General Public License for more details.
 */






typedef unsigned short Model1_apm_event_t;
typedef unsigned short Model1_apm_eventinfo_t;

struct Model1_apm_bios_info {
 Model1___u16 Model1_version;
 Model1___u16 Model1_cseg;
 __u32 Model1_offset;
 Model1___u16 Model1_cseg_16;
 Model1___u16 Model1_dseg;
 Model1___u16 Model1_flags;
 Model1___u16 Model1_cseg_len;
 Model1___u16 Model1_cseg_16_len;
 Model1___u16 Model1_dseg_len;
};


/*
 * Power states
 */
/*
 * Events (results of Get PM Event)
 */
/*
 * Error codes
 */
/*
 * APM Device IDs
 */
/*
 * Battery status
 */


/*
 * APM defined capability bit flags
 */
/*
 * ioctl operations
 */






/* Results of APM Installation Check */






/*
 * Data for APM that is persistent across module unload/load
 */
struct Model1_apm_info {
 struct Model1_apm_bios_info Model1_bios;
 unsigned short Model1_connection_version;
 int Model1_get_power_status_broken;
 int Model1_get_power_status_swabinminutes;
 int Model1_allow_ints;
 int Model1_forbid_idle;
 int Model1_realmode_power_off;
 int Model1_disabled;
};

/*
 * The APM function codes
 */
/*
 * Function code for APM_FUNC_RESUME_TIMER
 */




/*
 * Function code for APM_FUNC_RESUME_ON_RING
 */




/*
 * Function code for APM_FUNC_TIMER_STATUS
 */




/*
 * in arch/i386/kernel/setup.c
 */
extern struct Model1_apm_info Model1_apm_info;

/*
 * This is the "All Devices" ID communicated to the BIOS
 */
/*
 * linux/include/linux/edd.h
 *  Copyright (C) 2002, 2003, 2004 Dell Inc.
 *  by Matt Domsch <Matt_Domsch@dell.com>
 *
 * structures and definitions for the int 13h, ax={41,48}h
 * BIOS Enhanced Disk Drive Services
 * This is based on the T13 group document D1572 Revision 0 (August 14 2002)
 * available at http://www.t13.org/docs2002/d1572r0.pdf.  It is
 * very similar to D1484 Revision 3 http://www.t13.org/docs2002/d1484r3.pdf
 *
 * In a nutshell, arch/{i386,x86_64}/boot/setup.S populates a scratch
 * table in the boot_params that contains a list of BIOS-enumerated
 * boot devices.
 * In arch/{i386,x86_64}/kernel/setup.c, this information is
 * transferred into the edd structure, and in drivers/firmware/edd.c, that
 * information is used to identify BIOS boot disk.  The code in setup.S
 * is very sensitive to the size of these structures.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License v2.0 as published by
 * the Free Software Foundation
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 */




/*
 * linux/include/linux/edd.h
 *  Copyright (C) 2002, 2003, 2004 Dell Inc.
 *  by Matt Domsch <Matt_Domsch@dell.com>
 *
 * structures and definitions for the int 13h, ax={41,48}h
 * BIOS Enhanced Disk Drive Services
 * This is based on the T13 group document D1572 Revision 0 (August 14 2002)
 * available at http://www.t13.org/docs2002/d1572r0.pdf.  It is
 * very similar to D1484 Revision 3 http://www.t13.org/docs2002/d1484r3.pdf
 *
 * In a nutshell, arch/{i386,x86_64}/boot/setup.S populates a scratch
 * table in the boot_params that contains a list of BIOS-enumerated
 * boot devices.
 * In arch/{i386,x86_64}/kernel/setup.c, this information is
 * transferred into the edd structure, and in drivers/firmware/edd.c, that
 * information is used to identify BIOS boot disk.  The code in setup.S
 * is very sensitive to the size of these structures.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License v2.0 as published by
 * the Free Software Foundation
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 */
struct Model1_edd_device_params {
 Model1___u16 Model1_length;
 Model1___u16 Model1_info_flags;
 __u32 Model1_num_default_cylinders;
 __u32 Model1_num_default_heads;
 __u32 Model1_sectors_per_track;
 __u64 Model1_number_of_sectors;
 Model1___u16 Model1_bytes_per_sector;
 __u32 Model1_dpte_ptr; /* 0xFFFFFFFF for our purposes */
 Model1___u16 Model1_key; /* = 0xBEDD */
 __u8 Model1_device_path_info_length; /* = 44 */
 __u8 Model1_reserved2;
 Model1___u16 Model1_reserved3;
 __u8 Model1_host_bus_type[4];
 __u8 Model1_interface_type[8];
 union {
  struct {
   Model1___u16 Model1_base_address;
   Model1___u16 Model1_reserved1;
   __u32 Model1_reserved2;
  } __attribute__ ((packed)) Model1_isa;
  struct {
   __u8 Model1_bus;
   __u8 Model1_slot;
   __u8 Model1_function;
   __u8 Model1_channel;
   __u32 Model1_reserved;
  } __attribute__ ((packed)) Model1_pci;
  /* pcix is same as pci */
  struct {
   __u64 Model1_reserved;
  } __attribute__ ((packed)) Model1_ibnd;
  struct {
   __u64 Model1_reserved;
  } __attribute__ ((packed)) Model1_xprs;
  struct {
   __u64 Model1_reserved;
  } __attribute__ ((packed)) Model1_htpt;
  struct {
   __u64 Model1_reserved;
  } __attribute__ ((packed)) Model1_unknown;
 } Model1_interface_path;
 union {
  struct {
   __u8 Model1_device;
   __u8 Model1_reserved1;
   Model1___u16 Model1_reserved2;
   __u32 Model1_reserved3;
   __u64 Model1_reserved4;
  } __attribute__ ((packed)) Model1_ata;
  struct {
   __u8 Model1_device;
   __u8 Model1_lun;
   __u8 Model1_reserved1;
   __u8 Model1_reserved2;
   __u32 Model1_reserved3;
   __u64 Model1_reserved4;
  } __attribute__ ((packed)) Model1_atapi;
  struct {
   Model1___u16 Model1_id;
   __u64 Model1_lun;
   Model1___u16 Model1_reserved1;
   __u32 Model1_reserved2;
  } __attribute__ ((packed)) Model1_scsi;
  struct {
   __u64 Model1_serial_number;
   __u64 Model1_reserved;
  } __attribute__ ((packed)) Model1_usb;
  struct {
   __u64 Model1_eui;
   __u64 Model1_reserved;
  } __attribute__ ((packed)) Model1_i1394;
  struct {
   __u64 Model1_wwid;
   __u64 Model1_lun;
  } __attribute__ ((packed)) Model1_fibre;
  struct {
   __u64 Model1_identity_tag;
   __u64 Model1_reserved;
  } __attribute__ ((packed)) Model1_i2o;
  struct {
   __u32 Model1_array_number;
   __u32 Model1_reserved1;
   __u64 Model1_reserved2;
  } __attribute__ ((packed)) Model1_raid;
  struct {
   __u8 Model1_device;
   __u8 Model1_reserved1;
   Model1___u16 Model1_reserved2;
   __u32 Model1_reserved3;
   __u64 Model1_reserved4;
  } __attribute__ ((packed)) Model1_sata;
  struct {
   __u64 Model1_reserved1;
   __u64 Model1_reserved2;
  } __attribute__ ((packed)) Model1_unknown;
 } Model1_device_path;
 __u8 Model1_reserved4;
 __u8 Model1_checksum;
} __attribute__ ((packed));

struct Model1_edd_info {
 __u8 Model1_device;
 __u8 Model1_version;
 Model1___u16 Model1_interface_support;
 Model1___u16 Model1_legacy_max_cylinder;
 __u8 Model1_legacy_max_head;
 __u8 Model1_legacy_sectors_per_track;
 struct Model1_edd_device_params Model1_params;
} __attribute__ ((packed));

struct Model1_edd {
 unsigned int Model1_mbr_signature[16];
 struct Model1_edd_info Model1_edd_info[6];
 unsigned char Model1_mbr_signature_nr;
 unsigned char Model1_edd_info_nr;
};


extern struct Model1_edd Model1_edd;





/*
 * Legacy E820 BIOS limits us to 128 (E820MAX) nodes due to the
 * constrained space in the zeropage.  If we have more nodes than
 * that, and if we've booted off EFI firmware, then the EFI tables
 * passed us from the EFI firmware can list more nodes.  Size our
 * internal memory map tables to have room for these additional
 * nodes, based on up to three entries per node for which the
 * kernel was built: MAX_NUMNODES == (1 << CONFIG_NODES_SHIFT),
 * plus E820MAX, allowing space for the possible duplicate E820
 * entries that might need room in the same arrays, prior to the
 * call to sanitize_e820_map() to remove duplicates.  The allowance
 * of three memory map entries per node is "enough" entries for
 * the initial hardware platform motivating this mechanism to make
 * use of additional EFI map entries.  Future platforms may want
 * to allow more than three entries per node or otherwise refine
 * this size.
 */
/*
 * This is a non-standardized way to represent ADR or NVDIMM regions that
 * persist over a reboot.  The kernel will ignore their special capabilities
 * unless the CONFIG_X86_PMEM_LEGACY option is set.
 *
 * ( Note that older platforms also used 6 for the same type of memory,
 *   but newer versions switched to 12 as 6 was assigned differently.  Some
 *   time they will learn... )
 */


/*
 * reserved RAM used by kernel itself
 * if CONFIG_INTEL_TXT is enabled, memory of this type will be
 * included in the S3 integrity calculation and so should not include
 * any memory that BIOS might alter over the S3 transition
 */




struct Model1_e820entry {
 __u64 Model1_addr; /* start of memory segment */
 __u64 Model1_size; /* size of memory segment */
 __u32 Model1_type; /* type of memory segment */
} __attribute__((packed));

struct Model1_e820map {
 __u32 Model1_nr_map;
 struct Model1_e820entry Model1_map[(128 + 3 * (1 << 6))];
};

/* see comment in arch/x86/kernel/e820.c */
extern struct Model1_e820map Model1_e820;
extern struct Model1_e820map Model1_e820_saved;

extern unsigned long Model1_pci_mem_start;
extern int Model1_e820_any_mapped(Model1_u64 Model1_start, Model1_u64 Model1_end, unsigned Model1_type);
extern int Model1_e820_all_mapped(Model1_u64 Model1_start, Model1_u64 Model1_end, unsigned Model1_type);
extern void Model1_e820_add_region(Model1_u64 Model1_start, Model1_u64 Model1_size, int Model1_type);
extern void Model1_e820_print_map(char *Model1_who);
extern int
Model1_sanitize_e820_map(struct Model1_e820entry *Model1_biosmap, int Model1_max_nr_map, Model1_u32 *Model1_pnr_map);
extern Model1_u64 Model1_e820_update_range(Model1_u64 Model1_start, Model1_u64 Model1_size, unsigned Model1_old_type,
          unsigned Model1_new_type);
extern Model1_u64 Model1_e820_remove_range(Model1_u64 Model1_start, Model1_u64 Model1_size, unsigned Model1_old_type,
        int Model1_checktype);
extern void Model1_update_e820(void);
extern void Model1_e820_setup_gap(void);
extern int Model1_e820_search_gap(unsigned long *Model1_gapstart, unsigned long *Model1_gapsize,
   unsigned long Model1_start_addr, unsigned long long Model1_end_addr);
struct Model1_setup_data;
extern void Model1_parse_e820_ext(Model1_u64 Model1_phys_addr, Model1_u32 Model1_data_len);



extern void Model1_e820_mark_nosave_regions(unsigned long Model1_limit_pfn);






extern unsigned long Model1_e820_end_of_ram_pfn(void);
extern unsigned long Model1_e820_end_of_low_ram_pfn(void);
extern Model1_u64 Model1_early_reserve_e820(Model1_u64 Model1_sizet, Model1_u64 Model1_align);

void Model1_memblock_x86_fill(void);
void Model1_memblock_find_dma_reserve(void);

extern void Model1_finish_e820_parsing(void);
extern void Model1_e820_reserve_resources(void);
extern void Model1_e820_reserve_resources_late(void);
extern void Model1_setup_memory_map(void);
extern char *Model1_default_machine_specific_memory_setup(void);

/*
 * Returns true iff the specified range [s,e) is completely contained inside
 * the ISA region.
 */
static inline __attribute__((no_instrument_function)) bool Model1_is_ISA_range(Model1_u64 Model1_s, Model1_u64 Model1_e)
{
 return Model1_s >= 0xa0000 && Model1_e <= 0x100000;
}



/*
 * ioport.h	Definitions of routines for detecting, reserving and
 *		allocating system resources.
 *
 * Authors:	Linus Torvalds
 */







/*
 * Resources are tree-like, allowing
 * nesting etc..
 */
struct Model1_resource {
 Model1_resource_size_t Model1_start;
 Model1_resource_size_t Model1_end;
 const char *Model1_name;
 unsigned long Model1_flags;
 unsigned long Model1_desc;
 struct Model1_resource *Model1_parent, *Model1_sibling, *Model1_child;
};

/*
 * IO resources have these defined flags.
 *
 * PCI devices expose these flags to userspace in the "resource" sysfs file,
 * so don't move them.
 */
/* I/O resource extended types */


/* PnP IRQ specific bits (IORESOURCE_BITS) */







/* PnP DMA specific bits (IORESOURCE_BITS) */
/* PnP memory I/O specific bits (IORESOURCE_BITS) */
/* PnP I/O specific bits (IORESOURCE_BITS) */




/* PCI ROM control bits (IORESOURCE_BITS) */



/* PCI control bits.  Shares IORESOURCE_BITS with above PCI ROM.  */



/*
 * I/O Resource Descriptors
 *
 * Descriptors are used by walk_iomem_res_desc() and region_intersects()
 * for searching a specific resource range in the iomem table.  Assign
 * a new descriptor when a resource range supports the search interfaces.
 * Otherwise, resource.desc must be set to IORES_DESC_NONE (0).
 */
enum {
 Model1_IORES_DESC_NONE = 0,
 Model1_IORES_DESC_CRASH_KERNEL = 1,
 Model1_IORES_DESC_ACPI_TABLES = 2,
 Model1_IORES_DESC_ACPI_NV_STORAGE = 3,
 Model1_IORES_DESC_PERSISTENT_MEMORY = 4,
 Model1_IORES_DESC_PERSISTENT_MEMORY_LEGACY = 5,
};

/* helpers to define resources */
/* PC/ISA/whatever - the normal PC address spaces: IO and memory */
extern struct Model1_resource Model1_ioport_resource;
extern struct Model1_resource Model1_iomem_resource;

extern struct Model1_resource *Model1_request_resource_conflict(struct Model1_resource *Model1_root, struct Model1_resource *Model1_new);
extern int Model1_request_resource(struct Model1_resource *Model1_root, struct Model1_resource *Model1_new);
extern int Model1_release_resource(struct Model1_resource *Model1_new);
void Model1_release_child_resources(struct Model1_resource *Model1_new);
extern void Model1_reserve_region_with_split(struct Model1_resource *Model1_root,
        Model1_resource_size_t Model1_start, Model1_resource_size_t Model1_end,
        const char *Model1_name);
extern struct Model1_resource *Model1_insert_resource_conflict(struct Model1_resource *Model1_parent, struct Model1_resource *Model1_new);
extern int Model1_insert_resource(struct Model1_resource *Model1_parent, struct Model1_resource *Model1_new);
extern void Model1_insert_resource_expand_to_fit(struct Model1_resource *Model1_root, struct Model1_resource *Model1_new);
extern int Model1_remove_resource(struct Model1_resource *old);
extern void Model1_arch_remove_reservations(struct Model1_resource *Model1_avail);
extern int Model1_allocate_resource(struct Model1_resource *Model1_root, struct Model1_resource *Model1_new,
        Model1_resource_size_t Model1_size, Model1_resource_size_t Model1_min,
        Model1_resource_size_t Model1_max, Model1_resource_size_t Model1_align,
        Model1_resource_size_t (*Model1_alignf)(void *,
             const struct Model1_resource *,
             Model1_resource_size_t,
             Model1_resource_size_t),
        void *Model1_alignf_data);
struct Model1_resource *Model1_lookup_resource(struct Model1_resource *Model1_root, Model1_resource_size_t Model1_start);
int Model1_adjust_resource(struct Model1_resource *Model1_res, Model1_resource_size_t Model1_start,
      Model1_resource_size_t Model1_size);
Model1_resource_size_t Model1_resource_alignment(struct Model1_resource *Model1_res);
static inline __attribute__((no_instrument_function)) Model1_resource_size_t Model1_resource_size(const struct Model1_resource *Model1_res)
{
 return Model1_res->Model1_end - Model1_res->Model1_start + 1;
}
static inline __attribute__((no_instrument_function)) unsigned long Model1_resource_type(const struct Model1_resource *Model1_res)
{
 return Model1_res->Model1_flags & 0x00001f00;
}
static inline __attribute__((no_instrument_function)) unsigned long Model1_resource_ext_type(const struct Model1_resource *Model1_res)
{
 return Model1_res->Model1_flags & 0x01000000;
}
/* True iff r1 completely contains r2 */
static inline __attribute__((no_instrument_function)) bool Model1_resource_contains(struct Model1_resource *Model1_r1, struct Model1_resource *Model1_r2)
{
 if (Model1_resource_type(Model1_r1) != Model1_resource_type(Model1_r2))
  return false;
 if (Model1_r1->Model1_flags & 0x20000000 || Model1_r2->Model1_flags & 0x20000000)
  return false;
 return Model1_r1->Model1_start <= Model1_r2->Model1_start && Model1_r1->Model1_end >= Model1_r2->Model1_end;
}


/* Convenience shorthand with allocation */
extern struct Model1_resource * Model1___request_region(struct Model1_resource *,
     Model1_resource_size_t Model1_start,
     Model1_resource_size_t Model1_n,
     const char *Model1_name, int Model1_flags);

/* Compatibility cruft */



extern void Model1___release_region(struct Model1_resource *, Model1_resource_size_t,
    Model1_resource_size_t);





/* Wrappers for managed devices */
struct Model1_device;

extern int Model1_devm_request_resource(struct Model1_device *Model1_dev, struct Model1_resource *Model1_root,
     struct Model1_resource *Model1_new);
extern void Model1_devm_release_resource(struct Model1_device *Model1_dev, struct Model1_resource *Model1_new);






extern struct Model1_resource * Model1___devm_request_region(struct Model1_device *Model1_dev,
    struct Model1_resource *Model1_parent, Model1_resource_size_t Model1_start,
    Model1_resource_size_t Model1_n, const char *Model1_name);






extern void Model1___devm_release_region(struct Model1_device *Model1_dev, struct Model1_resource *Model1_parent,
      Model1_resource_size_t Model1_start, Model1_resource_size_t Model1_n);
extern int Model1_iomem_map_sanity_check(Model1_resource_size_t Model1_addr, unsigned long Model1_size);
extern int Model1_iomem_is_exclusive(Model1_u64 Model1_addr);

extern int
Model1_walk_system_ram_range(unsigned long Model1_start_pfn, unsigned long Model1_nr_pages,
  void *Model1_arg, int (*func)(unsigned long, unsigned long, void *));
extern int
Model1_walk_system_ram_res(Model1_u64 Model1_start, Model1_u64 Model1_end, void *Model1_arg,
      int (*func)(Model1_u64, Model1_u64, void *));
extern int
Model1_walk_iomem_res_desc(unsigned long Model1_desc, unsigned long Model1_flags, Model1_u64 Model1_start, Model1_u64 Model1_end,
      void *Model1_arg, int (*func)(Model1_u64, Model1_u64, void *));

/* True if any part of r1 overlaps r2 */
static inline __attribute__((no_instrument_function)) bool Model1_resource_overlaps(struct Model1_resource *Model1_r1, struct Model1_resource *Model1_r2)
{
       return (Model1_r1->Model1_start <= Model1_r2->Model1_end && Model1_r1->Model1_end >= Model1_r2->Model1_start);
}
/*
 * Include file for the interface to IST BIOS
 * Copyright 2002 Andy Grover <andrew.grover@intel.com>
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the
 * Free Software Foundation; either version 2, or (at your option) any
 * later version.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 * General Public License for more details.
 */




/*
 * Include file for the interface to IST BIOS
 * Copyright 2002 Andy Grover <andrew.grover@intel.com>
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the
 * Free Software Foundation; either version 2, or (at your option) any
 * later version.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 * General Public License for more details.
 */







struct Model1_ist_info {
 __u32 Model1_signature;
 __u32 Model1_command;
 __u32 Model1_event;
 __u32 Model1_perf_level;
};


extern struct Model1_ist_info Model1_ist_info;






struct Model1_edid_info {
 unsigned char Model1_dummy[128];
};


extern struct Model1_edid_info Model1_edid_info;

/* extensible setup data list node */
struct Model1_setup_data {
 __u64 Model1_next;
 __u32 Model1_type;
 __u32 Model1_len;
 __u8 Model1_data[0];
};

struct Model1_setup_header {
 __u8 Model1_setup_sects;
 Model1___u16 Model1_root_flags;
 __u32 Model1_syssize;
 Model1___u16 Model1_ram_size;
 Model1___u16 Model1_vid_mode;
 Model1___u16 Model1_root_dev;
 Model1___u16 Model1_boot_flag;
 Model1___u16 Model1_jump;
 __u32 Model1_header;
 Model1___u16 Model1_version;
 __u32 Model1_realmode_swtch;
 Model1___u16 Model1_start_sys;
 Model1___u16 Model1_kernel_version;
 __u8 Model1_type_of_loader;
 __u8 Model1_loadflags;
 Model1___u16 Model1_setup_move_size;
 __u32 Model1_code32_start;
 __u32 Model1_ramdisk_image;
 __u32 Model1_ramdisk_size;
 __u32 Model1_bootsect_kludge;
 Model1___u16 Model1_heap_end_ptr;
 __u8 Model1_ext_loader_ver;
 __u8 Model1_ext_loader_type;
 __u32 Model1_cmd_line_ptr;
 __u32 Model1_initrd_addr_max;
 __u32 Model1_kernel_alignment;
 __u8 Model1_relocatable_kernel;
 __u8 Model1_min_alignment;
 Model1___u16 Model1_xloadflags;
 __u32 Model1_cmdline_size;
 __u32 Model1_hardware_subarch;
 __u64 Model1_hardware_subarch_data;
 __u32 Model1_payload_offset;
 __u32 Model1_payload_length;
 __u64 Model1_setup_data;
 __u64 Model1_pref_address;
 __u32 Model1_init_size;
 __u32 Model1_handover_offset;
} __attribute__((packed));

struct Model1_sys_desc_table {
 Model1___u16 Model1_length;
 __u8 Model1_table[14];
};

/* Gleaned from OFW's set-parameters in cpu/x86/pc/linux.fth */
struct Model1_olpc_ofw_header {
 __u32 Model1_ofw_magic; /* OFW signature */
 __u32 Model1_ofw_version;
 __u32 Model1_cif_handler; /* callback into OFW */
 __u32 Model1_irq_desc_table;
} __attribute__((packed));

struct Model1_efi_info {
 __u32 Model1_efi_loader_signature;
 __u32 Model1_efi_systab;
 __u32 Model1_efi_memdesc_size;
 __u32 Model1_efi_memdesc_version;
 __u32 Model1_efi_memmap;
 __u32 Model1_efi_memmap_size;
 __u32 Model1_efi_systab_hi;
 __u32 Model1_efi_memmap_hi;
};

/* The so-called "zeropage" */
struct Model1_boot_params {
 struct Model1_screen_info Model1_screen_info; /* 0x000 */
 struct Model1_apm_bios_info Model1_apm_bios_info; /* 0x040 */
 __u8 Model1__pad2[4]; /* 0x054 */
 __u64 Model1_tboot_addr; /* 0x058 */
 struct Model1_ist_info Model1_ist_info; /* 0x060 */
 __u8 Model1__pad3[16]; /* 0x070 */
 __u8 Model1_hd0_info[16]; /* obsolete! */ /* 0x080 */
 __u8 Model1_hd1_info[16]; /* obsolete! */ /* 0x090 */
 struct Model1_sys_desc_table Model1_sys_desc_table; /* obsolete! */ /* 0x0a0 */
 struct Model1_olpc_ofw_header Model1_olpc_ofw_header; /* 0x0b0 */
 __u32 Model1_ext_ramdisk_image; /* 0x0c0 */
 __u32 Model1_ext_ramdisk_size; /* 0x0c4 */
 __u32 Model1_ext_cmd_line_ptr; /* 0x0c8 */
 __u8 Model1__pad4[116]; /* 0x0cc */
 struct Model1_edid_info Model1_edid_info; /* 0x140 */
 struct Model1_efi_info Model1_efi_info; /* 0x1c0 */
 __u32 Model1_alt_mem_k; /* 0x1e0 */
 __u32 Model1_scratch; /* Scratch field! */ /* 0x1e4 */
 __u8 Model1_e820_entries; /* 0x1e8 */
 __u8 Model1_eddbuf_entries; /* 0x1e9 */
 __u8 Model1_edd_mbr_sig_buf_entries; /* 0x1ea */
 __u8 Model1_kbd_status; /* 0x1eb */
 __u8 Model1__pad5[3]; /* 0x1ec */
 /*
	 * The sentinel is set to a nonzero value (0xff) in header.S.
	 *
	 * A bootloader is supposed to only take setup_header and put
	 * it into a clean boot_params buffer. If it turns out that
	 * it is clumsy or too generous with the buffer, it most
	 * probably will pick up the sentinel variable too. The fact
	 * that this variable then is still 0xff will let kernel
	 * know that some variables in boot_params are invalid and
	 * kernel should zero out certain portions of boot_params.
	 */
 __u8 Model1_sentinel; /* 0x1ef */
 __u8 Model1__pad6[1]; /* 0x1f0 */
 struct Model1_setup_header Model1_hdr; /* setup header */ /* 0x1f1 */
 __u8 Model1__pad7[0x290-0x1f1-sizeof(struct Model1_setup_header)];
 __u32 Model1_edd_mbr_sig_buffer[16]; /* 0x290 */
 struct Model1_e820entry Model1_e820_map[128]; /* 0x2d0 */
 __u8 Model1__pad8[48]; /* 0xcd0 */
 struct Model1_edd_info Model1_eddbuf[6]; /* 0xd00 */
 __u8 Model1__pad9[276]; /* 0xeec */
} __attribute__((packed));

/**
 * enum x86_hardware_subarch - x86 hardware subarchitecture
 *
 * The x86 hardware_subarch and hardware_subarch_data were added as of the x86
 * boot protocol 2.07 to help distinguish and support custom x86 boot
 * sequences. This enum represents accepted values for the x86
 * hardware_subarch.  Custom x86 boot sequences (not X86_SUBARCH_PC) do not
 * have or simply *cannot* make use of natural stubs like BIOS or EFI, the
 * hardware_subarch can be used on the Linux entry path to revector to a
 * subarchitecture stub when needed. This subarchitecture stub can be used to
 * set up Linux boot parameters or for special care to account for nonstandard
 * handling of page tables.
 *
 * These enums should only ever be used by x86 code, and the code that uses
 * it should be well contained and compartamentalized.
 *
 * KVM and Xen HVM do not have a subarch as these are expected to follow
 * standard x86 boot entries. If there is a genuine need for "hypervisor" type
 * that should be considered separately in the future. Future guest types
 * should seriously consider working with standard x86 boot stubs such as
 * the BIOS or EFI boot stubs.
 *
 * WARNING: this enum is only used for legacy hacks, for platform features that
 *	    are not easily enumerated or discoverable. You should not ever use
 *	    this for new features.
 *
 * @X86_SUBARCH_PC: Should be used if the hardware is enumerable using standard
 *	PC mechanisms (PCI, ACPI) and doesn't need a special boot flow.
 * @X86_SUBARCH_LGUEST: Used for x86 hypervisor demo, lguest
 * @X86_SUBARCH_XEN: Used for Xen guest types which follow the PV boot path,
 * 	which start at asm startup_xen() entry point and later jump to the C
 * 	xen_start_kernel() entry point. Both domU and dom0 type of guests are
 * 	currently supportd through this PV boot path.
 * @X86_SUBARCH_INTEL_MID: Used for Intel MID (Mobile Internet Device) platform
 *	systems which do not have the PCI legacy interfaces.
 * @X86_SUBARCH_CE4100: Used for Intel CE media processor (CE4100) SoC for
 * 	for settop boxes and media devices, the use of a subarch for CE4100
 * 	is more of a hack...
 */
enum Model1_x86_hardware_subarch {
 Model1_X86_SUBARCH_PC = 0,
 Model1_X86_SUBARCH_LGUEST,
 Model1_X86_SUBARCH_XEN,
 Model1_X86_SUBARCH_INTEL_MID,
 Model1_X86_SUBARCH_CE4100,
 Model1_X86_NR_SUBARCHS,
};

struct Model1_mpc_bus;
struct Model1_mpc_cpu;
struct Model1_mpc_table;
struct Model1_cpuinfo_x86;

/**
 * struct x86_init_mpparse - platform specific mpparse ops
 * @mpc_record:			platform specific mpc record accounting
 * @setup_ioapic_ids:		platform specific ioapic id override
 * @mpc_apic_id:		platform specific mpc apic id assignment
 * @smp_read_mpc_oem:		platform specific oem mpc table setup
 * @mpc_oem_pci_bus:		platform specific pci bus setup (default NULL)
 * @mpc_oem_bus_info:		platform specific mpc bus info
 * @find_smp_config:		find the smp configuration
 * @get_smp_config:		get the smp configuration
 */
struct Model1_x86_init_mpparse {
 void (*Model1_mpc_record)(unsigned int Model1_mode);
 void (*Model1_setup_ioapic_ids)(void);
 int (*Model1_mpc_apic_id)(struct Model1_mpc_cpu *Model1_m);
 void (*Model1_smp_read_mpc_oem)(struct Model1_mpc_table *Model1_mpc);
 void (*Model1_mpc_oem_pci_bus)(struct Model1_mpc_bus *Model1_m);
 void (*Model1_mpc_oem_bus_info)(struct Model1_mpc_bus *Model1_m, char *Model1_name);
 void (*Model1_find_smp_config)(void);
 void (*Model1_get_smp_config)(unsigned int Model1_early);
};

/**
 * struct x86_init_resources - platform specific resource related ops
 * @probe_roms:			probe BIOS roms
 * @reserve_resources:		reserve the standard resources for the
 *				platform
 * @memory_setup:		platform specific memory setup
 *
 */
struct Model1_x86_init_resources {
 void (*Model1_probe_roms)(void);
 void (*Model1_reserve_resources)(void);
 char *(*Model1_memory_setup)(void);
};

/**
 * struct x86_init_irqs - platform specific interrupt setup
 * @pre_vector_init:		init code to run before interrupt vectors
 *				are set up.
 * @intr_init:			interrupt init code
 * @trap_init:			platform specific trap setup
 */
struct Model1_x86_init_irqs {
 void (*Model1_pre_vector_init)(void);
 void (*Model1_intr_init)(void);
 void (*Model1_trap_init)(void);
};

/**
 * struct x86_init_oem - oem platform specific customizing functions
 * @arch_setup:			platform specific architecure setup
 * @banner:			print a platform specific banner
 */
struct Model1_x86_init_oem {
 void (*Model1_arch_setup)(void);
 void (*Model1_banner)(void);
};

/**
 * struct x86_init_paging - platform specific paging functions
 * @pagetable_init:	platform specific paging initialization call to setup
 *			the kernel pagetables and prepare accessors functions.
 *			Callback must call paging_init(). Called once after the
 *			direct mapping for phys memory is available.
 */
struct Model1_x86_init_paging {
 void (*Model1_pagetable_init)(void);
};

/**
 * struct x86_init_timers - platform specific timer setup
 * @setup_perpcu_clockev:	set up the per cpu clock event device for the
 *				boot cpu
 * @timer_init:			initialize the platform timer (default PIT/HPET)
 * @wallclock_init:		init the wallclock device
 */
struct Model1_x86_init_timers {
 void (*Model1_setup_percpu_clockev)(void);
 void (*Model1_timer_init)(void);
 void (*Model1_wallclock_init)(void);
};

/**
 * struct x86_init_iommu - platform specific iommu setup
 * @iommu_init:			platform specific iommu setup
 */
struct Model1_x86_init_iommu {
 int (*Model1_iommu_init)(void);
};

/**
 * struct x86_init_pci - platform specific pci init functions
 * @arch_init:			platform specific pci arch init call
 * @init:			platform specific pci subsystem init
 * @init_irq:			platform specific pci irq init
 * @fixup_irqs:			platform specific pci irq fixup
 */
struct Model1_x86_init_pci {
 int (*Model1_arch_init)(void);
 int (*Model1_init)(void);
 void (*Model1_init_irq)(void);
 void (*Model1_fixup_irqs)(void);
};

/**
 * struct x86_init_ops - functions for platform specific setup
 *
 */
struct Model1_x86_init_ops {
 struct Model1_x86_init_resources Model1_resources;
 struct Model1_x86_init_mpparse Model1_mpparse;
 struct Model1_x86_init_irqs Model1_irqs;
 struct Model1_x86_init_oem Model1_oem;
 struct Model1_x86_init_paging Model1_paging;
 struct Model1_x86_init_timers Model1_timers;
 struct Model1_x86_init_iommu Model1_iommu;
 struct Model1_x86_init_pci Model1_pci;
};

/**
 * struct x86_cpuinit_ops - platform specific cpu hotplug setups
 * @setup_percpu_clockev:	set up the per cpu clock event device
 * @early_percpu_clock_init:	early init of the per cpu clock event device
 */
struct Model1_x86_cpuinit_ops {
 void (*Model1_setup_percpu_clockev)(void);
 void (*Model1_early_percpu_clock_init)(void);
 void (*Model1_fixup_cpu_id)(struct Model1_cpuinfo_x86 *Model1_c, int Model1_node);
};

struct Model1_timespec;

/**
 * struct x86_legacy_devices - legacy x86 devices
 *
 * @pnpbios: this platform can have a PNPBIOS. If this is disabled the platform
 * 	is known to never have a PNPBIOS.
 *
 * These are devices known to require LPC or ISA bus. The definition of legacy
 * devices adheres to the ACPI 5.2.9.3 IA-PC Boot Architecture flag
 * ACPI_FADT_LEGACY_DEVICES. These devices consist of user visible devices on
 * the LPC or ISA bus. User visible devices are devices that have end-user
 * accessible connectors (for example, LPT parallel port). Legacy devices on
 * the LPC bus consist for example of serial and parallel ports, PS/2 keyboard
 * / mouse, and the floppy disk controller. A system that lacks all known
 * legacy devices can assume all devices can be detected exclusively via
 * standard device enumeration mechanisms including the ACPI namespace.
 *
 * A system which has does not have ACPI_FADT_LEGACY_DEVICES enabled must not
 * have any of the legacy devices enumerated below present.
 */
struct Model1_x86_legacy_devices {
 int Model1_pnpbios;
};

/**
 * struct x86_legacy_features - legacy x86 features
 *
 * @rtc: this device has a CMOS real-time clock present
 * @reserve_bios_regions: boot code will search for the EBDA address and the
 * 	start of the 640k - 1M BIOS region.  If false, the platform must
 * 	ensure that its memory map correctly reserves sub-1MB regions as needed.
 * @devices: legacy x86 devices, refer to struct x86_legacy_devices
 * 	documentation for further details.
 */
struct Model1_x86_legacy_features {
 int Model1_rtc;
 int Model1_reserve_bios_regions;
 struct Model1_x86_legacy_devices Model1_devices;
};

/**
 * struct x86_platform_ops - platform specific runtime functions
 * @calibrate_cpu:		calibrate CPU
 * @calibrate_tsc:		calibrate TSC, if different from CPU
 * @get_wallclock:		get time from HW clock like RTC etc.
 * @set_wallclock:		set time back to HW clock
 * @is_untracked_pat_range	exclude from PAT logic
 * @nmi_init			enable NMI on cpus
 * @i8042_detect		pre-detect if i8042 controller exists
 * @save_sched_clock_state:	save state for sched_clock() on suspend
 * @restore_sched_clock_state:	restore state for sched_clock() on resume
 * @apic_post_init:		adjust apic if neeeded
 * @legacy:			legacy features
 * @set_legacy_features:	override legacy features. Use of this callback
 * 				is highly discouraged. You should only need
 * 				this if your hardware platform requires further
 * 				custom fine tuning far beyong what may be
 * 				possible in x86_early_init_platform_quirks() by
 * 				only using the current x86_hardware_subarch
 * 				semantics.
 */
struct Model1_x86_platform_ops {
 unsigned long (*Model1_calibrate_cpu)(void);
 unsigned long (*Model1_calibrate_tsc)(void);
 void (*Model1_get_wallclock)(struct Model1_timespec *Model1_ts);
 int (*Model1_set_wallclock)(const struct Model1_timespec *Model1_ts);
 void (*Model1_iommu_shutdown)(void);
 bool (*Model1_is_untracked_pat_range)(Model1_u64 Model1_start, Model1_u64 Model1_end);
 void (*Model1_nmi_init)(void);
 unsigned char (*Model1_get_nmi_reason)(void);
 int (*Model1_i8042_detect)(void);
 void (*Model1_save_sched_clock_state)(void);
 void (*Model1_restore_sched_clock_state)(void);
 void (*Model1_apic_post_init)(void);
 struct Model1_x86_legacy_features Model1_legacy;
 void (*Model1_set_legacy_features)(void);
};

struct Model1_pci_dev;

struct Model1_x86_msi_ops {
 int (*Model1_setup_msi_irqs)(struct Model1_pci_dev *Model1_dev, int Model1_nvec, int Model1_type);
 void (*Model1_teardown_msi_irq)(unsigned int Model1_irq);
 void (*Model1_teardown_msi_irqs)(struct Model1_pci_dev *Model1_dev);
 void (*Model1_restore_msi_irqs)(struct Model1_pci_dev *Model1_dev);
};

struct Model1_x86_io_apic_ops {
 unsigned int (*Model1_read) (unsigned int Model1_apic, unsigned int Model1_reg);
 void (*Model1_disable)(void);
};

extern struct Model1_x86_init_ops Model1_x86_init;
extern struct Model1_x86_cpuinit_ops Model1_x86_cpuinit;
extern struct Model1_x86_platform_ops Model1_x86_platform;
extern struct Model1_x86_msi_ops Model1_x86_msi;
extern struct Model1_x86_io_apic_ops Model1_x86_io_apic_ops;

extern void Model1_x86_early_init_platform_quirks(void);
extern void Model1_x86_init_noop(void);
extern void Model1_x86_init_uint_noop(unsigned int unused);



/*
 * Constants for various Intel APICs. (local APIC, IOAPIC, etc.)
 *
 * Alan Cox <Alan.Cox@linux.org>, 1995.
 * Ingo Molnar <mingo@redhat.com>, 1999, 2000
 */




/*
 * This is the IO-APIC register space as specified
 * by Intel docs:
 */
/*
 * All x86-64 systems are xAPIC compatible.
 * In the following, "apicid" is a physical APIC ID.
 */
/*
 * the local APIC register structure, memory mapped. Not terribly well
 * tested, but we might eventually use this one in the future - the
 * problem why we cannot use it right now is the P5 APIC, it has an
 * errata which cannot take 8-bit reads and writes, only 32-bit ones ...
 */


struct Model1_local_apic {

/*000*/ struct { unsigned int Model1___reserved[4]; } Model1___reserved_01;

/*010*/ struct { unsigned int Model1___reserved[4]; } Model1___reserved_02;

/*020*/ struct { /* APIC ID Register */
  unsigned int Model1___reserved_1 : 24,
   Model1_phys_apic_id : 4,
   Model1___reserved_2 : 4;
  unsigned int Model1___reserved[3];
 } Model1_id;

/*030*/ const
 struct { /* APIC Version Register */
  unsigned int Model1_version : 8,
   Model1___reserved_1 : 8,
   Model1_max_lvt : 8,
   Model1___reserved_2 : 8;
  unsigned int Model1___reserved[3];
 } Model1_version;

/*040*/ struct { unsigned int Model1___reserved[4]; } Model1___reserved_03;

/*050*/ struct { unsigned int Model1___reserved[4]; } Model1___reserved_04;

/*060*/ struct { unsigned int Model1___reserved[4]; } Model1___reserved_05;

/*070*/ struct { unsigned int Model1___reserved[4]; } Model1___reserved_06;

/*080*/ struct { /* Task Priority Register */
  unsigned int Model1_priority : 8,
   Model1___reserved_1 : 24;
  unsigned int Model1___reserved_2[3];
 } Model1_tpr;

/*090*/ const
 struct { /* Arbitration Priority Register */
  unsigned int Model1_priority : 8,
   Model1___reserved_1 : 24;
  unsigned int Model1___reserved_2[3];
 } Model1_apr;

/*0A0*/ const
 struct { /* Processor Priority Register */
  unsigned int Model1_priority : 8,
   Model1___reserved_1 : 24;
  unsigned int Model1___reserved_2[3];
 } Model1_ppr;

/*0B0*/ struct { /* End Of Interrupt Register */
  unsigned int Model1_eoi;
  unsigned int Model1___reserved[3];
 } Model1_eoi;

/*0C0*/ struct { unsigned int Model1___reserved[4]; } Model1___reserved_07;

/*0D0*/ struct { /* Logical Destination Register */
  unsigned int Model1___reserved_1 : 24,
   Model1_logical_dest : 8;
  unsigned int Model1___reserved_2[3];
 } Model1_ldr;

/*0E0*/ struct { /* Destination Format Register */
  unsigned int Model1___reserved_1 : 28,
   Model1_model : 4;
  unsigned int Model1___reserved_2[3];
 } Model1_dfr;

/*0F0*/ struct { /* Spurious Interrupt Vector Register */
  unsigned int Model1_spurious_vector : 8,
   Model1_apic_enabled : 1,
   Model1_focus_cpu : 1,
   Model1___reserved_2 : 22;
  unsigned int Model1___reserved_3[3];
 } Model1_svr;

/*100*/ struct { /* In Service Register */
/*170*/ unsigned int Model1_bitfield;
  unsigned int Model1___reserved[3];
 } Model1_isr [8];

/*180*/ struct { /* Trigger Mode Register */
/*1F0*/ unsigned int Model1_bitfield;
  unsigned int Model1___reserved[3];
 } Model1_tmr [8];

/*200*/ struct { /* Interrupt Request Register */
/*270*/ unsigned int Model1_bitfield;
  unsigned int Model1___reserved[3];
 } Model1_irr [8];

/*280*/ union { /* Error Status Register */
  struct {
   unsigned int Model1_send_cs_error : 1,
    Model1_receive_cs_error : 1,
    Model1_send_accept_error : 1,
    Model1_receive_accept_error : 1,
    Model1___reserved_1 : 1,
    Model1_send_illegal_vector : 1,
    Model1_receive_illegal_vector : 1,
    Model1_illegal_register_address : 1,
    Model1___reserved_2 : 24;
   unsigned int Model1___reserved_3[3];
  } Model1_error_bits;
  struct {
   unsigned int Model1_errors;
   unsigned int Model1___reserved_3[3];
  } Model1_all_errors;
 } Model1_esr;

/*290*/ struct { unsigned int Model1___reserved[4]; } Model1___reserved_08;

/*2A0*/ struct { unsigned int Model1___reserved[4]; } Model1___reserved_09;

/*2B0*/ struct { unsigned int Model1___reserved[4]; } Model1___reserved_10;

/*2C0*/ struct { unsigned int Model1___reserved[4]; } Model1___reserved_11;

/*2D0*/ struct { unsigned int Model1___reserved[4]; } Model1___reserved_12;

/*2E0*/ struct { unsigned int Model1___reserved[4]; } Model1___reserved_13;

/*2F0*/ struct { unsigned int Model1___reserved[4]; } Model1___reserved_14;

/*300*/ struct { /* Interrupt Command Register 1 */
  unsigned int Model1_vector : 8,
   Model1_delivery_mode : 3,
   Model1_destination_mode : 1,
   Model1_delivery_status : 1,
   Model1___reserved_1 : 1,
   Model1_level : 1,
   Model1_trigger : 1,
   Model1___reserved_2 : 2,
   Model1_shorthand : 2,
   Model1___reserved_3 : 12;
  unsigned int Model1___reserved_4[3];
 } Model1_icr1;

/*310*/ struct { /* Interrupt Command Register 2 */
  union {
   unsigned int Model1___reserved_1 : 24,
    Model1_phys_dest : 4,
    Model1___reserved_2 : 4;
   unsigned int Model1___reserved_3 : 24,
    Model1_logical_dest : 8;
  } Model1_dest;
  unsigned int Model1___reserved_4[3];
 } Model1_icr2;

/*320*/ struct { /* LVT - Timer */
  unsigned int Model1_vector : 8,
   Model1___reserved_1 : 4,
   Model1_delivery_status : 1,
   Model1___reserved_2 : 3,
   Model1_mask : 1,
   Model1_timer_mode : 1,
   Model1___reserved_3 : 14;
  unsigned int Model1___reserved_4[3];
 } Model1_lvt_timer;

/*330*/ struct { /* LVT - Thermal Sensor */
  unsigned int Model1_vector : 8,
   Model1_delivery_mode : 3,
   Model1___reserved_1 : 1,
   Model1_delivery_status : 1,
   Model1___reserved_2 : 3,
   Model1_mask : 1,
   Model1___reserved_3 : 15;
  unsigned int Model1___reserved_4[3];
 } Model1_lvt_thermal;

/*340*/ struct { /* LVT - Performance Counter */
  unsigned int Model1_vector : 8,
   Model1_delivery_mode : 3,
   Model1___reserved_1 : 1,
   Model1_delivery_status : 1,
   Model1___reserved_2 : 3,
   Model1_mask : 1,
   Model1___reserved_3 : 15;
  unsigned int Model1___reserved_4[3];
 } Model1_lvt_pc;

/*350*/ struct { /* LVT - LINT0 */
  unsigned int Model1_vector : 8,
   Model1_delivery_mode : 3,
   Model1___reserved_1 : 1,
   Model1_delivery_status : 1,
   Model1_polarity : 1,
   Model1_remote_irr : 1,
   Model1_trigger : 1,
   Model1_mask : 1,
   Model1___reserved_2 : 15;
  unsigned int Model1___reserved_3[3];
 } Model1_lvt_lint0;

/*360*/ struct { /* LVT - LINT1 */
  unsigned int Model1_vector : 8,
   Model1_delivery_mode : 3,
   Model1___reserved_1 : 1,
   Model1_delivery_status : 1,
   Model1_polarity : 1,
   Model1_remote_irr : 1,
   Model1_trigger : 1,
   Model1_mask : 1,
   Model1___reserved_2 : 15;
  unsigned int Model1___reserved_3[3];
 } Model1_lvt_lint1;

/*370*/ struct { /* LVT - Error */
  unsigned int Model1_vector : 8,
   Model1___reserved_1 : 4,
   Model1_delivery_status : 1,
   Model1___reserved_2 : 3,
   Model1_mask : 1,
   Model1___reserved_3 : 15;
  unsigned int Model1___reserved_4[3];
 } Model1_lvt_error;

/*380*/ struct { /* Timer Initial Count Register */
  unsigned int Model1_initial_count;
  unsigned int Model1___reserved_2[3];
 } Model1_timer_icr;

/*390*/ const
 struct { /* Timer Current Count Register */
  unsigned int Model1_curr_count;
  unsigned int Model1___reserved_2[3];
 } Model1_timer_ccr;

/*3A0*/ struct { unsigned int Model1___reserved[4]; } Model1___reserved_16;

/*3B0*/ struct { unsigned int Model1___reserved[4]; } Model1___reserved_17;

/*3C0*/ struct { unsigned int Model1___reserved[4]; } Model1___reserved_18;

/*3D0*/ struct { unsigned int Model1___reserved[4]; } Model1___reserved_19;

/*3E0*/ struct { /* Timer Divide Configuration Register */
  unsigned int Model1_divisor : 4,
   Model1___reserved_1 : 28;
  unsigned int Model1___reserved_2[3];
 } Model1_timer_dcr;

/*3F0*/ struct { unsigned int Model1___reserved[4]; } Model1___reserved_20;

} __attribute__ ((packed));
enum Model1_ioapic_irq_destination_types {
 Model1_dest_Fixed = 0,
 Model1_dest_LowestPrio = 1,
 Model1_dest_SMI = 2,
 Model1_dest__reserved_1 = 3,
 Model1_dest_NMI = 4,
 Model1_dest_INIT = 5,
 Model1_dest__reserved_2 = 6,
 Model1_dest_ExtINT = 7
};

extern int Model1_apic_version[];
extern int Model1_pic_mode;
/* Each PCI slot may be a combo card with its own bus.  4 IRQ pins per slot. */
extern unsigned long Model1_mp_bus_not_pci[(((256) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];

extern unsigned int Model1_boot_cpu_physical_apicid;
extern unsigned long Model1_mp_lapic_addr;


extern int Model1_smp_found_config;




static inline __attribute__((no_instrument_function)) void Model1_get_smp_config(void)
{
 Model1_x86_init.Model1_mpparse.Model1_get_smp_config(0);
}

static inline __attribute__((no_instrument_function)) void Model1_early_get_smp_config(void)
{
 Model1_x86_init.Model1_mpparse.Model1_get_smp_config(1);
}

static inline __attribute__((no_instrument_function)) void Model1_find_smp_config(void)
{
 Model1_x86_init.Model1_mpparse.Model1_find_smp_config();
}


extern void Model1_early_reserve_e820_mpc_new(void);
extern int Model1_enable_update_mptable;
extern int Model1_default_mpc_apic_id(struct Model1_mpc_cpu *Model1_m);
extern void Model1_default_smp_read_mpc_oem(struct Model1_mpc_table *Model1_mpc);

extern void Model1_default_mpc_oem_bus_info(struct Model1_mpc_bus *Model1_m, char *Model1_str);



extern void Model1_default_find_smp_config(void);
extern void Model1_default_get_smp_config(unsigned int Model1_early);
int Model1_generic_processor_info(int Model1_apicid, int Model1_version);



struct Model1_physid_mask {
 unsigned long Model1_mask[(((32768) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];
};

typedef struct Model1_physid_mask Model1_physid_mask_t;
static inline __attribute__((no_instrument_function)) unsigned long Model1_physids_coerce(Model1_physid_mask_t *Model1_map)
{
 return Model1_map->Model1_mask[0];
}

static inline __attribute__((no_instrument_function)) void Model1_physids_promote(unsigned long Model1_physids, Model1_physid_mask_t *Model1_map)
{
 Model1_bitmap_zero((*Model1_map).Model1_mask, 32768);
 Model1_map->Model1_mask[0] = Model1_physids;
}

static inline __attribute__((no_instrument_function)) void Model1_physid_set_mask_of_physid(int Model1_physid, Model1_physid_mask_t *Model1_map)
{
 Model1_bitmap_zero((*Model1_map).Model1_mask, 32768);
 Model1_set_bit(Model1_physid, (*Model1_map).Model1_mask);
}




extern Model1_physid_mask_t Model1_phys_cpu_present_map;




/*
 *  pm.h - Power management interface
 *
 *  Copyright (C) 2000 Andrew Henroid
 *
 *  This program is free software; you can redistribute it and/or modify
 *  it under the terms of the GNU General Public License as published by
 *  the Free Software Foundation; either version 2 of the License, or
 *  (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU General Public License for more details.
 *
 *  You should have received a copy of the GNU General Public License
 *  along with this program; if not, write to the Free Software
 *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
 */
/*
 * Callbacks for platform drivers to implement.
 */
extern void (*Model1_pm_power_off)(void);
extern void (*Model1_pm_power_off_prepare)(void);

struct Model1_device; /* we have a circular dep with device.h */

extern void Model1_pm_vt_switch_required(struct Model1_device *Model1_dev, bool Model1_required);
extern void Model1_pm_vt_switch_unregister(struct Model1_device *Model1_dev);
/*
 * Device power management
 */

struct Model1_device;


extern const char Model1_power_group_name[]; /* = "power" */




typedef struct Model1_pm_message {
 int Model1_event;
} Model1_pm_message_t;

/**
 * struct dev_pm_ops - device PM callbacks
 *
 * Several device power state transitions are externally visible, affecting
 * the state of pending I/O queues and (for drivers that touch hardware)
 * interrupts, wakeups, DMA, and other hardware state.  There may also be
 * internal transitions to various low-power modes which are transparent
 * to the rest of the driver stack (such as a driver that's ON gating off
 * clocks which are not in active use).
 *
 * The externally visible transitions are handled with the help of callbacks
 * included in this structure in such a way that two levels of callbacks are
 * involved.  First, the PM core executes callbacks provided by PM domains,
 * device types, classes and bus types.  They are the subsystem-level callbacks
 * supposed to execute callbacks provided by device drivers, although they may
 * choose not to do that.  If the driver callbacks are executed, they have to
 * collaborate with the subsystem-level callbacks to achieve the goals
 * appropriate for the given system transition, given transition phase and the
 * subsystem the device belongs to.
 *
 * @prepare: The principal role of this callback is to prevent new children of
 *	the device from being registered after it has returned (the driver's
 *	subsystem and generally the rest of the kernel is supposed to prevent
 *	new calls to the probe method from being made too once @prepare() has
 *	succeeded).  If @prepare() detects a situation it cannot handle (e.g.
 *	registration of a child already in progress), it may return -EAGAIN, so
 *	that the PM core can execute it once again (e.g. after a new child has
 *	been registered) to recover from the race condition.
 *	This method is executed for all kinds of suspend transitions and is
 *	followed by one of the suspend callbacks: @suspend(), @freeze(), or
 *	@poweroff().  If the transition is a suspend to memory or standby (that
 *	is, not related to hibernation), the return value of @prepare() may be
 *	used to indicate to the PM core to leave the device in runtime suspend
 *	if applicable.  Namely, if @prepare() returns a positive number, the PM
 *	core will understand that as a declaration that the device appears to be
 *	runtime-suspended and it may be left in that state during the entire
 *	transition and during the subsequent resume if all of its descendants
 *	are left in runtime suspend too.  If that happens, @complete() will be
 *	executed directly after @prepare() and it must ensure the proper
 *	functioning of the device after the system resume.
 *	The PM core executes subsystem-level @prepare() for all devices before
 *	starting to invoke suspend callbacks for any of them, so generally
 *	devices may be assumed to be functional or to respond to runtime resume
 *	requests while @prepare() is being executed.  However, device drivers
 *	may NOT assume anything about the availability of user space at that
 *	time and it is NOT valid to request firmware from within @prepare()
 *	(it's too late to do that).  It also is NOT valid to allocate
 *	substantial amounts of memory from @prepare() in the GFP_KERNEL mode.
 *	[To work around these limitations, drivers may register suspend and
 *	hibernation notifiers to be executed before the freezing of tasks.]
 *
 * @complete: Undo the changes made by @prepare().  This method is executed for
 *	all kinds of resume transitions, following one of the resume callbacks:
 *	@resume(), @thaw(), @restore().  Also called if the state transition
 *	fails before the driver's suspend callback: @suspend(), @freeze() or
 *	@poweroff(), can be executed (e.g. if the suspend callback fails for one
 *	of the other devices that the PM core has unsuccessfully attempted to
 *	suspend earlier).
 *	The PM core executes subsystem-level @complete() after it has executed
 *	the appropriate resume callbacks for all devices.  If the corresponding
 *	@prepare() at the beginning of the suspend transition returned a
 *	positive number and the device was left in runtime suspend (without
 *	executing any suspend and resume callbacks for it), @complete() will be
 *	the only callback executed for the device during resume.  In that case,
 *	@complete() must be prepared to do whatever is necessary to ensure the
 *	proper functioning of the device after the system resume.  To this end,
 *	@complete() can check the power.direct_complete flag of the device to
 *	learn whether (unset) or not (set) the previous suspend and resume
 *	callbacks have been executed for it.
 *
 * @suspend: Executed before putting the system into a sleep state in which the
 *	contents of main memory are preserved.  The exact action to perform
 *	depends on the device's subsystem (PM domain, device type, class or bus
 *	type), but generally the device must be quiescent after subsystem-level
 *	@suspend() has returned, so that it doesn't do any I/O or DMA.
 *	Subsystem-level @suspend() is executed for all devices after invoking
 *	subsystem-level @prepare() for all of them.
 *
 * @suspend_late: Continue operations started by @suspend().  For a number of
 *	devices @suspend_late() may point to the same callback routine as the
 *	runtime suspend callback.
 *
 * @resume: Executed after waking the system up from a sleep state in which the
 *	contents of main memory were preserved.  The exact action to perform
 *	depends on the device's subsystem, but generally the driver is expected
 *	to start working again, responding to hardware events and software
 *	requests (the device itself may be left in a low-power state, waiting
 *	for a runtime resume to occur).  The state of the device at the time its
 *	driver's @resume() callback is run depends on the platform and subsystem
 *	the device belongs to.  On most platforms, there are no restrictions on
 *	availability of resources like clocks during @resume().
 *	Subsystem-level @resume() is executed for all devices after invoking
 *	subsystem-level @resume_noirq() for all of them.
 *
 * @resume_early: Prepare to execute @resume().  For a number of devices
 *	@resume_early() may point to the same callback routine as the runtime
 *	resume callback.
 *
 * @freeze: Hibernation-specific, executed before creating a hibernation image.
 *	Analogous to @suspend(), but it should not enable the device to signal
 *	wakeup events or change its power state.  The majority of subsystems
 *	(with the notable exception of the PCI bus type) expect the driver-level
 *	@freeze() to save the device settings in memory to be used by @restore()
 *	during the subsequent resume from hibernation.
 *	Subsystem-level @freeze() is executed for all devices after invoking
 *	subsystem-level @prepare() for all of them.
 *
 * @freeze_late: Continue operations started by @freeze().  Analogous to
 *	@suspend_late(), but it should not enable the device to signal wakeup
 *	events or change its power state.
 *
 * @thaw: Hibernation-specific, executed after creating a hibernation image OR
 *	if the creation of an image has failed.  Also executed after a failing
 *	attempt to restore the contents of main memory from such an image.
 *	Undo the changes made by the preceding @freeze(), so the device can be
 *	operated in the same way as immediately before the call to @freeze().
 *	Subsystem-level @thaw() is executed for all devices after invoking
 *	subsystem-level @thaw_noirq() for all of them.  It also may be executed
 *	directly after @freeze() in case of a transition error.
 *
 * @thaw_early: Prepare to execute @thaw().  Undo the changes made by the
 *	preceding @freeze_late().
 *
 * @poweroff: Hibernation-specific, executed after saving a hibernation image.
 *	Analogous to @suspend(), but it need not save the device's settings in
 *	memory.
 *	Subsystem-level @poweroff() is executed for all devices after invoking
 *	subsystem-level @prepare() for all of them.
 *
 * @poweroff_late: Continue operations started by @poweroff().  Analogous to
 *	@suspend_late(), but it need not save the device's settings in memory.
 *
 * @restore: Hibernation-specific, executed after restoring the contents of main
 *	memory from a hibernation image, analogous to @resume().
 *
 * @restore_early: Prepare to execute @restore(), analogous to @resume_early().
 *
 * @suspend_noirq: Complete the actions started by @suspend().  Carry out any
 *	additional operations required for suspending the device that might be
 *	racing with its driver's interrupt handler, which is guaranteed not to
 *	run while @suspend_noirq() is being executed.
 *	It generally is expected that the device will be in a low-power state
 *	(appropriate for the target system sleep state) after subsystem-level
 *	@suspend_noirq() has returned successfully.  If the device can generate
 *	system wakeup signals and is enabled to wake up the system, it should be
 *	configured to do so at that time.  However, depending on the platform
 *	and device's subsystem, @suspend() or @suspend_late() may be allowed to
 *	put the device into the low-power state and configure it to generate
 *	wakeup signals, in which case it generally is not necessary to define
 *	@suspend_noirq().
 *
 * @resume_noirq: Prepare for the execution of @resume() by carrying out any
 *	operations required for resuming the device that might be racing with
 *	its driver's interrupt handler, which is guaranteed not to run while
 *	@resume_noirq() is being executed.
 *
 * @freeze_noirq: Complete the actions started by @freeze().  Carry out any
 *	additional operations required for freezing the device that might be
 *	racing with its driver's interrupt handler, which is guaranteed not to
 *	run while @freeze_noirq() is being executed.
 *	The power state of the device should not be changed by either @freeze(),
 *	or @freeze_late(), or @freeze_noirq() and it should not be configured to
 *	signal system wakeup by any of these callbacks.
 *
 * @thaw_noirq: Prepare for the execution of @thaw() by carrying out any
 *	operations required for thawing the device that might be racing with its
 *	driver's interrupt handler, which is guaranteed not to run while
 *	@thaw_noirq() is being executed.
 *
 * @poweroff_noirq: Complete the actions started by @poweroff().  Analogous to
 *	@suspend_noirq(), but it need not save the device's settings in memory.
 *
 * @restore_noirq: Prepare for the execution of @restore() by carrying out any
 *	operations required for thawing the device that might be racing with its
 *	driver's interrupt handler, which is guaranteed not to run while
 *	@restore_noirq() is being executed.  Analogous to @resume_noirq().
 *
 * All of the above callbacks, except for @complete(), return error codes.
 * However, the error codes returned by the resume operations, @resume(),
 * @thaw(), @restore(), @resume_noirq(), @thaw_noirq(), and @restore_noirq(), do
 * not cause the PM core to abort the resume transition during which they are
 * returned.  The error codes returned in those cases are only printed by the PM
 * core to the system logs for debugging purposes.  Still, it is recommended
 * that drivers only return error codes from their resume methods in case of an
 * unrecoverable failure (i.e. when the device being handled refuses to resume
 * and becomes unusable) to allow us to modify the PM core in the future, so
 * that it can avoid attempting to handle devices that failed to resume and
 * their children.
 *
 * It is allowed to unregister devices while the above callbacks are being
 * executed.  However, a callback routine must NOT try to unregister the device
 * it was called for, although it may unregister children of that device (for
 * example, if it detects that a child was unplugged while the system was
 * asleep).
 *
 * Refer to Documentation/power/devices.txt for more information about the role
 * of the above callbacks in the system suspend process.
 *
 * There also are callbacks related to runtime power management of devices.
 * Again, these callbacks are executed by the PM core only for subsystems
 * (PM domains, device types, classes and bus types) and the subsystem-level
 * callbacks are supposed to invoke the driver callbacks.  Moreover, the exact
 * actions to be performed by a device driver's callbacks generally depend on
 * the platform and subsystem the device belongs to.
 *
 * @runtime_suspend: Prepare the device for a condition in which it won't be
 *	able to communicate with the CPU(s) and RAM due to power management.
 *	This need not mean that the device should be put into a low-power state.
 *	For example, if the device is behind a link which is about to be turned
 *	off, the device may remain at full power.  If the device does go to low
 *	power and is capable of generating runtime wakeup events, remote wakeup
 *	(i.e., a hardware mechanism allowing the device to request a change of
 *	its power state via an interrupt) should be enabled for it.
 *
 * @runtime_resume: Put the device into the fully active state in response to a
 *	wakeup event generated by hardware or at the request of software.  If
 *	necessary, put the device into the full-power state and restore its
 *	registers, so that it is fully operational.
 *
 * @runtime_idle: Device appears to be inactive and it might be put into a
 *	low-power state if all of the necessary conditions are satisfied.
 *	Check these conditions, and return 0 if it's appropriate to let the PM
 *	core queue a suspend request for the device.
 *
 * Refer to Documentation/power/runtime_pm.txt for more information about the
 * role of the above callbacks in device runtime power management.
 *
 */

struct Model1_dev_pm_ops {
 int (*Model1_prepare)(struct Model1_device *Model1_dev);
 void (*Model1_complete)(struct Model1_device *Model1_dev);
 int (*Model1_suspend)(struct Model1_device *Model1_dev);
 int (*Model1_resume)(struct Model1_device *Model1_dev);
 int (*Model1_freeze)(struct Model1_device *Model1_dev);
 int (*Model1_thaw)(struct Model1_device *Model1_dev);
 int (*Model1_poweroff)(struct Model1_device *Model1_dev);
 int (*Model1_restore)(struct Model1_device *Model1_dev);
 int (*Model1_suspend_late)(struct Model1_device *Model1_dev);
 int (*Model1_resume_early)(struct Model1_device *Model1_dev);
 int (*Model1_freeze_late)(struct Model1_device *Model1_dev);
 int (*Model1_thaw_early)(struct Model1_device *Model1_dev);
 int (*Model1_poweroff_late)(struct Model1_device *Model1_dev);
 int (*Model1_restore_early)(struct Model1_device *Model1_dev);
 int (*Model1_suspend_noirq)(struct Model1_device *Model1_dev);
 int (*Model1_resume_noirq)(struct Model1_device *Model1_dev);
 int (*Model1_freeze_noirq)(struct Model1_device *Model1_dev);
 int (*Model1_thaw_noirq)(struct Model1_device *Model1_dev);
 int (*Model1_poweroff_noirq)(struct Model1_device *Model1_dev);
 int (*Model1_restore_noirq)(struct Model1_device *Model1_dev);
 int (*Model1_runtime_suspend)(struct Model1_device *Model1_dev);
 int (*Model1_runtime_resume)(struct Model1_device *Model1_dev);
 int (*Model1_runtime_idle)(struct Model1_device *Model1_dev);
};
/*
 * Use this if you want to use the same suspend and resume callbacks for suspend
 * to RAM and hibernation.
 */





/*
 * Use this for defining a set of PM operations to be used in all situations
 * (system suspend, hibernation or runtime PM).
 * NOTE: In general, system suspend callbacks, .suspend() and .resume(), should
 * be different from the corresponding runtime PM callbacks, .runtime_suspend(),
 * and .runtime_resume(), because .runtime_suspend() always works on an already
 * quiescent device, while .suspend() should assume that the device may be doing
 * something when it is called (it should ensure that the device will be
 * quiescent after it has returned).  Therefore it's better to point the "late"
 * suspend and "early" resume callback pointers, .suspend_late() and
 * .resume_early(), to the same routines as .runtime_suspend() and
 * .runtime_resume(), respectively (and analogously for hibernation).
 */






/**
 * PM_EVENT_ messages
 *
 * The following PM_EVENT_ messages are defined for the internal use of the PM
 * core, in order to provide a mechanism allowing the high level suspend and
 * hibernation code to convey the necessary information to the device PM core
 * code:
 *
 * ON		No transition.
 *
 * FREEZE	System is going to hibernate, call ->prepare() and ->freeze()
 *		for all devices.
 *
 * SUSPEND	System is going to suspend, call ->prepare() and ->suspend()
 *		for all devices.
 *
 * HIBERNATE	Hibernation image has been saved, call ->prepare() and
 *		->poweroff() for all devices.
 *
 * QUIESCE	Contents of main memory are going to be restored from a (loaded)
 *		hibernation image, call ->prepare() and ->freeze() for all
 *		devices.
 *
 * RESUME	System is resuming, call ->resume() and ->complete() for all
 *		devices.
 *
 * THAW		Hibernation image has been created, call ->thaw() and
 *		->complete() for all devices.
 *
 * RESTORE	Contents of main memory have been restored from a hibernation
 *		image, call ->restore() and ->complete() for all devices.
 *
 * RECOVER	Creation of a hibernation image or restoration of the main
 *		memory contents from a hibernation image has failed, call
 *		->thaw() and ->complete() for all devices.
 *
 * The following PM_EVENT_ messages are defined for internal use by
 * kernel subsystems.  They are never issued by the PM core.
 *
 * USER_SUSPEND		Manual selective suspend was issued by userspace.
 *
 * USER_RESUME		Manual selective resume was issued by userspace.
 *
 * REMOTE_WAKEUP	Remote-wakeup request was received from the device.
 *
 * AUTO_SUSPEND		Automatic (device idle) runtime suspend was
 *			initiated by the subsystem.
 *
 * AUTO_RESUME		Automatic (device needed) runtime resume was
 *			requested by a driver.
 */
/**
 * Device run-time power management status.
 *
 * These status labels are used internally by the PM core to indicate the
 * current status of a device with respect to the PM core operations.  They do
 * not reflect the actual power state of the device or its status as seen by the
 * driver.
 *
 * RPM_ACTIVE		Device is fully operational.  Indicates that the device
 *			bus type's ->runtime_resume() callback has completed
 *			successfully.
 *
 * RPM_SUSPENDED	Device bus type's ->runtime_suspend() callback has
 *			completed successfully.  The device is regarded as
 *			suspended.
 *
 * RPM_RESUMING		Device bus type's ->runtime_resume() callback is being
 *			executed.
 *
 * RPM_SUSPENDING	Device bus type's ->runtime_suspend() callback is being
 *			executed.
 */

enum Model1_rpm_status {
 Model1_RPM_ACTIVE = 0,
 Model1_RPM_RESUMING,
 Model1_RPM_SUSPENDED,
 Model1_RPM_SUSPENDING,
};

/**
 * Device run-time power management request types.
 *
 * RPM_REQ_NONE		Do nothing.
 *
 * RPM_REQ_IDLE		Run the device bus type's ->runtime_idle() callback
 *
 * RPM_REQ_SUSPEND	Run the device bus type's ->runtime_suspend() callback
 *
 * RPM_REQ_AUTOSUSPEND	Same as RPM_REQ_SUSPEND, but not until the device has
 *			been inactive for as long as power.autosuspend_delay
 *
 * RPM_REQ_RESUME	Run the device bus type's ->runtime_resume() callback
 */

enum Model1_rpm_request {
 Model1_RPM_REQ_NONE = 0,
 Model1_RPM_REQ_IDLE,
 Model1_RPM_REQ_SUSPEND,
 Model1_RPM_REQ_AUTOSUSPEND,
 Model1_RPM_REQ_RESUME,
};

struct Model1_wakeup_source;
struct Model1_wake_irq;
struct Model1_pm_domain_data;

struct Model1_pm_subsys_data {
 Model1_spinlock_t Model1_lock;
 unsigned int Model1_refcount;






};

struct Model1_dev_pm_info {
 Model1_pm_message_t Model1_power_state;
 unsigned int Model1_can_wakeup:1;
 unsigned int Model1_async_suspend:1;
 bool Model1_is_prepared:1; /* Owned by the PM core */
 bool Model1_is_suspended:1; /* Ditto */
 bool Model1_is_noirq_suspended:1;
 bool Model1_is_late_suspended:1;
 bool Model1_early_init:1; /* Owned by the PM core */
 bool Model1_direct_complete:1; /* Owned by the PM core */
 Model1_spinlock_t Model1_lock;

 struct Model1_list_head Model1_entry;
 struct Model1_completion Model1_completion;
 struct Model1_wakeup_source *Model1_wakeup;
 bool Model1_wakeup_path:1;
 bool Model1_syscore:1;
 bool Model1_no_pm_callbacks:1; /* Owned by the PM core */




 struct Model1_timer_list Model1_suspend_timer;
 unsigned long Model1_timer_expires;
 struct Model1_work_struct Model1_work;
 Model1_wait_queue_head_t Model1_wait_queue;
 struct Model1_wake_irq *Model1_wakeirq;
 Model1_atomic_t Model1_usage_count;
 Model1_atomic_t Model1_child_count;
 unsigned int Model1_disable_depth:3;
 unsigned int Model1_idle_notification:1;
 unsigned int Model1_request_pending:1;
 unsigned int Model1_deferred_resume:1;
 unsigned int Model1_run_wake:1;
 unsigned int Model1_runtime_auto:1;
 bool Model1_ignore_children:1;
 unsigned int Model1_no_callbacks:1;
 unsigned int Model1_irq_safe:1;
 unsigned int Model1_use_autosuspend:1;
 unsigned int Model1_timer_autosuspends:1;
 unsigned int Model1_memalloc_noio:1;
 enum Model1_rpm_request Model1_request;
 enum Model1_rpm_status Model1_runtime_status;
 int Model1_runtime_error;
 int Model1_autosuspend_delay;
 unsigned long Model1_last_busy;
 unsigned long Model1_active_jiffies;
 unsigned long Model1_suspended_jiffies;
 unsigned long Model1_accounting_timestamp;

 struct Model1_pm_subsys_data *Model1_subsys_data; /* Owned by the subsystem. */
 void (*Model1_set_latency_tolerance)(struct Model1_device *, Model1_s32);
 struct Model1_dev_pm_qos *Model1_qos;
};

extern void Model1_update_pm_runtime_accounting(struct Model1_device *Model1_dev);
extern int Model1_dev_pm_get_subsys_data(struct Model1_device *Model1_dev);
extern void Model1_dev_pm_put_subsys_data(struct Model1_device *Model1_dev);

/*
 * Power domains provide callbacks that are executed during system suspend,
 * hibernation, system resume and during runtime PM transitions along with
 * subsystem-level and driver-level callbacks.
 *
 * @detach: Called when removing a device from the domain.
 * @activate: Called before executing probe routines for bus types and drivers.
 * @sync: Called after successful driver probe.
 * @dismiss: Called after unsuccessful driver probe and after driver removal.
 */
struct Model1_dev_pm_domain {
 struct Model1_dev_pm_ops Model1_ops;
 void (*Model1_detach)(struct Model1_device *Model1_dev, bool Model1_power_off);
 int (*Model1_activate)(struct Model1_device *Model1_dev);
 void (*Model1_sync)(struct Model1_device *Model1_dev);
 void (*Model1_dismiss)(struct Model1_device *Model1_dev);
};

/*
 * The PM_EVENT_ messages are also used by drivers implementing the legacy
 * suspend framework, based on the ->suspend() and ->resume() callbacks common
 * for suspend and hibernation transitions, according to the rules below.
 */

/* Necessary, because several drivers use PM_EVENT_PRETHAW */


/*
 * One transition is triggered by resume(), after a suspend() call; the
 * message is implicit:
 *
 * ON		Driver starts working again, responding to hardware events
 *		and software requests.  The hardware may have gone through
 *		a power-off reset, or it may have maintained state from the
 *		previous suspend() which the driver will rely on while
 *		resuming.  On most platforms, there are no restrictions on
 *		availability of resources like clocks during resume().
 *
 * Other transitions are triggered by messages sent using suspend().  All
 * these transitions quiesce the driver, so that I/O queues are inactive.
 * That commonly entails turning off IRQs and DMA; there may be rules
 * about how to quiesce that are specific to the bus or the device's type.
 * (For example, network drivers mark the link state.)  Other details may
 * differ according to the message:
 *
 * SUSPEND	Quiesce, enter a low power device state appropriate for
 *		the upcoming system state (such as PCI_D3hot), and enable
 *		wakeup events as appropriate.
 *
 * HIBERNATE	Enter a low power device state appropriate for the hibernation
 *		state (eg. ACPI S4) and enable wakeup events as appropriate.
 *
 * FREEZE	Quiesce operations so that a consistent image can be saved;
 *		but do NOT otherwise enter a low power device state, and do
 *		NOT emit system wakeup events.
 *
 * PRETHAW	Quiesce as if for FREEZE; additionally, prepare for restoring
 *		the system from a snapshot taken after an earlier FREEZE.
 *		Some drivers will need to reset their hardware state instead
 *		of preserving it, to ensure that it's never mistaken for the
 *		state which that earlier snapshot had set up.
 *
 * A minimally power-aware driver treats all messages as SUSPEND, fully
 * reinitializes its device during resume() -- whether or not it was reset
 * during the suspend/resume cycle -- and can't issue wakeup events.
 *
 * More power-aware drivers may also use low power states at runtime as
 * well as during system sleep states like PM_SUSPEND_STANDBY.  They may
 * be able to use wakeup events to exit from runtime low-power states,
 * or from system low-power states such as standby or suspend-to-RAM.
 */


extern void Model1_device_pm_lock(void);
extern void Model1_dpm_resume_start(Model1_pm_message_t Model1_state);
extern void Model1_dpm_resume_end(Model1_pm_message_t Model1_state);
extern void Model1_dpm_resume_noirq(Model1_pm_message_t Model1_state);
extern void Model1_dpm_resume_early(Model1_pm_message_t Model1_state);
extern void Model1_dpm_resume(Model1_pm_message_t Model1_state);
extern void Model1_dpm_complete(Model1_pm_message_t Model1_state);

extern void Model1_device_pm_unlock(void);
extern int Model1_dpm_suspend_end(Model1_pm_message_t Model1_state);
extern int Model1_dpm_suspend_start(Model1_pm_message_t Model1_state);
extern int Model1_dpm_suspend_noirq(Model1_pm_message_t Model1_state);
extern int Model1_dpm_suspend_late(Model1_pm_message_t Model1_state);
extern int Model1_dpm_suspend(Model1_pm_message_t Model1_state);
extern int Model1_dpm_prepare(Model1_pm_message_t Model1_state);

extern void Model1___suspend_report_result(const char *Model1_function, void *Model1_fn, int Model1_ret);






extern int Model1_device_pm_wait_for_dev(struct Model1_device *Model1_sub, struct Model1_device *Model1_dev);
extern void Model1_dpm_for_each_dev(void *Model1_data, void (*Model1_fn)(struct Model1_device *, void *));

extern int Model1_pm_generic_prepare(struct Model1_device *Model1_dev);
extern int Model1_pm_generic_suspend_late(struct Model1_device *Model1_dev);
extern int Model1_pm_generic_suspend_noirq(struct Model1_device *Model1_dev);
extern int Model1_pm_generic_suspend(struct Model1_device *Model1_dev);
extern int Model1_pm_generic_resume_early(struct Model1_device *Model1_dev);
extern int Model1_pm_generic_resume_noirq(struct Model1_device *Model1_dev);
extern int Model1_pm_generic_resume(struct Model1_device *Model1_dev);
extern int Model1_pm_generic_freeze_noirq(struct Model1_device *Model1_dev);
extern int Model1_pm_generic_freeze_late(struct Model1_device *Model1_dev);
extern int Model1_pm_generic_freeze(struct Model1_device *Model1_dev);
extern int Model1_pm_generic_thaw_noirq(struct Model1_device *Model1_dev);
extern int Model1_pm_generic_thaw_early(struct Model1_device *Model1_dev);
extern int Model1_pm_generic_thaw(struct Model1_device *Model1_dev);
extern int Model1_pm_generic_restore_noirq(struct Model1_device *Model1_dev);
extern int Model1_pm_generic_restore_early(struct Model1_device *Model1_dev);
extern int Model1_pm_generic_restore(struct Model1_device *Model1_dev);
extern int Model1_pm_generic_poweroff_noirq(struct Model1_device *Model1_dev);
extern int Model1_pm_generic_poweroff_late(struct Model1_device *Model1_dev);
extern int Model1_pm_generic_poweroff(struct Model1_device *Model1_dev);
extern void Model1_pm_generic_complete(struct Model1_device *Model1_dev);
extern void Model1_pm_complete_with_resume_check(struct Model1_device *Model1_dev);
/* How to reorder dpm_list after device_move() */
enum Model1_dpm_order {
 Model1_DPM_ORDER_NONE,
 Model1_DPM_ORDER_DEV_AFTER_PARENT,
 Model1_DPM_ORDER_PARENT_BEFORE_DEV,
 Model1_DPM_ORDER_DEV_LAST,
};





/*
 * fixmap.h: compile-time virtual memory allocation
 *
 * This file is subject to the terms and conditions of the GNU General Public
 * License.  See the file "COPYING" in the main directory of this archive
 * for more details.
 *
 * Copyright (C) 1998 Ingo Molnar
 *
 * Support of BIGMEM added by Gerhard Wichert, Siemens AG, July 1999
 * x86_32 and x86_64 integration by Gustavo F. Padovan, February 2009
 */










/*
 *  Copyright (C) 2001 Paul Diefenbaugh <paul.s.diefenbaugh@intel.com>
 *  Copyright (C) 2001 Patrick Mochel <mochel@osdl.org>
 *
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *
 *  This program is free software; you can redistribute it and/or modify
 *  it under the terms of the GNU General Public License as published by
 *  the Free Software Foundation; either version 2 of the License, or
 *  (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU General Public License for more details.
 *
 *  You should have received a copy of the GNU General Public License
 *  along with this program; if not, write to the Free Software
 *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
 *
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 */


/* _PDC bit definition for Intel processors */






/*
 * Written by: Matthew Dobson, IBM Corporation
 *
 * Copyright (C) 2002, IBM Corp.
 *
 * All rights reserved.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or
 * NON INFRINGEMENT.  See the GNU General Public License for more
 * details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
 *
 * Send feedback to <colpatch@us.ibm.com>
 */



/*
 * to preserve the visibility of NUMA_NO_NODE definition,
 * moved to there from here.  May be used independent of
 * CONFIG_NUMA.
 */
/* Mappings between logical cpu number and node number */
extern __attribute__((section(".data..percpu" ""))) __typeof__(int) Model1_x86_cpu_to_node_map; extern __typeof__(int) *Model1_x86_cpu_to_node_map_early_ptr; extern __typeof__(int) Model1_x86_cpu_to_node_map_early_map[];
/* Same function but used if called before per_cpu areas are setup */
static inline __attribute__((no_instrument_function)) int Model1_early_cpu_to_node(int Model1_cpu)
{
 return *((Model1_x86_cpu_to_node_map_early_ptr) ? &(Model1_x86_cpu_to_node_map_early_ptr)[Model1_cpu] : &(*({ do { const void *Model1___vpp_verify = (typeof((&(Model1_x86_cpu_to_node_map)) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); ({ unsigned long Model1___ptr; __asm__ ("" : "=r"(Model1___ptr) : "0"((typeof(*((&(Model1_x86_cpu_to_node_map)))) *)((&(Model1_x86_cpu_to_node_map))))); (typeof((typeof(*((&(Model1_x86_cpu_to_node_map)))) *)((&(Model1_x86_cpu_to_node_map))))) (Model1___ptr + (((Model1___per_cpu_offset[(Model1_cpu)])))); }); })));
}



/* Mappings between node number and cpus on that node. */
extern Model1_cpumask_var_t Model1_node_to_cpumask_map[(1 << 6)];




/* Returns a pointer to the cpumask of CPUs on Node 'node'. */
static inline __attribute__((no_instrument_function)) const struct Model1_cpumask *Model1_cpumask_of_node(int Model1_node)
{
 return Model1_node_to_cpumask_map[Model1_node];
}


extern void Model1_setup_node_to_cpumask_map(void);

/*
 * Returns the number of the node containing Node 'node'. This
 * architecture is flat, so it is a pretty simple function!
 */




extern int Model1___node_distance(int, int);
/*
 * linux/include/asm-generic/topology.h
 *
 * Written by: Matthew Dobson, IBM Corporation
 *
 * Copyright (C) 2002, IBM Corp.
 *
 * All rights reserved.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or
 * NON INFRINGEMENT.  See the GNU General Public License for more
 * details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
 *
 * Send feedback to <colpatch@us.ibm.com>
 */

extern const struct Model1_cpumask *Model1_cpu_coregroup_mask(int Model1_cpu);
extern unsigned int Model1___max_logical_packages;


extern int Model1___max_smt_threads;

static inline __attribute__((no_instrument_function)) int Model1_topology_max_smt_threads(void)
{
 return Model1___max_smt_threads;
}

int Model1_topology_update_package_map(unsigned int Model1_apicid, unsigned int Model1_cpu);
extern int Model1_topology_phys_to_logical_pkg(unsigned int Model1_pkg);
static inline __attribute__((no_instrument_function)) void Model1_arch_fix_phys_package_id(int Model1_num, Model1_u32 Model1_slot)
{
}

struct Model1_pci_bus;
int Model1_x86_pci_root_bus_node(int Model1_bus);
void Model1_x86_pci_root_bus_resources(int Model1_bus, struct Model1_list_head *Model1_resources);






/*
 * Too small node sizes may confuse the VM badly. Usually they
 * result from BIOS bugs. So dont recognize nodes as standalone
 * NUMA entities that have less than this amount of RAM listed:
 */


extern int Model1_numa_off;

/*
 * __apicid_to_node[] stores the raw mapping between physical apicid and
 * node and is used to initialize cpu_to_node mapping.
 *
 * The mapping may be overridden by apic->numa_cpu_node() on 32bit and thus
 * should be accessed by the accessors - set_apicid_to_node() and
 * numa_cpu_node().
 */
extern Model1_s16 Model1___apicid_to_node[32768];
extern Model1_nodemask_t Model1_numa_nodes_parsed __attribute__ ((__section__(".init.data")));

extern int __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_numa_add_memblk(int Model1_nodeid, Model1_u64 Model1_start, Model1_u64 Model1_end);
extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_numa_set_distance(int Model1_from, int Model1_to, int Model1_distance);

static inline __attribute__((no_instrument_function)) void Model1_set_apicid_to_node(int Model1_apicid, Model1_s16 Model1_node)
{
 Model1___apicid_to_node[Model1_apicid] = Model1_node;
}

extern int Model1_numa_cpu_node(int Model1_cpu);
extern void Model1_numa_set_node(int Model1_cpu, int Model1_node);
extern void Model1_numa_clear_node(int Model1_cpu);
extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_init_cpu_to_node(void);
extern void Model1_numa_add_cpu(int Model1_cpu);
extern void Model1_numa_remove_cpu(int Model1_cpu);
/*
 * fixmap.h: compile-time virtual memory allocation
 *
 * This file is subject to the terms and conditions of the GNU General Public
 * License.  See the file "COPYING" in the main directory of this archive
 * for more details.
 *
 * Copyright (C) 1998 Ingo Molnar
 *
 * Support of BIGMEM added by Gerhard Wichert, Siemens AG, July 1999
 * x86_32 and x86_64 integration by Gustavo F. Padovan, February 2009
 */







/*
 * The x86 doesn't have a mmu context, but
 * we put the segment information here.
 */
typedef struct {

 struct Model1_ldt_struct *Model1_ldt;



 /* True if mm supports a task running in 32 bit compatibility mode. */
 unsigned short Model1_ia32_compat;


 struct Model1_mutex Model1_lock;
 void *Model1_vdso; /* vdso base address */
 const struct Model1_vdso_image *Model1_vdso_image; /* vdso image in use */

 Model1_atomic_t Model1_perf_rdpmc_allowed; /* nonzero if rdpmc is allowed */
} Model1_mm_context_t;


void Model1_leave_mm(int Model1_cpu);








/*
 * This file contains the definitions for the x86 IO instructions
 * inb/inw/inl/outb/outw/outl and the "string versions" of the same
 * (insb/insw/insl/outsb/outsw/outsl). You can also use "pausing"
 * versions of the single-IO instructions (inb_p/inw_p/..).
 *
 * This file is not meant to be obfuscating: it's just complicated
 * to (a) handle it all in a way that makes gcc able to optimize it
 * as well as possible and (b) trying to avoid writing the same thing
 * over and over again with slight variations and possibly making a
 * mistake somewhere.
 */

/*
 * Thanks to James van Artsdalen for a better timing-fix than
 * the two short jumps: using outb's to a nonexistent port seems
 * to guarantee better timings even on fast machines.
 *
 * On the other hand, I'd like to be sure of a non-existent port:
 * I feel a bit unsafe about using 0x80 (should be safe, though)
 *
 *		Linus
 */

 /*
  *  Bit simplified and optimized by Jan Hubicka
  *  Support of BIGMEM added by Gerhard Wichert, Siemens AG, July 1999.
  *
  *  isa_memset_io, isa_memcpy_fromio, isa_memcpy_toio added,
  *  isa_read[wl] and isa_write[wl] fixed
  *  - Arnaldo Carvalho de Melo <acme@conectiva.com.br>
  */













/*
 * early_ioremap() and early_iounmap() are for temporary early boot-time
 * mappings, before the real ioremap() is functional.
 */
extern void *Model1_early_ioremap(Model1_resource_size_t Model1_phys_addr,
       unsigned long Model1_size);
extern void *Model1_early_memremap(Model1_resource_size_t Model1_phys_addr,
       unsigned long Model1_size);
extern void *Model1_early_memremap_ro(Model1_resource_size_t Model1_phys_addr,
          unsigned long Model1_size);
extern void Model1_early_iounmap(void *Model1_addr, unsigned long Model1_size);
extern void Model1_early_memunmap(void *Model1_addr, unsigned long Model1_size);

/*
 * Weak function called by early_ioremap_reset(). It does nothing, but
 * architectures may provide their own version to do any needed cleanups.
 */
extern void Model1_early_ioremap_shutdown(void);


/* Arch-specific initialization */
extern void Model1_early_ioremap_init(void);

/* Generic initialization called by architecture code */
extern void Model1_early_ioremap_setup(void);

/*
 * Called as last step in paging_init() so library can act
 * accordingly for subsequent map/unmap requests.
 */
extern void Model1_early_ioremap_reset(void);

/*
 * Early copy from unmapped memory to kernel mapped memory.
 */
extern void Model1_copy_from_early_mem(void *Model1_dest, Model1_phys_addr_t Model1_src,
    unsigned long Model1_size);
static inline __attribute__((no_instrument_function)) unsigned char Model1_readb(const volatile void *Model1_addr) { unsigned char Model1_ret; asm volatile("mov" "b" " %1,%0":"=q" (Model1_ret) :"m" (*(volatile unsigned char *)Model1_addr) :"memory"); return Model1_ret; }
static inline __attribute__((no_instrument_function)) unsigned short Model1_readw(const volatile void *Model1_addr) { unsigned short Model1_ret; asm volatile("mov" "w" " %1,%0":"=r" (Model1_ret) :"m" (*(volatile unsigned short *)Model1_addr) :"memory"); return Model1_ret; }
static inline __attribute__((no_instrument_function)) unsigned int Model1_readl(const volatile void *Model1_addr) { unsigned int Model1_ret; asm volatile("mov" "l" " %1,%0":"=r" (Model1_ret) :"m" (*(volatile unsigned int *)Model1_addr) :"memory"); return Model1_ret; }

static inline __attribute__((no_instrument_function)) unsigned char Model1___readb(const volatile void *Model1_addr) { unsigned char Model1_ret; asm volatile("mov" "b" " %1,%0":"=q" (Model1_ret) :"m" (*(volatile unsigned char *)Model1_addr) ); return Model1_ret; }
static inline __attribute__((no_instrument_function)) unsigned short Model1___readw(const volatile void *Model1_addr) { unsigned short Model1_ret; asm volatile("mov" "w" " %1,%0":"=r" (Model1_ret) :"m" (*(volatile unsigned short *)Model1_addr) ); return Model1_ret; }
static inline __attribute__((no_instrument_function)) unsigned int Model1___readl(const volatile void *Model1_addr) { unsigned int Model1_ret; asm volatile("mov" "l" " %1,%0":"=r" (Model1_ret) :"m" (*(volatile unsigned int *)Model1_addr) ); return Model1_ret; }

static inline __attribute__((no_instrument_function)) void Model1_writeb(unsigned char Model1_val, volatile void *Model1_addr) { asm volatile("mov" "b" " %0,%1": :"q" (Model1_val), "m" (*(volatile unsigned char *)Model1_addr) :"memory"); }
static inline __attribute__((no_instrument_function)) void Model1_writew(unsigned short Model1_val, volatile void *Model1_addr) { asm volatile("mov" "w" " %0,%1": :"r" (Model1_val), "m" (*(volatile unsigned short *)Model1_addr) :"memory"); }
static inline __attribute__((no_instrument_function)) void Model1_writel(unsigned int Model1_val, volatile void *Model1_addr) { asm volatile("mov" "l" " %0,%1": :"r" (Model1_val), "m" (*(volatile unsigned int *)Model1_addr) :"memory"); }

static inline __attribute__((no_instrument_function)) void Model1___writeb(unsigned char Model1_val, volatile void *Model1_addr) { asm volatile("mov" "b" " %0,%1": :"q" (Model1_val), "m" (*(volatile unsigned char *)Model1_addr) ); }
static inline __attribute__((no_instrument_function)) void Model1___writew(unsigned short Model1_val, volatile void *Model1_addr) { asm volatile("mov" "w" " %0,%1": :"r" (Model1_val), "m" (*(volatile unsigned short *)Model1_addr) ); }
static inline __attribute__((no_instrument_function)) void Model1___writel(unsigned int Model1_val, volatile void *Model1_addr) { asm volatile("mov" "l" " %0,%1": :"r" (Model1_val), "m" (*(volatile unsigned int *)Model1_addr) ); }
static inline __attribute__((no_instrument_function)) unsigned long Model1_readq(const volatile void *Model1_addr) { unsigned long Model1_ret; asm volatile("mov" "q" " %1,%0":"=r" (Model1_ret) :"m" (*(volatile unsigned long *)Model1_addr) :"memory"); return Model1_ret; }
static inline __attribute__((no_instrument_function)) void Model1_writeq(unsigned long Model1_val, volatile void *Model1_addr) { asm volatile("mov" "q" " %0,%1": :"r" (Model1_val), "m" (*(volatile unsigned long *)Model1_addr) :"memory"); }







/* Let people know that we have them */





/**
 *	virt_to_phys	-	map virtual addresses to physical
 *	@address: address to remap
 *
 *	The returned physical address is the physical (CPU) mapping for
 *	the memory address given. It is only valid to use this function on
 *	addresses directly mapped or allocated via kmalloc.
 *
 *	This function does not give bus mappings for DMA transfers. In
 *	almost all conceivable cases a device driver should not be using
 *	this function
 */

static inline __attribute__((no_instrument_function)) Model1_phys_addr_t Model1_virt_to_phys(volatile void *Model1_address)
{
 return Model1___phys_addr_nodebug((unsigned long)(Model1_address));
}

/**
 *	phys_to_virt	-	map physical address to virtual
 *	@address: address to remap
 *
 *	The returned virtual address is a current CPU mapping for
 *	the memory address given. It is only valid to use this function on
 *	addresses that have a kernel mapping
 *
 *	This function does not handle bus mappings for DMA transfers. In
 *	almost all conceivable cases a device driver should not be using
 *	this function
 */

static inline __attribute__((no_instrument_function)) void *Model1_phys_to_virt(Model1_phys_addr_t Model1_address)
{
 return ((void *)((unsigned long)(Model1_address)+((unsigned long)(0xffff880000000000UL))));
}

/*
 * Change "struct page" to physical address.
 */


/*
 * ISA I/O bus memory addresses are 1:1 with the physical address.
 * However, we truncate the address to unsigned int to avoid undesirable
 * promitions in legacy drivers.
 */
static inline __attribute__((no_instrument_function)) unsigned int Model1_isa_virt_to_bus(volatile void *Model1_address)
{
 return (unsigned int)Model1_virt_to_phys(Model1_address);
}



/*
 * However PCI ones are not necessarily 1:1 and therefore these interfaces
 * are forbidden in portable PCI drivers.
 *
 * Allow them on x86 for legacy drivers, though.
 */



/**
 * ioremap     -   map bus memory into CPU space
 * @offset:    bus address of the memory
 * @size:      size of the resource to map
 *
 * ioremap performs a platform specific sequence of operations to
 * make bus memory CPU accessible via the readb/readw/readl/writeb/
 * writew/writel functions and the other mmio helpers. The returned
 * address is not guaranteed to be usable directly as a virtual
 * address.
 *
 * If the area you are trying to map is a PCI BAR you should have a
 * look at pci_iomap().
 */
extern void *Model1_ioremap_nocache(Model1_resource_size_t Model1_offset, unsigned long Model1_size);
extern void *Model1_ioremap_uc(Model1_resource_size_t Model1_offset, unsigned long Model1_size);


extern void *Model1_ioremap_cache(Model1_resource_size_t Model1_offset, unsigned long Model1_size);
extern void *Model1_ioremap_prot(Model1_resource_size_t Model1_offset, unsigned long Model1_size,
    unsigned long Model1_prot_val);

/*
 * The default ioremap() behavior is non-cached:
 */
static inline __attribute__((no_instrument_function)) void *Model1_ioremap(Model1_resource_size_t Model1_offset, unsigned long Model1_size)
{
 return Model1_ioremap_nocache(Model1_offset, Model1_size);
}

extern void Model1_iounmap(volatile void *Model1_addr);

extern void Model1_set_iounmap_nonlazy(void);










/*
 * These are the "generic" interfaces for doing new-style
 * memory-mapped or PIO accesses. Architectures may do
 * their own arch-optimized versions, these just act as
 * wrappers around the old-style IO register access functions:
 * read[bwl]/write[bwl]/in[bwl]/out[bwl]
 *
 * Don't include this directly, include it from <asm/io.h>.
 */

/*
 * Read/write from/to an (offsettable) iomem cookie. It might be a PIO
 * access or a MMIO access, these functions don't care. The info is
 * encoded in the hardware mapping set up by the mapping functions
 * (or the cookie itself, depending on implementation and hw).
 *
 * The generic routines just encode the PIO/MMIO as part of the
 * cookie, and coldly assume that the MMIO IO mappings are not
 * in the low address range. Architectures for which this is not
 * true can't use this generic implementation.
 */
extern unsigned int Model1_ioread8(void *);
extern unsigned int Model1_ioread16(void *);
extern unsigned int Model1_ioread16be(void *);
extern unsigned int Model1_ioread32(void *);
extern unsigned int Model1_ioread32be(void *);

extern Model1_u64 Model1_ioread64(void *);
extern Model1_u64 Model1_ioread64be(void *);


extern void Model1_iowrite8(Model1_u8, void *);
extern void Model1_iowrite16(Model1_u16, void *);
extern void Model1_iowrite16be(Model1_u16, void *);
extern void Model1_iowrite32(Model1_u32, void *);
extern void Model1_iowrite32be(Model1_u32, void *);

extern void Model1_iowrite64(Model1_u64, void *);
extern void Model1_iowrite64be(Model1_u64, void *);


/*
 * "string" versions of the above. Note that they
 * use native byte ordering for the accesses (on
 * the assumption that IO and memory agree on a
 * byte order, and CPU byteorder is irrelevant).
 *
 * They do _not_ update the port address. If you
 * want MMIO that copies stuff laid out in MMIO
 * memory across multiple ports, use "memcpy_toio()"
 * and friends.
 */
extern void Model1_ioread8_rep(void *Model1_port, void *Model1_buf, unsigned long Model1_count);
extern void Model1_ioread16_rep(void *Model1_port, void *Model1_buf, unsigned long Model1_count);
extern void Model1_ioread32_rep(void *Model1_port, void *Model1_buf, unsigned long Model1_count);

extern void Model1_iowrite8_rep(void *Model1_port, const void *Model1_buf, unsigned long Model1_count);
extern void Model1_iowrite16_rep(void *Model1_port, const void *Model1_buf, unsigned long Model1_count);
extern void Model1_iowrite32_rep(void *Model1_port, const void *Model1_buf, unsigned long Model1_count);


/* Create a virtual mapping cookie for an IO port range */
extern void *Model1_ioport_map(unsigned long Model1_port, unsigned int Model1_nr);
extern void Model1_ioport_unmap(void *);
/* Destroy a virtual mapping cookie for a PCI BAR (memory or IO) */
struct Model1_pci_dev;
extern void Model1_pci_iounmap(struct Model1_pci_dev *Model1_dev, void *);







/* Generic I/O port emulation, based on MN10300 code
 *
 * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.
 * Written by David Howells (dhowells@redhat.com)
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public Licence
 * as published by the Free Software Foundation; either version
 * 2 of the Licence, or (at your option) any later version.
 */



struct Model1_pci_dev;

/* Create a virtual mapping cookie for a PCI BAR (memory or IO) */
extern void *Model1_pci_iomap(struct Model1_pci_dev *Model1_dev, int Model1_bar, unsigned long Model1_max);
extern void *Model1_pci_iomap_wc(struct Model1_pci_dev *Model1_dev, int Model1_bar, unsigned long Model1_max);
extern void *Model1_pci_iomap_range(struct Model1_pci_dev *Model1_dev, int Model1_bar,
         unsigned long Model1_offset,
         unsigned long Model1_maxlen);
extern void *Model1_pci_iomap_wc_range(struct Model1_pci_dev *Model1_dev, int Model1_bar,
     unsigned long Model1_offset,
     unsigned long Model1_maxlen);
/* Create a virtual mapping cookie for a port on a given PCI device.
 * Do not call this directly, it exists to make it easier for architectures
 * to override */

/*
 * Convert a virtual cached pointer to an uncached pointer
 */


static inline __attribute__((no_instrument_function)) void
Model1_memset_io(volatile void *Model1_addr, unsigned char Model1_val, Model1_size_t Model1_count)
{
 memset((void *)Model1_addr, Model1_val, Model1_count);
}

static inline __attribute__((no_instrument_function)) void
Model1_memcpy_fromio(void *Model1_dst, const volatile void *Model1_src, Model1_size_t Model1_count)
{
 ({ Model1_size_t Model1___len = (Model1_count); void *Model1___ret; if (__builtin_constant_p(Model1_count) && Model1___len >= 64) Model1___ret = Model1___memcpy((Model1_dst), ((const void *)Model1_src), Model1___len); else Model1___ret = __builtin_memcpy((Model1_dst), ((const void *)Model1_src), Model1___len); Model1___ret; });
}

static inline __attribute__((no_instrument_function)) void
Model1_memcpy_toio(volatile void *Model1_dst, const void *Model1_src, Model1_size_t Model1_count)
{
 ({ Model1_size_t Model1___len = (Model1_count); void *Model1___ret; if (__builtin_constant_p(Model1_count) && Model1___len >= 64) Model1___ret = Model1___memcpy(((void *)Model1_dst), (Model1_src), Model1___len); else Model1___ret = __builtin_memcpy(((void *)Model1_dst), (Model1_src), Model1___len); Model1___ret; });
}

/*
 * ISA space is 'always mapped' on a typical x86 system, no need to
 * explicitly ioremap() it. The fact that the ISA IO space is mapped
 * to PAGE_OFFSET is pure coincidence - it does not mean ISA values
 * are physical addresses. The following constant pointer can be
 * used as the IO-area pointer (it can be iounmapped as well, so the
 * analogy with PCI is quite large):
 */


/*
 *	Cache management
 *
 *	This needed for two cases
 *	1. Out of order aware processors
 *	2. Accidentally out of order processors (PPro errata #51)
 */

static inline __attribute__((no_instrument_function)) void Model1_flush_write_buffers(void)
{



}



extern void Model1_native_io_delay(void);

extern int Model1_io_delay_type;
extern void Model1_io_delay_init(void);





static inline __attribute__((no_instrument_function)) void Model1_slow_down_io(void)
{
 Model1_native_io_delay();





}
static inline __attribute__((no_instrument_function)) void Model1_outb(unsigned char Model1_value, int Model1_port) { asm volatile("out" "b" " %" "b" "0, %w1" : : "a"(Model1_value), "Nd"(Model1_port)); } static inline __attribute__((no_instrument_function)) unsigned char Model1_inb(int Model1_port) { unsigned char Model1_value; asm volatile("in" "b" " %w1, %" "b" "0" : "=a"(Model1_value) : "Nd"(Model1_port)); return Model1_value; } static inline __attribute__((no_instrument_function)) void Model1_outb_p(unsigned char Model1_value, int Model1_port) { Model1_outb(Model1_value, Model1_port); Model1_slow_down_io(); } static inline __attribute__((no_instrument_function)) unsigned char Model1_inb_p(int Model1_port) { unsigned char Model1_value = Model1_inb(Model1_port); Model1_slow_down_io(); return Model1_value; } static inline __attribute__((no_instrument_function)) void Model1_outsb(int Model1_port, const void *Model1_addr, unsigned long Model1_count) { asm volatile("rep; outs" "b" : "+S"(Model1_addr), "+c"(Model1_count) : "d"(Model1_port)); } static inline __attribute__((no_instrument_function)) void Model1_insb(int Model1_port, void *Model1_addr, unsigned long Model1_count) { asm volatile("rep; ins" "b" : "+D"(Model1_addr), "+c"(Model1_count) : "d"(Model1_port)); }
static inline __attribute__((no_instrument_function)) void Model1_outw(unsigned short Model1_value, int Model1_port) { asm volatile("out" "w" " %" "w" "0, %w1" : : "a"(Model1_value), "Nd"(Model1_port)); } static inline __attribute__((no_instrument_function)) unsigned short Model1_inw(int Model1_port) { unsigned short Model1_value; asm volatile("in" "w" " %w1, %" "w" "0" : "=a"(Model1_value) : "Nd"(Model1_port)); return Model1_value; } static inline __attribute__((no_instrument_function)) void Model1_outw_p(unsigned short Model1_value, int Model1_port) { Model1_outw(Model1_value, Model1_port); Model1_slow_down_io(); } static inline __attribute__((no_instrument_function)) unsigned short Model1_inw_p(int Model1_port) { unsigned short Model1_value = Model1_inw(Model1_port); Model1_slow_down_io(); return Model1_value; } static inline __attribute__((no_instrument_function)) void Model1_outsw(int Model1_port, const void *Model1_addr, unsigned long Model1_count) { asm volatile("rep; outs" "w" : "+S"(Model1_addr), "+c"(Model1_count) : "d"(Model1_port)); } static inline __attribute__((no_instrument_function)) void Model1_insw(int Model1_port, void *Model1_addr, unsigned long Model1_count) { asm volatile("rep; ins" "w" : "+D"(Model1_addr), "+c"(Model1_count) : "d"(Model1_port)); }
static inline __attribute__((no_instrument_function)) void Model1_outl(unsigned int Model1_value, int Model1_port) { asm volatile("out" "l" " %" "" "0, %w1" : : "a"(Model1_value), "Nd"(Model1_port)); } static inline __attribute__((no_instrument_function)) unsigned int Model1_inl(int Model1_port) { unsigned int Model1_value; asm volatile("in" "l" " %w1, %" "" "0" : "=a"(Model1_value) : "Nd"(Model1_port)); return Model1_value; } static inline __attribute__((no_instrument_function)) void Model1_outl_p(unsigned int Model1_value, int Model1_port) { Model1_outl(Model1_value, Model1_port); Model1_slow_down_io(); } static inline __attribute__((no_instrument_function)) unsigned int Model1_inl_p(int Model1_port) { unsigned int Model1_value = Model1_inl(Model1_port); Model1_slow_down_io(); return Model1_value; } static inline __attribute__((no_instrument_function)) void Model1_outsl(int Model1_port, const void *Model1_addr, unsigned long Model1_count) { asm volatile("rep; outs" "l" : "+S"(Model1_addr), "+c"(Model1_count) : "d"(Model1_port)); } static inline __attribute__((no_instrument_function)) void Model1_insl(int Model1_port, void *Model1_addr, unsigned long Model1_count) { asm volatile("rep; ins" "l" : "+D"(Model1_addr), "+c"(Model1_count) : "d"(Model1_port)); }

extern void *Model1_xlate_dev_mem_ptr(Model1_phys_addr_t Model1_phys);
extern void Model1_unxlate_dev_mem_ptr(Model1_phys_addr_t Model1_phys, void *Model1_addr);

extern int Model1_ioremap_change_attr(unsigned long Model1_vaddr, unsigned long Model1_size,
    enum Model1_page_cache_mode Model1_pcm);
extern void *Model1_ioremap_wc(Model1_resource_size_t Model1_offset, unsigned long Model1_size);
extern void *Model1_ioremap_wt(Model1_resource_size_t Model1_offset, unsigned long Model1_size);

extern bool Model1_is_early_ioremap_ptep(Model1_pte_t *Model1_ptep);
extern int __attribute__((warn_unused_result)) Model1_arch_phys_wc_index(int Model1_handle);


extern int __attribute__((warn_unused_result)) Model1_arch_phys_wc_add(unsigned long Model1_base,
      unsigned long Model1_size);
extern void Model1_arch_phys_wc_del(int Model1_handle);

/* This must match data at realmode.S */
struct Model1_real_mode_header {
 Model1_u32 Model1_text_start;
 Model1_u32 Model1_ro_end;
 /* SMP trampoline */
 Model1_u32 Model1_trampoline_start;
 Model1_u32 Model1_trampoline_status;
 Model1_u32 Model1_trampoline_header;

 Model1_u32 Model1_trampoline_pgd;

 /* ACPI S3 wakeup */

 Model1_u32 Model1_wakeup_start;
 Model1_u32 Model1_wakeup_header;

 /* APM/BIOS reboot */
 Model1_u32 Model1_machine_real_restart_asm;

 Model1_u32 Model1_machine_real_restart_seg;

};

/* This must match data at trampoline_32/64.S */
struct Model1_trampoline_header {






 Model1_u64 Model1_start;
 Model1_u64 Model1_efer;
 Model1_u32 Model1_cr4;

};

extern struct Model1_real_mode_header *Model1_real_mode_header;
extern unsigned char Model1_real_mode_blob_end[];

extern unsigned long Model1_init_rsp;
extern unsigned long Model1_initial_code;
extern unsigned long Model1_initial_gs;

extern unsigned char Model1_real_mode_blob[];
extern unsigned char Model1_real_mode_relocs[];





extern unsigned char Model1_secondary_startup_64[];


static inline __attribute__((no_instrument_function)) Model1_size_t Model1_real_mode_size_needed(void)
{
 if (Model1_real_mode_header)
  return 0; /* already allocated. */

 return ((((Model1_real_mode_blob_end - Model1_real_mode_blob)) + ((typeof((Model1_real_mode_blob_end - Model1_real_mode_blob)))((((1UL) << 12))) - 1)) & ~((typeof((Model1_real_mode_blob_end - Model1_real_mode_blob)))((((1UL) << 12))) - 1));
}

void Model1_set_real_mode_mem(Model1_phys_addr_t Model1_mem, Model1_size_t Model1_size);
void Model1_reserve_real_mode(void);






extern int Model1_acpi_lapic;
extern int Model1_acpi_ioapic;
extern int Model1_acpi_noirq;
extern int Model1_acpi_strict;
extern int Model1_acpi_disabled;
extern int Model1_acpi_pci_disabled;
extern int Model1_acpi_skip_timer_override;
extern int Model1_acpi_use_timer_override;
extern int Model1_acpi_fix_pin2_polarity;
extern int Model1_acpi_disable_cmcff;

extern Model1_u8 Model1_acpi_sci_flags;
extern int Model1_acpi_sci_override_gsi;
void Model1_acpi_pic_sci_set_trigger(unsigned int, Model1_u16);

extern int (*Model1___acpi_register_gsi)(struct Model1_device *Model1_dev, Model1_u32 Model1_gsi,
      int Model1_trigger, int Model1_polarity);
extern void (*Model1___acpi_unregister_gsi)(Model1_u32 Model1_gsi);

static inline __attribute__((no_instrument_function)) void Model1_disable_acpi(void)
{
 Model1_acpi_disabled = 1;
 Model1_acpi_pci_disabled = 1;
 Model1_acpi_noirq = 1;
}

extern int Model1_acpi_gsi_to_irq(Model1_u32 Model1_gsi, unsigned int *Model1_irq);

static inline __attribute__((no_instrument_function)) void Model1_acpi_noirq_set(void) { Model1_acpi_noirq = 1; }
static inline __attribute__((no_instrument_function)) void Model1_acpi_disable_pci(void)
{
 Model1_acpi_pci_disabled = 1;
 Model1_acpi_noirq_set();
}

/* Low-level suspend routine. */
extern int (*Model1_acpi_suspend_lowlevel)(void);

/* Physical address to resume after wakeup */


/*
 * Check if the CPU can handle C2 and deeper
 */
static inline __attribute__((no_instrument_function)) unsigned int Model1_acpi_processor_cstate_check(unsigned int Model1_max_cstate)
{
 /*
	 * Early models (<=5) of AMD Opterons are not supposed to go into
	 * C2 state.
	 *
	 * Steppings 0x0A and later are good
	 */
 if (Model1_boot_cpu_data.Model1_x86 == 0x0F &&
     Model1_boot_cpu_data.Model1_x86_vendor == 2 &&
     Model1_boot_cpu_data.Model1_x86_model <= 0x05 &&
     Model1_boot_cpu_data.Model1_x86_mask < 0x0A)
  return 1;
 else if (Model1_amd_e400_c1e_detected)
  return 1;
 else
  return Model1_max_cstate;
}

static inline __attribute__((no_instrument_function)) bool Model1_arch_has_acpi_pdc(void)
{
 struct Model1_cpuinfo_x86 *Model1_c = &(*({ do { const void *Model1___vpp_verify = (typeof((&(Model1_cpu_info)) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); ({ unsigned long Model1___ptr; __asm__ ("" : "=r"(Model1___ptr) : "0"((typeof(*((&(Model1_cpu_info)))) *)((&(Model1_cpu_info))))); (typeof((typeof(*((&(Model1_cpu_info)))) *)((&(Model1_cpu_info))))) (Model1___ptr + (((Model1___per_cpu_offset[(0)])))); }); }));
 return (Model1_c->Model1_x86_vendor == 0 ||
  Model1_c->Model1_x86_vendor == 5);
}

static inline __attribute__((no_instrument_function)) void Model1_arch_acpi_set_pdc_bits(Model1_u32 *Model1_buf)
{
 struct Model1_cpuinfo_x86 *Model1_c = &(*({ do { const void *Model1___vpp_verify = (typeof((&(Model1_cpu_info)) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); ({ unsigned long Model1___ptr; __asm__ ("" : "=r"(Model1___ptr) : "0"((typeof(*((&(Model1_cpu_info)))) *)((&(Model1_cpu_info))))); (typeof((typeof(*((&(Model1_cpu_info)))) *)((&(Model1_cpu_info))))) (Model1___ptr + (((Model1___per_cpu_offset[(0)])))); }); }));

 Model1_buf[2] |= ((0x0010) | (0x0008) | (0x0002) | (0x0100) | (0x0200));

 if ((__builtin_constant_p(( 4*32+ 7)) && ( (((( 4*32+ 7))>>5)==(0) && (1UL<<((( 4*32+ 7))&31) & ((1<<(( 0*32+ 0) & 31))|(1<<(( 0*32+ 3)) & 31)|(1<<(( 0*32+ 5) & 31))|(1<<(( 0*32+ 6) & 31))| (1<<(( 0*32+ 8) & 31))|(1<<(( 0*32+13)) & 31)|(1<<(( 0*32+24) & 31))|(1<<(( 0*32+15) & 31))| (1<<(( 0*32+25) & 31))|(1<<(( 0*32+26) & 31))) )) || (((( 4*32+ 7))>>5)==(1) && (1UL<<((( 4*32+ 7))&31) & ((1<<(( 1*32+29) & 31))|0) )) || (((( 4*32+ 7))>>5)==(2) && (1UL<<((( 4*32+ 7))&31) & 0 )) || (((( 4*32+ 7))>>5)==(3) && (1UL<<((( 4*32+ 7))&31) & ((1<<(( 3*32+20) & 31))) )) || (((( 4*32+ 7))>>5)==(4) && (1UL<<((( 4*32+ 7))&31) & (0) )) || (((( 4*32+ 7))>>5)==(5) && (1UL<<((( 4*32+ 7))&31) & 0 )) || (((( 4*32+ 7))>>5)==(6) && (1UL<<((( 4*32+ 7))&31) & 0 )) || (((( 4*32+ 7))>>5)==(7) && (1UL<<((( 4*32+ 7))&31) & 0 )) || (((( 4*32+ 7))>>5)==(8) && (1UL<<((( 4*32+ 7))&31) & 0 )) || (((( 4*32+ 7))>>5)==(9) && (1UL<<((( 4*32+ 7))&31) & 0 )) || (((( 4*32+ 7))>>5)==(10) && (1UL<<((( 4*32+ 7))&31) & 0 )) || (((( 4*32+ 7))>>5)==(11) && (1UL<<((( 4*32+ 7))&31) & 0 )) || (((( 4*32+ 7))>>5)==(12) && (1UL<<((( 4*32+ 7))&31) & 0 )) || (((( 4*32+ 7))>>5)==(13) && (1UL<<((( 4*32+ 7))&31) & 0 )) || (((( 4*32+ 7))>>5)==(14) && (1UL<<((( 4*32+ 7))&31) & 0 )) || (((( 4*32+ 7))>>5)==(15) && (1UL<<((( 4*32+ 7))&31) & 0 )) || (((( 4*32+ 7))>>5)==(16) && (1UL<<((( 4*32+ 7))&31) & 0 )) || (((( 4*32+ 7))>>5)==(17) && (1UL<<((( 4*32+ 7))&31) & 0 )) || (sizeof(struct { int:-!!(18 != 18); })) || (sizeof(struct { int:-!!(18 != 18); }))) ? 1 : (__builtin_constant_p((( 4*32+ 7))) ? Model1_constant_test_bit((( 4*32+ 7)), ((unsigned long *)((Model1_c)->Model1_x86_capability))) : Model1_variable_test_bit((( 4*32+ 7)), ((unsigned long *)((Model1_c)->Model1_x86_capability))))))
  Model1_buf[2] |= ((0x0008) | (0x0002) | (0x0020) | (0x0800) | (0x0001));

 if ((__builtin_constant_p(( 0*32+22)) && ( (((( 0*32+22))>>5)==(0) && (1UL<<((( 0*32+22))&31) & ((1<<(( 0*32+ 0) & 31))|(1<<(( 0*32+ 3)) & 31)|(1<<(( 0*32+ 5) & 31))|(1<<(( 0*32+ 6) & 31))| (1<<(( 0*32+ 8) & 31))|(1<<(( 0*32+13)) & 31)|(1<<(( 0*32+24) & 31))|(1<<(( 0*32+15) & 31))| (1<<(( 0*32+25) & 31))|(1<<(( 0*32+26) & 31))) )) || (((( 0*32+22))>>5)==(1) && (1UL<<((( 0*32+22))&31) & ((1<<(( 1*32+29) & 31))|0) )) || (((( 0*32+22))>>5)==(2) && (1UL<<((( 0*32+22))&31) & 0 )) || (((( 0*32+22))>>5)==(3) && (1UL<<((( 0*32+22))&31) & ((1<<(( 3*32+20) & 31))) )) || (((( 0*32+22))>>5)==(4) && (1UL<<((( 0*32+22))&31) & (0) )) || (((( 0*32+22))>>5)==(5) && (1UL<<((( 0*32+22))&31) & 0 )) || (((( 0*32+22))>>5)==(6) && (1UL<<((( 0*32+22))&31) & 0 )) || (((( 0*32+22))>>5)==(7) && (1UL<<((( 0*32+22))&31) & 0 )) || (((( 0*32+22))>>5)==(8) && (1UL<<((( 0*32+22))&31) & 0 )) || (((( 0*32+22))>>5)==(9) && (1UL<<((( 0*32+22))&31) & 0 )) || (((( 0*32+22))>>5)==(10) && (1UL<<((( 0*32+22))&31) & 0 )) || (((( 0*32+22))>>5)==(11) && (1UL<<((( 0*32+22))&31) & 0 )) || (((( 0*32+22))>>5)==(12) && (1UL<<((( 0*32+22))&31) & 0 )) || (((( 0*32+22))>>5)==(13) && (1UL<<((( 0*32+22))&31) & 0 )) || (((( 0*32+22))>>5)==(14) && (1UL<<((( 0*32+22))&31) & 0 )) || (((( 0*32+22))>>5)==(15) && (1UL<<((( 0*32+22))&31) & 0 )) || (((( 0*32+22))>>5)==(16) && (1UL<<((( 0*32+22))&31) & 0 )) || (((( 0*32+22))>>5)==(17) && (1UL<<((( 0*32+22))&31) & 0 )) || (sizeof(struct { int:-!!(18 != 18); })) || (sizeof(struct { int:-!!(18 != 18); }))) ? 1 : (__builtin_constant_p((( 0*32+22))) ? Model1_constant_test_bit((( 0*32+22)), ((unsigned long *)((Model1_c)->Model1_x86_capability))) : Model1_variable_test_bit((( 0*32+22)), ((unsigned long *)((Model1_c)->Model1_x86_capability))))))
  Model1_buf[2] |= (0x0004);

 /*
	 * If mwait/monitor is unsupported, C2/C3_FFH will be disabled
	 */
 if (!(__builtin_constant_p(( 4*32+ 3)) && ( (((( 4*32+ 3))>>5)==(0) && (1UL<<((( 4*32+ 3))&31) & ((1<<(( 0*32+ 0) & 31))|(1<<(( 0*32+ 3)) & 31)|(1<<(( 0*32+ 5) & 31))|(1<<(( 0*32+ 6) & 31))| (1<<(( 0*32+ 8) & 31))|(1<<(( 0*32+13)) & 31)|(1<<(( 0*32+24) & 31))|(1<<(( 0*32+15) & 31))| (1<<(( 0*32+25) & 31))|(1<<(( 0*32+26) & 31))) )) || (((( 4*32+ 3))>>5)==(1) && (1UL<<((( 4*32+ 3))&31) & ((1<<(( 1*32+29) & 31))|0) )) || (((( 4*32+ 3))>>5)==(2) && (1UL<<((( 4*32+ 3))&31) & 0 )) || (((( 4*32+ 3))>>5)==(3) && (1UL<<((( 4*32+ 3))&31) & ((1<<(( 3*32+20) & 31))) )) || (((( 4*32+ 3))>>5)==(4) && (1UL<<((( 4*32+ 3))&31) & (0) )) || (((( 4*32+ 3))>>5)==(5) && (1UL<<((( 4*32+ 3))&31) & 0 )) || (((( 4*32+ 3))>>5)==(6) && (1UL<<((( 4*32+ 3))&31) & 0 )) || (((( 4*32+ 3))>>5)==(7) && (1UL<<((( 4*32+ 3))&31) & 0 )) || (((( 4*32+ 3))>>5)==(8) && (1UL<<((( 4*32+ 3))&31) & 0 )) || (((( 4*32+ 3))>>5)==(9) && (1UL<<((( 4*32+ 3))&31) & 0 )) || (((( 4*32+ 3))>>5)==(10) && (1UL<<((( 4*32+ 3))&31) & 0 )) || (((( 4*32+ 3))>>5)==(11) && (1UL<<((( 4*32+ 3))&31) & 0 )) || (((( 4*32+ 3))>>5)==(12) && (1UL<<((( 4*32+ 3))&31) & 0 )) || (((( 4*32+ 3))>>5)==(13) && (1UL<<((( 4*32+ 3))&31) & 0 )) || (((( 4*32+ 3))>>5)==(14) && (1UL<<((( 4*32+ 3))&31) & 0 )) || (((( 4*32+ 3))>>5)==(15) && (1UL<<((( 4*32+ 3))&31) & 0 )) || (((( 4*32+ 3))>>5)==(16) && (1UL<<((( 4*32+ 3))&31) & 0 )) || (((( 4*32+ 3))>>5)==(17) && (1UL<<((( 4*32+ 3))&31) & 0 )) || (sizeof(struct { int:-!!(18 != 18); })) || (sizeof(struct { int:-!!(18 != 18); }))) ? 1 : (__builtin_constant_p((( 4*32+ 3))) ? Model1_constant_test_bit((( 4*32+ 3)), ((unsigned long *)((Model1_c)->Model1_x86_capability))) : Model1_variable_test_bit((( 4*32+ 3)), ((unsigned long *)((Model1_c)->Model1_x86_capability))))))
  Model1_buf[2] &= ~((0x0200));
}

static inline __attribute__((no_instrument_function)) bool Model1_acpi_has_cpu_in_madt(void)
{
 return !!Model1_acpi_lapic;
}
extern int Model1_x86_acpi_numa_init(void);









enum Model1_vsyscall_num {
 Model1___NR_vgettimeofday,
 Model1___NR_vtime,
 Model1___NR_vgetcpu,
};


/*
 * We can't declare FIXADDR_TOP as variable for x86_64 because vsyscall
 * uses fixmaps that relies on FIXADDR_TOP for proper address calculation.
 * Because of this, FIXADDR_TOP x86 integration was left as later work.
 */
/*
 * Here we define all the compile-time 'special' virtual
 * addresses. The point is to have a constant address at
 * compile time, but to set the physical address only
 * in the boot process.
 * for x86_32: We allocate these special addresses
 * from the end of virtual memory (0xfffff000) backwards.
 * Also this lets us do fail-safe vmalloc(), we
 * can guarantee that these special addresses and
 * vmalloc()-ed addresses never overlap.
 *
 * These 'compile-time allocated' memory buffers are
 * fixed-size 4k pages (or larger if used with an increment
 * higher than 1). Use set_fixmap(idx,phys) to associate
 * physical memory with fixmap indices.
 *
 * TLB entries of such buffers will not be flushed across
 * task switches.
 */
enum Model1_fixed_addresses {




 Model1_VSYSCALL_PAGE = (((((((-10UL << 20) + ((1UL) << 12))-1) | ((__typeof__((-10UL << 20) + ((1UL) << 12)))((1<<21)-1)))+1) - ((1UL) << 12)) - (-10UL << 20)) >> 12,


 Model1_FIX_DBGP_BASE,
 Model1_FIX_EARLYCON_MEM_BASE,

 Model1_FIX_OHCI1394_BASE,


 Model1_FIX_APIC_BASE, /* local (CPU) APIC) -- required for SMP or not */


 Model1_FIX_IO_APIC_BASE_0,
 Model1_FIX_IO_APIC_BASE_END = Model1_FIX_IO_APIC_BASE_0 + 128 - 1,

 Model1_FIX_RO_IDT, /* Virtual mapping for read-only IDT */
 Model1_FIX_TEXT_POKE1, /* reserve 2 pages for text_poke() */
 Model1_FIX_TEXT_POKE0, /* first page is last, because allocation is backward */



 Model1___end_of_permanent_fixed_addresses,

 /*
	 * 512 temporary boot-time mappings, used by early_ioremap(),
	 * before ioremap() is functional.
	 *
	 * If necessary we round it up to the next 512 pages boundary so
	 * that we can have a single pgd entry and a single pte table:
	 */



 Model1_FIX_BTMAP_END =
  (Model1___end_of_permanent_fixed_addresses ^
   (Model1___end_of_permanent_fixed_addresses + (64 * 8) - 1)) &
  -512
  ? Model1___end_of_permanent_fixed_addresses + (64 * 8) -
    (Model1___end_of_permanent_fixed_addresses & ((64 * 8) - 1))
  : Model1___end_of_permanent_fixed_addresses,
 Model1_FIX_BTMAP_BEGIN = Model1_FIX_BTMAP_END + (64 * 8) - 1,






 Model1___end_of_fixed_addresses
};


extern void Model1_reserve_top_address(unsigned long Model1_reserve);




extern int Model1_fixmaps_set;

extern Model1_pte_t *Model1_kmap_pte;

extern Model1_pte_t *Model1_pkmap_page_table;

void Model1___native_set_fixmap(enum Model1_fixed_addresses Model1_idx, Model1_pte_t Model1_pte);
void Model1_native_set_fixmap(enum Model1_fixed_addresses Model1_idx,
         Model1_phys_addr_t Model1_phys, Model1_pgprot_t Model1_flags);


static inline __attribute__((no_instrument_function)) void Model1___set_fixmap(enum Model1_fixed_addresses Model1_idx,
    Model1_phys_addr_t Model1_phys, Model1_pgprot_t Model1_flags)
{
 Model1_native_set_fixmap(Model1_idx, Model1_phys, Model1_flags);
}



/*
 * fixmap.h: compile-time virtual memory allocation
 *
 * This file is subject to the terms and conditions of the GNU General Public
 * License.  See the file "COPYING" in the main directory of this archive
 * for more details.
 *
 * Copyright (C) 1998 Ingo Molnar
 *
 * Support of BIGMEM added by Gerhard Wichert, Siemens AG, July 1999
 * x86_32 and x86_64 integration by Gustavo F. Padovan, February 2009
 * Break out common bits to asm-generic by Mark Salter, November 2013
 */
/*
 * 'index to address' translation. If anyone tries to use the idx
 * directly without translation, we catch the bug with a NULL-deference
 * kernel oops. Illegal ranges of incoming indices are caught too.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) unsigned long Model1_fix_to_virt(const unsigned int Model1_idx)
{
 do { bool Model1___cond = !(!(Model1_idx >= Model1___end_of_fixed_addresses)); extern void Model1___compiletime_assert_31(void) ; if (Model1___cond) Model1___compiletime_assert_31(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
 return (((((((-10UL << 20) + ((1UL) << 12))-1) | ((__typeof__((-10UL << 20) + ((1UL) << 12)))((1<<21)-1)))+1) - ((1UL) << 12)) - ((Model1_idx) << 12));
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_virt_to_fix(const unsigned long Model1_vaddr)
{
 do { if (__builtin_expect(!!(Model1_vaddr >= ((((((-10UL << 20) + ((1UL) << 12))-1) | ((__typeof__((-10UL << 20) + ((1UL) << 12)))((1<<21)-1)))+1) - ((1UL) << 12)) || Model1_vaddr < (((((((-10UL << 20) + ((1UL) << 12))-1) | ((__typeof__((-10UL << 20) + ((1UL) << 12)))((1<<21)-1)))+1) - ((1UL) << 12)) - (Model1___end_of_permanent_fixed_addresses << 12))), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/asm-generic/fixmap.h"), "i" (37), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);
 return ((((((((-10UL << 20) + ((1UL) << 12))-1) | ((__typeof__((-10UL << 20) + ((1UL) << 12)))((1<<21)-1)))+1) - ((1UL) << 12)) - ((Model1_vaddr)&(~(((1UL) << 12)-1)))) >> 12);
}

/*
 * Provide some reasonable defaults for page flags.
 * Not all architectures use all of these different types and some
 * architectures use different names.
 */
/* Return a pointer with offset calculated */
/*
 * Some hardware wants to get fixmapped without caching.
 */






/*
 * Some fixmaps are for IO
 */




void Model1___early_set_fixmap(enum Model1_fixed_addresses Model1_idx,
   Model1_phys_addr_t Model1_phys, Model1_pgprot_t Model1_flags);








struct Model1_notifier_block;
void Model1_idle_notifier_register(struct Model1_notifier_block *Model1_n);
void Model1_idle_notifier_unregister(struct Model1_notifier_block *Model1_n);


void Model1_enter_idle(void);
void Model1_exit_idle(void);






void Model1_amd_e400_remove_cpu(int Model1_cpu);



/*
 * Debugging macros
 */




/* Macros for apic_extnmi which controls external NMI masking */




/*
 * Define the default level of output to be very little
 * This can be turned up by using apic=verbose for more
 * information and apic=debug for _lots_ of information.
 * apic_verbosity is defined in apic.c
 */
static inline __attribute__((no_instrument_function)) void Model1_generic_apic_probe(void)
{
}




extern unsigned int Model1_apic_verbosity;
extern int Model1_local_apic_timer_c2_ok;

extern int Model1_disable_apic;
extern unsigned int Model1_lapic_timer_frequency;


extern void Model1___inquire_remote_apic(int Model1_apicid);






static inline __attribute__((no_instrument_function)) void Model1_default_inquire_remote_apic(int Model1_apicid)
{
 if (Model1_apic_verbosity >= 2)
  Model1___inquire_remote_apic(Model1_apicid);
}

/*
 * With 82489DX we can't rely on apic feature bit
 * retrieved via cpuid but still have to deal with
 * such an apic chip so we assume that SMP configuration
 * is found from MP table (64bit case uses ACPI mostly
 * which set smp presence flag as well so we are safe
 * to use this helper too).
 */
static inline __attribute__((no_instrument_function)) bool Model1_apic_from_smp_config(void)
{
 return Model1_smp_found_config && !Model1_disable_apic;
}

/*
 * Basic functions accessing APICs.
 */




extern int Model1_setup_profiling_timer(unsigned int);

static inline __attribute__((no_instrument_function)) void Model1_native_apic_mem_write(Model1_u32 Model1_reg, Model1_u32 Model1_v)
{
 volatile Model1_u32 *Model1_addr = (volatile Model1_u32 *)((Model1_fix_to_virt(Model1_FIX_APIC_BASE)) + Model1_reg);

 asm volatile ("661:\n\t" "movl %0, %P1" "\n662:\n" ".skip -(((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")) > 0) * " "((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")),0x90\n" "663" ":\n" ".pushsection .altinstructions,\"a\"\n" " .long 661b - .\n" " .long " "664""1""f - .\n" " .word " "(18*32 + (5))" "\n" " .byte " "663""b-661b" "\n" " .byte " "665""1""f-""664""1""f" "\n" " .byte " "663""b-662b" "\n" ".popsection\n" ".pushsection .altinstr_replacement, \"ax\"\n" "664""1"":\n\t" "xchgl %0, %P1" "\n" "665""1" ":\n\t" ".popsection" : "=r" (Model1_v), "=m" (*Model1_addr) : "i" (0), "0" (Model1_v), "m" (*Model1_addr));


}

static inline __attribute__((no_instrument_function)) Model1_u32 Model1_native_apic_mem_read(Model1_u32 Model1_reg)
{
 return *((volatile Model1_u32 *)((Model1_fix_to_virt(Model1_FIX_APIC_BASE)) + Model1_reg));
}

extern void Model1_native_apic_wait_icr_idle(void);
extern Model1_u32 Model1_native_safe_apic_wait_icr_idle(void);
extern void Model1_native_apic_icr_write(Model1_u32 Model1_low, Model1_u32 Model1_id);
extern Model1_u64 Model1_native_apic_icr_read(void);

static inline __attribute__((no_instrument_function)) bool Model1_apic_is_x2apic_enabled(void)
{
 Model1_u64 Model1_msr;

 if (Model1_rdmsrl_safe(0x0000001b, &Model1_msr))
  return false;
 return Model1_msr & (1UL << 10);
}

extern void Model1_enable_IR_x2apic(void);

extern int Model1_get_physical_broadcast(void);

extern int Model1_lapic_get_maxlvt(void);
extern void Model1_clear_local_APIC(void);
extern void Model1_disconnect_bsp_APIC(int Model1_virt_wire_setup);
extern void Model1_disable_local_APIC(void);
extern void Model1_lapic_shutdown(void);
extern void Model1_sync_Arb_IDs(void);
extern void Model1_init_bsp_APIC(void);
extern void Model1_setup_local_APIC(void);
extern void Model1_init_apic_mappings(void);
void Model1_register_lapic_address(unsigned long Model1_address);
extern void Model1_setup_boot_APIC_clock(void);
extern void Model1_setup_secondary_APIC_clock(void);
extern void Model1_lapic_update_tsc_freq(void);
extern int Model1_APIC_init_uniprocessor(void);


static inline __attribute__((no_instrument_function)) int Model1_apic_force_enable(unsigned long Model1_addr)
{
 return -1;
}




extern int Model1_apic_bsp_setup(bool Model1_upmode);
extern void Model1_apic_ap_setup(void);

/*
 * On 32bit this is mach-xxx local
 */

extern int Model1_apic_is_clustered_box(void);







extern int Model1_setup_APIC_eilvt(Model1_u8 Model1_lvt_off, Model1_u8 Model1_vector, Model1_u8 Model1_msg_type, Model1_u8 Model1_mask);
static inline __attribute__((no_instrument_function)) void Model1_check_x2apic(void) { }
static inline __attribute__((no_instrument_function)) void Model1_x2apic_setup(void) { }
static inline __attribute__((no_instrument_function)) int Model1_x2apic_enabled(void) { return 0; }
/*
 * Copyright 2004 James Cleverdon, IBM.
 * Subject to the GNU Public License, v.2
 *
 * Generic APIC sub-arch data struct.
 *
 * Hacked for x86-64 by James Cleverdon from i386 architecture code by
 * Martin Bligh, Andi Kleen, James Bottomley, John Stultz, and
 * James Cleverdon.
 */
struct Model1_apic {
 char *Model1_name;

 int (*Model1_probe)(void);
 int (*Model1_acpi_madt_oem_check)(char *Model1_oem_id, char *Model1_oem_table_id);
 int (*Model1_apic_id_valid)(int Model1_apicid);
 int (*Model1_apic_id_registered)(void);

 Model1_u32 Model1_irq_delivery_mode;
 Model1_u32 Model1_irq_dest_mode;

 const struct Model1_cpumask *(*Model1_target_cpus)(void);

 int Model1_disable_esr;

 int Model1_dest_logical;
 unsigned long (*Model1_check_apicid_used)(Model1_physid_mask_t *Model1_map, int Model1_apicid);

 void (*Model1_vector_allocation_domain)(int Model1_cpu, struct Model1_cpumask *Model1_retmask,
      const struct Model1_cpumask *Model1_mask);
 void (*Model1_init_apic_ldr)(void);

 void (*Model1_ioapic_phys_id_map)(Model1_physid_mask_t *Model1_phys_map, Model1_physid_mask_t *Model1_retmap);

 void (*Model1_setup_apic_routing)(void);
 int (*Model1_cpu_present_to_apicid)(int Model1_mps_cpu);
 void (*Model1_apicid_to_cpu_present)(int Model1_phys_apicid, Model1_physid_mask_t *Model1_retmap);
 int (*Model1_check_phys_apicid_present)(int Model1_phys_apicid);
 int (*Model1_phys_pkg_id)(int Model1_cpuid_apic, int Model1_index_msb);

 unsigned int (*Model1_get_apic_id)(unsigned long Model1_x);
 unsigned long (*Model1_set_apic_id)(unsigned int Model1_id);

 int (*Model1_cpu_mask_to_apicid_and)(const struct Model1_cpumask *Model1_cpumask,
          const struct Model1_cpumask *Model1_andmask,
          unsigned int *Model1_apicid);

 /* ipi */
 void (*Model1_send_IPI)(int Model1_cpu, int Model1_vector);
 void (*Model1_send_IPI_mask)(const struct Model1_cpumask *Model1_mask, int Model1_vector);
 void (*Model1_send_IPI_mask_allbutself)(const struct Model1_cpumask *Model1_mask,
      int Model1_vector);
 void (*Model1_send_IPI_allbutself)(int Model1_vector);
 void (*Model1_send_IPI_all)(int Model1_vector);
 void (*Model1_send_IPI_self)(int Model1_vector);

 /* wakeup_secondary_cpu */
 int (*Model1_wakeup_secondary_cpu)(int Model1_apicid, unsigned long Model1_start_eip);

 void (*Model1_inquire_remote_apic)(int Model1_apicid);

 /* apic ops */
 Model1_u32 (*Model1_read)(Model1_u32 Model1_reg);
 void (*Model1_write)(Model1_u32 Model1_reg, Model1_u32 Model1_v);
 /*
	 * ->eoi_write() has the same signature as ->write().
	 *
	 * Drivers can support both ->eoi_write() and ->write() by passing the same
	 * callback value. Kernel can override ->eoi_write() and fall back
	 * on write for EOI.
	 */
 void (*Model1_eoi_write)(Model1_u32 Model1_reg, Model1_u32 Model1_v);
 Model1_u64 (*Model1_icr_read)(void);
 void (*Model1_icr_write)(Model1_u32 Model1_low, Model1_u32 Model1_high);
 void (*Model1_wait_icr_idle)(void);
 Model1_u32 (*Model1_safe_wait_icr_idle)(void);
};

/*
 * Pointer to the local APIC driver in use on this system (there's
 * always just one such driver in use - the kernel decides via an
 * early probing process which one it picks - and then sticks to it):
 */
extern struct Model1_apic *Model1_apic;

/*
 * APIC drivers are probed based on how they are listed in the .apicdrivers
 * section. So the order is important and enforced by the ordering
 * of different apic driver files in the Makefile.
 *
 * For the files having two apic drivers, we use apic_drivers()
 * to enforce the order with in them.
 */
extern struct Model1_apic *Model1___apicdrivers[], *Model1___apicdrivers_end[];

/*
 * APIC functionality to boot other CPUs - only used on SMP:
 */

extern int Model1_wakeup_secondary_cpu_via_nmi(int Model1_apicid, unsigned long Model1_start_eip);




static inline __attribute__((no_instrument_function)) Model1_u32 Model1_apic_read(Model1_u32 Model1_reg)
{
 return Model1_apic->Model1_read(Model1_reg);
}

static inline __attribute__((no_instrument_function)) void Model1_apic_write(Model1_u32 Model1_reg, Model1_u32 Model1_val)
{
 Model1_apic->Model1_write(Model1_reg, Model1_val);
}

static inline __attribute__((no_instrument_function)) void Model1_apic_eoi(void)
{
 Model1_apic->Model1_eoi_write(0xB0, 0x0);
}

static inline __attribute__((no_instrument_function)) Model1_u64 Model1_apic_icr_read(void)
{
 return Model1_apic->Model1_icr_read();
}

static inline __attribute__((no_instrument_function)) void Model1_apic_icr_write(Model1_u32 Model1_low, Model1_u32 Model1_high)
{
 Model1_apic->Model1_icr_write(Model1_low, Model1_high);
}

static inline __attribute__((no_instrument_function)) void Model1_apic_wait_icr_idle(void)
{
 Model1_apic->Model1_wait_icr_idle();
}

static inline __attribute__((no_instrument_function)) Model1_u32 Model1_safe_apic_wait_icr_idle(void)
{
 return Model1_apic->Model1_safe_wait_icr_idle();
}

extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_apic_set_eoi_write(void (*Model1_eoi_write)(Model1_u32 Model1_reg, Model1_u32 Model1_v));
static inline __attribute__((no_instrument_function)) void Model1_ack_APIC_irq(void)
{
 /*
	 * ack_APIC_irq() actually gets compiled as a single instruction
	 * ... yummie.
	 */
 Model1_apic_eoi();
}

static inline __attribute__((no_instrument_function)) unsigned Model1_default_get_apic_id(unsigned long Model1_x)
{
 unsigned int Model1_ver = ((Model1_apic_read(0x30)) & 0xFFu);

 if (((Model1_ver) >= 0x14) || (__builtin_constant_p(( 3*32+26)) && ( (((( 3*32+26))>>5)==(0) && (1UL<<((( 3*32+26))&31) & ((1<<(( 0*32+ 0) & 31))|(1<<(( 0*32+ 3)) & 31)|(1<<(( 0*32+ 5) & 31))|(1<<(( 0*32+ 6) & 31))| (1<<(( 0*32+ 8) & 31))|(1<<(( 0*32+13)) & 31)|(1<<(( 0*32+24) & 31))|(1<<(( 0*32+15) & 31))| (1<<(( 0*32+25) & 31))|(1<<(( 0*32+26) & 31))) )) || (((( 3*32+26))>>5)==(1) && (1UL<<((( 3*32+26))&31) & ((1<<(( 1*32+29) & 31))|0) )) || (((( 3*32+26))>>5)==(2) && (1UL<<((( 3*32+26))&31) & 0 )) || (((( 3*32+26))>>5)==(3) && (1UL<<((( 3*32+26))&31) & ((1<<(( 3*32+20) & 31))) )) || (((( 3*32+26))>>5)==(4) && (1UL<<((( 3*32+26))&31) & (0) )) || (((( 3*32+26))>>5)==(5) && (1UL<<((( 3*32+26))&31) & 0 )) || (((( 3*32+26))>>5)==(6) && (1UL<<((( 3*32+26))&31) & 0 )) || (((( 3*32+26))>>5)==(7) && (1UL<<((( 3*32+26))&31) & 0 )) || (((( 3*32+26))>>5)==(8) && (1UL<<((( 3*32+26))&31) & 0 )) || (((( 3*32+26))>>5)==(9) && (1UL<<((( 3*32+26))&31) & 0 )) || (((( 3*32+26))>>5)==(10) && (1UL<<((( 3*32+26))&31) & 0 )) || (((( 3*32+26))>>5)==(11) && (1UL<<((( 3*32+26))&31) & 0 )) || (((( 3*32+26))>>5)==(12) && (1UL<<((( 3*32+26))&31) & 0 )) || (((( 3*32+26))>>5)==(13) && (1UL<<((( 3*32+26))&31) & 0 )) || (((( 3*32+26))>>5)==(14) && (1UL<<((( 3*32+26))&31) & 0 )) || (((( 3*32+26))>>5)==(15) && (1UL<<((( 3*32+26))&31) & 0 )) || (((( 3*32+26))>>5)==(16) && (1UL<<((( 3*32+26))&31) & 0 )) || (((( 3*32+26))>>5)==(17) && (1UL<<((( 3*32+26))&31) & 0 )) || (sizeof(struct { int:-!!(18 != 18); })) || (sizeof(struct { int:-!!(18 != 18); }))) ? 1 : (__builtin_constant_p((( 3*32+26))) ? Model1_constant_test_bit((( 3*32+26)), ((unsigned long *)((&Model1_boot_cpu_data)->Model1_x86_capability))) : Model1_variable_test_bit((( 3*32+26)), ((unsigned long *)((&Model1_boot_cpu_data)->Model1_x86_capability))))))
  return (Model1_x >> 24) & 0xFF;
 else
  return (Model1_x >> 24) & 0x0F;
}

/*
 * Warm reset vector position:
 */




extern void Model1_apic_send_IPI_self(int Model1_vector);

extern __attribute__((section(".data..percpu" ""))) __typeof__(int) Model1_x2apic_extra_bits;

extern int Model1_default_cpu_present_to_apicid(int Model1_mps_cpu);
extern int Model1_default_check_phys_apicid_present(int Model1_phys_apicid);


extern void Model1_generic_bigsmp_probe(void);








static inline __attribute__((no_instrument_function)) const struct Model1_cpumask *Model1_default_target_cpus(void)
{

 return ((const struct Model1_cpumask *)&Model1___cpu_online_mask);



}

static inline __attribute__((no_instrument_function)) const struct Model1_cpumask *Model1_online_target_cpus(void)
{
 return ((const struct Model1_cpumask *)&Model1___cpu_online_mask);
}

extern __attribute__((section(".data..percpu" "..read_mostly"))) __typeof__(Model1_u16) Model1_x86_bios_cpu_apicid; extern __typeof__(Model1_u16) *Model1_x86_bios_cpu_apicid_early_ptr; extern __typeof__(Model1_u16) Model1_x86_bios_cpu_apicid_early_map[];


static inline __attribute__((no_instrument_function)) unsigned int Model1_read_apic_id(void)
{
 unsigned int Model1_reg;

 Model1_reg = Model1_apic_read(0x20);

 return Model1_apic->Model1_get_apic_id(Model1_reg);
}

static inline __attribute__((no_instrument_function)) int Model1_default_apic_id_valid(int Model1_apicid)
{
 return (Model1_apicid < 255);
}

extern int Model1_default_acpi_madt_oem_check(char *, char *);

extern void Model1_default_setup_apic_routing(void);

extern struct Model1_apic Model1_apic_noop;
static inline __attribute__((no_instrument_function)) int
Model1_flat_cpu_mask_to_apicid_and(const struct Model1_cpumask *Model1_cpumask,
       const struct Model1_cpumask *Model1_andmask,
       unsigned int *Model1_apicid)
{
 unsigned long Model1_cpu_mask = ((Model1_cpumask)->Model1_bits)[0] &
     ((Model1_andmask)->Model1_bits)[0] &
     ((((const struct Model1_cpumask *)&Model1___cpu_online_mask))->Model1_bits)[0] &
     0xFFu;

 if (__builtin_expect(!!(Model1_cpu_mask), 1)) {
  *Model1_apicid = (unsigned int)Model1_cpu_mask;
  return 0;
 } else {
  return -22;
 }
}

extern int
Model1_default_cpu_mask_to_apicid_and(const struct Model1_cpumask *Model1_cpumask,
          const struct Model1_cpumask *Model1_andmask,
          unsigned int *Model1_apicid);

static inline __attribute__((no_instrument_function)) void
Model1_flat_vector_allocation_domain(int Model1_cpu, struct Model1_cpumask *Model1_retmask,
         const struct Model1_cpumask *Model1_mask)
{
 /* Careful. Some cpus do not strictly honor the set of cpus
	 * specified in the interrupt destination when using lowest
	 * priority interrupt delivery mode.
	 *
	 * In particular there was a hyperthreading cpu observed to
	 * deliver interrupts to the wrong hyperthread when only one
	 * hyperthread was specified in the interrupt desitination.
	 */
 Model1_cpumask_clear(Model1_retmask);
 ((Model1_retmask)->Model1_bits)[0] = 0xFFu;
}

static inline __attribute__((no_instrument_function)) void
Model1_default_vector_allocation_domain(int Model1_cpu, struct Model1_cpumask *Model1_retmask,
     const struct Model1_cpumask *Model1_mask)
{
 Model1_cpumask_copy(Model1_retmask, (Model1_get_cpu_mask(Model1_cpu)));
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_default_check_apicid_used(Model1_physid_mask_t *Model1_map, int Model1_apicid)
{
 return (__builtin_constant_p((Model1_apicid)) ? Model1_constant_test_bit((Model1_apicid), ((*Model1_map).Model1_mask)) : Model1_variable_test_bit((Model1_apicid), ((*Model1_map).Model1_mask)));
}

static inline __attribute__((no_instrument_function)) void Model1_default_ioapic_phys_id_map(Model1_physid_mask_t *Model1_phys_map, Model1_physid_mask_t *Model1_retmap)
{
 *Model1_retmap = *Model1_phys_map;
}

static inline __attribute__((no_instrument_function)) int Model1___default_cpu_present_to_apicid(int Model1_mps_cpu)
{
 if (Model1_mps_cpu < Model1_nr_cpu_ids && Model1_cpumask_test_cpu((Model1_mps_cpu), ((const struct Model1_cpumask *)&Model1___cpu_present_mask)))
  return (int)(*({ do { const void *Model1___vpp_verify = (typeof((&(Model1_x86_bios_cpu_apicid)) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); ({ unsigned long Model1___ptr; __asm__ ("" : "=r"(Model1___ptr) : "0"((typeof(*((&(Model1_x86_bios_cpu_apicid)))) *)((&(Model1_x86_bios_cpu_apicid))))); (typeof((typeof(*((&(Model1_x86_bios_cpu_apicid)))) *)((&(Model1_x86_bios_cpu_apicid))))) (Model1___ptr + (((Model1___per_cpu_offset[(Model1_mps_cpu)])))); }); }));
 else
  return 0xFFFFu;
}

static inline __attribute__((no_instrument_function)) int
Model1___default_check_phys_apicid_present(int Model1_phys_apicid)
{
 return (__builtin_constant_p((Model1_phys_apicid)) ? Model1_constant_test_bit((Model1_phys_apicid), ((Model1_phys_cpu_present_map).Model1_mask)) : Model1_variable_test_bit((Model1_phys_apicid), ((Model1_phys_cpu_present_map).Model1_mask)));
}
extern int Model1_default_cpu_present_to_apicid(int Model1_mps_cpu);
extern int Model1_default_check_phys_apicid_present(int Model1_phys_apicid);



extern void Model1_irq_enter(void);
extern void Model1_irq_exit(void);

static inline __attribute__((no_instrument_function)) void Model1_entering_irq(void)
{
 Model1_irq_enter();
 Model1_exit_idle();
}

static inline __attribute__((no_instrument_function)) void Model1_entering_ack_irq(void)
{
 Model1_entering_irq();
 Model1_ack_APIC_irq();
}

static inline __attribute__((no_instrument_function)) void Model1_ipi_entering_ack_irq(void)
{
 Model1_ack_APIC_irq();
 Model1_irq_enter();
}

static inline __attribute__((no_instrument_function)) void Model1_exiting_irq(void)
{
 Model1_irq_exit();
}

static inline __attribute__((no_instrument_function)) void Model1_exiting_ack_irq(void)
{
 Model1_irq_exit();
 /* Ack only at the end to avoid potential reentry */
 Model1_ack_APIC_irq();
}

extern void Model1_ioapic_zap_locks(void);











/*
 * Linux IRQ vector layout.
 *
 * There are 256 IDT entries (per CPU - each entry is 8 bytes) which can
 * be defined by Linux. They are used as a jump table by the CPU when a
 * given vector is triggered - by a CPU-external, CPU-internal or
 * software-triggered event.
 *
 * Linux sets the kernel code address each entry jumps to early during
 * bootup, and never changes them. This is the general layout of the
 * IDT entries:
 *
 *  Vectors   0 ...  31 : system traps and exceptions - hardcoded events
 *  Vectors  32 ... 127 : device interrupts
 *  Vector  128         : legacy int80 syscall interface
 *  Vectors 129 ... INVALIDATE_TLB_VECTOR_START-1 except 204 : device interrupts
 *  Vectors INVALIDATE_TLB_VECTOR_START ... 255 : special interrupts
 *
 * 64-bit x86 has per CPU IDT tables, 32-bit has one shared IDT table.
 *
 * This file enumerates the exact layout of them:
 */




/*
 * IDT vectors usable for external interrupt sources start at 0x20.
 * (0x80 is the syscall vector, 0x30-0x3f are for ISA)
 */

/*
 * We start allocating at 0x21 to spread out vectors evenly between
 * priority levels. (0x80 is the syscall vector)
 */


/*
 * Reserve the lowest usable vector (and hence lowest priority)  0x20 for
 * triggering cleanup after irq migration. 0x21-0x2f will still be used
 * for device interrupts.
 */




/*
 * Vectors 0x30-0x3f are used for ISA interrupts.
 *   round up to the next 16-vector boundary
 */


/*
 * Special IRQ vectors used by the SMP architecture, 0xf0-0xff
 *
 *  some of the following vectors are 'rare', they are merged
 *  into a single vector (CALL_FUNCTION_VECTOR) to save vector space.
 *  TLB, reschedule and local APIC vectors are performance-critical.
 */


/*
 * Sanity check
 */
/*
 * Generic system vector for platform specific use
 */



/*
 * IRQ work vector:
 */





/* Vector on which hypervisor callbacks will be delivered */


/* Vector for KVM to deliver posted interrupt IPI */




/*
 * Local APIC timer IRQ vector is on a different priority level,
 * to work around the 'lost local interrupt if more than 2 IRQ
 * sources per level' errata.
 */
/*
 * Size the maximum number of interrupts.
 *
 * If the irq_desc[] array has a sparse layout, we can size things
 * generously - it scales up linearly with the maximum number of CPUs,
 * and the maximum number of IO-APICs, whichever is higher.
 *
 * In other cases we size more conservatively, to not create too large
 * static arrays.
 */

/*
 * Intel IO-APIC support for SMP and UP systems.
 *
 * Copyright (C) 1997, 1998, 1999, 2000 Ingo Molnar
 */

/* I/O Unit Redirection Table */
/*
 * The structure of the IO-APIC:
 */
union Model1_IO_APIC_reg_00 {
 Model1_u32 Model1_raw;
 struct {
  Model1_u32 Model1___reserved_2 : 14,
   Model1_LTS : 1,
   Model1_delivery_type : 1,
   Model1___reserved_1 : 8,
   Model1_ID : 8;
 } __attribute__ ((packed)) Model1_bits;
};

union Model1_IO_APIC_reg_01 {
 Model1_u32 Model1_raw;
 struct {
  Model1_u32 Model1_version : 8,
   Model1___reserved_2 : 7,
   Model1_PRQ : 1,
   Model1_entries : 8,
   Model1___reserved_1 : 8;
 } __attribute__ ((packed)) Model1_bits;
};

union Model1_IO_APIC_reg_02 {
 Model1_u32 Model1_raw;
 struct {
  Model1_u32 Model1___reserved_2 : 24,
   Model1_arbitration : 4,
   Model1___reserved_1 : 4;
 } __attribute__ ((packed)) Model1_bits;
};

union Model1_IO_APIC_reg_03 {
 Model1_u32 Model1_raw;
 struct {
  Model1_u32 Model1_boot_DT : 1,
   Model1___reserved_1 : 31;
 } __attribute__ ((packed)) Model1_bits;
};

struct Model1_IO_APIC_route_entry {
 __u32 Model1_vector : 8,
  Model1_delivery_mode : 3, /* 000: FIXED
					 * 001: lowest prio
					 * 111: ExtINT
					 */
  Model1_dest_mode : 1, /* 0: physical, 1: logical */
  Model1_delivery_status : 1,
  Model1_polarity : 1,
  Model1_irr : 1,
  Model1_trigger : 1, /* 0: edge, 1: level */
  Model1_mask : 1, /* 0: enabled, 1: disabled */
  Model1___reserved_2 : 15;

 __u32 Model1___reserved_3 : 24,
  Model1_dest : 8;
} __attribute__ ((packed));

struct Model1_IR_IO_APIC_route_entry {
 __u64 Model1_vector : 8,
  Model1_zero : 3,
  Model1_index2 : 1,
  Model1_delivery_status : 1,
  Model1_polarity : 1,
  Model1_irr : 1,
  Model1_trigger : 1,
  Model1_mask : 1,
  Model1_reserved : 31,
  format : 1,
  Model1_index : 15;
} __attribute__ ((packed));

struct Model1_irq_alloc_info;
struct Model1_ioapic_domain_cfg;
/*
 * # of IO-APICs and # of IRQ routing registers
 */
extern int Model1_nr_ioapics;

extern int Model1_mpc_ioapic_id(int Model1_ioapic);
extern unsigned int Model1_mpc_ioapic_addr(int Model1_ioapic);

/* # of MP IRQ source entries */
extern int Model1_mp_irq_entries;

/* MP IRQ source entries */
extern struct Model1_mpc_intsrc Model1_mp_irqs[(256 * 4)];

/* 1 if "noapic" boot option passed */
extern int Model1_skip_ioapic_setup;

/* 1 if "noapic" boot option passed */
extern int Model1_noioapicquirk;

/* -1 if "noapic" boot option passed */
extern int Model1_noioapicreroute;

extern Model1_u32 Model1_gsi_top;

extern unsigned long Model1_io_apic_irqs;



/*
 * If we use the IO-APIC for IRQ routing, disable automatic
 * assignment of PCI IRQ's.
 */



struct Model1_irq_cfg;
extern void Model1_ioapic_insert_resources(void);
extern int Model1_arch_early_ioapic_init(void);

extern int Model1_save_ioapic_entries(void);
extern void Model1_mask_ioapic_entries(void);
extern int Model1_restore_ioapic_entries(void);

extern void Model1_setup_ioapic_ids_from_mpc(void);
extern void Model1_setup_ioapic_ids_from_mpc_nocheck(void);

extern int Model1_mp_find_ioapic(Model1_u32 Model1_gsi);
extern int Model1_mp_find_ioapic_pin(int Model1_ioapic, Model1_u32 Model1_gsi);
extern int Model1_mp_map_gsi_to_irq(Model1_u32 Model1_gsi, unsigned int Model1_flags,
        struct Model1_irq_alloc_info *Model1_info);
extern void Model1_mp_unmap_irq(int Model1_irq);
extern int Model1_mp_register_ioapic(int Model1_id, Model1_u32 Model1_address, Model1_u32 Model1_gsi_base,
         struct Model1_ioapic_domain_cfg *Model1_cfg);
extern int Model1_mp_unregister_ioapic(Model1_u32 Model1_gsi_base);
extern int Model1_mp_ioapic_registered(Model1_u32 Model1_gsi_base);

extern void Model1_ioapic_set_alloc_attr(struct Model1_irq_alloc_info *Model1_info,
      int Model1_node, int Model1_trigger, int Model1_polarity);

extern void Model1_mp_save_irq(struct Model1_mpc_intsrc *Model1_m);

extern void Model1_disable_ioapic_support(void);

extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_io_apic_init_mappings(void);
extern unsigned int Model1_native_io_apic_read(unsigned int Model1_apic, unsigned int Model1_reg);
extern void Model1_native_disable_io_apic(void);

static inline __attribute__((no_instrument_function)) unsigned int Model1_io_apic_read(unsigned int Model1_apic, unsigned int Model1_reg)
{
 return Model1_x86_io_apic_ops.Model1_read(Model1_apic, Model1_reg);
}

extern void Model1_setup_IO_APIC(void);
extern void Model1_enable_IO_APIC(void);
extern void Model1_disable_IO_APIC(void);
extern void Model1_setup_ioapic_dest(void);
extern int Model1_IO_APIC_get_PCI_irq_vector(int Model1_bus, int Model1_devfn, int Model1_pin);
extern void Model1_print_IO_APICs(void);





extern int Model1_smp_num_siblings;
extern unsigned int Model1_num_processors;

extern __attribute__((section(".data..percpu" "..read_mostly"))) __typeof__(Model1_cpumask_var_t) Model1_cpu_sibling_map;
extern __attribute__((section(".data..percpu" "..read_mostly"))) __typeof__(Model1_cpumask_var_t) Model1_cpu_core_map;
/* cpus sharing the last level cache: */
extern __attribute__((section(".data..percpu" "..read_mostly"))) __typeof__(Model1_cpumask_var_t) Model1_cpu_llc_shared_map;
extern __attribute__((section(".data..percpu" "..read_mostly"))) __typeof__(Model1_u16) Model1_cpu_llc_id;
extern __attribute__((section(".data..percpu" "..read_mostly"))) __typeof__(int) Model1_cpu_number;

static inline __attribute__((no_instrument_function)) struct Model1_cpumask *Model1_cpu_llc_shared_mask(int Model1_cpu)
{
 return (*({ do { const void *Model1___vpp_verify = (typeof((&(Model1_cpu_llc_shared_map)) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); ({ unsigned long Model1___ptr; __asm__ ("" : "=r"(Model1___ptr) : "0"((typeof(*((&(Model1_cpu_llc_shared_map)))) *)((&(Model1_cpu_llc_shared_map))))); (typeof((typeof(*((&(Model1_cpu_llc_shared_map)))) *)((&(Model1_cpu_llc_shared_map))))) (Model1___ptr + (((Model1___per_cpu_offset[(Model1_cpu)])))); }); }));
}

extern __attribute__((section(".data..percpu" "..read_mostly"))) __typeof__(Model1_u16) Model1_x86_cpu_to_apicid; extern __typeof__(Model1_u16) *Model1_x86_cpu_to_apicid_early_ptr; extern __typeof__(Model1_u16) Model1_x86_cpu_to_apicid_early_map[];
extern __attribute__((section(".data..percpu" "..read_mostly"))) __typeof__(Model1_u32) Model1_x86_cpu_to_acpiid; extern __typeof__(Model1_u32) *Model1_x86_cpu_to_acpiid_early_ptr; extern __typeof__(Model1_u32) Model1_x86_cpu_to_acpiid_early_map[];
extern __attribute__((section(".data..percpu" "..read_mostly"))) __typeof__(Model1_u16) Model1_x86_bios_cpu_apicid; extern __typeof__(Model1_u16) *Model1_x86_bios_cpu_apicid_early_ptr; extern __typeof__(Model1_u16) Model1_x86_bios_cpu_apicid_early_map[];




/* Static state in head.S used to set up a CPU */
extern unsigned long Model1_stack_start; /* Initial stack pointer address */

struct Model1_task_struct;

struct Model1_smp_ops {
 void (*Model1_smp_prepare_boot_cpu)(void);
 void (*Model1_smp_prepare_cpus)(unsigned Model1_max_cpus);
 void (*Model1_smp_cpus_done)(unsigned Model1_max_cpus);

 void (*Model1_stop_other_cpus)(int Model1_wait);
 void (*Model1_smp_send_reschedule)(int Model1_cpu);

 int (*Model1_cpu_up)(unsigned Model1_cpu, struct Model1_task_struct *Model1_tidle);
 int (*Model1_cpu_disable)(void);
 void (*Model1_cpu_die)(unsigned int Model1_cpu);
 void (*Model1_play_dead)(void);

 void (*Model1_send_call_func_ipi)(const struct Model1_cpumask *Model1_mask);
 void (*Model1_send_call_func_single_ipi)(int Model1_cpu);
};

/* Globals due to paravirt */
extern void Model1_set_cpu_sibling_map(int Model1_cpu);


extern struct Model1_smp_ops Model1_smp_ops;

static inline __attribute__((no_instrument_function)) void Model1_smp_send_stop(void)
{
 Model1_smp_ops.Model1_stop_other_cpus(0);
}

static inline __attribute__((no_instrument_function)) void Model1_stop_other_cpus(void)
{
 Model1_smp_ops.Model1_stop_other_cpus(1);
}

static inline __attribute__((no_instrument_function)) void Model1_smp_prepare_boot_cpu(void)
{
 Model1_smp_ops.Model1_smp_prepare_boot_cpu();
}

static inline __attribute__((no_instrument_function)) void Model1_smp_prepare_cpus(unsigned int Model1_max_cpus)
{
 Model1_smp_ops.Model1_smp_prepare_cpus(Model1_max_cpus);
}

static inline __attribute__((no_instrument_function)) void Model1_smp_cpus_done(unsigned int Model1_max_cpus)
{
 Model1_smp_ops.Model1_smp_cpus_done(Model1_max_cpus);
}

static inline __attribute__((no_instrument_function)) int Model1___cpu_up(unsigned int Model1_cpu, struct Model1_task_struct *Model1_tidle)
{
 return Model1_smp_ops.Model1_cpu_up(Model1_cpu, Model1_tidle);
}

static inline __attribute__((no_instrument_function)) int Model1___cpu_disable(void)
{
 return Model1_smp_ops.Model1_cpu_disable();
}

static inline __attribute__((no_instrument_function)) void Model1___cpu_die(unsigned int Model1_cpu)
{
 Model1_smp_ops.Model1_cpu_die(Model1_cpu);
}

static inline __attribute__((no_instrument_function)) void Model1_play_dead(void)
{
 Model1_smp_ops.Model1_play_dead();
}

static inline __attribute__((no_instrument_function)) void Model1_smp_send_reschedule(int Model1_cpu)
{
 Model1_smp_ops.Model1_smp_send_reschedule(Model1_cpu);
}

static inline __attribute__((no_instrument_function)) void Model1_arch_send_call_function_single_ipi(int Model1_cpu)
{
 Model1_smp_ops.Model1_send_call_func_single_ipi(Model1_cpu);
}

static inline __attribute__((no_instrument_function)) void Model1_arch_send_call_function_ipi_mask(const struct Model1_cpumask *Model1_mask)
{
 Model1_smp_ops.Model1_send_call_func_ipi(Model1_mask);
}

void Model1_cpu_disable_common(void);
void Model1_native_smp_prepare_boot_cpu(void);
void Model1_native_smp_prepare_cpus(unsigned int Model1_max_cpus);
void Model1_native_smp_cpus_done(unsigned int Model1_max_cpus);
void Model1_common_cpu_up(unsigned int Model1_cpunum, struct Model1_task_struct *Model1_tidle);
int Model1_native_cpu_up(unsigned int Model1_cpunum, struct Model1_task_struct *Model1_tidle);
int Model1_native_cpu_disable(void);
int Model1_common_cpu_die(unsigned int Model1_cpu);
void Model1_native_cpu_die(unsigned int Model1_cpu);
void Model1_hlt_play_dead(void);
void Model1_native_play_dead(void);
void Model1_play_dead_common(void);
void Model1_wbinvd_on_cpu(int Model1_cpu);
int Model1_wbinvd_on_all_cpus(void);

void Model1_native_send_call_func_ipi(const struct Model1_cpumask *Model1_mask);
void Model1_native_send_call_func_single_ipi(int Model1_cpu);
void Model1_x86_idle_thread_init(unsigned int Model1_cpu, struct Model1_task_struct *Model1_idle);

void Model1_smp_store_boot_cpu_info(void);
void Model1_smp_store_cpu_info(int Model1_id);
extern unsigned Model1_disabled_cpus;
extern int Model1_hard_smp_processor_id(void);

extern struct Model1_pglist_data *Model1_node_data[];



extern struct Model1_pglist_data *Model1_first_online_pgdat(void);
extern struct Model1_pglist_data *Model1_next_online_pgdat(struct Model1_pglist_data *Model1_pgdat);
extern struct Model1_zone *Model1_next_zone(struct Model1_zone *Model1_zone);

/**
 * for_each_online_pgdat - helper macro to iterate over all online nodes
 * @pgdat - pointer to a pg_data_t variable
 */




/**
 * for_each_zone - helper macro to iterate over all memory zones
 * @zone - pointer to struct zone variable
 *
 * The user only needs to declare the zone variable, for_each_zone
 * fills it in.
 */
static inline __attribute__((no_instrument_function)) struct Model1_zone *Model1_zonelist_zone(struct Model1_zoneref *Model1_zoneref)
{
 return Model1_zoneref->Model1_zone;
}

static inline __attribute__((no_instrument_function)) int Model1_zonelist_zone_idx(struct Model1_zoneref *Model1_zoneref)
{
 return Model1_zoneref->Model1_zone_idx;
}

static inline __attribute__((no_instrument_function)) int Model1_zonelist_node_idx(struct Model1_zoneref *Model1_zoneref)
{

 /* zone_to_nid not available in this context */
 return Model1_zoneref->Model1_zone->Model1_node;



}

struct Model1_zoneref *Model1___next_zones_zonelist(struct Model1_zoneref *Model1_z,
     enum Model1_zone_type Model1_highest_zoneidx,
     Model1_nodemask_t *Model1_nodes);

/**
 * next_zones_zonelist - Returns the next zone at or below highest_zoneidx within the allowed nodemask using a cursor within a zonelist as a starting point
 * @z - The cursor used as a starting point for the search
 * @highest_zoneidx - The zone index of the highest zone to return
 * @nodes - An optional nodemask to filter the zonelist with
 *
 * This function returns the next zone at or below a given zone index that is
 * within the allowed nodemask using a cursor as the starting point for the
 * search. The zoneref returned is a cursor that represents the current zone
 * being examined. It should be advanced by one before calling
 * next_zones_zonelist again.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) struct Model1_zoneref *Model1_next_zones_zonelist(struct Model1_zoneref *Model1_z,
     enum Model1_zone_type Model1_highest_zoneidx,
     Model1_nodemask_t *Model1_nodes)
{
 if (__builtin_expect(!!(!Model1_nodes && Model1_zonelist_zone_idx(Model1_z) <= Model1_highest_zoneidx), 1))
  return Model1_z;
 return Model1___next_zones_zonelist(Model1_z, Model1_highest_zoneidx, Model1_nodes);
}

/**
 * first_zones_zonelist - Returns the first zone at or below highest_zoneidx within the allowed nodemask in a zonelist
 * @zonelist - The zonelist to search for a suitable zone
 * @highest_zoneidx - The zone index of the highest zone to return
 * @nodes - An optional nodemask to filter the zonelist with
 * @zone - The first suitable zone found is returned via this parameter
 *
 * This function returns the first zone at or below a given zone index that is
 * within the allowed nodemask. The zoneref returned is a cursor that can be
 * used to iterate the zonelist with next_zones_zonelist by advancing it by
 * one before calling.
 */
static inline __attribute__((no_instrument_function)) struct Model1_zoneref *Model1_first_zones_zonelist(struct Model1_zonelist *Model1_zonelist,
     enum Model1_zone_type Model1_highest_zoneidx,
     Model1_nodemask_t *Model1_nodes)
{
 return Model1_next_zones_zonelist(Model1_zonelist->Model1__zonerefs,
       Model1_highest_zoneidx, Model1_nodes);
}

/**
 * for_each_zone_zonelist_nodemask - helper macro to iterate over valid zones in a zonelist at or below a given zone index and within a nodemask
 * @zone - The current zone in the iterator
 * @z - The current pointer within zonelist->zones being iterated
 * @zlist - The zonelist being iterated
 * @highidx - The zone index of the highest zone to return
 * @nodemask - Nodemask allowed by the allocator
 *
 * This iterator iterates though all zones at or below a given zone index and
 * within a given nodemask
 */
/**
 * for_each_zone_zonelist - helper macro to iterate over valid zones in a zonelist at or below a given zone index
 * @zone - The current zone in the iterator
 * @z - The current pointer within zonelist->zones being iterated
 * @zlist - The zonelist being iterated
 * @highidx - The zone index of the highest zone to return
 *
 * This iterator iterates though all zones at or below a given zone index.
 */
/*
 * SECTION_SHIFT    		#bits space required to store a section #
 *
 * PA_SECTION_SHIFT		physical address to/from section number
 * PFN_SECTION_SHIFT		pfn to/from section number
 */
struct Model1_page;
struct Model1_page_ext;
struct Model1_mem_section {
 /*
	 * This is, logically, a pointer to an array of struct
	 * pages.  However, it is stored with some other magic.
	 * (see sparse.c::sparse_init_one_section())
	 *
	 * Additionally during early boot we encode node id of
	 * the location of the section here to guide allocation.
	 * (see sparse.c::memory_present())
	 *
	 * Making it a UL at least makes someone do a cast
	 * before using it wrong.
	 */
 unsigned long Model1_section_mem_map;

 /* See declaration of similar field in struct zone */
 unsigned long *Model1_pageblock_flags;
 /*
	 * WARNING: mem_section must be a power-of-2 in size for the
	 * calculation and use of SECTION_ROOT_MASK to make sense.
	 */
};
extern struct Model1_mem_section *Model1_mem_section[((((1UL << (46 - 27))) + ((((1UL) << 12) / sizeof (struct Model1_mem_section))) - 1) / ((((1UL) << 12) / sizeof (struct Model1_mem_section))))];




static inline __attribute__((no_instrument_function)) struct Model1_mem_section *Model1___nr_to_section(unsigned long Model1_nr)
{
 if (!Model1_mem_section[((Model1_nr) / (((1UL) << 12) / sizeof (struct Model1_mem_section)))])
  return ((void *)0);
 return &Model1_mem_section[((Model1_nr) / (((1UL) << 12) / sizeof (struct Model1_mem_section)))][Model1_nr & ((((1UL) << 12) / sizeof (struct Model1_mem_section)) - 1)];
}
extern int Model1___section_nr(struct Model1_mem_section* Model1_ms);
extern unsigned long Model1_usemap_size(void);

/*
 * We use the lower bits of the mem_map pointer to store
 * a little bit of information.  There should be at least
 * 3 bits here due to 32-bit alignment.
 */






static inline __attribute__((no_instrument_function)) struct Model1_page *Model1___section_mem_map_addr(struct Model1_mem_section *section)
{
 unsigned long Model1_map = section->Model1_section_mem_map;
 Model1_map &= (~((1UL<<2)-1));
 return (struct Model1_page *)Model1_map;
}

static inline __attribute__((no_instrument_function)) int Model1_present_section(struct Model1_mem_section *section)
{
 return (section && (section->Model1_section_mem_map & (1UL<<0)));
}

static inline __attribute__((no_instrument_function)) int Model1_present_section_nr(unsigned long Model1_nr)
{
 return Model1_present_section(Model1___nr_to_section(Model1_nr));
}

static inline __attribute__((no_instrument_function)) int Model1_valid_section(struct Model1_mem_section *section)
{
 return (section && (section->Model1_section_mem_map & (1UL<<1)));
}

static inline __attribute__((no_instrument_function)) int Model1_valid_section_nr(unsigned long Model1_nr)
{
 return Model1_valid_section(Model1___nr_to_section(Model1_nr));
}

static inline __attribute__((no_instrument_function)) struct Model1_mem_section *Model1___pfn_to_section(unsigned long Model1_pfn)
{
 return Model1___nr_to_section(((Model1_pfn) >> (27 - 12)));
}


static inline __attribute__((no_instrument_function)) int Model1_pfn_valid(unsigned long Model1_pfn)
{
 if (((Model1_pfn) >> (27 - 12)) >= (1UL << (46 - 27)))
  return 0;
 return Model1_valid_section(Model1___nr_to_section(((Model1_pfn) >> (27 - 12))));
}


static inline __attribute__((no_instrument_function)) int Model1_pfn_present(unsigned long Model1_pfn)
{
 if (((Model1_pfn) >> (27 - 12)) >= (1UL << (46 - 27)))
  return 0;
 return Model1_present_section(Model1___nr_to_section(((Model1_pfn) >> (27 - 12))));
}

/*
 * These are _only_ used during initialisation, therefore they
 * can use __initdata ...  They could have names to indicate
 * this restriction.
 */
void Model1_sparse_init(void);





/*
 * During memory init memblocks map pfns to nids. The search is expensive and
 * this caches recent lookups. The implementation of __early_pfn_to_nid
 * may treat start/end as pfns or sections.
 */
struct Model1_mminit_pfnnid_cache {
 unsigned long Model1_last_start;
 unsigned long Model1_last_end;
 int Model1_last_nid;
};





void Model1_memory_present(int Model1_nid, unsigned long Model1_start, unsigned long Model1_end);
unsigned long __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_node_memmap_size_bytes(int, unsigned long, unsigned long);

/*
 * If it is possible to have holes within a MAX_ORDER_NR_PAGES, then we
 * need to check pfn validility within that MAX_ORDER_NR_PAGES block.
 * pfn_valid_within() should be used in this case; we optimise this away
 * when we have no holes within a MAX_ORDER_NR_PAGES block.
 */
static inline __attribute__((no_instrument_function)) bool Model1_memmap_valid_within(unsigned long Model1_pfn,
     struct Model1_page *Model1_page, struct Model1_zone *Model1_zone)
{
 return true;
}


/*
 * include/linux/topology.h
 *
 * Written by: Matthew Dobson, IBM Corporation
 *
 * Copyright (C) 2002, IBM Corp.
 *
 * All rights reserved.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or
 * NON INFRINGEMENT.  See the GNU General Public License for more
 * details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
 *
 * Send feedback to <colpatch@us.ibm.com>
 */










/*
 *	Generic SMP support
 *		Alan Cox. <alan@redhat.com>
 */









/*
 * Lock-less NULL terminated single linked list
 *
 * If there are multiple producers and multiple consumers, llist_add
 * can be used in producers and llist_del_all can be used in
 * consumers.  They can work simultaneously without lock.  But
 * llist_del_first can not be used here.  Because llist_del_first
 * depends on list->first->next does not changed if list->first is not
 * changed during its operation, but llist_del_first, llist_add,
 * llist_add (or llist_del_all, llist_add, llist_add) sequence in
 * another consumer may violate that.
 *
 * If there are multiple producers and one consumer, llist_add can be
 * used in producers and llist_del_all or llist_del_first can be used
 * in the consumer.
 *
 * This can be summarized as follow:
 *
 *           |   add    | del_first |  del_all
 * add       |    -     |     -     |     -
 * del_first |          |     L     |     L
 * del_all   |          |           |     -
 *
 * Where "-" stands for no lock is needed, while "L" stands for lock
 * is needed.
 *
 * The list entries deleted via llist_del_all can be traversed with
 * traversing function such as llist_for_each etc.  But the list
 * entries can not be traversed safely before deleted from the list.
 * The order of deleted entries is from the newest to the oldest added
 * one.  If you want to traverse from the oldest to the newest, you
 * must reverse the order by yourself before traversing.
 *
 * The basic atomic operation of this list is cmpxchg on long.  On
 * architectures that don't have NMI-safe cmpxchg implementation, the
 * list can NOT be used in NMI handlers.  So code that uses the list in
 * an NMI handler should depend on CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG.
 *
 * Copyright 2010,2011 Intel Corp.
 *   Author: Huang Ying <ying.huang@intel.com>
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public License version
 * 2 as published by the Free Software Foundation;
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
 */




struct Model1_llist_head {
 struct Model1_llist_node *Model1_first;
};

struct Model1_llist_node {
 struct Model1_llist_node *Model1_next;
};




/**
 * init_llist_head - initialize lock-less list head
 * @head:	the head for your lock-less list
 */
static inline __attribute__((no_instrument_function)) void Model1_init_llist_head(struct Model1_llist_head *Model1_list)
{
 Model1_list->Model1_first = ((void *)0);
}

/**
 * llist_entry - get the struct of this entry
 * @ptr:	the &struct llist_node pointer.
 * @type:	the type of the struct this is embedded in.
 * @member:	the name of the llist_node within the struct.
 */



/**
 * llist_for_each - iterate over some deleted entries of a lock-less list
 * @pos:	the &struct llist_node to use as a loop cursor
 * @node:	the first entry of deleted list entries
 *
 * In general, some entries of the lock-less list can be traversed
 * safely only after being deleted from list, so start with an entry
 * instead of list head.
 *
 * If being used on entries deleted from lock-less list directly, the
 * traverse order is from the newest to the oldest added entry.  If
 * you want to traverse from the oldest to the newest, you must
 * reverse the order by yourself before traversing.
 */



/**
 * llist_for_each_entry - iterate over some deleted entries of lock-less list of given type
 * @pos:	the type * to use as a loop cursor.
 * @node:	the fist entry of deleted list entries.
 * @member:	the name of the llist_node with the struct.
 *
 * In general, some entries of the lock-less list can be traversed
 * safely only after being removed from list, so start with an entry
 * instead of list head.
 *
 * If being used on entries deleted from lock-less list directly, the
 * traverse order is from the newest to the oldest added entry.  If
 * you want to traverse from the oldest to the newest, you must
 * reverse the order by yourself before traversing.
 */





/**
 * llist_for_each_entry_safe - iterate over some deleted entries of lock-less list of given type
 *			       safe against removal of list entry
 * @pos:	the type * to use as a loop cursor.
 * @n:		another type * to use as temporary storage
 * @node:	the first entry of deleted list entries.
 * @member:	the name of the llist_node with the struct.
 *
 * In general, some entries of the lock-less list can be traversed
 * safely only after being removed from list, so start with an entry
 * instead of list head.
 *
 * If being used on entries deleted from lock-less list directly, the
 * traverse order is from the newest to the oldest added entry.  If
 * you want to traverse from the oldest to the newest, you must
 * reverse the order by yourself before traversing.
 */






/**
 * llist_empty - tests whether a lock-less list is empty
 * @head:	the list to test
 *
 * Not guaranteed to be accurate or up to date.  Just a quick way to
 * test whether the list is empty without deleting something from the
 * list.
 */
static inline __attribute__((no_instrument_function)) bool Model1_llist_empty(const struct Model1_llist_head *Model1_head)
{
 return (*({ __attribute__((unused)) typeof(Model1_head->Model1_first) Model1___var = ( typeof(Model1_head->Model1_first)) 0; (volatile typeof(Model1_head->Model1_first) *)&(Model1_head->Model1_first); })) == ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model1_llist_node *Model1_llist_next(struct Model1_llist_node *Model1_node)
{
 return Model1_node->Model1_next;
}

extern bool Model1_llist_add_batch(struct Model1_llist_node *Model1_new_first,
       struct Model1_llist_node *Model1_new_last,
       struct Model1_llist_head *Model1_head);
/**
 * llist_add - add a new entry
 * @new:	new entry to be added
 * @head:	the head for your lock-less list
 *
 * Returns true if the list was empty prior to adding this entry.
 */
static inline __attribute__((no_instrument_function)) bool Model1_llist_add(struct Model1_llist_node *Model1_new, struct Model1_llist_head *Model1_head)
{
 return Model1_llist_add_batch(Model1_new, Model1_new, Model1_head);
}

/**
 * llist_del_all - delete all entries from lock-less list
 * @head:	the head of lock-less list to delete all entries
 *
 * If list is empty, return NULL, otherwise, delete all entries and
 * return the pointer to the first entry.  The order of entries
 * deleted is from the newest to the oldest added one.
 */
static inline __attribute__((no_instrument_function)) struct Model1_llist_node *Model1_llist_del_all(struct Model1_llist_head *Model1_head)
{
 return ({ __typeof__ (*((&Model1_head->Model1_first))) Model1___ret = ((((void *)0))); switch (sizeof(*((&Model1_head->Model1_first)))) { case 1: asm volatile ("" "xchg" "b %b0, %1\n" : "+q" (Model1___ret), "+m" (*((&Model1_head->Model1_first))) : : "memory", "cc"); break; case 2: asm volatile ("" "xchg" "w %w0, %1\n" : "+r" (Model1___ret), "+m" (*((&Model1_head->Model1_first))) : : "memory", "cc"); break; case 4: asm volatile ("" "xchg" "l %0, %1\n" : "+r" (Model1___ret), "+m" (*((&Model1_head->Model1_first))) : : "memory", "cc"); break; case 8: asm volatile ("" "xchg" "q %q0, %1\n" : "+r" (Model1___ret), "+m" (*((&Model1_head->Model1_first))) : : "memory", "cc"); break; default: Model1___xchg_wrong_size(); } Model1___ret; });
}

extern struct Model1_llist_node *Model1_llist_del_first(struct Model1_llist_head *Model1_head);

struct Model1_llist_node *Model1_llist_reverse_order(struct Model1_llist_node *Model1_head);

typedef void (*Model1_smp_call_func_t)(void *Model1_info);
struct Model1_call_single_data {
 struct Model1_llist_node Model1_llist;
 Model1_smp_call_func_t func;
 void *Model1_info;
 unsigned int Model1_flags;
};

/* total number of cpus in this system (may exceed NR_CPUS) */
extern unsigned int Model1_total_cpus;

int Model1_smp_call_function_single(int Model1_cpuid, Model1_smp_call_func_t func, void *Model1_info,
        int Model1_wait);

/*
 * Call a function on all processors
 */
int Model1_on_each_cpu(Model1_smp_call_func_t func, void *Model1_info, int Model1_wait);

/*
 * Call a function on processors specified by mask, which might include
 * the local one.
 */
void Model1_on_each_cpu_mask(const struct Model1_cpumask *Model1_mask, Model1_smp_call_func_t func,
  void *Model1_info, bool Model1_wait);

/*
 * Call a function on each processor for which the supplied function
 * cond_func returns a positive value. This may include the local
 * processor.
 */
void Model1_on_each_cpu_cond(bool (*Model1_cond_func)(int Model1_cpu, void *Model1_info),
  Model1_smp_call_func_t func, void *Model1_info, bool Model1_wait,
  Model1_gfp_t Model1_gfp_flags);

int Model1_smp_call_function_single_async(int Model1_cpu, struct Model1_call_single_data *Model1_csd);
/*
 * main cross-CPU interfaces, handles INIT, TLB flush, STOP, etc.
 * (defined in asm header):
 */

/*
 * stops all CPUs but the current one:
 */
extern void Model1_smp_send_stop(void);

/*
 * sends a 'reschedule' event to another CPU:
 */
extern void Model1_smp_send_reschedule(int Model1_cpu);


/*
 * Prepare machine for booting other CPUs.
 */
extern void Model1_smp_prepare_cpus(unsigned int Model1_max_cpus);

/*
 * Bring a CPU up
 */
extern int Model1___cpu_up(unsigned int Model1_cpunum, struct Model1_task_struct *Model1_tidle);

/*
 * Final polishing of CPUs
 */
extern void Model1_smp_cpus_done(unsigned int Model1_max_cpus);

/*
 * Call a function on all other processors
 */
int Model1_smp_call_function(Model1_smp_call_func_t func, void *Model1_info, int Model1_wait);
void Model1_smp_call_function_many(const struct Model1_cpumask *Model1_mask,
       Model1_smp_call_func_t func, void *Model1_info, bool Model1_wait);

int Model1_smp_call_function_any(const struct Model1_cpumask *Model1_mask,
     Model1_smp_call_func_t func, void *Model1_info, int Model1_wait);

void Model1_kick_all_cpus_sync(void);
void Model1_wake_up_all_idle_cpus(void);

/*
 * Generic and arch helpers
 */
void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_call_function_init(void);
void Model1_generic_smp_call_function_single_interrupt(void);



/*
 * Mark the boot cpu "online" so that it can call console drivers in
 * printk() and can access its per-cpu storage.
 */
void Model1_smp_prepare_boot_cpu(void);

extern unsigned int Model1_setup_max_cpus;
extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_setup_nr_cpu_ids(void);
extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_smp_init(void);
/*
 * smp_processor_id(): get the current CPU ID.
 *
 * if DEBUG_PREEMPT is enabled then we check whether it is
 * used in a preemption-safe way. (smp_processor_id() is safe
 * if it's used in a preemption-off critical section, or in
 * a thread that is bound to the current CPU.)
 *
 * NOTE: raw_smp_processor_id() is for internal use only
 * (smp_processor_id() is the preferred variant), but in rare
 * instances it might also be used to turn off false positives
 * (i.e. smp_processor_id() use that the debugging code reports but
 * which use for some reason is legal). Don't use this to hack around
 * the warning message, as your code might not work under PREEMPT.
 */
/*
 * Callback to arch code if there's nosmp or maxcpus=0 on the
 * boot command line:
 */
extern void Model1_arch_disable_smp_support(void);

extern void Model1_arch_enable_nonboot_cpus_begin(void);
extern void Model1_arch_enable_nonboot_cpus_end(void);

void Model1_smp_setup_processor_id(void);

/* SMP core functions */
int Model1_smpcfd_prepare_cpu(unsigned int Model1_cpu);
int Model1_smpcfd_dead_cpu(unsigned int Model1_cpu);
int Model1_smpcfd_dying_cpu(unsigned int Model1_cpu);
/* enough to cover all DEFINE_PER_CPUs in modules */






/* minimum unit size, also is the maximum supported allocation size */


/*
 * Percpu allocator can serve percpu allocations before slab is
 * initialized which allows slab to depend on the percpu allocator.
 * The following two parameters decide how much resource to
 * preallocate for this.  Keep PERCPU_DYNAMIC_RESERVE equal to or
 * larger than PERCPU_DYNAMIC_EARLY_SIZE.
 */



/*
 * PERCPU_DYNAMIC_RESERVE indicates the amount of free area to piggy
 * back on the first chunk for dynamic percpu allocation if arch is
 * manually allocating and mapping it for faster access (as a part of
 * large page mapping for example).
 *
 * The following values give between one and two pages of free space
 * after typical minimal boot (2-way SMP, single disk and NIC) with
 * both defconfig and a distro config on x86_64 and 32.  More
 * intelligent way to determine this would be nice.
 */






extern void *Model1_pcpu_base_addr;
extern const unsigned long *Model1_pcpu_unit_offsets;

struct Model1_pcpu_group_info {
 int Model1_nr_units; /* aligned # of units */
 unsigned long Model1_base_offset; /* base address offset */
 unsigned int *Model1_cpu_map; /* unit->cpu map, empty
						 * entries contain NR_CPUS */
};

struct Model1_pcpu_alloc_info {
 Model1_size_t Model1_static_size;
 Model1_size_t Model1_reserved_size;
 Model1_size_t Model1_dyn_size;
 Model1_size_t Model1_unit_size;
 Model1_size_t Model1_atom_size;
 Model1_size_t Model1_alloc_size;
 Model1_size_t Model1___ai_size; /* internal, don't use */
 int Model1_nr_groups; /* 0 if grouping unnecessary */
 struct Model1_pcpu_group_info Model1_groups[];
};

enum Model1_pcpu_fc {
 Model1_PCPU_FC_AUTO,
 Model1_PCPU_FC_EMBED,
 Model1_PCPU_FC_PAGE,

 Model1_PCPU_FC_NR,
};
extern const char * const Model1_pcpu_fc_names[Model1_PCPU_FC_NR];

extern enum Model1_pcpu_fc Model1_pcpu_chosen_fc;

typedef void * (*Model1_pcpu_fc_alloc_fn_t)(unsigned int Model1_cpu, Model1_size_t Model1_size,
         Model1_size_t Model1_align);
typedef void (*Model1_pcpu_fc_free_fn_t)(void *Model1_ptr, Model1_size_t Model1_size);
typedef void (*Model1_pcpu_fc_populate_pte_fn_t)(unsigned long Model1_addr);
typedef int (Model1_pcpu_fc_cpu_distance_fn_t)(unsigned int Model1_from, unsigned int Model1_to);

extern struct Model1_pcpu_alloc_info * __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_pcpu_alloc_alloc_info(int Model1_nr_groups,
            int Model1_nr_units);
extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_pcpu_free_alloc_info(struct Model1_pcpu_alloc_info *Model1_ai);

extern int __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_pcpu_setup_first_chunk(const struct Model1_pcpu_alloc_info *Model1_ai,
      void *Model1_base_addr);


extern int __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_pcpu_embed_first_chunk(Model1_size_t Model1_reserved_size, Model1_size_t Model1_dyn_size,
    Model1_size_t Model1_atom_size,
    Model1_pcpu_fc_cpu_distance_fn_t Model1_cpu_distance_fn,
    Model1_pcpu_fc_alloc_fn_t Model1_alloc_fn,
    Model1_pcpu_fc_free_fn_t Model1_free_fn);



extern int __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_pcpu_page_first_chunk(Model1_size_t Model1_reserved_size,
    Model1_pcpu_fc_alloc_fn_t Model1_alloc_fn,
    Model1_pcpu_fc_free_fn_t Model1_free_fn,
    Model1_pcpu_fc_populate_pte_fn_t Model1_populate_pte_fn);


extern void *Model1___alloc_reserved_percpu(Model1_size_t Model1_size, Model1_size_t Model1_align);
extern bool Model1_is_kernel_percpu_address(unsigned long Model1_addr);




extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_percpu_init_late(void);

extern void *Model1___alloc_percpu_gfp(Model1_size_t Model1_size, Model1_size_t Model1_align, Model1_gfp_t Model1_gfp);
extern void *Model1___alloc_percpu(Model1_size_t Model1_size, Model1_size_t Model1_align);
extern void Model1_free_percpu(void *Model1___pdata);
extern Model1_phys_addr_t Model1_per_cpu_ptr_to_phys(void *Model1_addr);
int Model1_arch_update_cpu_topology(void);

/* Conform to ACPI 2.0 SLIT distance definitions */






/*
 * If the distance between nodes in a system is larger than RECLAIM_DISTANCE
 * (in whatever arch specific measurement units returned by node_distance())
 * and node_reclaim_mode is enabled then the VM will only call node_reclaim()
 * on nodes within this distance.
 */







extern __attribute__((section(".data..percpu" ""))) __typeof__(int) Model1_numa_node;


/* Returns the number of the current Node. */
static inline __attribute__((no_instrument_function)) int Model1_numa_node_id(void)
{
 return ({ typeof(Model1_numa_node) Model1_pscr_ret__; do { const void *Model1___vpp_verify = (typeof((&(Model1_numa_node)) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); switch(sizeof(Model1_numa_node)) { case 1: Model1_pscr_ret__ = ({ typeof(Model1_numa_node) Model1_pfo_ret__; switch (sizeof(Model1_numa_node)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_numa_node)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_numa_node)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_numa_node)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_numa_node)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 2: Model1_pscr_ret__ = ({ typeof(Model1_numa_node) Model1_pfo_ret__; switch (sizeof(Model1_numa_node)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_numa_node)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_numa_node)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_numa_node)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_numa_node)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 4: Model1_pscr_ret__ = ({ typeof(Model1_numa_node) Model1_pfo_ret__; switch (sizeof(Model1_numa_node)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_numa_node)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_numa_node)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_numa_node)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_numa_node)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 8: Model1_pscr_ret__ = ({ typeof(Model1_numa_node) Model1_pfo_ret__; switch (sizeof(Model1_numa_node)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_numa_node)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_numa_node)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_numa_node)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_numa_node)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; default: Model1___bad_size_call_parameter(); break; } Model1_pscr_ret__; });
}



static inline __attribute__((no_instrument_function)) int Model1_cpu_to_node(int Model1_cpu)
{
 return (*({ do { const void *Model1___vpp_verify = (typeof((&(Model1_numa_node)) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); ({ unsigned long Model1___ptr; __asm__ ("" : "=r"(Model1___ptr) : "0"((typeof(*((&(Model1_numa_node)))) *)((&(Model1_numa_node))))); (typeof((typeof(*((&(Model1_numa_node)))) *)((&(Model1_numa_node))))) (Model1___ptr + (((Model1___per_cpu_offset[(Model1_cpu)])))); }); }));
}



static inline __attribute__((no_instrument_function)) void Model1_set_numa_node(int Model1_node)
{
 do { do { const void *Model1___vpp_verify = (typeof((&(Model1_numa_node)) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); switch(sizeof(Model1_numa_node)) { case 1: do { typedef typeof((Model1_numa_node)) Model1_pto_T__; if (0) { Model1_pto_T__ Model1_pto_tmp__; Model1_pto_tmp__ = (Model1_node); (void)Model1_pto_tmp__; } switch (sizeof((Model1_numa_node))) { case 1: asm("mov" "b %1,""%%""gs"":" "%" "0" : "+m" ((Model1_numa_node)) : "qi" ((Model1_pto_T__)(Model1_node))); break; case 2: asm("mov" "w %1,""%%""gs"":" "%" "0" : "+m" ((Model1_numa_node)) : "ri" ((Model1_pto_T__)(Model1_node))); break; case 4: asm("mov" "l %1,""%%""gs"":" "%" "0" : "+m" ((Model1_numa_node)) : "ri" ((Model1_pto_T__)(Model1_node))); break; case 8: asm("mov" "q %1,""%%""gs"":" "%" "0" : "+m" ((Model1_numa_node)) : "re" ((Model1_pto_T__)(Model1_node))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 2: do { typedef typeof((Model1_numa_node)) Model1_pto_T__; if (0) { Model1_pto_T__ Model1_pto_tmp__; Model1_pto_tmp__ = (Model1_node); (void)Model1_pto_tmp__; } switch (sizeof((Model1_numa_node))) { case 1: asm("mov" "b %1,""%%""gs"":" "%" "0" : "+m" ((Model1_numa_node)) : "qi" ((Model1_pto_T__)(Model1_node))); break; case 2: asm("mov" "w %1,""%%""gs"":" "%" "0" : "+m" ((Model1_numa_node)) : "ri" ((Model1_pto_T__)(Model1_node))); break; case 4: asm("mov" "l %1,""%%""gs"":" "%" "0" : "+m" ((Model1_numa_node)) : "ri" ((Model1_pto_T__)(Model1_node))); break; case 8: asm("mov" "q %1,""%%""gs"":" "%" "0" : "+m" ((Model1_numa_node)) : "re" ((Model1_pto_T__)(Model1_node))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 4: do { typedef typeof((Model1_numa_node)) Model1_pto_T__; if (0) { Model1_pto_T__ Model1_pto_tmp__; Model1_pto_tmp__ = (Model1_node); (void)Model1_pto_tmp__; } switch (sizeof((Model1_numa_node))) { case 1: asm("mov" "b %1,""%%""gs"":" "%" "0" : "+m" ((Model1_numa_node)) : "qi" ((Model1_pto_T__)(Model1_node))); break; case 2: asm("mov" "w %1,""%%""gs"":" "%" "0" : "+m" ((Model1_numa_node)) : "ri" ((Model1_pto_T__)(Model1_node))); break; case 4: asm("mov" "l %1,""%%""gs"":" "%" "0" : "+m" ((Model1_numa_node)) : "ri" ((Model1_pto_T__)(Model1_node))); break; case 8: asm("mov" "q %1,""%%""gs"":" "%" "0" : "+m" ((Model1_numa_node)) : "re" ((Model1_pto_T__)(Model1_node))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 8: do { typedef typeof((Model1_numa_node)) Model1_pto_T__; if (0) { Model1_pto_T__ Model1_pto_tmp__; Model1_pto_tmp__ = (Model1_node); (void)Model1_pto_tmp__; } switch (sizeof((Model1_numa_node))) { case 1: asm("mov" "b %1,""%%""gs"":" "%" "0" : "+m" ((Model1_numa_node)) : "qi" ((Model1_pto_T__)(Model1_node))); break; case 2: asm("mov" "w %1,""%%""gs"":" "%" "0" : "+m" ((Model1_numa_node)) : "ri" ((Model1_pto_T__)(Model1_node))); break; case 4: asm("mov" "l %1,""%%""gs"":" "%" "0" : "+m" ((Model1_numa_node)) : "ri" ((Model1_pto_T__)(Model1_node))); break; case 8: asm("mov" "q %1,""%%""gs"":" "%" "0" : "+m" ((Model1_numa_node)) : "re" ((Model1_pto_T__)(Model1_node))); break; default: Model1___bad_percpu_size(); } } while (0);break; default: Model1___bad_size_call_parameter();break; } } while (0);
}



static inline __attribute__((no_instrument_function)) void Model1_set_cpu_numa_node(int Model1_cpu, int Model1_node)
{
 (*({ do { const void *Model1___vpp_verify = (typeof((&(Model1_numa_node)) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); ({ unsigned long Model1___ptr; __asm__ ("" : "=r"(Model1___ptr) : "0"((typeof(*((&(Model1_numa_node)))) *)((&(Model1_numa_node))))); (typeof((typeof(*((&(Model1_numa_node)))) *)((&(Model1_numa_node))))) (Model1___ptr + (((Model1___per_cpu_offset[(Model1_cpu)])))); }); })) = Model1_node;
}
/* Returns the number of the nearest Node with memory */
static inline __attribute__((no_instrument_function)) int Model1_numa_mem_id(void)
{
 return Model1_numa_node_id();
}



static inline __attribute__((no_instrument_function)) int Model1_node_to_mem_node(int Model1_node)
{
 return Model1_node;
}



static inline __attribute__((no_instrument_function)) int Model1_cpu_to_mem(int Model1_cpu)
{
 return Model1_cpu_to_node(Model1_cpu);
}
static inline __attribute__((no_instrument_function)) const struct Model1_cpumask *Model1_cpu_smt_mask(int Model1_cpu)
{
 return ((*({ do { const void *Model1___vpp_verify = (typeof((&(Model1_cpu_sibling_map)) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); ({ unsigned long Model1___ptr; __asm__ ("" : "=r"(Model1___ptr) : "0"((typeof(*((&(Model1_cpu_sibling_map)))) *)((&(Model1_cpu_sibling_map))))); (typeof((typeof(*((&(Model1_cpu_sibling_map)))) *)((&(Model1_cpu_sibling_map))))) (Model1___ptr + (((Model1___per_cpu_offset[(Model1_cpu)])))); }); })));
}


static inline __attribute__((no_instrument_function)) const struct Model1_cpumask *Model1_cpu_cpu_mask(int Model1_cpu)
{
 return Model1_cpumask_of_node(Model1_cpu_to_node(Model1_cpu));
}

struct Model1_vm_area_struct;

/*
 * In case of changes, please don't forget to update
 * include/trace/events/mmflags.h and tools/perf/builtin-kmem.c
 */

/* Plain integer GFP bitmasks. Do not use this directly. */
/* If the above are modified, __GFP_BITS_SHIFT may need updating */

/*
 * Physical address zone modifiers (see linux/mmzone.h - low four bits)
 *
 * Do not put any conditional on these. If necessary modify the definitions
 * without the underscores and use them consistently. The definitions here may
 * be used in bit comparisons.
 */






/*
 * Page mobility and placement hints
 *
 * These flags provide hints about how mobile the page is. Pages with similar
 * mobility are placed within the same pageblocks to minimise problems due
 * to external fragmentation.
 *
 * __GFP_MOVABLE (also a zone modifier) indicates that the page can be
 *   moved by page migration during memory compaction or can be reclaimed.
 *
 * __GFP_RECLAIMABLE is used for slab allocations that specify
 *   SLAB_RECLAIM_ACCOUNT and whose pages can be freed via shrinkers.
 *
 * __GFP_WRITE indicates the caller intends to dirty the page. Where possible,
 *   these pages will be spread between local zones to avoid all the dirty
 *   pages being in one zone (fair zone allocation policy).
 *
 * __GFP_HARDWALL enforces the cpuset memory allocation policy.
 *
 * __GFP_THISNODE forces the allocation to be satisified from the requested
 *   node with no fallbacks or placement policy enforcements.
 *
 * __GFP_ACCOUNT causes the allocation to be accounted to kmemcg.
 */






/*
 * Watermark modifiers -- controls access to emergency reserves
 *
 * __GFP_HIGH indicates that the caller is high-priority and that granting
 *   the request is necessary before the system can make forward progress.
 *   For example, creating an IO context to clean pages.
 *
 * __GFP_ATOMIC indicates that the caller cannot reclaim or sleep and is
 *   high priority. Users are typically interrupt handlers. This may be
 *   used in conjunction with __GFP_HIGH
 *
 * __GFP_MEMALLOC allows access to all memory. This should only be used when
 *   the caller guarantees the allocation will allow more memory to be freed
 *   very shortly e.g. process exiting or swapping. Users either should
 *   be the MM or co-ordinating closely with the VM (e.g. swap over NFS).
 *
 * __GFP_NOMEMALLOC is used to explicitly forbid access to emergency reserves.
 *   This takes precedence over the __GFP_MEMALLOC flag if both are set.
 */





/*
 * Reclaim modifiers
 *
 * __GFP_IO can start physical IO.
 *
 * __GFP_FS can call down to the low-level FS. Clearing the flag avoids the
 *   allocator recursing into the filesystem which might already be holding
 *   locks.
 *
 * __GFP_DIRECT_RECLAIM indicates that the caller may enter direct reclaim.
 *   This flag can be cleared to avoid unnecessary delays when a fallback
 *   option is available.
 *
 * __GFP_KSWAPD_RECLAIM indicates that the caller wants to wake kswapd when
 *   the low watermark is reached and have it reclaim pages until the high
 *   watermark is reached. A caller may wish to clear this flag when fallback
 *   options are available and the reclaim is likely to disrupt the system. The
 *   canonical example is THP allocation where a fallback is cheap but
 *   reclaim/compaction may cause indirect stalls.
 *
 * __GFP_RECLAIM is shorthand to allow/forbid both direct and kswapd reclaim.
 *
 * __GFP_REPEAT: Try hard to allocate the memory, but the allocation attempt
 *   _might_ fail.  This depends upon the particular VM implementation.
 *
 * __GFP_NOFAIL: The VM implementation _must_ retry infinitely: the caller
 *   cannot handle allocation failures. New users should be evaluated carefully
 *   (and the flag should be used only when there is no reasonable failure
 *   policy) but it is definitely preferable to use the flag rather than
 *   opencode endless loop around allocator.
 *
 * __GFP_NORETRY: The VM implementation must not retry indefinitely and will
 *   return NULL when direct reclaim and memory compaction have failed to allow
 *   the allocation to succeed.  The OOM killer is not called with the current
 *   implementation.
 */
/*
 * Action modifiers
 *
 * __GFP_COLD indicates that the caller does not expect to be used in the near
 *   future. Where possible, a cache-cold page will be returned.
 *
 * __GFP_NOWARN suppresses allocation failure reports.
 *
 * __GFP_COMP address compound page metadata.
 *
 * __GFP_ZERO returns a zeroed page on success.
 *
 * __GFP_NOTRACK avoids tracking with kmemcheck.
 *
 * __GFP_NOTRACK_FALSE_POSITIVE is an alias of __GFP_NOTRACK. It's a means of
 *   distinguishing in the source between false positives and allocations that
 *   cannot be supported (e.g. page tables).
 *
 * __GFP_OTHER_NODE is for allocations that are on a remote node but that
 *   should not be accounted for as a remote allocation in vmstat. A
 *   typical user would be khugepaged collapsing a huge page on a remote
 *   node.
 */
/* Room for N __GFP_FOO bits */



/*
 * Useful GFP flag combinations that are commonly used. It is recommended
 * that subsystems start with one of these combinations and then set/clear
 * __GFP_FOO flags as necessary.
 *
 * GFP_ATOMIC users can not sleep and need the allocation to succeed. A lower
 *   watermark is applied to allow access to "atomic reserves"
 *
 * GFP_KERNEL is typical for kernel-internal allocations. The caller requires
 *   ZONE_NORMAL or a lower zone for direct access but can direct reclaim.
 *
 * GFP_KERNEL_ACCOUNT is the same as GFP_KERNEL, except the allocation is
 *   accounted to kmemcg.
 *
 * GFP_NOWAIT is for kernel allocations that should not stall for direct
 *   reclaim, start physical IO or use any filesystem callback.
 *
 * GFP_NOIO will use direct reclaim to discard clean pages or slab pages
 *   that do not require the starting of any physical IO.
 *
 * GFP_NOFS will use direct reclaim but will not use any filesystem interfaces.
 *
 * GFP_USER is for userspace allocations that also need to be directly
 *   accessibly by the kernel or hardware. It is typically used by hardware
 *   for buffers that are mapped to userspace (e.g. graphics) that hardware
 *   still must DMA to. cpuset limits are enforced for these allocations.
 *
 * GFP_DMA exists for historical reasons and should be avoided where possible.
 *   The flags indicates that the caller requires that the lowest zone be
 *   used (ZONE_DMA or 16M on x86-64). Ideally, this would be removed but
 *   it would require careful auditing as some users really require it and
 *   others use the flag to avoid lowmem reserves in ZONE_DMA and treat the
 *   lowest zone as a type of emergency reserve.
 *
 * GFP_DMA32 is similar to GFP_DMA except that the caller requires a 32-bit
 *   address.
 *
 * GFP_HIGHUSER is for userspace allocations that may be mapped to userspace,
 *   do not need to be directly accessible by the kernel but that cannot
 *   move once in use. An example may be a hardware allocation that maps
 *   data directly into userspace but has no addressing limitations.
 *
 * GFP_HIGHUSER_MOVABLE is for userspace allocations that the kernel does not
 *   need direct access to but can use kmap() when access is required. They
 *   are expected to be movable via page reclaim or page migration. Typically,
 *   pages on the LRU would also be allocated with GFP_HIGHUSER_MOVABLE.
 *
 * GFP_TRANSHUGE and GFP_TRANSHUGE_LIGHT are used for THP allocations. They are
 *   compound allocations that will generally fail quickly if memory is not
 *   available and will not wake kswapd/kcompactd on failure. The _LIGHT
 *   version does not attempt reclaim/compaction at all and is by default used
 *   in page fault path, while the non-light is used by khugepaged.
 */
/* Convert GFP flags to their corresponding migrate type */



static inline __attribute__((no_instrument_function)) int Model1_gfpflags_to_migratetype(const Model1_gfp_t Model1_gfp_flags)
{
 ((void)(sizeof(( long)((Model1_gfp_flags & ((( Model1_gfp_t)0x10u)|(( Model1_gfp_t)0x08u))) == ((( Model1_gfp_t)0x10u)|(( Model1_gfp_t)0x08u))))));
 do { bool Model1___cond = !(!((1UL << 3) != 0x08u)); extern void Model1___compiletime_assert_270(void) ; if (Model1___cond) Model1___compiletime_assert_270(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
 do { bool Model1___cond = !(!((0x08u >> 3) != Model1_MIGRATE_MOVABLE)); extern void Model1___compiletime_assert_271(void) ; if (Model1___cond) Model1___compiletime_assert_271(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);

 if (__builtin_expect(!!(Model1_page_group_by_mobility_disabled), 0))
  return Model1_MIGRATE_UNMOVABLE;

 /* Group based on mobility */
 return (Model1_gfp_flags & ((( Model1_gfp_t)0x10u)|(( Model1_gfp_t)0x08u))) >> 3;
}



static inline __attribute__((no_instrument_function)) bool Model1_gfpflags_allow_blocking(const Model1_gfp_t Model1_gfp_flags)
{
 return !!(Model1_gfp_flags & (( Model1_gfp_t)0x400000u));
}
/*
 * GFP_ZONE_TABLE is a word size bitstring that is used for looking up the
 * zone to use given the lowest 4 bits of gfp_t. Entries are ZONE_SHIFT long
 * and there are 16 of them to cover all possible combinations of
 * __GFP_DMA, __GFP_DMA32, __GFP_MOVABLE and __GFP_HIGHMEM.
 *
 * The zone fallback order is MOVABLE=>HIGHMEM=>NORMAL=>DMA32=>DMA.
 * But GFP_MOVABLE is not only a zone specifier but also an allocation
 * policy. Therefore __GFP_MOVABLE plus another zone selector is valid.
 * Only 1 bit of the lowest 3 bits (DMA,DMA32,HIGHMEM) can be set to "1".
 *
 *       bit       result
 *       =================
 *       0x0    => NORMAL
 *       0x1    => DMA or NORMAL
 *       0x2    => HIGHMEM or NORMAL
 *       0x3    => BAD (DMA+HIGHMEM)
 *       0x4    => DMA32 or DMA or NORMAL
 *       0x5    => BAD (DMA+DMA32)
 *       0x6    => BAD (HIGHMEM+DMA32)
 *       0x7    => BAD (HIGHMEM+DMA32+DMA)
 *       0x8    => NORMAL (MOVABLE+0)
 *       0x9    => DMA or NORMAL (MOVABLE+DMA)
 *       0xa    => MOVABLE (Movable is valid only if HIGHMEM is set too)
 *       0xb    => BAD (MOVABLE+HIGHMEM+DMA)
 *       0xc    => DMA32 (MOVABLE+DMA32)
 *       0xd    => BAD (MOVABLE+DMA32+DMA)
 *       0xe    => BAD (MOVABLE+DMA32+HIGHMEM)
 *       0xf    => BAD (MOVABLE+DMA32+HIGHMEM+DMA)
 *
 * GFP_ZONES_SHIFT must be <= 2 on 32 bit platforms.
 */
/*
 * GFP_ZONE_BAD is a bitmap for all combinations of __GFP_DMA, __GFP_DMA32
 * __GFP_HIGHMEM and __GFP_MOVABLE that are not permitted. One flag per
 * entry starting with bit 0. Bit is set if the combination is not
 * allowed.
 */
static inline __attribute__((no_instrument_function)) enum Model1_zone_type Model1_gfp_zone(Model1_gfp_t Model1_flags)
{
 enum Model1_zone_type Model1_z;
 int Model1_bit = ( int) (Model1_flags & ((( Model1_gfp_t)0x01u)|(( Model1_gfp_t)0x02u)|(( Model1_gfp_t)0x04u)|(( Model1_gfp_t)0x08u)));

 Model1_z = (( (Model1_ZONE_NORMAL << 0 * 2) | (Model1_ZONE_DMA << 0x01u * 2) | (Model1_ZONE_NORMAL << 0x02u * 2) | (Model1_ZONE_DMA32 << 0x04u * 2) | (Model1_ZONE_NORMAL << 0x08u * 2) | (Model1_ZONE_DMA << (0x08u | 0x01u) * 2) | (Model1_ZONE_MOVABLE << (0x08u | 0x02u) * 2) | (Model1_ZONE_DMA32 << (0x08u | 0x04u) * 2)) >> (Model1_bit * 2)) &
      ((1 << 2) - 1);
 ((void)(sizeof(( long)((( 1 << (0x01u | 0x02u) | 1 << (0x01u | 0x04u) | 1 << (0x04u | 0x02u) | 1 << (0x01u | 0x04u | 0x02u) | 1 << (0x08u | 0x02u | 0x01u) | 1 << (0x08u | 0x04u | 0x01u) | 1 << (0x08u | 0x04u | 0x02u) | 1 << (0x08u | 0x04u | 0x01u | 0x02u) ) >> Model1_bit) & 1))));
 return Model1_z;
}

/*
 * There is only one page-allocator function, and two main namespaces to
 * it. The alloc_page*() variants return 'struct page *' and as such
 * can allocate highmem pages, the *get*page*() variants return
 * virtual kernel addresses to the allocated page(s).
 */

static inline __attribute__((no_instrument_function)) int Model1_gfp_zonelist(Model1_gfp_t Model1_flags)
{

 if (__builtin_expect(!!(Model1_flags & (( Model1_gfp_t)0x40000u)), 0))
  return Model1_ZONELIST_NOFALLBACK;

 return Model1_ZONELIST_FALLBACK;
}

/*
 * We get the zone list from the current node and the gfp_mask.
 * This zone list contains a maximum of MAXNODES*MAX_NR_ZONES zones.
 * There are two zonelists per node, one for all zones with memory and
 * one containing just zones from the node the zonelist belongs to.
 *
 * For the normal case of non-DISCONTIGMEM systems the NODE_DATA() gets
 * optimized to &contig_page_data at compile-time.
 */
static inline __attribute__((no_instrument_function)) struct Model1_zonelist *Model1_node_zonelist(int Model1_nid, Model1_gfp_t Model1_flags)
{
 return (Model1_node_data[Model1_nid])->Model1_node_zonelists + Model1_gfp_zonelist(Model1_flags);
}


static inline __attribute__((no_instrument_function)) void Model1_arch_free_page(struct Model1_page *Model1_page, int Model1_order) { }


static inline __attribute__((no_instrument_function)) void Model1_arch_alloc_page(struct Model1_page *Model1_page, int Model1_order) { }


struct Model1_page *
Model1___alloc_pages_nodemask(Model1_gfp_t Model1_gfp_mask, unsigned int Model1_order,
         struct Model1_zonelist *Model1_zonelist, Model1_nodemask_t *Model1_nodemask);

static inline __attribute__((no_instrument_function)) struct Model1_page *
Model1___alloc_pages(Model1_gfp_t Model1_gfp_mask, unsigned int Model1_order,
  struct Model1_zonelist *Model1_zonelist)
{
 return Model1___alloc_pages_nodemask(Model1_gfp_mask, Model1_order, Model1_zonelist, ((void *)0));
}

/*
 * Allocate pages, preferring the node given as nid. The node must be valid and
 * online. For more general interface, see alloc_pages_node().
 */
static inline __attribute__((no_instrument_function)) struct Model1_page *
Model1___alloc_pages_node(int Model1_nid, Model1_gfp_t Model1_gfp_mask, unsigned int Model1_order)
{
 ((void)(sizeof(( long)(Model1_nid < 0 || Model1_nid >= (1 << 6)))));
 ((void)(sizeof(( long)(!Model1_node_state((Model1_nid), Model1_N_ONLINE)))));

 return Model1___alloc_pages(Model1_gfp_mask, Model1_order, Model1_node_zonelist(Model1_nid, Model1_gfp_mask));
}

/*
 * Allocate pages, preferring the node given as nid. When nid == NUMA_NO_NODE,
 * prefer the current CPU's closest node. Otherwise node must be valid and
 * online.
 */
static inline __attribute__((no_instrument_function)) struct Model1_page *Model1_alloc_pages_node(int Model1_nid, Model1_gfp_t Model1_gfp_mask,
      unsigned int Model1_order)
{
 if (Model1_nid == (-1))
  Model1_nid = Model1_numa_mem_id();

 return Model1___alloc_pages_node(Model1_nid, Model1_gfp_mask, Model1_order);
}


extern struct Model1_page *Model1_alloc_pages_current(Model1_gfp_t Model1_gfp_mask, unsigned Model1_order);

static inline __attribute__((no_instrument_function)) struct Model1_page *
Model1_alloc_pages(Model1_gfp_t Model1_gfp_mask, unsigned int Model1_order)
{
 return Model1_alloc_pages_current(Model1_gfp_mask, Model1_order);
}
extern struct Model1_page *Model1_alloc_pages_vma(Model1_gfp_t Model1_gfp_mask, int Model1_order,
   struct Model1_vm_area_struct *Model1_vma, unsigned long Model1_addr,
   int Model1_node, bool Model1_hugepage);
extern unsigned long Model1___get_free_pages(Model1_gfp_t Model1_gfp_mask, unsigned int Model1_order);
extern unsigned long Model1_get_zeroed_page(Model1_gfp_t Model1_gfp_mask);

void *Model1_alloc_pages_exact(Model1_size_t Model1_size, Model1_gfp_t Model1_gfp_mask);
void Model1_free_pages_exact(void *Model1_virt, Model1_size_t Model1_size);
void * __attribute__ ((__section__(".meminit.text"))) __attribute__((no_instrument_function)) Model1_alloc_pages_exact_nid(int Model1_nid, Model1_size_t Model1_size, Model1_gfp_t Model1_gfp_mask);







extern void Model1___free_pages(struct Model1_page *Model1_page, unsigned int Model1_order);
extern void Model1_free_pages(unsigned long Model1_addr, unsigned int Model1_order);
extern void Model1_free_hot_cold_page(struct Model1_page *Model1_page, bool Model1_cold);
extern void Model1_free_hot_cold_page_list(struct Model1_list_head *Model1_list, bool Model1_cold);

struct Model1_page_frag_cache;
extern void *Model1___alloc_page_frag(struct Model1_page_frag_cache *Model1_nc,
          unsigned int Model1_fragsz, Model1_gfp_t Model1_gfp_mask);
extern void Model1___free_page_frag(void *Model1_addr);




void Model1_page_alloc_init(void);
void Model1_drain_zone_pages(struct Model1_zone *Model1_zone, struct Model1_per_cpu_pages *Model1_pcp);
void Model1_drain_all_pages(struct Model1_zone *Model1_zone);
void Model1_drain_local_pages(struct Model1_zone *Model1_zone);

void Model1_page_alloc_init_late(void);

/*
 * gfp_allowed_mask is set to GFP_BOOT_MASK during early boot to restrict what
 * GFP flags are used before interrupts are enabled. Once interrupts are
 * enabled, it is set to __GFP_BITS_MASK while the system is running. During
 * hibernation, it is used by PM to avoid I/O during memory allocation while
 * devices are suspended.
 */
extern Model1_gfp_t Model1_gfp_allowed_mask;

/* Returns true if the gfp_mask allows use of ALLOC_NO_WATERMARK */
bool Model1_gfp_pfmemalloc_allowed(Model1_gfp_t Model1_gfp_mask);

extern void Model1_pm_restrict_gfp_mask(void);
extern void Model1_pm_restore_gfp_mask(void);


extern bool Model1_pm_suspended_storage(void);












struct Model1_task_struct;

extern int Model1_debug_locks;
extern int Model1_debug_locks_silent;


static inline __attribute__((no_instrument_function)) int Model1___debug_locks_off(void)
{
 return ({ __typeof__ (*((&Model1_debug_locks))) Model1___ret = ((0)); switch (sizeof(*((&Model1_debug_locks)))) { case 1: asm volatile ("" "xchg" "b %b0, %1\n" : "+q" (Model1___ret), "+m" (*((&Model1_debug_locks))) : : "memory", "cc"); break; case 2: asm volatile ("" "xchg" "w %w0, %1\n" : "+r" (Model1___ret), "+m" (*((&Model1_debug_locks))) : : "memory", "cc"); break; case 4: asm volatile ("" "xchg" "l %0, %1\n" : "+r" (Model1___ret), "+m" (*((&Model1_debug_locks))) : : "memory", "cc"); break; case 8: asm volatile ("" "xchg" "q %q0, %1\n" : "+r" (Model1___ret), "+m" (*((&Model1_debug_locks))) : : "memory", "cc"); break; default: Model1___xchg_wrong_size(); } Model1___ret; });
}

/*
 * Generic 'turn off all lock debugging' function:
 */
extern int Model1_debug_locks_off(void);
struct Model1_task_struct;







static inline __attribute__((no_instrument_function)) void Model1_debug_show_all_locks(void)
{
}

static inline __attribute__((no_instrument_function)) void Model1_debug_show_held_locks(struct Model1_task_struct *Model1_task)
{
}

static inline __attribute__((no_instrument_function)) void
Model1_debug_check_no_locks_freed(const void *Model1_from, unsigned long Model1_len)
{
}

static inline __attribute__((no_instrument_function)) void
Model1_debug_check_no_locks_held(void)
{
}











/*
 * Architecture-neutral AT_ values in 0-17, leave some room
 * for more of them, start the x86-specific ones at 32.
 */





/* entries in ARCH_DLINFO: */

/* Symbolic values for the entries in the auxiliary table
   put on the initial stack */
/* AT_* values 18 through 22 are reserved */


  /* number of "#define AT_.*" above, minus {AT_NULL, AT_IGNORE, AT_NOTELF} */










/*
 * User-space Probes (UProbes)
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
 *
 * Copyright (C) IBM Corporation, 2008-2012
 * Authors:
 *	Srikar Dronamraju
 *	Jim Keniston
 * Copyright (C) 2011-2012 Red Hat, Inc., Peter Zijlstra
 */





struct Model1_vm_area_struct;
struct Model1_mm_struct;
struct Model1_inode;
struct Model1_notifier_block;
struct Model1_page;






enum Model1_uprobe_filter_ctx {
 Model1_UPROBE_FILTER_REGISTER,
 Model1_UPROBE_FILTER_UNREGISTER,
 Model1_UPROBE_FILTER_MMAP,
};

struct Model1_uprobe_consumer {
 int (*Model1_handler)(struct Model1_uprobe_consumer *Model1_self, struct Model1_pt_regs *Model1_regs);
 int (*Model1_ret_handler)(struct Model1_uprobe_consumer *Model1_self,
    unsigned long func,
    struct Model1_pt_regs *Model1_regs);
 bool (*Model1_filter)(struct Model1_uprobe_consumer *Model1_self,
    enum Model1_uprobe_filter_ctx Model1_ctx,
    struct Model1_mm_struct *Model1_mm);

 struct Model1_uprobe_consumer *Model1_next;
};
struct Model1_uprobes_state {
};



static inline __attribute__((no_instrument_function)) int
Model1_uprobe_register(struct Model1_inode *Model1_inode, Model1_loff_t Model1_offset, struct Model1_uprobe_consumer *Model1_uc)
{
 return -38;
}
static inline __attribute__((no_instrument_function)) int
Model1_uprobe_apply(struct Model1_inode *Model1_inode, Model1_loff_t Model1_offset, struct Model1_uprobe_consumer *Model1_uc, bool Model1_add)
{
 return -38;
}
static inline __attribute__((no_instrument_function)) void
Model1_uprobe_unregister(struct Model1_inode *Model1_inode, Model1_loff_t Model1_offset, struct Model1_uprobe_consumer *Model1_uc)
{
}
static inline __attribute__((no_instrument_function)) int Model1_uprobe_mmap(struct Model1_vm_area_struct *Model1_vma)
{
 return 0;
}
static inline __attribute__((no_instrument_function)) void
Model1_uprobe_munmap(struct Model1_vm_area_struct *Model1_vma, unsigned long Model1_start, unsigned long Model1_end)
{
}
static inline __attribute__((no_instrument_function)) void Model1_uprobe_start_dup_mmap(void)
{
}
static inline __attribute__((no_instrument_function)) void Model1_uprobe_end_dup_mmap(void)
{
}
static inline __attribute__((no_instrument_function)) void
Model1_uprobe_dup_mmap(struct Model1_mm_struct *Model1_oldmm, struct Model1_mm_struct *Model1_newmm)
{
}
static inline __attribute__((no_instrument_function)) void Model1_uprobe_notify_resume(struct Model1_pt_regs *Model1_regs)
{
}
static inline __attribute__((no_instrument_function)) bool Model1_uprobe_deny_signal(void)
{
 return false;
}
static inline __attribute__((no_instrument_function)) void Model1_uprobe_free_utask(struct Model1_task_struct *Model1_t)
{
}
static inline __attribute__((no_instrument_function)) void Model1_uprobe_copy_process(struct Model1_task_struct *Model1_t, unsigned long Model1_flags)
{
}
static inline __attribute__((no_instrument_function)) void Model1_uprobe_clear_state(struct Model1_mm_struct *Model1_mm)
{
}
struct Model1_address_space;
struct Model1_mem_cgroup;






/*
 * Each physical page in the system has a struct page associated with
 * it to keep track of whatever it is we are using the page for at the
 * moment. Note that we have no way to track which tasks are using
 * a page, though if it is a pagecache page, rmap structures can tell us
 * who is mapping it.
 *
 * The objects in struct page are organized in double word blocks in
 * order to allows us to use atomic double word operations on portions
 * of struct page. That is currently only used by slub but the arrangement
 * allows the use of atomic double word operations on the flags/mapping
 * and lru list pointers also.
 */
struct Model1_page {
 /* First double word block */
 unsigned long Model1_flags; /* Atomic flags, some possibly
					 * updated asynchronously */
 union {
  struct Model1_address_space *Model1_mapping; /* If low bit clear, points to
						 * inode address_space, or NULL.
						 * If page mapped as anonymous
						 * memory, low bit is set, and
						 * it points to anon_vma object:
						 * see PAGE_MAPPING_ANON below.
						 */
  void *Model1_s_mem; /* slab first object */
  Model1_atomic_t Model1_compound_mapcount; /* first tail page */
  /* page_deferred_list().next	 -- second tail page */
 };

 /* Second double word */
 union {
  unsigned long Model1_index; /* Our offset within mapping. */
  void *Model1_freelist; /* sl[aou]b first free object */
  /* page_deferred_list().prev	-- second tail page */
 };

 union {


  /* Used for cmpxchg_double in slub */
  unsigned long Model1_counters;
  struct {

   union {
    /*
				 * Count of ptes mapped in mms, to show when
				 * page is mapped & limit reverse map searches.
				 *
				 * Extra information about page type may be
				 * stored here for pages that are never mapped,
				 * in which case the value MUST BE <= -2.
				 * See page-flags.h for more details.
				 */
    Model1_atomic_t Model1__mapcount;

    unsigned int Model1_active; /* SLAB */
    struct { /* SLUB */
     unsigned Model1_inuse:16;
     unsigned Model1_objects:15;
     unsigned Model1_frozen:1;
    };
    int Model1_units; /* SLOB */
   };
   /*
			 * Usage count, *USE WRAPPER FUNCTION* when manual
			 * accounting. See page_ref.h
			 */
   Model1_atomic_t Model1__refcount;
  };
 };

 /*
	 * Third double word block
	 *
	 * WARNING: bit 0 of the first word encode PageTail(). That means
	 * the rest users of the storage space MUST NOT use the bit to
	 * avoid collision and false-positive PageTail().
	 */
 union {
  struct Model1_list_head Model1_lru; /* Pageout list, eg. active_list
					 * protected by zone_lru_lock !
					 * Can be used as a generic list
					 * by the page owner.
					 */
  struct Model1_dev_pagemap *Model1_pgmap; /* ZONE_DEVICE pages are never on an
					    * lru or handled by a slab
					    * allocator, this points to the
					    * hosting device page map.
					    */
  struct { /* slub per cpu partial pages */
   struct Model1_page *Model1_next; /* Next partial slab */

   int Model1_pages; /* Nr of partial slabs left */
   int Model1_pobjects; /* Approximate # of objects */




  };

  struct Model1_callback_head Model1_callback_head; /* Used by SLAB
						 * when destroying via RCU
						 */
  /* Tail pages of compound page */
  struct {
   unsigned long Model1_compound_head; /* If bit zero is set */

   /* First tail page only */

   /*
			 * On 64 bit system we have enough space in struct page
			 * to encode compound_dtor and compound_order with
			 * unsigned int. It can help compiler generate better or
			 * smaller code on some archtectures.
			 */
   unsigned int Model1_compound_dtor;
   unsigned int Model1_compound_order;




  };
 };

 /* Remainder is not double word aligned */
 union {
  unsigned long Model1_private; /* Mapping-private opaque data:
					 	 * usually used for buffer_heads
						 * if PagePrivate set; used for
						 * swp_entry_t if PageSwapCache;
						 * indicates order in the buddy
						 * system if PG_buddy is set.
						 */




  Model1_spinlock_t Model1_ptl;


  struct Model1_kmem_cache *Model1_slab_cache; /* SL[AU]B: Pointer to slab */
 };





 /*
	 * On machines where all RAM is mapped into kernel address space,
	 * we can simply calculate the virtual address. On machines with
	 * highmem some memory is mapped into kernel virtual memory
	 * dynamically, so we need a place to store that address.
	 * Note that this field could be 16 bits on x86 ... ;)
	 *
	 * Architectures with slow multiplication can define
	 * WANT_PAGE_VIRTUAL in asm/page.h
	 */
}
/*
 * The struct page can be forced to be double word aligned so that atomic ops
 * on double words work. The SLUB allocator can make use of such a feature.
 */

 __attribute__((aligned(2 * sizeof(unsigned long))))

;

struct Model1_page_frag {
 struct Model1_page *Model1_page;

 __u32 Model1_offset;
 __u32 Model1_size;




};




struct Model1_page_frag_cache {
 void * Model1_va;

 Model1___u16 Model1_offset;
 Model1___u16 Model1_size;



 /* we maintain a pagecount bias, so that we dont dirty cache line
	 * containing page->_refcount every time we allocate a fragment.
	 */
 unsigned int Model1_pagecnt_bias;
 bool Model1_pfmemalloc;
};

typedef unsigned long Model1_vm_flags_t;

/*
 * A region containing a mapping of a non-memory backed file under NOMMU
 * conditions.  These are held in a global tree and are pinned by the VMAs that
 * map parts of them.
 */
struct Model1_vm_region {
 struct Model1_rb_node Model1_vm_rb; /* link in global region tree */
 Model1_vm_flags_t Model1_vm_flags; /* VMA vm_flags */
 unsigned long Model1_vm_start; /* start address of region */
 unsigned long Model1_vm_end; /* region initialised to here */
 unsigned long Model1_vm_top; /* region allocated to here */
 unsigned long Model1_vm_pgoff; /* the offset in vm_file corresponding to vm_start */
 struct Model1_file *Model1_vm_file; /* the backing file or NULL */

 int Model1_vm_usage; /* region usage count (access under nommu_region_sem) */
 bool Model1_vm_icache_flushed : 1; /* true if the icache has been flushed for
						* this region */
};
struct Model1_vm_userfaultfd_ctx {};


/*
 * This struct defines a memory VMM memory area. There is one of these
 * per VM-area/task.  A VM area is any part of the process virtual memory
 * space that has a special rule for the page-fault handlers (ie a shared
 * library, the executable area etc).
 */
struct Model1_vm_area_struct {
 /* The first cache line has the info for VMA tree walking. */

 unsigned long Model1_vm_start; /* Our start address within vm_mm. */
 unsigned long Model1_vm_end; /* The first byte after our end address
					   within vm_mm. */

 /* linked list of VM areas per task, sorted by address */
 struct Model1_vm_area_struct *Model1_vm_next, *Model1_vm_prev;

 struct Model1_rb_node Model1_vm_rb;

 /*
	 * Largest free memory gap in bytes to the left of this VMA.
	 * Either between this VMA and vma->vm_prev, or between one of the
	 * VMAs below us in the VMA rbtree and its ->vm_prev. This helps
	 * get_unmapped_area find a free area of the right size.
	 */
 unsigned long Model1_rb_subtree_gap;

 /* Second cache line starts here. */

 struct Model1_mm_struct *Model1_vm_mm; /* The address space we belong to. */
 Model1_pgprot_t Model1_vm_page_prot; /* Access permissions of this VMA. */
 unsigned long Model1_vm_flags; /* Flags, see mm.h. */

 /*
	 * For areas with an address space and backing store,
	 * linkage into the address_space->i_mmap interval tree.
	 */
 struct {
  struct Model1_rb_node Model1_rb;
  unsigned long Model1_rb_subtree_last;
 } Model1_shared;

 /*
	 * A file's MAP_PRIVATE vma can be in both i_mmap tree and anon_vma
	 * list, after a COW of one of the file pages.	A MAP_SHARED vma
	 * can only be in the i_mmap tree.  An anonymous MAP_PRIVATE, stack
	 * or brk vma (with NULL file) can only be in an anon_vma list.
	 */
 struct Model1_list_head Model1_anon_vma_chain; /* Serialized by mmap_sem &
					  * page_table_lock */
 struct Model1_anon_vma *Model1_anon_vma; /* Serialized by page_table_lock */

 /* Function pointers to deal with this struct. */
 const struct Model1_vm_operations_struct *Model1_vm_ops;

 /* Information about our backing store: */
 unsigned long Model1_vm_pgoff; /* Offset (within vm_file) in PAGE_SIZE
					   units */
 struct Model1_file * Model1_vm_file; /* File we map to (can be NULL). */
 void * Model1_vm_private_data; /* was vm_pte (shared mem) */





 struct Model1_mempolicy *Model1_vm_policy; /* NUMA policy for the VMA */

 struct Model1_vm_userfaultfd_ctx Model1_vm_userfaultfd_ctx;
};

struct Model1_core_thread {
 struct Model1_task_struct *Model1_task;
 struct Model1_core_thread *Model1_next;
};

struct Model1_core_state {
 Model1_atomic_t Model1_nr_threads;
 struct Model1_core_thread Model1_dumper;
 struct Model1_completion Model1_startup;
};

enum {
 Model1_MM_FILEPAGES, /* Resident file mapping pages */
 Model1_MM_ANONPAGES, /* Resident anonymous pages */
 Model1_MM_SWAPENTS, /* Anonymous swap entries */
 Model1_MM_SHMEMPAGES, /* Resident shared memory pages */
 Model1_NR_MM_COUNTERS
};



/* per-thread cached information, */
struct Model1_task_rss_stat {
 int Model1_events; /* for synchronization threshold */
 int Model1_count[Model1_NR_MM_COUNTERS];
};


struct Model1_mm_rss_stat {
 Model1_atomic_long_t Model1_count[Model1_NR_MM_COUNTERS];
};

struct Model1_kioctx_table;
struct Model1_mm_struct {
 struct Model1_vm_area_struct *Model1_mmap; /* list of VMAs */
 struct Model1_rb_root Model1_mm_rb;
 Model1_u32 Model1_vmacache_seqnum; /* per-thread vmacache */

 unsigned long (*Model1_get_unmapped_area) (struct Model1_file *Model1_filp,
    unsigned long Model1_addr, unsigned long Model1_len,
    unsigned long Model1_pgoff, unsigned long Model1_flags);

 unsigned long Model1_mmap_base; /* base of mmap area */
 unsigned long Model1_mmap_legacy_base; /* base of mmap area in bottom-up allocations */
 unsigned long Model1_task_size; /* size of task vm space */
 unsigned long Model1_highest_vm_end; /* highest vma end address */
 Model1_pgd_t * Model1_pgd;
 Model1_atomic_t Model1_mm_users; /* How many users with user space? */
 Model1_atomic_t Model1_mm_count; /* How many references to "struct mm_struct" (users count as 1) */
 Model1_atomic_long_t Model1_nr_ptes; /* PTE page table pages */

 Model1_atomic_long_t Model1_nr_pmds; /* PMD page table pages */

 int Model1_map_count; /* number of VMAs */

 Model1_spinlock_t Model1_page_table_lock; /* Protects page tables and some counters */
 struct Model1_rw_semaphore Model1_mmap_sem;

 struct Model1_list_head Model1_mmlist; /* List of maybe swapped mm's.	These are globally strung
						 * together off init_mm.mmlist, and are protected
						 * by mmlist_lock
						 */


 unsigned long Model1_hiwater_rss; /* High-watermark of RSS usage */
 unsigned long Model1_hiwater_vm; /* High-water virtual memory usage */

 unsigned long Model1_total_vm; /* Total pages mapped */
 unsigned long Model1_locked_vm; /* Pages that have PG_mlocked set */
 unsigned long Model1_pinned_vm; /* Refcount permanently increased */
 unsigned long Model1_data_vm; /* VM_WRITE & ~VM_SHARED & ~VM_STACK */
 unsigned long Model1_exec_vm; /* VM_EXEC & ~VM_WRITE & ~VM_STACK */
 unsigned long Model1_stack_vm; /* VM_STACK */
 unsigned long Model1_def_flags;
 unsigned long Model1_start_code, Model1_end_code, Model1_start_data, Model1_end_data;
 unsigned long Model1_start_brk, Model1_brk, Model1_start_stack;
 unsigned long Model1_arg_start, Model1_arg_end, Model1_env_start, Model1_env_end;

 unsigned long Model1_saved_auxv[(2*(2 + 20 + 1))]; /* for /proc/PID/auxv */

 /*
	 * Special counters, in some configurations protected by the
	 * page_table_lock, in other configurations by being atomic.
	 */
 struct Model1_mm_rss_stat Model1_rss_stat;

 struct Model1_linux_binfmt *Model1_binfmt;

 Model1_cpumask_var_t Model1_cpu_vm_mask_var;

 /* Architecture-specific MM context */
 Model1_mm_context_t Model1_context;

 unsigned long Model1_flags; /* Must use atomic bitops to access the bits */

 struct Model1_core_state *Model1_core_state; /* coredumping support */

 Model1_spinlock_t Model1_ioctx_lock;
 struct Model1_kioctx_table *Model1_ioctx_table;
 /* store ref to file /proc/<pid>/exe symlink points to */
 struct Model1_file *Model1_exe_file;

 struct Model1_mmu_notifier_mm *Model1_mmu_notifier_mm;
 /*
	 * An operation with batched TLB flushing is going on. Anything that
	 * can move process memory needs to flush the TLB when moving a
	 * PROT_NONE or PROT_NUMA mapped page.
	 */
 bool Model1_tlb_flush_pending;

 struct Model1_uprobes_state Model1_uprobes_state;





 Model1_atomic_long_t Model1_hugetlb_usage;


 struct Model1_work_struct Model1_async_put_work;

};

static inline __attribute__((no_instrument_function)) void Model1_mm_init_cpumask(struct Model1_mm_struct *Model1_mm)
{



 Model1_cpumask_clear(Model1_mm->Model1_cpu_vm_mask_var);
}

/* Future-safe accessor for struct mm_struct's cpu_vm_mask. */
static inline __attribute__((no_instrument_function)) Model1_cpumask_t *Model1_mm_cpumask(struct Model1_mm_struct *Model1_mm)
{
 return Model1_mm->Model1_cpu_vm_mask_var;
}


/*
 * Memory barriers to keep this state in sync are graciously provided by
 * the page table locks, outside of which no page table modifications happen.
 * The barriers below prevent the compiler from re-ordering the instructions
 * around the memory barriers that are already present in the code.
 */
static inline __attribute__((no_instrument_function)) bool Model1_mm_tlb_flush_pending(struct Model1_mm_struct *Model1_mm)
{
 __asm__ __volatile__("": : :"memory");
 return Model1_mm->Model1_tlb_flush_pending;
}
static inline __attribute__((no_instrument_function)) void Model1_set_tlb_flush_pending(struct Model1_mm_struct *Model1_mm)
{
 Model1_mm->Model1_tlb_flush_pending = true;

 /*
	 * Guarantee that the tlb_flush_pending store does not leak into the
	 * critical section updating the page tables
	 */
 __asm__ __volatile__("": : :"memory");
}
/* Clearing is done after a TLB flush, which also provides a barrier. */
static inline __attribute__((no_instrument_function)) void Model1_clear_tlb_flush_pending(struct Model1_mm_struct *Model1_mm)
{
 __asm__ __volatile__("": : :"memory");
 Model1_mm->Model1_tlb_flush_pending = false;
}
struct Model1_vm_fault;

struct Model1_vm_special_mapping {
 const char *Model1_name; /* The name, e.g. "[vdso]". */

 /*
	 * If .fault is not provided, this points to a
	 * NULL-terminated array of pages that back the special mapping.
	 *
	 * This must not be NULL unless .fault is provided.
	 */
 struct Model1_page **Model1_pages;

 /*
	 * If non-NULL, then this is called to resolve page faults
	 * on the special mapping.  If used, .pages is not checked.
	 */
 int (*fault)(const struct Model1_vm_special_mapping *Model1_sm,
       struct Model1_vm_area_struct *Model1_vma,
       struct Model1_vm_fault *Model1_vmf);

 int (*Model1_mremap)(const struct Model1_vm_special_mapping *Model1_sm,
       struct Model1_vm_area_struct *Model1_new_vma);
};

enum Model1_tlb_flush_reason {
 Model1_TLB_FLUSH_ON_TASK_SWITCH,
 Model1_TLB_REMOTE_SHOOTDOWN,
 Model1_TLB_LOCAL_SHOOTDOWN,
 Model1_TLB_LOCAL_MM_SHOOTDOWN,
 Model1_TLB_REMOTE_SEND_IPI,
 Model1_NR_TLB_FLUSH_REASONS,
};

 /*
  * A swap entry has to fit into a "unsigned long", as the entry is hidden
  * in the "index" field of the swapper address space.
  */
typedef struct {
 unsigned long Model1_val;
} Model1_swp_entry_t;


/*
 * Percpu refcounts:
 * (C) 2012 Google, Inc.
 * Author: Kent Overstreet <koverstreet@google.com>
 *
 * This implements a refcount with similar semantics to atomic_t - atomic_inc(),
 * atomic_dec_and_test() - but percpu.
 *
 * There's one important difference between percpu refs and normal atomic_t
 * refcounts; you have to keep track of your initial refcount, and then when you
 * start shutting down you call percpu_ref_kill() _before_ dropping the initial
 * refcount.
 *
 * The refcount will have a range of 0 to ((1U << 31) - 1), i.e. one bit less
 * than an atomic_t - this is because of the way shutdown works, see
 * percpu_ref_kill()/PERCPU_COUNT_BIAS.
 *
 * Before you call percpu_ref_kill(), percpu_ref_put() does not check for the
 * refcount hitting 0 - it can't, if it was in percpu mode. percpu_ref_kill()
 * puts the ref back in single atomic_t mode, collecting the per cpu refs and
 * issuing the appropriate barriers, and then marks the ref as shutting down so
 * that percpu_ref_put() will check for the ref hitting 0.  After it returns,
 * it's safe to drop the initial ref.
 *
 * USAGE:
 *
 * See fs/aio.c for some example usage; it's used there for struct kioctx, which
 * is created when userspaces calls io_setup(), and destroyed when userspace
 * calls io_destroy() or the process exits.
 *
 * In the aio code, kill_ioctx() is called when we wish to destroy a kioctx; it
 * calls percpu_ref_kill(), then hlist_del_rcu() and synchronize_rcu() to remove
 * the kioctx from the proccess's list of kioctxs - after that, there can't be
 * any new users of the kioctx (from lookup_ioctx()) and it's then safe to drop
 * the initial ref with percpu_ref_put().
 *
 * Code that does a two stage shutdown like this often needs some kind of
 * explicit synchronization to ensure the initial refcount can only be dropped
 * once - percpu_ref_kill() does this for you, it returns true once and false if
 * someone else already called it. The aio code uses it this way, but it's not
 * necessary if the code has some other mechanism to synchronize teardown.
 * around.
 */
struct Model1_percpu_ref;
typedef void (Model1_percpu_ref_func_t)(struct Model1_percpu_ref *);

/* flags set in the lower bits of percpu_ref->percpu_count_ptr */
enum {
 Model1___PERCPU_REF_ATOMIC = 1LU << 0, /* operating in atomic mode */
 Model1___PERCPU_REF_DEAD = 1LU << 1, /* (being) killed */
 Model1___PERCPU_REF_ATOMIC_DEAD = Model1___PERCPU_REF_ATOMIC | Model1___PERCPU_REF_DEAD,

 Model1___PERCPU_REF_FLAG_BITS = 2,
};

/* @flags for percpu_ref_init() */
enum {
 /*
	 * Start w/ ref == 1 in atomic mode.  Can be switched to percpu
	 * operation using percpu_ref_switch_to_percpu().  If initialized
	 * with this flag, the ref will stay in atomic mode until
	 * percpu_ref_switch_to_percpu() is invoked on it.
	 */
 Model1_PERCPU_REF_INIT_ATOMIC = 1 << 0,

 /*
	 * Start dead w/ ref == 0 in atomic mode.  Must be revived with
	 * percpu_ref_reinit() before used.  Implies INIT_ATOMIC.
	 */
 Model1_PERCPU_REF_INIT_DEAD = 1 << 1,
};

struct Model1_percpu_ref {
 Model1_atomic_long_t Model1_count;
 /*
	 * The low bit of the pointer indicates whether the ref is in percpu
	 * mode; if set, then get/put will manipulate the atomic_t.
	 */
 unsigned long Model1_percpu_count_ptr;
 Model1_percpu_ref_func_t *Model1_release;
 Model1_percpu_ref_func_t *Model1_confirm_switch;
 bool Model1_force_atomic:1;
 struct Model1_callback_head Model1_rcu;
};

int __attribute__((warn_unused_result)) Model1_percpu_ref_init(struct Model1_percpu_ref *Model1_ref,
     Model1_percpu_ref_func_t *Model1_release, unsigned int Model1_flags,
     Model1_gfp_t Model1_gfp);
void Model1_percpu_ref_exit(struct Model1_percpu_ref *Model1_ref);
void Model1_percpu_ref_switch_to_atomic(struct Model1_percpu_ref *Model1_ref,
     Model1_percpu_ref_func_t *Model1_confirm_switch);
void Model1_percpu_ref_switch_to_percpu(struct Model1_percpu_ref *Model1_ref);
void Model1_percpu_ref_kill_and_confirm(struct Model1_percpu_ref *Model1_ref,
     Model1_percpu_ref_func_t *Model1_confirm_kill);
void Model1_percpu_ref_reinit(struct Model1_percpu_ref *Model1_ref);

/**
 * percpu_ref_kill - drop the initial ref
 * @ref: percpu_ref to kill
 *
 * Must be used to drop the initial ref on a percpu refcount; must be called
 * precisely once before shutdown.
 *
 * Puts @ref in non percpu mode, then does a call_rcu() before gathering up the
 * percpu counters and dropping the initial ref.
 */
static inline __attribute__((no_instrument_function)) void Model1_percpu_ref_kill(struct Model1_percpu_ref *Model1_ref)
{
 Model1_percpu_ref_kill_and_confirm(Model1_ref, ((void *)0));
}

/*
 * Internal helper.  Don't use outside percpu-refcount proper.  The
 * function doesn't return the pointer and let the caller test it for NULL
 * because doing so forces the compiler to generate two conditional
 * branches as it can't assume that @ref->percpu_count is not NULL.
 */
static inline __attribute__((no_instrument_function)) bool Model1___ref_is_percpu(struct Model1_percpu_ref *Model1_ref,
       unsigned long **Model1_percpu_countp)
{
 unsigned long Model1_percpu_ptr;

 /*
	 * The value of @ref->percpu_count_ptr is tested for
	 * !__PERCPU_REF_ATOMIC, which may be set asynchronously, and then
	 * used as a pointer.  If the compiler generates a separate fetch
	 * when using it as a pointer, __PERCPU_REF_ATOMIC may be set in
	 * between contaminating the pointer value, meaning that
	 * READ_ONCE() is required when fetching it.
	 */
 Model1_percpu_ptr = ({ union { typeof(Model1_ref->Model1_percpu_count_ptr) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&(Model1_ref->Model1_percpu_count_ptr), Model1___u.Model1___c, sizeof(Model1_ref->Model1_percpu_count_ptr)); else Model1___read_once_size_nocheck(&(Model1_ref->Model1_percpu_count_ptr), Model1___u.Model1___c, sizeof(Model1_ref->Model1_percpu_count_ptr)); Model1___u.Model1___val; });

 /* paired with smp_store_release() in __percpu_ref_switch_to_percpu() */
 do { } while (0);

 /*
	 * Theoretically, the following could test just ATOMIC; however,
	 * then we'd have to mask off DEAD separately as DEAD may be
	 * visible without ATOMIC if we race with percpu_ref_kill().  DEAD
	 * implies ATOMIC anyway.  Test them together.
	 */
 if (__builtin_expect(!!(Model1_percpu_ptr & Model1___PERCPU_REF_ATOMIC_DEAD), 0))
  return false;

 *Model1_percpu_countp = (unsigned long *)Model1_percpu_ptr;
 return true;
}

/**
 * percpu_ref_get_many - increment a percpu refcount
 * @ref: percpu_ref to get
 * @nr: number of references to get
 *
 * Analogous to atomic_long_add().
 *
 * This function is safe to call as long as @ref is between init and exit.
 */
static inline __attribute__((no_instrument_function)) void Model1_percpu_ref_get_many(struct Model1_percpu_ref *Model1_ref, unsigned long Model1_nr)
{
 unsigned long *Model1_percpu_count;

 Model1_rcu_read_lock_sched();

 if (Model1___ref_is_percpu(Model1_ref, &Model1_percpu_count))
  do { do { const void *Model1___vpp_verify = (typeof((&(*Model1_percpu_count)) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); switch(sizeof(*Model1_percpu_count)) { case 1: do { typedef typeof((*Model1_percpu_count)) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(Model1_nr) && ((Model1_nr) == 1 || (Model1_nr) == -1)) ? (int)(Model1_nr) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (Model1_nr); (void)Model1_pao_tmp__; } switch (sizeof((*Model1_percpu_count))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "qi" ((Model1_pao_T__)(Model1_nr))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(Model1_nr))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(Model1_nr))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "re" ((Model1_pao_T__)(Model1_nr))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 2: do { typedef typeof((*Model1_percpu_count)) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(Model1_nr) && ((Model1_nr) == 1 || (Model1_nr) == -1)) ? (int)(Model1_nr) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (Model1_nr); (void)Model1_pao_tmp__; } switch (sizeof((*Model1_percpu_count))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "qi" ((Model1_pao_T__)(Model1_nr))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(Model1_nr))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(Model1_nr))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "re" ((Model1_pao_T__)(Model1_nr))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 4: do { typedef typeof((*Model1_percpu_count)) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(Model1_nr) && ((Model1_nr) == 1 || (Model1_nr) == -1)) ? (int)(Model1_nr) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (Model1_nr); (void)Model1_pao_tmp__; } switch (sizeof((*Model1_percpu_count))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "qi" ((Model1_pao_T__)(Model1_nr))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(Model1_nr))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(Model1_nr))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "re" ((Model1_pao_T__)(Model1_nr))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 8: do { typedef typeof((*Model1_percpu_count)) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(Model1_nr) && ((Model1_nr) == 1 || (Model1_nr) == -1)) ? (int)(Model1_nr) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (Model1_nr); (void)Model1_pao_tmp__; } switch (sizeof((*Model1_percpu_count))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "qi" ((Model1_pao_T__)(Model1_nr))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(Model1_nr))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(Model1_nr))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "re" ((Model1_pao_T__)(Model1_nr))); break; default: Model1___bad_percpu_size(); } } while (0);break; default: Model1___bad_size_call_parameter();break; } } while (0);
 else
  Model1_atomic_long_add(Model1_nr, &Model1_ref->Model1_count);

 Model1_rcu_read_unlock_sched();
}

/**
 * percpu_ref_get - increment a percpu refcount
 * @ref: percpu_ref to get
 *
 * Analagous to atomic_long_inc().
 *
 * This function is safe to call as long as @ref is between init and exit.
 */
static inline __attribute__((no_instrument_function)) void Model1_percpu_ref_get(struct Model1_percpu_ref *Model1_ref)
{
 Model1_percpu_ref_get_many(Model1_ref, 1);
}

/**
 * percpu_ref_tryget - try to increment a percpu refcount
 * @ref: percpu_ref to try-get
 *
 * Increment a percpu refcount unless its count already reached zero.
 * Returns %true on success; %false on failure.
 *
 * This function is safe to call as long as @ref is between init and exit.
 */
static inline __attribute__((no_instrument_function)) bool Model1_percpu_ref_tryget(struct Model1_percpu_ref *Model1_ref)
{
 unsigned long *Model1_percpu_count;
 int Model1_ret;

 Model1_rcu_read_lock_sched();

 if (Model1___ref_is_percpu(Model1_ref, &Model1_percpu_count)) {
  do { do { const void *Model1___vpp_verify = (typeof((&(*Model1_percpu_count)) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); switch(sizeof(*Model1_percpu_count)) { case 1: do { typedef typeof((*Model1_percpu_count)) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (1); (void)Model1_pao_tmp__; } switch (sizeof((*Model1_percpu_count))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "qi" ((Model1_pao_T__)(1))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(1))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(1))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "re" ((Model1_pao_T__)(1))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 2: do { typedef typeof((*Model1_percpu_count)) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (1); (void)Model1_pao_tmp__; } switch (sizeof((*Model1_percpu_count))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "qi" ((Model1_pao_T__)(1))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(1))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(1))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "re" ((Model1_pao_T__)(1))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 4: do { typedef typeof((*Model1_percpu_count)) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (1); (void)Model1_pao_tmp__; } switch (sizeof((*Model1_percpu_count))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "qi" ((Model1_pao_T__)(1))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(1))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(1))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "re" ((Model1_pao_T__)(1))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 8: do { typedef typeof((*Model1_percpu_count)) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (1); (void)Model1_pao_tmp__; } switch (sizeof((*Model1_percpu_count))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "qi" ((Model1_pao_T__)(1))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(1))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(1))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "re" ((Model1_pao_T__)(1))); break; default: Model1___bad_percpu_size(); } } while (0);break; default: Model1___bad_size_call_parameter();break; } } while (0);
  Model1_ret = true;
 } else {
  Model1_ret = Model1_atomic64_add_unless(((Model1_atomic64_t *)(&Model1_ref->Model1_count)), 1, 0);
 }

 Model1_rcu_read_unlock_sched();

 return Model1_ret;
}

/**
 * percpu_ref_tryget_live - try to increment a live percpu refcount
 * @ref: percpu_ref to try-get
 *
 * Increment a percpu refcount unless it has already been killed.  Returns
 * %true on success; %false on failure.
 *
 * Completion of percpu_ref_kill() in itself doesn't guarantee that this
 * function will fail.  For such guarantee, percpu_ref_kill_and_confirm()
 * should be used.  After the confirm_kill callback is invoked, it's
 * guaranteed that no new reference will be given out by
 * percpu_ref_tryget_live().
 *
 * This function is safe to call as long as @ref is between init and exit.
 */
static inline __attribute__((no_instrument_function)) bool Model1_percpu_ref_tryget_live(struct Model1_percpu_ref *Model1_ref)
{
 unsigned long *Model1_percpu_count;
 int Model1_ret = false;

 Model1_rcu_read_lock_sched();

 if (Model1___ref_is_percpu(Model1_ref, &Model1_percpu_count)) {
  do { do { const void *Model1___vpp_verify = (typeof((&(*Model1_percpu_count)) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); switch(sizeof(*Model1_percpu_count)) { case 1: do { typedef typeof((*Model1_percpu_count)) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (1); (void)Model1_pao_tmp__; } switch (sizeof((*Model1_percpu_count))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "qi" ((Model1_pao_T__)(1))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(1))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(1))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "re" ((Model1_pao_T__)(1))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 2: do { typedef typeof((*Model1_percpu_count)) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (1); (void)Model1_pao_tmp__; } switch (sizeof((*Model1_percpu_count))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "qi" ((Model1_pao_T__)(1))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(1))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(1))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "re" ((Model1_pao_T__)(1))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 4: do { typedef typeof((*Model1_percpu_count)) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (1); (void)Model1_pao_tmp__; } switch (sizeof((*Model1_percpu_count))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "qi" ((Model1_pao_T__)(1))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(1))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(1))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "re" ((Model1_pao_T__)(1))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 8: do { typedef typeof((*Model1_percpu_count)) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (1); (void)Model1_pao_tmp__; } switch (sizeof((*Model1_percpu_count))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "qi" ((Model1_pao_T__)(1))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(1))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(1))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "re" ((Model1_pao_T__)(1))); break; default: Model1___bad_percpu_size(); } } while (0);break; default: Model1___bad_size_call_parameter();break; } } while (0);
  Model1_ret = true;
 } else if (!(Model1_ref->Model1_percpu_count_ptr & Model1___PERCPU_REF_DEAD)) {
  Model1_ret = Model1_atomic64_add_unless(((Model1_atomic64_t *)(&Model1_ref->Model1_count)), 1, 0);
 }

 Model1_rcu_read_unlock_sched();

 return Model1_ret;
}

/**
 * percpu_ref_put_many - decrement a percpu refcount
 * @ref: percpu_ref to put
 * @nr: number of references to put
 *
 * Decrement the refcount, and if 0, call the release function (which was passed
 * to percpu_ref_init())
 *
 * This function is safe to call as long as @ref is between init and exit.
 */
static inline __attribute__((no_instrument_function)) void Model1_percpu_ref_put_many(struct Model1_percpu_ref *Model1_ref, unsigned long Model1_nr)
{
 unsigned long *Model1_percpu_count;

 Model1_rcu_read_lock_sched();

 if (Model1___ref_is_percpu(Model1_ref, &Model1_percpu_count))
  do { do { const void *Model1___vpp_verify = (typeof((&(*Model1_percpu_count)) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); switch(sizeof(*Model1_percpu_count)) { case 1: do { typedef typeof((*Model1_percpu_count)) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(-(typeof(*Model1_percpu_count))(Model1_nr)) && ((-(typeof(*Model1_percpu_count))(Model1_nr)) == 1 || (-(typeof(*Model1_percpu_count))(Model1_nr)) == -1)) ? (int)(-(typeof(*Model1_percpu_count))(Model1_nr)) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (-(typeof(*Model1_percpu_count))(Model1_nr)); (void)Model1_pao_tmp__; } switch (sizeof((*Model1_percpu_count))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "qi" ((Model1_pao_T__)(-(typeof(*Model1_percpu_count))(Model1_nr)))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(-(typeof(*Model1_percpu_count))(Model1_nr)))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(-(typeof(*Model1_percpu_count))(Model1_nr)))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "re" ((Model1_pao_T__)(-(typeof(*Model1_percpu_count))(Model1_nr)))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 2: do { typedef typeof((*Model1_percpu_count)) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(-(typeof(*Model1_percpu_count))(Model1_nr)) && ((-(typeof(*Model1_percpu_count))(Model1_nr)) == 1 || (-(typeof(*Model1_percpu_count))(Model1_nr)) == -1)) ? (int)(-(typeof(*Model1_percpu_count))(Model1_nr)) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (-(typeof(*Model1_percpu_count))(Model1_nr)); (void)Model1_pao_tmp__; } switch (sizeof((*Model1_percpu_count))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "qi" ((Model1_pao_T__)(-(typeof(*Model1_percpu_count))(Model1_nr)))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(-(typeof(*Model1_percpu_count))(Model1_nr)))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(-(typeof(*Model1_percpu_count))(Model1_nr)))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "re" ((Model1_pao_T__)(-(typeof(*Model1_percpu_count))(Model1_nr)))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 4: do { typedef typeof((*Model1_percpu_count)) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(-(typeof(*Model1_percpu_count))(Model1_nr)) && ((-(typeof(*Model1_percpu_count))(Model1_nr)) == 1 || (-(typeof(*Model1_percpu_count))(Model1_nr)) == -1)) ? (int)(-(typeof(*Model1_percpu_count))(Model1_nr)) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (-(typeof(*Model1_percpu_count))(Model1_nr)); (void)Model1_pao_tmp__; } switch (sizeof((*Model1_percpu_count))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "qi" ((Model1_pao_T__)(-(typeof(*Model1_percpu_count))(Model1_nr)))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(-(typeof(*Model1_percpu_count))(Model1_nr)))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(-(typeof(*Model1_percpu_count))(Model1_nr)))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "re" ((Model1_pao_T__)(-(typeof(*Model1_percpu_count))(Model1_nr)))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 8: do { typedef typeof((*Model1_percpu_count)) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(-(typeof(*Model1_percpu_count))(Model1_nr)) && ((-(typeof(*Model1_percpu_count))(Model1_nr)) == 1 || (-(typeof(*Model1_percpu_count))(Model1_nr)) == -1)) ? (int)(-(typeof(*Model1_percpu_count))(Model1_nr)) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (-(typeof(*Model1_percpu_count))(Model1_nr)); (void)Model1_pao_tmp__; } switch (sizeof((*Model1_percpu_count))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "qi" ((Model1_pao_T__)(-(typeof(*Model1_percpu_count))(Model1_nr)))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(-(typeof(*Model1_percpu_count))(Model1_nr)))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "ri" ((Model1_pao_T__)(-(typeof(*Model1_percpu_count))(Model1_nr)))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_percpu_count)) : "re" ((Model1_pao_T__)(-(typeof(*Model1_percpu_count))(Model1_nr)))); break; default: Model1___bad_percpu_size(); } } while (0);break; default: Model1___bad_size_call_parameter();break; } } while (0);
 else if (__builtin_expect(!!(Model1_atomic_long_sub_and_test(Model1_nr, &Model1_ref->Model1_count)), 0))
  Model1_ref->Model1_release(Model1_ref);

 Model1_rcu_read_unlock_sched();
}

/**
 * percpu_ref_put - decrement a percpu refcount
 * @ref: percpu_ref to put
 *
 * Decrement the refcount, and if 0, call the release function (which was passed
 * to percpu_ref_init())
 *
 * This function is safe to call as long as @ref is between init and exit.
 */
static inline __attribute__((no_instrument_function)) void Model1_percpu_ref_put(struct Model1_percpu_ref *Model1_ref)
{
 Model1_percpu_ref_put_many(Model1_ref, 1);
}

/**
 * percpu_ref_is_dying - test whether a percpu refcount is dying or dead
 * @ref: percpu_ref to test
 *
 * Returns %true if @ref is dying or dead.
 *
 * This function is safe to call as long as @ref is between init and exit
 * and the caller is responsible for synchronizing against state changes.
 */
static inline __attribute__((no_instrument_function)) bool Model1_percpu_ref_is_dying(struct Model1_percpu_ref *Model1_ref)
{
 return Model1_ref->Model1_percpu_count_ptr & Model1___PERCPU_REF_DEAD;
}

/**
 * percpu_ref_is_zero - test whether a percpu refcount reached zero
 * @ref: percpu_ref to test
 *
 * Returns %true if @ref reached zero.
 *
 * This function is safe to call as long as @ref is between init and exit.
 */
static inline __attribute__((no_instrument_function)) bool Model1_percpu_ref_is_zero(struct Model1_percpu_ref *Model1_ref)
{
 unsigned long *Model1_percpu_count;

 if (Model1___ref_is_percpu(Model1_ref, &Model1_percpu_count))
  return false;
 return !Model1_atomic_long_read(&Model1_ref->Model1_count);
}








/*
 *  bit-based spin_lock()
 *
 * Don't use this unless you really need to: spin_lock() and spin_unlock()
 * are significantly faster.
 */
static inline __attribute__((no_instrument_function)) void Model1_bit_spin_lock(int bitnum, unsigned long *Model1_addr)
{
 /*
	 * Assuming the lock is uncontended, this never enters
	 * the body of the outer loop. If it is contended, then
	 * within the inner loop a non-atomic test is used to
	 * busywait with less bus contention for a good time to
	 * attempt to acquire the lock bit.
	 */
 __asm__ __volatile__("": : :"memory");

 while (__builtin_expect(!!(Model1_test_and_set_bit_lock(bitnum, Model1_addr)), 0)) {
  __asm__ __volatile__("": : :"memory");
  do {
   Model1_cpu_relax();
  } while ((__builtin_constant_p((bitnum)) ? Model1_constant_test_bit((bitnum), (Model1_addr)) : Model1_variable_test_bit((bitnum), (Model1_addr))));
  __asm__ __volatile__("": : :"memory");
 }

 (void)0;
}

/*
 * Return true if it was acquired
 */
static inline __attribute__((no_instrument_function)) int Model1_bit_spin_trylock(int bitnum, unsigned long *Model1_addr)
{
 __asm__ __volatile__("": : :"memory");

 if (__builtin_expect(!!(Model1_test_and_set_bit_lock(bitnum, Model1_addr)), 0)) {
  __asm__ __volatile__("": : :"memory");
  return 0;
 }

 (void)0;
 return 1;
}

/*
 *  bit-based spin_unlock()
 */
static inline __attribute__((no_instrument_function)) void Model1_bit_spin_unlock(int bitnum, unsigned long *Model1_addr)
{




 Model1_clear_bit_unlock(bitnum, Model1_addr);

 __asm__ __volatile__("": : :"memory");
 (void)0;
}

/*
 *  bit-based spin_unlock()
 *  non-atomic version, which can be used eg. if the bit lock itself is
 *  protecting the rest of the flags in the word.
 */
static inline __attribute__((no_instrument_function)) void Model1___bit_spin_unlock(int bitnum, unsigned long *Model1_addr)
{




 Model1___clear_bit_unlock(bitnum, Model1_addr);

 __asm__ __volatile__("": : :"memory");
 (void)0;
}

/*
 * Return true if the lock is held.
 */
static inline __attribute__((no_instrument_function)) int Model1_bit_spin_is_locked(int bitnum, unsigned long *Model1_addr)
{

 return (__builtin_constant_p((bitnum)) ? Model1_constant_test_bit((bitnum), (Model1_addr)) : Model1_variable_test_bit((bitnum), (Model1_addr)));





}



/*
 * This struct is used to pass information from page reclaim to the shrinkers.
 * We consolidate the values for easier extention later.
 *
 * The 'gfpmask' refers to the allocation we are currently trying to
 * fulfil.
 */
struct Model1_shrink_control {
 Model1_gfp_t Model1_gfp_mask;

 /*
	 * How many objects scan_objects should scan and try to reclaim.
	 * This is reset before every call, so it is safe for callees
	 * to modify.
	 */
 unsigned long Model1_nr_to_scan;

 /* current node being shrunk (for NUMA aware shrinkers) */
 int Model1_nid;

 /* current memcg being shrunk (for memcg aware shrinkers) */
 struct Model1_mem_cgroup *Model1_memcg;
};


/*
 * A callback you can register to apply pressure to ageable caches.
 *
 * @count_objects should return the number of freeable items in the cache. If
 * there are no objects to free or the number of freeable items cannot be
 * determined, it should return 0. No deadlock checks should be done during the
 * count callback - the shrinker relies on aggregating scan counts that couldn't
 * be executed due to potential deadlocks to be run at a later call when the
 * deadlock condition is no longer pending.
 *
 * @scan_objects will only be called if @count_objects returned a non-zero
 * value for the number of freeable objects. The callout should scan the cache
 * and attempt to free items from the cache. It should then return the number
 * of objects freed during the scan, or SHRINK_STOP if progress cannot be made
 * due to potential deadlocks. If SHRINK_STOP is returned, then no further
 * attempts to call the @scan_objects will be made from the current reclaim
 * context.
 *
 * @flags determine the shrinker abilities, like numa awareness
 */
struct Model1_shrinker {
 unsigned long (*Model1_count_objects)(struct Model1_shrinker *,
           struct Model1_shrink_control *Model1_sc);
 unsigned long (*Model1_scan_objects)(struct Model1_shrinker *,
          struct Model1_shrink_control *Model1_sc);

 int Model1_seeks; /* seeks to recreate an obj */
 long Model1_batch; /* reclaim batch size, 0 = default */
 unsigned long Model1_flags;

 /* These are for internal use */
 struct Model1_list_head Model1_list;
 /* objs pending delete, per node */
 Model1_atomic_long_t *Model1_nr_deferred;
};


/* Flags */



extern int Model1_register_shrinker(struct Model1_shrinker *);
extern void Model1_unregister_shrinker(struct Model1_shrinker *);









/*
 * Resource control/accounting header file for linux
 */

/*
 * Definition of struct rusage taken from BSD 4.3 Reno
 * 
 * We don't support all of these yet, but we might as well have them....
 * Otherwise, each time we add new items, programs which depend on this
 * structure will lose.  This reduces the chances of that happening.
 */





struct Model1_rusage {
 struct Model1_timeval Model1_ru_utime; /* user time used */
 struct Model1_timeval Model1_ru_stime; /* system time used */
 Model1___kernel_long_t Model1_ru_maxrss; /* maximum resident set size */
 Model1___kernel_long_t Model1_ru_ixrss; /* integral shared memory size */
 Model1___kernel_long_t Model1_ru_idrss; /* integral unshared data size */
 Model1___kernel_long_t Model1_ru_isrss; /* integral unshared stack size */
 Model1___kernel_long_t Model1_ru_minflt; /* page reclaims */
 Model1___kernel_long_t Model1_ru_majflt; /* page faults */
 Model1___kernel_long_t Model1_ru_nswap; /* swaps */
 Model1___kernel_long_t Model1_ru_inblock; /* block input operations */
 Model1___kernel_long_t Model1_ru_oublock; /* block output operations */
 Model1___kernel_long_t Model1_ru_msgsnd; /* messages sent */
 Model1___kernel_long_t Model1_ru_msgrcv; /* messages received */
 Model1___kernel_long_t Model1_ru_nsignals; /* signals received */
 Model1___kernel_long_t Model1_ru_nvcsw; /* voluntary context switches */
 Model1___kernel_long_t Model1_ru_nivcsw; /* involuntary " */
};

struct Model1_rlimit {
 Model1___kernel_ulong_t Model1_rlim_cur;
 Model1___kernel_ulong_t Model1_rlim_max;
};



struct Model1_rlimit64 {
 __u64 Model1_rlim_cur;
 __u64 Model1_rlim_max;
};
/*
 * Limit the stack by to some sane default: root can always
 * increase this limit if needed..  8MB seems reasonable.
 */


/*
 * GPG2 wants 64kB of mlocked memory, to make sure pass phrases
 * and other sensitive information are never written to disk.
 */


/*
 * Due to binary compatibility, the actual resource numbers
 * may be different for different linux versions..
 */







/*
 * Resource limit IDs
 *
 * ( Compatibility detail: there are architectures that have
 *   a different rlimit ID order in the 5-9 range and want
 *   to keep that order for binary compatibility. The reasons
 *   are historic and all new rlimits are identical across all
 *   arches. If an arch has such special order for some rlimits
 *   then it defines them prior including asm-generic/resource.h. )
 */
/*
 * SuS says limits have to be unsigned.
 * Which makes a ton more sense anyway.
 *
 * Some architectures override this (for compatibility reasons):
 */


/*
 * boot-time rlimit defaults for the init task:
 */


struct Model1_task_struct;

int Model1_getrusage(struct Model1_task_struct *Model1_p, int Model1_who, struct Model1_rusage *Model1_ru);
int Model1_do_prlimit(struct Model1_task_struct *Model1_tsk, unsigned int Model1_resource,
  struct Model1_rlimit *Model1_new_rlim, struct Model1_rlimit *Model1_old_rlim);









struct Model1_task_struct;
struct Model1_pt_regs;


struct Model1_stack_trace {
 unsigned int Model1_nr_entries, Model1_max_entries;
 unsigned long *Model1_entries;
 int Model1_skip; /* input argument: How many entries to skip */
};

extern void Model1_save_stack_trace(struct Model1_stack_trace *Model1_trace);
extern void Model1_save_stack_trace_regs(struct Model1_pt_regs *Model1_regs,
      struct Model1_stack_trace *Model1_trace);
extern void Model1_save_stack_trace_tsk(struct Model1_task_struct *Model1_tsk,
    struct Model1_stack_trace *Model1_trace);

extern void Model1_print_stack_trace(struct Model1_stack_trace *Model1_trace, int Model1_spaces);
extern int Model1_snprint_stack_trace(char *Model1_buf, Model1_size_t Model1_size,
   struct Model1_stack_trace *Model1_trace, int Model1_spaces);


extern void Model1_save_stack_trace_user(struct Model1_stack_trace *Model1_trace);
/*
 * A generic stack depot implementation
 *
 * Author: Alexander Potapenko <glider@google.com>
 * Copyright (C) 2016 Google, Inc.
 *
 * Based on code by Dmitry Chernenkov.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 */




typedef Model1_u32 Model1_depot_stack_handle_t;

struct Model1_stack_trace;

Model1_depot_stack_handle_t Model1_depot_save_stack(struct Model1_stack_trace *Model1_trace, Model1_gfp_t Model1_flags);

void Model1_depot_fetch_stack(Model1_depot_stack_handle_t Model1_handle, struct Model1_stack_trace *Model1_trace);

struct Model1_pglist_data;
struct Model1_page_ext_operations {
 bool (*Model1_need)(void);
 void (*Model1_init)(void);
};
struct Model1_page_ext;

static inline __attribute__((no_instrument_function)) void Model1_pgdat_page_ext_init(struct Model1_pglist_data *Model1_pgdat)
{
}

static inline __attribute__((no_instrument_function)) struct Model1_page_ext *Model1_lookup_page_ext(struct Model1_page *Model1_page)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) void Model1_page_ext_init(void)
{
}

static inline __attribute__((no_instrument_function)) void Model1_page_ext_init_flatmem(void)
{
}






/*
 * Macros for manipulating and testing page->flags
 */
/*
 * Various page->flags bits:
 *
 * PG_reserved is set for special pages, which can never be swapped out. Some
 * of them might not even exist (eg empty_bad_page)...
 *
 * The PG_private bitflag is set on pagecache pages if they contain filesystem
 * specific data (which is normally at page->private). It can be used by
 * private allocations for its own usage.
 *
 * During initiation of disk I/O, PG_locked is set. This bit is set before I/O
 * and cleared when writeback _starts_ or when read _completes_. PG_writeback
 * is set before writeback starts and cleared when it finishes.
 *
 * PG_locked also pins a page in pagecache, and blocks truncation of the file
 * while it is held.
 *
 * page_waitqueue(page) is a wait queue of all tasks waiting for the page
 * to become unlocked.
 *
 * PG_uptodate tells whether the page's contents is valid.  When a read
 * completes, the page becomes uptodate, unless a disk I/O error happened.
 *
 * PG_referenced, PG_reclaim are used for page reclaim for anonymous and
 * file-backed pagecache (see mm/vmscan.c).
 *
 * PG_error is set to indicate that an I/O error occurred on this page.
 *
 * PG_arch_1 is an architecture specific page state bit.  The generic code
 * guarantees that this bit is cleared for a page when it first is entered into
 * the page cache.
 *
 * PG_highmem pages are not permanently mapped into the kernel virtual address
 * space, they need to be kmapped separately for doing IO on the pages.  The
 * struct page (these bits with information) are always mapped into kernel
 * address space...
 *
 * PG_hwpoison indicates that a page got corrupted in hardware and contains
 * data with incorrect ECC bits that triggered a machine check. Accessing is
 * not safe since it may cause another machine check. Don't touch!
 */

/*
 * Don't use the *_dontuse flags.  Use the macros.  Otherwise you'll break
 * locked- and dirty-page accounting.
 *
 * The page flags field is split into two parts, the main flags area
 * which extends from the low bits upwards, and the fields area which
 * extends from the high bits downwards.
 *
 *  | FIELD | ... | FLAGS |
 *  N-1           ^       0
 *               (NR_PAGEFLAGS)
 *
 * The fields area is reserved for fields mapping zone, node (for NUMA) and
 * SPARSEMEM section (for variants of SPARSEMEM that require section ids like
 * SPARSEMEM_EXTREME with !SPARSEMEM_VMEMMAP).
 */
enum Model1_pageflags {
 Model1_PG_locked, /* Page is locked. Don't touch. */
 Model1_PG_error,
 Model1_PG_referenced,
 Model1_PG_uptodate,
 Model1_PG_dirty,
 Model1_PG_lru,
 Model1_PG_active,
 Model1_PG_slab,
 Model1_PG_owner_priv_1, /* Owner use. If pagecache, fs may use*/
 Model1_PG_arch_1,
 Model1_PG_reserved,
 Model1_PG_private, /* If pagecache, has fs-private data */
 Model1_PG_private_2, /* If pagecache, has fs aux data */
 Model1_PG_writeback, /* Page is under writeback */
 Model1_PG_head, /* A head page */
 Model1_PG_swapcache, /* Swap page: swp_entry_t in private */
 Model1_PG_mappedtodisk, /* Has blocks allocated on-disk */
 Model1_PG_reclaim, /* To be reclaimed asap */
 Model1_PG_swapbacked, /* Page is backed by RAM/swap */
 Model1_PG_unevictable, /* Page is "unevictable"  */

 Model1_PG_mlocked, /* Page is vma mlocked */


 Model1_PG_uncached, /* Page has been mapped as uncached */
 Model1___NR_PAGEFLAGS,

 /* Filesystems */
 Model1_PG_checked = Model1_PG_owner_priv_1,

 /* Two page bits are conscripted by FS-Cache to maintain local caching
	 * state.  These bits are set on pages belonging to the netfs's inodes
	 * when those inodes are being locally cached.
	 */
 Model1_PG_fscache = Model1_PG_private_2, /* page backed by cache */

 /* XEN */
 /* Pinned in Xen as a read-only pagetable page. */
 Model1_PG_pinned = Model1_PG_owner_priv_1,
 /* Pinned as part of domain save (see xen_mm_pin_all()). */
 Model1_PG_savepinned = Model1_PG_dirty,
 /* Has a grant mapping of another (foreign) domain's page. */
 Model1_PG_foreign = Model1_PG_owner_priv_1,

 /* SLOB */
 Model1_PG_slob_free = Model1_PG_private,

 /* Compound pages. Stored in first tail page's flags */
 Model1_PG_double_map = Model1_PG_private_2,

 /* non-lru isolated movable page */
 Model1_PG_isolated = Model1_PG_reclaim,
};



struct Model1_page; /* forward declaration */

static inline __attribute__((no_instrument_function)) struct Model1_page *Model1_compound_head(struct Model1_page *Model1_page)
{
 unsigned long Model1_head = ({ union { typeof(Model1_page->Model1_compound_head) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&(Model1_page->Model1_compound_head), Model1___u.Model1___c, sizeof(Model1_page->Model1_compound_head)); else Model1___read_once_size_nocheck(&(Model1_page->Model1_compound_head), Model1___u.Model1___c, sizeof(Model1_page->Model1_compound_head)); Model1___u.Model1___val; });

 if (__builtin_expect(!!(Model1_head & 1), 0))
  return (struct Model1_page *) (Model1_head - 1);
 return Model1_page;
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PageTail(struct Model1_page *Model1_page)
{
 return ({ union { typeof(Model1_page->Model1_compound_head) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&(Model1_page->Model1_compound_head), Model1___u.Model1___c, sizeof(Model1_page->Model1_compound_head)); else Model1___read_once_size_nocheck(&(Model1_page->Model1_compound_head), Model1___u.Model1___c, sizeof(Model1_page->Model1_compound_head)); Model1___u.Model1___val; }) & 1;
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PageCompound(struct Model1_page *Model1_page)
{
 return (__builtin_constant_p((Model1_PG_head)) ? Model1_constant_test_bit((Model1_PG_head), (&Model1_page->Model1_flags)) : Model1_variable_test_bit((Model1_PG_head), (&Model1_page->Model1_flags))) || Model1_PageTail(Model1_page);
}

/*
 * Page flags policies wrt compound pages
 *
 * PF_ANY:
 *     the page flag is relevant for small, head and tail pages.
 *
 * PF_HEAD:
 *     for compound page all operations related to the page flag applied to
 *     head page.
 *
 * PF_NO_TAIL:
 *     modifications of the page flag must be done on small or head pages,
 *     checks can be done on tail pages too.
 *
 * PF_NO_COMPOUND:
 *     the page flag is not relevant for compound pages.
 */
/*
 * Macros to create function definitions for page flags
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PageLocked(struct Model1_page *Model1_page) { return (__builtin_constant_p((Model1_PG_locked)) ? Model1_constant_test_bit((Model1_PG_locked), (&({ ((void)(sizeof(( long)(0 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags)) : Model1_variable_test_bit((Model1_PG_locked), (&({ ((void)(sizeof(( long)(0 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___SetPageLocked(struct Model1_page *Model1_page) { Model1___set_bit(Model1_PG_locked, &({ ((void)(sizeof(( long)(1 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___ClearPageLocked(struct Model1_page *Model1_page) { Model1___clear_bit(Model1_PG_locked, &({ ((void)(sizeof(( long)(1 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PageError(struct Model1_page *Model1_page) { return (__builtin_constant_p((Model1_PG_error)) ? Model1_constant_test_bit((Model1_PG_error), (&({ ((void)(sizeof(( long)(0 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags)) : Model1_variable_test_bit((Model1_PG_error), (&({ ((void)(sizeof(( long)(0 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_SetPageError(struct Model1_page *Model1_page) { Model1_set_bit(Model1_PG_error, &({ ((void)(sizeof(( long)(1 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_ClearPageError(struct Model1_page *Model1_page) { Model1_clear_bit(Model1_PG_error, &({ ((void)(sizeof(( long)(1 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_TestClearPageError(struct Model1_page *Model1_page) { return Model1_test_and_clear_bit(Model1_PG_error, &({ ((void)(sizeof(( long)(1 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PageReferenced(struct Model1_page *Model1_page) { return (__builtin_constant_p((Model1_PG_referenced)) ? Model1_constant_test_bit((Model1_PG_referenced), (&Model1_compound_head(Model1_page)->Model1_flags)) : Model1_variable_test_bit((Model1_PG_referenced), (&Model1_compound_head(Model1_page)->Model1_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_SetPageReferenced(struct Model1_page *Model1_page) { Model1_set_bit(Model1_PG_referenced, &Model1_compound_head(Model1_page)->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_ClearPageReferenced(struct Model1_page *Model1_page) { Model1_clear_bit(Model1_PG_referenced, &Model1_compound_head(Model1_page)->Model1_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_TestClearPageReferenced(struct Model1_page *Model1_page) { return Model1_test_and_clear_bit(Model1_PG_referenced, &Model1_compound_head(Model1_page)->Model1_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___SetPageReferenced(struct Model1_page *Model1_page) { Model1___set_bit(Model1_PG_referenced, &Model1_compound_head(Model1_page)->Model1_flags); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PageDirty(struct Model1_page *Model1_page) { return (__builtin_constant_p((Model1_PG_dirty)) ? Model1_constant_test_bit((Model1_PG_dirty), (&Model1_compound_head(Model1_page)->Model1_flags)) : Model1_variable_test_bit((Model1_PG_dirty), (&Model1_compound_head(Model1_page)->Model1_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_SetPageDirty(struct Model1_page *Model1_page) { Model1_set_bit(Model1_PG_dirty, &Model1_compound_head(Model1_page)->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_ClearPageDirty(struct Model1_page *Model1_page) { Model1_clear_bit(Model1_PG_dirty, &Model1_compound_head(Model1_page)->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_TestSetPageDirty(struct Model1_page *Model1_page) { return Model1_test_and_set_bit(Model1_PG_dirty, &Model1_compound_head(Model1_page)->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_TestClearPageDirty(struct Model1_page *Model1_page) { return Model1_test_and_clear_bit(Model1_PG_dirty, &Model1_compound_head(Model1_page)->Model1_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___ClearPageDirty(struct Model1_page *Model1_page) { Model1___clear_bit(Model1_PG_dirty, &Model1_compound_head(Model1_page)->Model1_flags); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PageLRU(struct Model1_page *Model1_page) { return (__builtin_constant_p((Model1_PG_lru)) ? Model1_constant_test_bit((Model1_PG_lru), (&Model1_compound_head(Model1_page)->Model1_flags)) : Model1_variable_test_bit((Model1_PG_lru), (&Model1_compound_head(Model1_page)->Model1_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_SetPageLRU(struct Model1_page *Model1_page) { Model1_set_bit(Model1_PG_lru, &Model1_compound_head(Model1_page)->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_ClearPageLRU(struct Model1_page *Model1_page) { Model1_clear_bit(Model1_PG_lru, &Model1_compound_head(Model1_page)->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___ClearPageLRU(struct Model1_page *Model1_page) { Model1___clear_bit(Model1_PG_lru, &Model1_compound_head(Model1_page)->Model1_flags); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PageActive(struct Model1_page *Model1_page) { return (__builtin_constant_p((Model1_PG_active)) ? Model1_constant_test_bit((Model1_PG_active), (&Model1_compound_head(Model1_page)->Model1_flags)) : Model1_variable_test_bit((Model1_PG_active), (&Model1_compound_head(Model1_page)->Model1_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_SetPageActive(struct Model1_page *Model1_page) { Model1_set_bit(Model1_PG_active, &Model1_compound_head(Model1_page)->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_ClearPageActive(struct Model1_page *Model1_page) { Model1_clear_bit(Model1_PG_active, &Model1_compound_head(Model1_page)->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___ClearPageActive(struct Model1_page *Model1_page) { Model1___clear_bit(Model1_PG_active, &Model1_compound_head(Model1_page)->Model1_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_TestClearPageActive(struct Model1_page *Model1_page) { return Model1_test_and_clear_bit(Model1_PG_active, &Model1_compound_head(Model1_page)->Model1_flags); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PageSlab(struct Model1_page *Model1_page) { return (__builtin_constant_p((Model1_PG_slab)) ? Model1_constant_test_bit((Model1_PG_slab), (&({ ((void)(sizeof(( long)(0 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags)) : Model1_variable_test_bit((Model1_PG_slab), (&({ ((void)(sizeof(( long)(0 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___SetPageSlab(struct Model1_page *Model1_page) { Model1___set_bit(Model1_PG_slab, &({ ((void)(sizeof(( long)(1 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___ClearPageSlab(struct Model1_page *Model1_page) { Model1___clear_bit(Model1_PG_slab, &({ ((void)(sizeof(( long)(1 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PageSlobFree(struct Model1_page *Model1_page) { return (__builtin_constant_p((Model1_PG_slob_free)) ? Model1_constant_test_bit((Model1_PG_slob_free), (&({ ((void)(sizeof(( long)(0 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags)) : Model1_variable_test_bit((Model1_PG_slob_free), (&({ ((void)(sizeof(( long)(0 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___SetPageSlobFree(struct Model1_page *Model1_page) { Model1___set_bit(Model1_PG_slob_free, &({ ((void)(sizeof(( long)(1 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___ClearPageSlobFree(struct Model1_page *Model1_page) { Model1___clear_bit(Model1_PG_slob_free, &({ ((void)(sizeof(( long)(1 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PageChecked(struct Model1_page *Model1_page) { return (__builtin_constant_p((Model1_PG_checked)) ? Model1_constant_test_bit((Model1_PG_checked), (&({ ((void)(sizeof(( long)(0 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags)) : Model1_variable_test_bit((Model1_PG_checked), (&({ ((void)(sizeof(( long)(0 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_SetPageChecked(struct Model1_page *Model1_page) { Model1_set_bit(Model1_PG_checked, &({ ((void)(sizeof(( long)(1 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_ClearPageChecked(struct Model1_page *Model1_page) { Model1_clear_bit(Model1_PG_checked, &({ ((void)(sizeof(( long)(1 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags); } /* Used by some filesystems */

/* Xen */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PagePinned(struct Model1_page *Model1_page) { return (__builtin_constant_p((Model1_PG_pinned)) ? Model1_constant_test_bit((Model1_PG_pinned), (&({ ((void)(sizeof(( long)(0 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags)) : Model1_variable_test_bit((Model1_PG_pinned), (&({ ((void)(sizeof(( long)(0 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_SetPagePinned(struct Model1_page *Model1_page) { Model1_set_bit(Model1_PG_pinned, &({ ((void)(sizeof(( long)(1 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_ClearPagePinned(struct Model1_page *Model1_page) { Model1_clear_bit(Model1_PG_pinned, &({ ((void)(sizeof(( long)(1 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_TestSetPagePinned(struct Model1_page *Model1_page) { return Model1_test_and_set_bit(Model1_PG_pinned, &({ ((void)(sizeof(( long)(1 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_TestClearPagePinned(struct Model1_page *Model1_page) { return Model1_test_and_clear_bit(Model1_PG_pinned, &({ ((void)(sizeof(( long)(1 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PageSavePinned(struct Model1_page *Model1_page) { return (__builtin_constant_p((Model1_PG_savepinned)) ? Model1_constant_test_bit((Model1_PG_savepinned), (&({ ((void)(sizeof(( long)(0 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags)) : Model1_variable_test_bit((Model1_PG_savepinned), (&({ ((void)(sizeof(( long)(0 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_SetPageSavePinned(struct Model1_page *Model1_page) { Model1_set_bit(Model1_PG_savepinned, &({ ((void)(sizeof(( long)(1 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_ClearPageSavePinned(struct Model1_page *Model1_page) { Model1_clear_bit(Model1_PG_savepinned, &({ ((void)(sizeof(( long)(1 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags); };
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PageForeign(struct Model1_page *Model1_page) { return (__builtin_constant_p((Model1_PG_foreign)) ? Model1_constant_test_bit((Model1_PG_foreign), (&({ ((void)(sizeof(( long)(0 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags)) : Model1_variable_test_bit((Model1_PG_foreign), (&({ ((void)(sizeof(( long)(0 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_SetPageForeign(struct Model1_page *Model1_page) { Model1_set_bit(Model1_PG_foreign, &({ ((void)(sizeof(( long)(1 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_ClearPageForeign(struct Model1_page *Model1_page) { Model1_clear_bit(Model1_PG_foreign, &({ ((void)(sizeof(( long)(1 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags); };

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PageReserved(struct Model1_page *Model1_page) { return (__builtin_constant_p((Model1_PG_reserved)) ? Model1_constant_test_bit((Model1_PG_reserved), (&({ ((void)(sizeof(( long)(0 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags)) : Model1_variable_test_bit((Model1_PG_reserved), (&({ ((void)(sizeof(( long)(0 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_SetPageReserved(struct Model1_page *Model1_page) { Model1_set_bit(Model1_PG_reserved, &({ ((void)(sizeof(( long)(1 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_ClearPageReserved(struct Model1_page *Model1_page) { Model1_clear_bit(Model1_PG_reserved, &({ ((void)(sizeof(( long)(1 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___ClearPageReserved(struct Model1_page *Model1_page) { Model1___clear_bit(Model1_PG_reserved, &({ ((void)(sizeof(( long)(1 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PageSwapBacked(struct Model1_page *Model1_page) { return (__builtin_constant_p((Model1_PG_swapbacked)) ? Model1_constant_test_bit((Model1_PG_swapbacked), (&({ ((void)(sizeof(( long)(0 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags)) : Model1_variable_test_bit((Model1_PG_swapbacked), (&({ ((void)(sizeof(( long)(0 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_SetPageSwapBacked(struct Model1_page *Model1_page) { Model1_set_bit(Model1_PG_swapbacked, &({ ((void)(sizeof(( long)(1 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_ClearPageSwapBacked(struct Model1_page *Model1_page) { Model1_clear_bit(Model1_PG_swapbacked, &({ ((void)(sizeof(( long)(1 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___ClearPageSwapBacked(struct Model1_page *Model1_page) { Model1___clear_bit(Model1_PG_swapbacked, &({ ((void)(sizeof(( long)(1 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___SetPageSwapBacked(struct Model1_page *Model1_page) { Model1___set_bit(Model1_PG_swapbacked, &({ ((void)(sizeof(( long)(1 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags); }

/*
 * Private page markings that may be used by the filesystem that owns the page
 * for its own purposes.
 * - PG_private and PG_private_2 cause releasepage() and co to be invoked
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PagePrivate(struct Model1_page *Model1_page) { return (__builtin_constant_p((Model1_PG_private)) ? Model1_constant_test_bit((Model1_PG_private), (&Model1_page->Model1_flags)) : Model1_variable_test_bit((Model1_PG_private), (&Model1_page->Model1_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_SetPagePrivate(struct Model1_page *Model1_page) { Model1_set_bit(Model1_PG_private, &Model1_page->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_ClearPagePrivate(struct Model1_page *Model1_page) { Model1_clear_bit(Model1_PG_private, &Model1_page->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___SetPagePrivate(struct Model1_page *Model1_page) { Model1___set_bit(Model1_PG_private, &Model1_page->Model1_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___ClearPagePrivate(struct Model1_page *Model1_page) { Model1___clear_bit(Model1_PG_private, &Model1_page->Model1_flags); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PagePrivate2(struct Model1_page *Model1_page) { return (__builtin_constant_p((Model1_PG_private_2)) ? Model1_constant_test_bit((Model1_PG_private_2), (&Model1_page->Model1_flags)) : Model1_variable_test_bit((Model1_PG_private_2), (&Model1_page->Model1_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_SetPagePrivate2(struct Model1_page *Model1_page) { Model1_set_bit(Model1_PG_private_2, &Model1_page->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_ClearPagePrivate2(struct Model1_page *Model1_page) { Model1_clear_bit(Model1_PG_private_2, &Model1_page->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_TestSetPagePrivate2(struct Model1_page *Model1_page) { return Model1_test_and_set_bit(Model1_PG_private_2, &Model1_page->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_TestClearPagePrivate2(struct Model1_page *Model1_page) { return Model1_test_and_clear_bit(Model1_PG_private_2, &Model1_page->Model1_flags); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PageOwnerPriv1(struct Model1_page *Model1_page) { return (__builtin_constant_p((Model1_PG_owner_priv_1)) ? Model1_constant_test_bit((Model1_PG_owner_priv_1), (&Model1_page->Model1_flags)) : Model1_variable_test_bit((Model1_PG_owner_priv_1), (&Model1_page->Model1_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_SetPageOwnerPriv1(struct Model1_page *Model1_page) { Model1_set_bit(Model1_PG_owner_priv_1, &Model1_page->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_ClearPageOwnerPriv1(struct Model1_page *Model1_page) { Model1_clear_bit(Model1_PG_owner_priv_1, &Model1_page->Model1_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_TestClearPageOwnerPriv1(struct Model1_page *Model1_page) { return Model1_test_and_clear_bit(Model1_PG_owner_priv_1, &Model1_page->Model1_flags); }

/*
 * Only test-and-set exist for PG_writeback.  The unconditional operators are
 * risky: they bypass page accounting.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PageWriteback(struct Model1_page *Model1_page) { return (__builtin_constant_p((Model1_PG_writeback)) ? Model1_constant_test_bit((Model1_PG_writeback), (&({ ((void)(sizeof(( long)(0 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags)) : Model1_variable_test_bit((Model1_PG_writeback), (&({ ((void)(sizeof(( long)(0 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags))); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_TestSetPageWriteback(struct Model1_page *Model1_page) { return Model1_test_and_set_bit(Model1_PG_writeback, &({ ((void)(sizeof(( long)(1 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_TestClearPageWriteback(struct Model1_page *Model1_page) { return Model1_test_and_clear_bit(Model1_PG_writeback, &({ ((void)(sizeof(( long)(1 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PageMappedToDisk(struct Model1_page *Model1_page) { return (__builtin_constant_p((Model1_PG_mappedtodisk)) ? Model1_constant_test_bit((Model1_PG_mappedtodisk), (&({ ((void)(sizeof(( long)(0 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags)) : Model1_variable_test_bit((Model1_PG_mappedtodisk), (&({ ((void)(sizeof(( long)(0 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_SetPageMappedToDisk(struct Model1_page *Model1_page) { Model1_set_bit(Model1_PG_mappedtodisk, &({ ((void)(sizeof(( long)(1 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_ClearPageMappedToDisk(struct Model1_page *Model1_page) { Model1_clear_bit(Model1_PG_mappedtodisk, &({ ((void)(sizeof(( long)(1 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags); }

/* PG_readahead is only used for reads; PG_reclaim is only for writes */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PageReclaim(struct Model1_page *Model1_page) { return (__builtin_constant_p((Model1_PG_reclaim)) ? Model1_constant_test_bit((Model1_PG_reclaim), (&({ ((void)(sizeof(( long)(0 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags)) : Model1_variable_test_bit((Model1_PG_reclaim), (&({ ((void)(sizeof(( long)(0 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_SetPageReclaim(struct Model1_page *Model1_page) { Model1_set_bit(Model1_PG_reclaim, &({ ((void)(sizeof(( long)(1 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_ClearPageReclaim(struct Model1_page *Model1_page) { Model1_clear_bit(Model1_PG_reclaim, &({ ((void)(sizeof(( long)(1 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_TestClearPageReclaim(struct Model1_page *Model1_page) { return Model1_test_and_clear_bit(Model1_PG_reclaim, &({ ((void)(sizeof(( long)(1 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags); }
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PageReadahead(struct Model1_page *Model1_page) { return (__builtin_constant_p((Model1_PG_reclaim)) ? Model1_constant_test_bit((Model1_PG_reclaim), (&({ ((void)(sizeof(( long)(0 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags)) : Model1_variable_test_bit((Model1_PG_reclaim), (&({ ((void)(sizeof(( long)(0 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_SetPageReadahead(struct Model1_page *Model1_page) { Model1_set_bit(Model1_PG_reclaim, &({ ((void)(sizeof(( long)(1 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_ClearPageReadahead(struct Model1_page *Model1_page) { Model1_clear_bit(Model1_PG_reclaim, &({ ((void)(sizeof(( long)(1 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_TestClearPageReadahead(struct Model1_page *Model1_page) { return Model1_test_and_clear_bit(Model1_PG_reclaim, &({ ((void)(sizeof(( long)(1 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags); }
static inline __attribute__((no_instrument_function)) int Model1_PageHighMem(const struct Model1_page *Model1_page) { return 0; } static inline __attribute__((no_instrument_function)) void Model1_SetPageHighMem(struct Model1_page *Model1_page) { } static inline __attribute__((no_instrument_function)) void Model1_ClearPageHighMem(struct Model1_page *Model1_page) { }



static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PageSwapCache(struct Model1_page *Model1_page) { return (__builtin_constant_p((Model1_PG_swapcache)) ? Model1_constant_test_bit((Model1_PG_swapcache), (&({ ((void)(sizeof(( long)(0 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags)) : Model1_variable_test_bit((Model1_PG_swapcache), (&({ ((void)(sizeof(( long)(0 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_SetPageSwapCache(struct Model1_page *Model1_page) { Model1_set_bit(Model1_PG_swapcache, &({ ((void)(sizeof(( long)(1 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_ClearPageSwapCache(struct Model1_page *Model1_page) { Model1_clear_bit(Model1_PG_swapcache, &({ ((void)(sizeof(( long)(1 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags); }




static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PageUnevictable(struct Model1_page *Model1_page) { return (__builtin_constant_p((Model1_PG_unevictable)) ? Model1_constant_test_bit((Model1_PG_unevictable), (&Model1_compound_head(Model1_page)->Model1_flags)) : Model1_variable_test_bit((Model1_PG_unevictable), (&Model1_compound_head(Model1_page)->Model1_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_SetPageUnevictable(struct Model1_page *Model1_page) { Model1_set_bit(Model1_PG_unevictable, &Model1_compound_head(Model1_page)->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_ClearPageUnevictable(struct Model1_page *Model1_page) { Model1_clear_bit(Model1_PG_unevictable, &Model1_compound_head(Model1_page)->Model1_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___ClearPageUnevictable(struct Model1_page *Model1_page) { Model1___clear_bit(Model1_PG_unevictable, &Model1_compound_head(Model1_page)->Model1_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_TestClearPageUnevictable(struct Model1_page *Model1_page) { return Model1_test_and_clear_bit(Model1_PG_unevictable, &Model1_compound_head(Model1_page)->Model1_flags); }


static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PageMlocked(struct Model1_page *Model1_page) { return (__builtin_constant_p((Model1_PG_mlocked)) ? Model1_constant_test_bit((Model1_PG_mlocked), (&({ ((void)(sizeof(( long)(0 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags)) : Model1_variable_test_bit((Model1_PG_mlocked), (&({ ((void)(sizeof(( long)(0 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_SetPageMlocked(struct Model1_page *Model1_page) { Model1_set_bit(Model1_PG_mlocked, &({ ((void)(sizeof(( long)(1 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_ClearPageMlocked(struct Model1_page *Model1_page) { Model1_clear_bit(Model1_PG_mlocked, &({ ((void)(sizeof(( long)(1 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___ClearPageMlocked(struct Model1_page *Model1_page) { Model1___clear_bit(Model1_PG_mlocked, &({ ((void)(sizeof(( long)(1 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags); }
 static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_TestSetPageMlocked(struct Model1_page *Model1_page) { return Model1_test_and_set_bit(Model1_PG_mlocked, &({ ((void)(sizeof(( long)(1 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_TestClearPageMlocked(struct Model1_page *Model1_page) { return Model1_test_and_clear_bit(Model1_PG_mlocked, &({ ((void)(sizeof(( long)(1 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags); }






static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PageUncached(struct Model1_page *Model1_page) { return (__builtin_constant_p((Model1_PG_uncached)) ? Model1_constant_test_bit((Model1_PG_uncached), (&({ ((void)(sizeof(( long)(0 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags)) : Model1_variable_test_bit((Model1_PG_uncached), (&({ ((void)(sizeof(( long)(0 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_SetPageUncached(struct Model1_page *Model1_page) { Model1_set_bit(Model1_PG_uncached, &({ ((void)(sizeof(( long)(1 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_ClearPageUncached(struct Model1_page *Model1_page) { Model1_clear_bit(Model1_PG_uncached, &({ ((void)(sizeof(( long)(1 && Model1_PageCompound(Model1_page))))); Model1_page;})->Model1_flags); }
static inline __attribute__((no_instrument_function)) int Model1_PageHWPoison(const struct Model1_page *Model1_page) { return 0; } static inline __attribute__((no_instrument_function)) void Model1_SetPageHWPoison(struct Model1_page *Model1_page) { } static inline __attribute__((no_instrument_function)) void Model1_ClearPageHWPoison(struct Model1_page *Model1_page) { }
/*
 * On an anonymous page mapped into a user virtual memory area,
 * page->mapping points to its anon_vma, not to a struct address_space;
 * with the PAGE_MAPPING_ANON bit set to distinguish it.  See rmap.h.
 *
 * On an anonymous page in a VM_MERGEABLE area, if CONFIG_KSM is enabled,
 * the PAGE_MAPPING_MOVABLE bit may be set along with the PAGE_MAPPING_ANON
 * bit; and then page->mapping points, not to an anon_vma, but to a private
 * structure which KSM associates with that merged page.  See ksm.h.
 *
 * PAGE_MAPPING_KSM without PAGE_MAPPING_ANON is used for non-lru movable
 * page and then page->mapping points a struct address_space.
 *
 * Please note that, confusingly, "page_mapping" refers to the inode
 * address_space which maps the page from disk; whereas "page_mapped"
 * refers to user virtual address space into which the page is mapped.
 */





static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PageMappingFlags(struct Model1_page *Model1_page)
{
 return ((unsigned long)Model1_page->Model1_mapping & (0x1 | 0x2)) != 0;
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PageAnon(struct Model1_page *Model1_page)
{
 Model1_page = Model1_compound_head(Model1_page);
 return ((unsigned long)Model1_page->Model1_mapping & 0x1) != 0;
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1___PageMovable(struct Model1_page *Model1_page)
{
 return ((unsigned long)Model1_page->Model1_mapping & (0x1 | 0x2)) ==
    0x2;
}
static inline __attribute__((no_instrument_function)) int Model1_PageKsm(const struct Model1_page *Model1_page) { return 0; }


Model1_u64 Model1_stable_page_flags(struct Model1_page *Model1_page);

static inline __attribute__((no_instrument_function)) int Model1_PageUptodate(struct Model1_page *Model1_page)
{
 int Model1_ret;
 Model1_page = Model1_compound_head(Model1_page);
 Model1_ret = (__builtin_constant_p((Model1_PG_uptodate)) ? Model1_constant_test_bit((Model1_PG_uptodate), (&(Model1_page)->Model1_flags)) : Model1_variable_test_bit((Model1_PG_uptodate), (&(Model1_page)->Model1_flags)));
 /*
	 * Must ensure that the data we read out of the page is loaded
	 * _after_ we've loaded page->flags to check for PageUptodate.
	 * We can skip the barrier if the page is not uptodate, because
	 * we wouldn't be reading anything from it.
	 *
	 * See SetPageUptodate() for the other side of the story.
	 */
 if (Model1_ret)
  __asm__ __volatile__("": : :"memory");

 return Model1_ret;
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___SetPageUptodate(struct Model1_page *Model1_page)
{
 ((void)(sizeof(( long)(Model1_PageTail(Model1_page)))));
 __asm__ __volatile__("": : :"memory");
 Model1___set_bit(Model1_PG_uptodate, &Model1_page->Model1_flags);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_SetPageUptodate(struct Model1_page *Model1_page)
{
 ((void)(sizeof(( long)(Model1_PageTail(Model1_page)))));
 /*
	 * Memory barrier must be issued before setting the PG_uptodate bit,
	 * so that all previous stores issued in order to bring the page
	 * uptodate are actually visible before PageUptodate becomes true.
	 */
 __asm__ __volatile__("": : :"memory");
 Model1_set_bit(Model1_PG_uptodate, &Model1_page->Model1_flags);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_ClearPageUptodate(struct Model1_page *Model1_page) { Model1_clear_bit(Model1_PG_uptodate, &({ ((void)(sizeof(( long)(1 && Model1_PageTail(Model1_page))))); Model1_compound_head(Model1_page);})->Model1_flags); }

int Model1_test_clear_page_writeback(struct Model1_page *Model1_page);
int Model1___test_set_page_writeback(struct Model1_page *Model1_page, bool Model1_keep_write);






static inline __attribute__((no_instrument_function)) void Model1_set_page_writeback(struct Model1_page *Model1_page)
{
 Model1___test_set_page_writeback(Model1_page, false);
}

static inline __attribute__((no_instrument_function)) void Model1_set_page_writeback_keepwrite(struct Model1_page *Model1_page)
{
 Model1___test_set_page_writeback(Model1_page, true);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PageHead(struct Model1_page *Model1_page) { return (__builtin_constant_p((Model1_PG_head)) ? Model1_constant_test_bit((Model1_PG_head), (&Model1_page->Model1_flags)) : Model1_variable_test_bit((Model1_PG_head), (&Model1_page->Model1_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___SetPageHead(struct Model1_page *Model1_page) { Model1___set_bit(Model1_PG_head, &Model1_page->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___ClearPageHead(struct Model1_page *Model1_page) { Model1___clear_bit(Model1_PG_head, &Model1_page->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_ClearPageHead(struct Model1_page *Model1_page) { Model1_clear_bit(Model1_PG_head, &Model1_page->Model1_flags); }

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_set_compound_head(struct Model1_page *Model1_page, struct Model1_page *Model1_head)
{
 ({ union { typeof(Model1_page->Model1_compound_head) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(Model1_page->Model1_compound_head)) ((unsigned long)Model1_head + 1) }; Model1___write_once_size(&(Model1_page->Model1_compound_head), Model1___u.Model1___c, sizeof(Model1_page->Model1_compound_head)); Model1___u.Model1___val; });
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_clear_compound_head(struct Model1_page *Model1_page)
{
 ({ union { typeof(Model1_page->Model1_compound_head) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(Model1_page->Model1_compound_head)) (0) }; Model1___write_once_size(&(Model1_page->Model1_compound_head), Model1___u.Model1___c, sizeof(Model1_page->Model1_compound_head)); Model1___u.Model1___val; });
}
int Model1_PageHuge(struct Model1_page *Model1_page);
int Model1_PageHeadHuge(struct Model1_page *Model1_page);
bool Model1_page_huge_active(struct Model1_page *Model1_page);
static inline __attribute__((no_instrument_function)) int Model1_PageTransHuge(const struct Model1_page *Model1_page) { return 0; }
static inline __attribute__((no_instrument_function)) int Model1_PageTransCompound(const struct Model1_page *Model1_page) { return 0; }
static inline __attribute__((no_instrument_function)) int Model1_PageTransCompoundMap(const struct Model1_page *Model1_page) { return 0; }
static inline __attribute__((no_instrument_function)) int Model1_PageTransTail(const struct Model1_page *Model1_page) { return 0; }
static inline __attribute__((no_instrument_function)) int Model1_PageDoubleMap(const struct Model1_page *Model1_page) { return 0; } static inline __attribute__((no_instrument_function)) void Model1_SetPageDoubleMap(struct Model1_page *Model1_page) { } static inline __attribute__((no_instrument_function)) void Model1_ClearPageDoubleMap(struct Model1_page *Model1_page) { }
 static inline __attribute__((no_instrument_function)) int Model1_TestSetPageDoubleMap(struct Model1_page *Model1_page) { return 0; }
 static inline __attribute__((no_instrument_function)) int Model1_TestClearPageDoubleMap(struct Model1_page *Model1_page) { return 0; }


/*
 * For pages that are never mapped to userspace, page->mapcount may be
 * used for storing extra information about page type. Any value used
 * for this purpose must be <= -2, but it's better start not too close
 * to -2 so that an underflow of the page_mapcount() won't be mistaken
 * for a special page.
 */
/*
 * PageBuddy() indicate that the page is free and in the buddy system
 * (see mm/page_alloc.c).
 */

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PageBuddy(struct Model1_page *Model1_page) { return Model1_atomic_read(&Model1_page->Model1__mapcount) == (-128); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___SetPageBuddy(struct Model1_page *Model1_page) { ((void)(sizeof(( long)(Model1_atomic_read(&Model1_page->Model1__mapcount) != -1)))); Model1_atomic_set(&Model1_page->Model1__mapcount, (-128)); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___ClearPageBuddy(struct Model1_page *Model1_page) { ((void)(sizeof(( long)(!Model1_PageBuddy(Model1_page))))); Model1_atomic_set(&Model1_page->Model1__mapcount, -1); }

/*
 * PageBalloon() is set on pages that are on the balloon page list
 * (see mm/balloon_compaction.c).
 */

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PageBalloon(struct Model1_page *Model1_page) { return Model1_atomic_read(&Model1_page->Model1__mapcount) == (-256); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___SetPageBalloon(struct Model1_page *Model1_page) { ((void)(sizeof(( long)(Model1_atomic_read(&Model1_page->Model1__mapcount) != -1)))); Model1_atomic_set(&Model1_page->Model1__mapcount, (-256)); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___ClearPageBalloon(struct Model1_page *Model1_page) { ((void)(sizeof(( long)(!Model1_PageBalloon(Model1_page))))); Model1_atomic_set(&Model1_page->Model1__mapcount, -1); }

/*
 * If kmemcg is enabled, the buddy allocator will set PageKmemcg() on
 * pages allocated with __GFP_ACCOUNT. It gets cleared on page free.
 */

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PageKmemcg(struct Model1_page *Model1_page) { return Model1_atomic_read(&Model1_page->Model1__mapcount) == (-512); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___SetPageKmemcg(struct Model1_page *Model1_page) { ((void)(sizeof(( long)(Model1_atomic_read(&Model1_page->Model1__mapcount) != -1)))); Model1_atomic_set(&Model1_page->Model1__mapcount, (-512)); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___ClearPageKmemcg(struct Model1_page *Model1_page) { ((void)(sizeof(( long)(!Model1_PageKmemcg(Model1_page))))); Model1_atomic_set(&Model1_page->Model1__mapcount, -1); }

extern bool Model1_is_free_buddy_page(struct Model1_page *Model1_page);

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_PageIsolated(struct Model1_page *Model1_page) { return (__builtin_constant_p((Model1_PG_isolated)) ? Model1_constant_test_bit((Model1_PG_isolated), (&Model1_page->Model1_flags)) : Model1_variable_test_bit((Model1_PG_isolated), (&Model1_page->Model1_flags))); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___SetPageIsolated(struct Model1_page *Model1_page) { Model1___set_bit(Model1_PG_isolated, &Model1_page->Model1_flags); } static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___ClearPageIsolated(struct Model1_page *Model1_page) { Model1___clear_bit(Model1_PG_isolated, &Model1_page->Model1_flags); };

/*
 * If network-based swap is enabled, sl*b must keep track of whether pages
 * were allocated from pfmemalloc reserves.
 */
static inline __attribute__((no_instrument_function)) int Model1_PageSlabPfmemalloc(struct Model1_page *Model1_page)
{
 ((void)(sizeof(( long)(!Model1_PageSlab(Model1_page)))));
 return Model1_PageActive(Model1_page);
}

static inline __attribute__((no_instrument_function)) void Model1_SetPageSlabPfmemalloc(struct Model1_page *Model1_page)
{
 ((void)(sizeof(( long)(!Model1_PageSlab(Model1_page)))));
 Model1_SetPageActive(Model1_page);
}

static inline __attribute__((no_instrument_function)) void Model1___ClearPageSlabPfmemalloc(struct Model1_page *Model1_page)
{
 ((void)(sizeof(( long)(!Model1_PageSlab(Model1_page)))));
 Model1___ClearPageActive(Model1_page);
}

static inline __attribute__((no_instrument_function)) void Model1_ClearPageSlabPfmemalloc(struct Model1_page *Model1_page)
{
 ((void)(sizeof(( long)(!Model1_PageSlab(Model1_page)))));
 Model1_ClearPageActive(Model1_page);
}







/*
 * Flags checked when a page is freed.  Pages being freed should not have
 * these flags set.  It they are, there is a problem.
 */







/*
 * Flags checked when a page is prepped for return by the page allocator.
 * Pages being prepped should not have these flags set.  It they are set,
 * there has been a kernel bug or struct page corruption.
 *
 * __PG_HWPOISON is exceptional because it needs to be kept beyond page's
 * alloc-free cycle to prevent from reusing the page.
 */





/**
 * page_has_private - Determine if page has private stuff
 * @page: The page to be checked
 *
 * Determine if a page has private stuff, indicating that release routines
 * should be invoked upon it.
 */
static inline __attribute__((no_instrument_function)) int Model1_page_has_private(struct Model1_page *Model1_page)
{
 return !!(Model1_page->Model1_flags & (1UL << Model1_PG_private | 1UL << Model1_PG_private_2));
}


extern struct Model1_tracepoint Model1___tracepoint_page_ref_set;
extern struct Model1_tracepoint Model1___tracepoint_page_ref_mod;
extern struct Model1_tracepoint Model1___tracepoint_page_ref_mod_and_test;
extern struct Model1_tracepoint Model1___tracepoint_page_ref_mod_and_return;
extern struct Model1_tracepoint Model1___tracepoint_page_ref_mod_unless;
extern struct Model1_tracepoint Model1___tracepoint_page_ref_freeze;
extern struct Model1_tracepoint Model1___tracepoint_page_ref_unfreeze;
static inline __attribute__((no_instrument_function)) void Model1___page_ref_set(struct Model1_page *Model1_page, int Model1_v)
{
}
static inline __attribute__((no_instrument_function)) void Model1___page_ref_mod(struct Model1_page *Model1_page, int Model1_v)
{
}
static inline __attribute__((no_instrument_function)) void Model1___page_ref_mod_and_test(struct Model1_page *Model1_page, int Model1_v, int Model1_ret)
{
}
static inline __attribute__((no_instrument_function)) void Model1___page_ref_mod_and_return(struct Model1_page *Model1_page, int Model1_v, int Model1_ret)
{
}
static inline __attribute__((no_instrument_function)) void Model1___page_ref_mod_unless(struct Model1_page *Model1_page, int Model1_v, int Model1_u)
{
}
static inline __attribute__((no_instrument_function)) void Model1___page_ref_freeze(struct Model1_page *Model1_page, int Model1_v, int Model1_ret)
{
}
static inline __attribute__((no_instrument_function)) void Model1___page_ref_unfreeze(struct Model1_page *Model1_page, int Model1_v)
{
}



static inline __attribute__((no_instrument_function)) int Model1_page_ref_count(struct Model1_page *Model1_page)
{
 return Model1_atomic_read(&Model1_page->Model1__refcount);
}

static inline __attribute__((no_instrument_function)) int Model1_page_count(struct Model1_page *Model1_page)
{
 return Model1_atomic_read(&Model1_compound_head(Model1_page)->Model1__refcount);
}

static inline __attribute__((no_instrument_function)) void Model1_set_page_count(struct Model1_page *Model1_page, int Model1_v)
{
 Model1_atomic_set(&Model1_page->Model1__refcount, Model1_v);
 if (false)
  Model1___page_ref_set(Model1_page, Model1_v);
}

/*
 * Setup the page count before being freed into the page allocator for
 * the first time (boot or memory hotplug)
 */
static inline __attribute__((no_instrument_function)) void Model1_init_page_count(struct Model1_page *Model1_page)
{
 Model1_set_page_count(Model1_page, 1);
}

static inline __attribute__((no_instrument_function)) void Model1_page_ref_add(struct Model1_page *Model1_page, int Model1_nr)
{
 Model1_atomic_add(Model1_nr, &Model1_page->Model1__refcount);
 if (false)
  Model1___page_ref_mod(Model1_page, Model1_nr);
}

static inline __attribute__((no_instrument_function)) void Model1_page_ref_sub(struct Model1_page *Model1_page, int Model1_nr)
{
 Model1_atomic_sub(Model1_nr, &Model1_page->Model1__refcount);
 if (false)
  Model1___page_ref_mod(Model1_page, -Model1_nr);
}

static inline __attribute__((no_instrument_function)) void Model1_page_ref_inc(struct Model1_page *Model1_page)
{
 Model1_atomic_inc(&Model1_page->Model1__refcount);
 if (false)
  Model1___page_ref_mod(Model1_page, 1);
}

static inline __attribute__((no_instrument_function)) void Model1_page_ref_dec(struct Model1_page *Model1_page)
{
 Model1_atomic_dec(&Model1_page->Model1__refcount);
 if (false)
  Model1___page_ref_mod(Model1_page, -1);
}

static inline __attribute__((no_instrument_function)) int Model1_page_ref_sub_and_test(struct Model1_page *Model1_page, int Model1_nr)
{
 int Model1_ret = Model1_atomic_sub_and_test(Model1_nr, &Model1_page->Model1__refcount);

 if (false)
  Model1___page_ref_mod_and_test(Model1_page, -Model1_nr, Model1_ret);
 return Model1_ret;
}

static inline __attribute__((no_instrument_function)) int Model1_page_ref_inc_return(struct Model1_page *Model1_page)
{
 int Model1_ret = (Model1_atomic_add_return(1, &Model1_page->Model1__refcount));

 if (false)
  Model1___page_ref_mod_and_return(Model1_page, 1, Model1_ret);
 return Model1_ret;
}

static inline __attribute__((no_instrument_function)) int Model1_page_ref_dec_and_test(struct Model1_page *Model1_page)
{
 int Model1_ret = Model1_atomic_dec_and_test(&Model1_page->Model1__refcount);

 if (false)
  Model1___page_ref_mod_and_test(Model1_page, -1, Model1_ret);
 return Model1_ret;
}

static inline __attribute__((no_instrument_function)) int Model1_page_ref_dec_return(struct Model1_page *Model1_page)
{
 int Model1_ret = (Model1_atomic_sub_return(1, &Model1_page->Model1__refcount));

 if (false)
  Model1___page_ref_mod_and_return(Model1_page, -1, Model1_ret);
 return Model1_ret;
}

static inline __attribute__((no_instrument_function)) int Model1_page_ref_add_unless(struct Model1_page *Model1_page, int Model1_nr, int Model1_u)
{
 int Model1_ret = Model1_atomic_add_unless(&Model1_page->Model1__refcount, Model1_nr, Model1_u);

 if (false)
  Model1___page_ref_mod_unless(Model1_page, Model1_nr, Model1_ret);
 return Model1_ret;
}

static inline __attribute__((no_instrument_function)) int Model1_page_ref_freeze(struct Model1_page *Model1_page, int Model1_count)
{
 int Model1_ret = __builtin_expect(!!(Model1_atomic_cmpxchg(&Model1_page->Model1__refcount, Model1_count, 0) == Model1_count), 1);

 if (false)
  Model1___page_ref_freeze(Model1_page, Model1_count, Model1_ret);
 return Model1_ret;
}

static inline __attribute__((no_instrument_function)) void Model1_page_ref_unfreeze(struct Model1_page *Model1_page, int Model1_count)
{
 ((void)(sizeof(( long)(Model1_page_count(Model1_page) != 0))));
 ((void)(sizeof(( long)(Model1_count == 0))));

 Model1_atomic_set(&Model1_page->Model1__refcount, Model1_count);
 if (false)
  Model1___page_ref_unfreeze(Model1_page, Model1_count);
}

struct Model1_mempolicy;
struct Model1_anon_vma;
struct Model1_anon_vma_chain;
struct Model1_file_ra_state;
struct Model1_user_struct;
struct Model1_writeback_control;
struct Model1_bdi_writeback;
static inline __attribute__((no_instrument_function)) void Model1_set_max_mapnr(unsigned long Model1_limit) { }


extern unsigned long Model1_totalram_pages;
extern void * Model1_high_memory;
extern int Model1_page_cluster;


extern int Model1_sysctl_legacy_va_layout;





extern const int Model1_mmap_rnd_bits_min;
extern const int Model1_mmap_rnd_bits_max;
extern int Model1_mmap_rnd_bits __attribute__((__section__(".data..read_mostly")));


extern const int Model1_mmap_rnd_compat_bits_min;
extern const int Model1_mmap_rnd_compat_bits_max;
extern int Model1_mmap_rnd_compat_bits __attribute__((__section__(".data..read_mostly")));












/*
 * Macro to mark a page protection value as UC-
 */
void Model1_ptdump_walk_pgd_level(struct Model1_seq_file *Model1_m, Model1_pgd_t *Model1_pgd);
void Model1_ptdump_walk_pgd_level_checkwx(void);







/*
 * ZERO_PAGE is a global shared page that is always zero: used
 * for zero-mapped memory areas etc..
 */
extern unsigned long Model1_empty_zero_page[((1UL) << 12) / sizeof(unsigned long)]
          ;


extern Model1_spinlock_t Model1_pgd_lock;
extern struct Model1_list_head Model1_pgd_list;

extern struct Model1_mm_struct *Model1_pgd_page_get_mm(struct Model1_page *Model1_page);
/*
 * The following only work if pte_present() is true.
 * Undefined behaviour if not..
 */
static inline __attribute__((no_instrument_function)) int Model1_pte_dirty(Model1_pte_t Model1_pte)
{
 return Model1_pte_flags(Model1_pte) & (((Model1_pteval_t)(1)) << 6);
}


static inline __attribute__((no_instrument_function)) Model1_u32 Model1_read_pkru(void)
{
 if ((__builtin_constant_p((16*32+ 4)) && ( ((((16*32+ 4))>>5)==(0) && (1UL<<(((16*32+ 4))&31) & ((1<<(( 0*32+ 0) & 31))|(1<<(( 0*32+ 3)) & 31)|(1<<(( 0*32+ 5) & 31))|(1<<(( 0*32+ 6) & 31))| (1<<(( 0*32+ 8) & 31))|(1<<(( 0*32+13)) & 31)|(1<<(( 0*32+24) & 31))|(1<<(( 0*32+15) & 31))| (1<<(( 0*32+25) & 31))|(1<<(( 0*32+26) & 31))) )) || ((((16*32+ 4))>>5)==(1) && (1UL<<(((16*32+ 4))&31) & ((1<<(( 1*32+29) & 31))|0) )) || ((((16*32+ 4))>>5)==(2) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(3) && (1UL<<(((16*32+ 4))&31) & ((1<<(( 3*32+20) & 31))) )) || ((((16*32+ 4))>>5)==(4) && (1UL<<(((16*32+ 4))&31) & (0) )) || ((((16*32+ 4))>>5)==(5) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(6) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(7) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(8) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(9) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(10) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(11) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(12) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(13) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(14) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(15) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(16) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(17) && (1UL<<(((16*32+ 4))&31) & 0 )) || (sizeof(struct { int:-!!(18 != 18); })) || (sizeof(struct { int:-!!(18 != 18); }))) ? 1 : (__builtin_constant_p(((16*32+ 4))) ? Model1_constant_test_bit(((16*32+ 4)), ((unsigned long *)((&Model1_boot_cpu_data)->Model1_x86_capability))) : Model1_variable_test_bit(((16*32+ 4)), ((unsigned long *)((&Model1_boot_cpu_data)->Model1_x86_capability))))))
  return Model1___read_pkru();
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model1_write_pkru(Model1_u32 Model1_pkru)
{
 if ((__builtin_constant_p((16*32+ 4)) && ( ((((16*32+ 4))>>5)==(0) && (1UL<<(((16*32+ 4))&31) & ((1<<(( 0*32+ 0) & 31))|(1<<(( 0*32+ 3)) & 31)|(1<<(( 0*32+ 5) & 31))|(1<<(( 0*32+ 6) & 31))| (1<<(( 0*32+ 8) & 31))|(1<<(( 0*32+13)) & 31)|(1<<(( 0*32+24) & 31))|(1<<(( 0*32+15) & 31))| (1<<(( 0*32+25) & 31))|(1<<(( 0*32+26) & 31))) )) || ((((16*32+ 4))>>5)==(1) && (1UL<<(((16*32+ 4))&31) & ((1<<(( 1*32+29) & 31))|0) )) || ((((16*32+ 4))>>5)==(2) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(3) && (1UL<<(((16*32+ 4))&31) & ((1<<(( 3*32+20) & 31))) )) || ((((16*32+ 4))>>5)==(4) && (1UL<<(((16*32+ 4))&31) & (0) )) || ((((16*32+ 4))>>5)==(5) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(6) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(7) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(8) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(9) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(10) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(11) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(12) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(13) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(14) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(15) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(16) && (1UL<<(((16*32+ 4))&31) & 0 )) || ((((16*32+ 4))>>5)==(17) && (1UL<<(((16*32+ 4))&31) & 0 )) || (sizeof(struct { int:-!!(18 != 18); })) || (sizeof(struct { int:-!!(18 != 18); }))) ? 1 : (__builtin_constant_p(((16*32+ 4))) ? Model1_constant_test_bit(((16*32+ 4)), ((unsigned long *)((&Model1_boot_cpu_data)->Model1_x86_capability))) : Model1_variable_test_bit(((16*32+ 4)), ((unsigned long *)((&Model1_boot_cpu_data)->Model1_x86_capability))))))
  Model1___write_pkru(Model1_pkru);
}

static inline __attribute__((no_instrument_function)) int Model1_pte_young(Model1_pte_t Model1_pte)
{
 return Model1_pte_flags(Model1_pte) & (((Model1_pteval_t)(1)) << 5);
}

static inline __attribute__((no_instrument_function)) int Model1_pmd_dirty(Model1_pmd_t Model1_pmd)
{
 return Model1_pmd_flags(Model1_pmd) & (((Model1_pteval_t)(1)) << 6);
}

static inline __attribute__((no_instrument_function)) int Model1_pmd_young(Model1_pmd_t Model1_pmd)
{
 return Model1_pmd_flags(Model1_pmd) & (((Model1_pteval_t)(1)) << 5);
}

static inline __attribute__((no_instrument_function)) int Model1_pte_write(Model1_pte_t Model1_pte)
{
 return Model1_pte_flags(Model1_pte) & (((Model1_pteval_t)(1)) << 1);
}

static inline __attribute__((no_instrument_function)) int Model1_pte_huge(Model1_pte_t Model1_pte)
{
 return Model1_pte_flags(Model1_pte) & (((Model1_pteval_t)(1)) << 7);
}

static inline __attribute__((no_instrument_function)) int Model1_pte_global(Model1_pte_t Model1_pte)
{
 return Model1_pte_flags(Model1_pte) & (((Model1_pteval_t)(1)) << 8);
}

static inline __attribute__((no_instrument_function)) int Model1_pte_exec(Model1_pte_t Model1_pte)
{
 return !(Model1_pte_flags(Model1_pte) & (((Model1_pteval_t)(1)) << 63));
}

static inline __attribute__((no_instrument_function)) int Model1_pte_special(Model1_pte_t Model1_pte)
{
 return Model1_pte_flags(Model1_pte) & (((Model1_pteval_t)(1)) << 9);
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_pte_pfn(Model1_pte_t Model1_pte)
{
 return (Model1_native_pte_val(Model1_pte) & ((Model1_pteval_t)(((signed long)(~(((1UL) << 12)-1))) & ((Model1_phys_addr_t)((1ULL << 46) - 1))))) >> 12;
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_pmd_pfn(Model1_pmd_t Model1_pmd)
{
 return (Model1_native_pmd_val(Model1_pmd) & Model1_pmd_pfn_mask(Model1_pmd)) >> 12;
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_pud_pfn(Model1_pud_t Model1_pud)
{
 return (Model1_native_pud_val(Model1_pud) & Model1_pud_pfn_mask(Model1_pud)) >> 12;
}



static inline __attribute__((no_instrument_function)) int Model1_pmd_large(Model1_pmd_t Model1_pte)
{
 return Model1_pmd_flags(Model1_pte) & (((Model1_pteval_t)(1)) << 7);
}
static inline __attribute__((no_instrument_function)) Model1_pte_t Model1_pte_set_flags(Model1_pte_t Model1_pte, Model1_pteval_t Model1_set)
{
 Model1_pteval_t Model1_v = Model1_native_pte_val(Model1_pte);

 return Model1_native_make_pte(Model1_v | Model1_set);
}

static inline __attribute__((no_instrument_function)) Model1_pte_t Model1_pte_clear_flags(Model1_pte_t Model1_pte, Model1_pteval_t Model1_clear)
{
 Model1_pteval_t Model1_v = Model1_native_pte_val(Model1_pte);

 return Model1_native_make_pte(Model1_v & ~Model1_clear);
}

static inline __attribute__((no_instrument_function)) Model1_pte_t Model1_pte_mkclean(Model1_pte_t Model1_pte)
{
 return Model1_pte_clear_flags(Model1_pte, (((Model1_pteval_t)(1)) << 6));
}

static inline __attribute__((no_instrument_function)) Model1_pte_t Model1_pte_mkold(Model1_pte_t Model1_pte)
{
 return Model1_pte_clear_flags(Model1_pte, (((Model1_pteval_t)(1)) << 5));
}

static inline __attribute__((no_instrument_function)) Model1_pte_t Model1_pte_wrprotect(Model1_pte_t Model1_pte)
{
 return Model1_pte_clear_flags(Model1_pte, (((Model1_pteval_t)(1)) << 1));
}

static inline __attribute__((no_instrument_function)) Model1_pte_t Model1_pte_mkexec(Model1_pte_t Model1_pte)
{
 return Model1_pte_clear_flags(Model1_pte, (((Model1_pteval_t)(1)) << 63));
}

static inline __attribute__((no_instrument_function)) Model1_pte_t Model1_pte_mkdirty(Model1_pte_t Model1_pte)
{
 return Model1_pte_set_flags(Model1_pte, (((Model1_pteval_t)(1)) << 6) | (((Model1_pteval_t)(0))));
}

static inline __attribute__((no_instrument_function)) Model1_pte_t Model1_pte_mkyoung(Model1_pte_t Model1_pte)
{
 return Model1_pte_set_flags(Model1_pte, (((Model1_pteval_t)(1)) << 5));
}

static inline __attribute__((no_instrument_function)) Model1_pte_t Model1_pte_mkwrite(Model1_pte_t Model1_pte)
{
 return Model1_pte_set_flags(Model1_pte, (((Model1_pteval_t)(1)) << 1));
}

static inline __attribute__((no_instrument_function)) Model1_pte_t Model1_pte_mkhuge(Model1_pte_t Model1_pte)
{
 return Model1_pte_set_flags(Model1_pte, (((Model1_pteval_t)(1)) << 7));
}

static inline __attribute__((no_instrument_function)) Model1_pte_t Model1_pte_clrhuge(Model1_pte_t Model1_pte)
{
 return Model1_pte_clear_flags(Model1_pte, (((Model1_pteval_t)(1)) << 7));
}

static inline __attribute__((no_instrument_function)) Model1_pte_t Model1_pte_mkglobal(Model1_pte_t Model1_pte)
{
 return Model1_pte_set_flags(Model1_pte, (((Model1_pteval_t)(1)) << 8));
}

static inline __attribute__((no_instrument_function)) Model1_pte_t Model1_pte_clrglobal(Model1_pte_t Model1_pte)
{
 return Model1_pte_clear_flags(Model1_pte, (((Model1_pteval_t)(1)) << 8));
}

static inline __attribute__((no_instrument_function)) Model1_pte_t Model1_pte_mkspecial(Model1_pte_t Model1_pte)
{
 return Model1_pte_set_flags(Model1_pte, (((Model1_pteval_t)(1)) << 9));
}

static inline __attribute__((no_instrument_function)) Model1_pte_t Model1_pte_mkdevmap(Model1_pte_t Model1_pte)
{
 return Model1_pte_set_flags(Model1_pte, (((Model1_pteval_t)(1)) << 9)|(((Model1_u64)(1)) << 58));
}

static inline __attribute__((no_instrument_function)) Model1_pmd_t Model1_pmd_set_flags(Model1_pmd_t Model1_pmd, Model1_pmdval_t Model1_set)
{
 Model1_pmdval_t Model1_v = Model1_native_pmd_val(Model1_pmd);

 return Model1_native_make_pmd(Model1_v | Model1_set);
}

static inline __attribute__((no_instrument_function)) Model1_pmd_t Model1_pmd_clear_flags(Model1_pmd_t Model1_pmd, Model1_pmdval_t Model1_clear)
{
 Model1_pmdval_t Model1_v = Model1_native_pmd_val(Model1_pmd);

 return Model1_native_make_pmd(Model1_v & ~Model1_clear);
}

static inline __attribute__((no_instrument_function)) Model1_pmd_t Model1_pmd_mkold(Model1_pmd_t Model1_pmd)
{
 return Model1_pmd_clear_flags(Model1_pmd, (((Model1_pteval_t)(1)) << 5));
}

static inline __attribute__((no_instrument_function)) Model1_pmd_t Model1_pmd_mkclean(Model1_pmd_t Model1_pmd)
{
 return Model1_pmd_clear_flags(Model1_pmd, (((Model1_pteval_t)(1)) << 6));
}

static inline __attribute__((no_instrument_function)) Model1_pmd_t Model1_pmd_wrprotect(Model1_pmd_t Model1_pmd)
{
 return Model1_pmd_clear_flags(Model1_pmd, (((Model1_pteval_t)(1)) << 1));
}

static inline __attribute__((no_instrument_function)) Model1_pmd_t Model1_pmd_mkdirty(Model1_pmd_t Model1_pmd)
{
 return Model1_pmd_set_flags(Model1_pmd, (((Model1_pteval_t)(1)) << 6) | (((Model1_pteval_t)(0))));
}

static inline __attribute__((no_instrument_function)) Model1_pmd_t Model1_pmd_mkdevmap(Model1_pmd_t Model1_pmd)
{
 return Model1_pmd_set_flags(Model1_pmd, (((Model1_u64)(1)) << 58));
}

static inline __attribute__((no_instrument_function)) Model1_pmd_t Model1_pmd_mkhuge(Model1_pmd_t Model1_pmd)
{
 return Model1_pmd_set_flags(Model1_pmd, (((Model1_pteval_t)(1)) << 7));
}

static inline __attribute__((no_instrument_function)) Model1_pmd_t Model1_pmd_mkyoung(Model1_pmd_t Model1_pmd)
{
 return Model1_pmd_set_flags(Model1_pmd, (((Model1_pteval_t)(1)) << 5));
}

static inline __attribute__((no_instrument_function)) Model1_pmd_t Model1_pmd_mkwrite(Model1_pmd_t Model1_pmd)
{
 return Model1_pmd_set_flags(Model1_pmd, (((Model1_pteval_t)(1)) << 1));
}

static inline __attribute__((no_instrument_function)) Model1_pmd_t Model1_pmd_mknotpresent(Model1_pmd_t Model1_pmd)
{
 return Model1_pmd_clear_flags(Model1_pmd, (((Model1_pteval_t)(1)) << 0) | (((Model1_pteval_t)(1)) << 8));
}


static inline __attribute__((no_instrument_function)) int Model1_pte_soft_dirty(Model1_pte_t Model1_pte)
{
 return Model1_pte_flags(Model1_pte) & (((Model1_pteval_t)(0)));
}

static inline __attribute__((no_instrument_function)) int Model1_pmd_soft_dirty(Model1_pmd_t Model1_pmd)
{
 return Model1_pmd_flags(Model1_pmd) & (((Model1_pteval_t)(0)));
}

static inline __attribute__((no_instrument_function)) Model1_pte_t Model1_pte_mksoft_dirty(Model1_pte_t Model1_pte)
{
 return Model1_pte_set_flags(Model1_pte, (((Model1_pteval_t)(0))));
}

static inline __attribute__((no_instrument_function)) Model1_pmd_t Model1_pmd_mksoft_dirty(Model1_pmd_t Model1_pmd)
{
 return Model1_pmd_set_flags(Model1_pmd, (((Model1_pteval_t)(0))));
}

static inline __attribute__((no_instrument_function)) Model1_pte_t Model1_pte_clear_soft_dirty(Model1_pte_t Model1_pte)
{
 return Model1_pte_clear_flags(Model1_pte, (((Model1_pteval_t)(0))));
}

static inline __attribute__((no_instrument_function)) Model1_pmd_t Model1_pmd_clear_soft_dirty(Model1_pmd_t Model1_pmd)
{
 return Model1_pmd_clear_flags(Model1_pmd, (((Model1_pteval_t)(0))));
}



/*
 * Mask out unsupported bits in a present pgprot.  Non-present pgprots
 * can use those bits for other purposes, so leave them be.
 */
static inline __attribute__((no_instrument_function)) Model1_pgprotval_t Model1_massage_pgprot(Model1_pgprot_t Model1_pgprot)
{
 Model1_pgprotval_t Model1_protval = ((Model1_pgprot).Model1_pgprot);

 if (Model1_protval & (((Model1_pteval_t)(1)) << 0))
  Model1_protval &= Model1___supported_pte_mask;

 return Model1_protval;
}

static inline __attribute__((no_instrument_function)) Model1_pte_t Model1_pfn_pte(unsigned long Model1_page_nr, Model1_pgprot_t Model1_pgprot)
{
 return Model1_native_make_pte(((Model1_phys_addr_t)Model1_page_nr << 12) | Model1_massage_pgprot(Model1_pgprot));

}

static inline __attribute__((no_instrument_function)) Model1_pmd_t Model1_pfn_pmd(unsigned long Model1_page_nr, Model1_pgprot_t Model1_pgprot)
{
 return Model1_native_make_pmd(((Model1_phys_addr_t)Model1_page_nr << 12) | Model1_massage_pgprot(Model1_pgprot));

}

static inline __attribute__((no_instrument_function)) Model1_pte_t Model1_pte_modify(Model1_pte_t Model1_pte, Model1_pgprot_t Model1_newprot)
{
 Model1_pteval_t Model1_val = Model1_native_pte_val(Model1_pte);

 /*
	 * Chop off the NX bit (if present), and add the NX portion of
	 * the newprot (if present):
	 */
 Model1_val &= (((Model1_pteval_t)(((signed long)(~(((1UL) << 12)-1))) & ((Model1_phys_addr_t)((1ULL << 46) - 1)))) | (((Model1_pteval_t)(1)) << 4) | (((Model1_pteval_t)(1)) << 3) | (((Model1_pteval_t)(1)) << 9) | (((Model1_pteval_t)(1)) << 5) | (((Model1_pteval_t)(1)) << 6) | (((Model1_pteval_t)(0))));
 Model1_val |= Model1_massage_pgprot(Model1_newprot) & ~(((Model1_pteval_t)(((signed long)(~(((1UL) << 12)-1))) & ((Model1_phys_addr_t)((1ULL << 46) - 1)))) | (((Model1_pteval_t)(1)) << 4) | (((Model1_pteval_t)(1)) << 3) | (((Model1_pteval_t)(1)) << 9) | (((Model1_pteval_t)(1)) << 5) | (((Model1_pteval_t)(1)) << 6) | (((Model1_pteval_t)(0))));

 return Model1_native_make_pte(Model1_val);
}

static inline __attribute__((no_instrument_function)) Model1_pmd_t Model1_pmd_modify(Model1_pmd_t Model1_pmd, Model1_pgprot_t Model1_newprot)
{
 Model1_pmdval_t Model1_val = Model1_native_pmd_val(Model1_pmd);

 Model1_val &= ((((Model1_pteval_t)(((signed long)(~(((1UL) << 12)-1))) & ((Model1_phys_addr_t)((1ULL << 46) - 1)))) | (((Model1_pteval_t)(1)) << 4) | (((Model1_pteval_t)(1)) << 3) | (((Model1_pteval_t)(1)) << 9) | (((Model1_pteval_t)(1)) << 5) | (((Model1_pteval_t)(1)) << 6) | (((Model1_pteval_t)(0)))) | (((Model1_pteval_t)(1)) << 7));
 Model1_val |= Model1_massage_pgprot(Model1_newprot) & ~((((Model1_pteval_t)(((signed long)(~(((1UL) << 12)-1))) & ((Model1_phys_addr_t)((1ULL << 46) - 1)))) | (((Model1_pteval_t)(1)) << 4) | (((Model1_pteval_t)(1)) << 3) | (((Model1_pteval_t)(1)) << 9) | (((Model1_pteval_t)(1)) << 5) | (((Model1_pteval_t)(1)) << 6) | (((Model1_pteval_t)(0)))) | (((Model1_pteval_t)(1)) << 7));

 return Model1_native_make_pmd(Model1_val);
}

/* mprotect needs to preserve PAT bits when updating vm_page_prot */

static inline __attribute__((no_instrument_function)) Model1_pgprot_t Model1_pgprot_modify(Model1_pgprot_t Model1_oldprot, Model1_pgprot_t Model1_newprot)
{
 Model1_pgprotval_t Model1_preservebits = ((Model1_oldprot).Model1_pgprot) & (((Model1_pteval_t)(((signed long)(~(((1UL) << 12)-1))) & ((Model1_phys_addr_t)((1ULL << 46) - 1)))) | (((Model1_pteval_t)(1)) << 4) | (((Model1_pteval_t)(1)) << 3) | (((Model1_pteval_t)(1)) << 9) | (((Model1_pteval_t)(1)) << 5) | (((Model1_pteval_t)(1)) << 6) | (((Model1_pteval_t)(0))));
 Model1_pgprotval_t Model1_addbits = ((Model1_newprot).Model1_pgprot);
 return ((Model1_pgprot_t) { (Model1_preservebits | Model1_addbits) } );
}







static inline __attribute__((no_instrument_function)) int Model1_is_new_memtype_allowed(Model1_u64 Model1_paddr, unsigned long Model1_size,
      enum Model1_page_cache_mode Model1_pcm,
      enum Model1_page_cache_mode Model1_new_pcm)
{
 /*
	 * PAT type is always WB for untracked ranges, so no need to check.
	 */
 if (Model1_x86_platform.Model1_is_untracked_pat_range(Model1_paddr, Model1_paddr + Model1_size))
  return 1;

 /*
	 * Certain new memtypes are not allowed with certain
	 * requested memtype:
	 * - request is uncached, return cannot be write-back
	 * - request is write-combine, return cannot be write-back
	 * - request is write-through, return cannot be write-back
	 * - request is write-through, return cannot be write-combine
	 */
 if ((Model1_pcm == Model1__PAGE_CACHE_MODE_UC_MINUS &&
      Model1_new_pcm == Model1__PAGE_CACHE_MODE_WB) ||
     (Model1_pcm == Model1__PAGE_CACHE_MODE_WC &&
      Model1_new_pcm == Model1__PAGE_CACHE_MODE_WB) ||
     (Model1_pcm == Model1__PAGE_CACHE_MODE_WT &&
      Model1_new_pcm == Model1__PAGE_CACHE_MODE_WB) ||
     (Model1_pcm == Model1__PAGE_CACHE_MODE_WT &&
      Model1_new_pcm == Model1__PAGE_CACHE_MODE_WC)) {
  return 0;
 }

 return 1;
}

Model1_pmd_t *Model1_populate_extra_pmd(unsigned long Model1_vaddr);
Model1_pte_t *Model1_populate_extra_pte(unsigned long Model1_vaddr);














/*
 * This file contains the functions and defines necessary to modify and use
 * the x86-64 page table tree.
 */




extern Model1_pud_t Model1_level3_kernel_pgt[512];
extern Model1_pud_t Model1_level3_ident_pgt[512];
extern Model1_pmd_t Model1_level2_kernel_pgt[512];
extern Model1_pmd_t Model1_level2_fixmap_pgt[512];
extern Model1_pmd_t Model1_level2_ident_pgt[512];
extern Model1_pte_t Model1_level1_fixmap_pgt[512];
extern Model1_pgd_t Model1_init_level4_pgt[];



extern void Model1_paging_init(void);
struct Model1_mm_struct;

void Model1_set_pte_vaddr_pud(Model1_pud_t *Model1_pud_page, unsigned long Model1_vaddr, Model1_pte_t Model1_new_pte);


static inline __attribute__((no_instrument_function)) void Model1_native_pte_clear(struct Model1_mm_struct *Model1_mm, unsigned long Model1_addr,
        Model1_pte_t *Model1_ptep)
{
 *Model1_ptep = Model1_native_make_pte(0);
}

static inline __attribute__((no_instrument_function)) void Model1_native_set_pte(Model1_pte_t *Model1_ptep, Model1_pte_t Model1_pte)
{
 *Model1_ptep = Model1_pte;
}

static inline __attribute__((no_instrument_function)) void Model1_native_set_pte_atomic(Model1_pte_t *Model1_ptep, Model1_pte_t Model1_pte)
{
 Model1_native_set_pte(Model1_ptep, Model1_pte);
}

static inline __attribute__((no_instrument_function)) void Model1_native_set_pmd(Model1_pmd_t *Model1_pmdp, Model1_pmd_t Model1_pmd)
{
 *Model1_pmdp = Model1_pmd;
}

static inline __attribute__((no_instrument_function)) void Model1_native_pmd_clear(Model1_pmd_t *Model1_pmd)
{
 Model1_native_set_pmd(Model1_pmd, Model1_native_make_pmd(0));
}

static inline __attribute__((no_instrument_function)) Model1_pte_t Model1_native_ptep_get_and_clear(Model1_pte_t *Model1_xp)
{

 return Model1_native_make_pte(({ __typeof__ (*((&Model1_xp->Model1_pte))) Model1___ret = ((0)); switch (sizeof(*((&Model1_xp->Model1_pte)))) { case 1: asm volatile ("" "xchg" "b %b0, %1\n" : "+q" (Model1___ret), "+m" (*((&Model1_xp->Model1_pte))) : : "memory", "cc"); break; case 2: asm volatile ("" "xchg" "w %w0, %1\n" : "+r" (Model1___ret), "+m" (*((&Model1_xp->Model1_pte))) : : "memory", "cc"); break; case 4: asm volatile ("" "xchg" "l %0, %1\n" : "+r" (Model1___ret), "+m" (*((&Model1_xp->Model1_pte))) : : "memory", "cc"); break; case 8: asm volatile ("" "xchg" "q %q0, %1\n" : "+r" (Model1___ret), "+m" (*((&Model1_xp->Model1_pte))) : : "memory", "cc"); break; default: Model1___xchg_wrong_size(); } Model1___ret; }));







}

static inline __attribute__((no_instrument_function)) Model1_pmd_t Model1_native_pmdp_get_and_clear(Model1_pmd_t *Model1_xp)
{

 return Model1_native_make_pmd(({ __typeof__ (*((&Model1_xp->Model1_pmd))) Model1___ret = ((0)); switch (sizeof(*((&Model1_xp->Model1_pmd)))) { case 1: asm volatile ("" "xchg" "b %b0, %1\n" : "+q" (Model1___ret), "+m" (*((&Model1_xp->Model1_pmd))) : : "memory", "cc"); break; case 2: asm volatile ("" "xchg" "w %w0, %1\n" : "+r" (Model1___ret), "+m" (*((&Model1_xp->Model1_pmd))) : : "memory", "cc"); break; case 4: asm volatile ("" "xchg" "l %0, %1\n" : "+r" (Model1___ret), "+m" (*((&Model1_xp->Model1_pmd))) : : "memory", "cc"); break; case 8: asm volatile ("" "xchg" "q %q0, %1\n" : "+r" (Model1___ret), "+m" (*((&Model1_xp->Model1_pmd))) : : "memory", "cc"); break; default: Model1___xchg_wrong_size(); } Model1___ret; }));







}

static inline __attribute__((no_instrument_function)) void Model1_native_set_pud(Model1_pud_t *Model1_pudp, Model1_pud_t Model1_pud)
{
 *Model1_pudp = Model1_pud;
}

static inline __attribute__((no_instrument_function)) void Model1_native_pud_clear(Model1_pud_t *Model1_pud)
{
 Model1_native_set_pud(Model1_pud, Model1_native_make_pud(0));
}

static inline __attribute__((no_instrument_function)) void Model1_native_set_pgd(Model1_pgd_t *Model1_pgdp, Model1_pgd_t Model1_pgd)
{
 *Model1_pgdp = Model1_pgd;
}

static inline __attribute__((no_instrument_function)) void Model1_native_pgd_clear(Model1_pgd_t *Model1_pgd)
{
 Model1_native_set_pgd(Model1_pgd, Model1_native_make_pgd(0));
}

extern void Model1_sync_global_pgds(unsigned long Model1_start, unsigned long Model1_end,
        int Model1_removed);

/*
 * Conversion functions: convert a page and protection to a page entry,
 * and a page entry and page directory to the page they refer to.
 */

/*
 * Level 4 access.
 */
static inline __attribute__((no_instrument_function)) int Model1_pgd_large(Model1_pgd_t Model1_pgd) { return 0; }


/* PUD - Level3 access */

/* PMD  - Level 2 access */

/* PTE - Level 1 access. */

/* x86-64 always has all page tables mapped. */



/*
 * Encode and de-code a swap entry
 *
 * |     ...            | 11| 10|  9|8|7|6|5| 4| 3|2|1|0| <- bit number
 * |     ...            |SW3|SW2|SW1|G|L|D|A|CD|WT|U|W|P| <- bit names
 * | OFFSET (14->63) | TYPE (9-13)  |0|X|X|X| X| X|X|X|0| <- swp entry
 *
 * G (8) is aliased and used as a PROT_NONE indicator for
 * !present ptes.  We need to start storing swap entries above
 * there.  We also need to avoid using A and D because of an
 * erratum where they can be incorrectly set by hardware on
 * non-present PTEs.
 */


/* Place the offset above the type: */
extern int Model1_kern_addr_valid(unsigned long Model1_addr);
extern void Model1_cleanup_highmap(void);
/* fs/proc/kcore.c */







extern void Model1_init_extra_mapping_uc(unsigned long Model1_phys, unsigned long Model1_size);
extern void Model1_init_extra_mapping_wb(unsigned long Model1_phys, unsigned long Model1_size);







static inline __attribute__((no_instrument_function)) int Model1_pte_none(Model1_pte_t Model1_pte)
{
 return !(Model1_pte.Model1_pte & ~(((((Model1_pteval_t)(1)) << 6) | (((Model1_pteval_t)(1)) << 5))));
}


static inline __attribute__((no_instrument_function)) int Model1_pte_same(Model1_pte_t Model1_a, Model1_pte_t Model1_b)
{
 return Model1_a.Model1_pte == Model1_b.Model1_pte;
}

static inline __attribute__((no_instrument_function)) int Model1_pte_present(Model1_pte_t Model1_a)
{
 return Model1_pte_flags(Model1_a) & ((((Model1_pteval_t)(1)) << 0) | (((Model1_pteval_t)(1)) << 8));
}


static inline __attribute__((no_instrument_function)) int Model1_pte_devmap(Model1_pte_t Model1_a)
{
 return (Model1_pte_flags(Model1_a) & (((Model1_u64)(1)) << 58)) == (((Model1_u64)(1)) << 58);
}



static inline __attribute__((no_instrument_function)) bool Model1_pte_accessible(struct Model1_mm_struct *Model1_mm, Model1_pte_t Model1_a)
{
 if (Model1_pte_flags(Model1_a) & (((Model1_pteval_t)(1)) << 0))
  return true;

 if ((Model1_pte_flags(Model1_a) & (((Model1_pteval_t)(1)) << 8)) &&
   Model1_mm_tlb_flush_pending(Model1_mm))
  return true;

 return false;
}

static inline __attribute__((no_instrument_function)) int Model1_pte_hidden(Model1_pte_t Model1_pte)
{
 return Model1_pte_flags(Model1_pte) & (((Model1_pteval_t)(0)));
}

static inline __attribute__((no_instrument_function)) int Model1_pmd_present(Model1_pmd_t Model1_pmd)
{
 /*
	 * Checking for _PAGE_PSE is needed too because
	 * split_huge_page will temporarily clear the present bit (but
	 * the _PAGE_PSE flag will remain set at all times while the
	 * _PAGE_PRESENT bit is clear).
	 */
 return Model1_pmd_flags(Model1_pmd) & ((((Model1_pteval_t)(1)) << 0) | (((Model1_pteval_t)(1)) << 8) | (((Model1_pteval_t)(1)) << 7));
}
static inline __attribute__((no_instrument_function)) int Model1_pmd_none(Model1_pmd_t Model1_pmd)
{
 /* Only check low word on 32-bit platforms, since it might be
	   out of sync with upper half. */
 unsigned long Model1_val = Model1_native_pmd_val(Model1_pmd);
 return (Model1_val & ~((((Model1_pteval_t)(1)) << 6) | (((Model1_pteval_t)(1)) << 5))) == 0;
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_pmd_page_vaddr(Model1_pmd_t Model1_pmd)
{
 return (unsigned long)((void *)((unsigned long)(Model1_native_pmd_val(Model1_pmd) & Model1_pmd_pfn_mask(Model1_pmd))+((unsigned long)(0xffff880000000000UL))));
}

/*
 * Currently stuck as a macro due to indirect forward reference to
 * linux/mmzone.h's __section_mem_map_addr() definition:
 */



/*
 * the pmd page can be thought of an array like this: pmd_t[PTRS_PER_PMD]
 *
 * this macro returns the index of the entry in the pmd page which would
 * control the given virtual address
 */
static inline __attribute__((no_instrument_function)) unsigned long Model1_pmd_index(unsigned long Model1_address)
{
 return (Model1_address >> 21) & (512 - 1);
}

/*
 * Conversion functions: convert a page and protection to a page entry,
 * and a page entry and page directory to the page they refer to.
 *
 * (Currently stuck as a macro because of indirect forward reference
 * to linux/mm.h:page_to_nid())
 */


/*
 * the pte page can be thought of an array like this: pte_t[PTRS_PER_PTE]
 *
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline __attribute__((no_instrument_function)) unsigned long Model1_pte_index(unsigned long Model1_address)
{
 return (Model1_address >> 12) & (512 - 1);
}

static inline __attribute__((no_instrument_function)) Model1_pte_t *Model1_pte_offset_kernel(Model1_pmd_t *Model1_pmd, unsigned long Model1_address)
{
 return (Model1_pte_t *)Model1_pmd_page_vaddr(*Model1_pmd) + Model1_pte_index(Model1_address);
}

static inline __attribute__((no_instrument_function)) int Model1_pmd_bad(Model1_pmd_t Model1_pmd)
{
 return (Model1_pmd_flags(Model1_pmd) & ~(((Model1_pteval_t)(1)) << 2)) != ((((Model1_pteval_t)(1)) << 0) | (((Model1_pteval_t)(1)) << 1) | (((Model1_pteval_t)(1)) << 5) | (((Model1_pteval_t)(1)) << 6));
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_pages_to_mb(unsigned long Model1_npg)
{
 return Model1_npg >> (20 - 12);
}


static inline __attribute__((no_instrument_function)) int Model1_pud_none(Model1_pud_t Model1_pud)
{
 return (Model1_native_pud_val(Model1_pud) & ~(((((Model1_pteval_t)(1)) << 6) | (((Model1_pteval_t)(1)) << 5)))) == 0;
}

static inline __attribute__((no_instrument_function)) int Model1_pud_present(Model1_pud_t Model1_pud)
{
 return Model1_pud_flags(Model1_pud) & (((Model1_pteval_t)(1)) << 0);
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_pud_page_vaddr(Model1_pud_t Model1_pud)
{
 return (unsigned long)((void *)((unsigned long)(Model1_native_pud_val(Model1_pud) & Model1_pud_pfn_mask(Model1_pud))+((unsigned long)(0xffff880000000000UL))));
}

/*
 * Currently stuck as a macro due to indirect forward reference to
 * linux/mmzone.h's __section_mem_map_addr() definition:
 */



/* Find an entry in the second-level page table.. */
static inline __attribute__((no_instrument_function)) Model1_pmd_t *Model1_pmd_offset(Model1_pud_t *Model1_pud, unsigned long Model1_address)
{
 return (Model1_pmd_t *)Model1_pud_page_vaddr(*Model1_pud) + Model1_pmd_index(Model1_address);
}

static inline __attribute__((no_instrument_function)) int Model1_pud_large(Model1_pud_t Model1_pud)
{
 return (Model1_native_pud_val(Model1_pud) & ((((Model1_pteval_t)(1)) << 7) | (((Model1_pteval_t)(1)) << 0))) ==
  ((((Model1_pteval_t)(1)) << 7) | (((Model1_pteval_t)(1)) << 0));
}

static inline __attribute__((no_instrument_function)) int Model1_pud_bad(Model1_pud_t Model1_pud)
{
 return (Model1_pud_flags(Model1_pud) & ~(((((Model1_pteval_t)(1)) << 0) | (((Model1_pteval_t)(1)) << 1) | (((Model1_pteval_t)(1)) << 5) | (((Model1_pteval_t)(1)) << 6)) | (((Model1_pteval_t)(1)) << 2))) != 0;
}
static inline __attribute__((no_instrument_function)) int Model1_pgd_present(Model1_pgd_t Model1_pgd)
{
 return Model1_pgd_flags(Model1_pgd) & (((Model1_pteval_t)(1)) << 0);
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_pgd_page_vaddr(Model1_pgd_t Model1_pgd)
{
 return (unsigned long)((void *)((unsigned long)((unsigned long)Model1_native_pgd_val(Model1_pgd) & ((Model1_pteval_t)(((signed long)(~(((1UL) << 12)-1))) & ((Model1_phys_addr_t)((1ULL << 46) - 1)))))+((unsigned long)(0xffff880000000000UL))));
}

/*
 * Currently stuck as a macro due to indirect forward reference to
 * linux/mmzone.h's __section_mem_map_addr() definition:
 */


/* to find an entry in a page-table-directory. */
static inline __attribute__((no_instrument_function)) unsigned long Model1_pud_index(unsigned long Model1_address)
{
 return (Model1_address >> 30) & (512 - 1);
}

static inline __attribute__((no_instrument_function)) Model1_pud_t *Model1_pud_offset(Model1_pgd_t *Model1_pgd, unsigned long Model1_address)
{
 return (Model1_pud_t *)Model1_pgd_page_vaddr(*Model1_pgd) + Model1_pud_index(Model1_address);
}

static inline __attribute__((no_instrument_function)) int Model1_pgd_bad(Model1_pgd_t Model1_pgd)
{
 return (Model1_pgd_flags(Model1_pgd) & ~(((Model1_pteval_t)(1)) << 2)) != ((((Model1_pteval_t)(1)) << 0) | (((Model1_pteval_t)(1)) << 1) | (((Model1_pteval_t)(1)) << 5) | (((Model1_pteval_t)(1)) << 6));
}

static inline __attribute__((no_instrument_function)) int Model1_pgd_none(Model1_pgd_t Model1_pgd)
{
 /*
	 * There is no need to do a workaround for the KNL stray
	 * A/D bit erratum here.  PGDs only point to page tables
	 * except on 32-bit non-PAE which is not supported on
	 * KNL.
	 */
 return !Model1_native_pgd_val(Model1_pgd);
}




/*
 * the pgd page can be thought of an array like this: pgd_t[PTRS_PER_PGD]
 *
 * this macro returns the index of the entry in the pgd page which would
 * control the given virtual address
 */


/*
 * pgd_offset() returns a (pgd_t *)
 * pgd_index() is used get the offset into the pgd page's array of pgd_t's;
 */

/*
 * a shortcut which implies the use of the kernel's pgd, instead
 * of a process's
 */
extern int Model1_direct_gbpages;
void Model1_init_mem_mapping(void);
void Model1_early_alloc_pgt_buf(void);


/* Realmode trampoline initialization. */
extern Model1_pgd_t Model1_trampoline_pgd_entry;
static inline __attribute__((no_instrument_function)) void __attribute__ ((__section__(".meminit.text"))) __attribute__((no_instrument_function)) Model1_init_trampoline_default(void)
{
 /* Default trampoline pgd value */
 Model1_trampoline_pgd_entry = Model1_init_level4_pgt[((((0xffff880000000000UL)) >> 39) & (512 - 1))];
}
/* local pte updates need not use xchg for locking */
static inline __attribute__((no_instrument_function)) Model1_pte_t Model1_native_local_ptep_get_and_clear(Model1_pte_t *Model1_ptep)
{
 Model1_pte_t Model1_res = *Model1_ptep;

 /* Pure native function needs no input for mm, addr */
 Model1_native_pte_clear(((void *)0), 0, Model1_ptep);
 return Model1_res;
}

static inline __attribute__((no_instrument_function)) Model1_pmd_t Model1_native_local_pmdp_get_and_clear(Model1_pmd_t *Model1_pmdp)
{
 Model1_pmd_t Model1_res = *Model1_pmdp;

 Model1_native_pmd_clear(Model1_pmdp);
 return Model1_res;
}

static inline __attribute__((no_instrument_function)) void Model1_native_set_pte_at(struct Model1_mm_struct *Model1_mm, unsigned long Model1_addr,
         Model1_pte_t *Model1_ptep , Model1_pte_t Model1_pte)
{
 Model1_native_set_pte(Model1_ptep, Model1_pte);
}

static inline __attribute__((no_instrument_function)) void Model1_native_set_pmd_at(struct Model1_mm_struct *Model1_mm, unsigned long Model1_addr,
         Model1_pmd_t *Model1_pmdp , Model1_pmd_t Model1_pmd)
{
 Model1_native_set_pmd(Model1_pmdp, Model1_pmd);
}


/*
 * Rules for using pte_update - it must be called after any PTE update which
 * has not been done using the set_pte / clear_pte interfaces.  It is used by
 * shadow mode hypervisors to resynchronize the shadow page tables.  Kernel PTE
 * updates should either be sets, clears, or set_pte_atomic for P->P
 * transitions, which means this hook should only be called for user PTEs.
 * This hook implies a P->P protection or access change has taken place, which
 * requires a subsequent TLB flush.
 */



/*
 * We only update the dirty/accessed state if we set
 * the dirty bit by hand in the kernel, since the hardware
 * will do the accessed bit for us, and we don't want to
 * race with other CPU's that might be updating the dirty
 * bit at the same time.
 */
struct Model1_vm_area_struct;


extern int Model1_ptep_set_access_flags(struct Model1_vm_area_struct *Model1_vma,
     unsigned long Model1_address, Model1_pte_t *Model1_ptep,
     Model1_pte_t Model1_entry, int Model1_dirty);


extern int Model1_ptep_test_and_clear_young(struct Model1_vm_area_struct *Model1_vma,
         unsigned long Model1_addr, Model1_pte_t *Model1_ptep);


extern int Model1_ptep_clear_flush_young(struct Model1_vm_area_struct *Model1_vma,
      unsigned long Model1_address, Model1_pte_t *Model1_ptep);


static inline __attribute__((no_instrument_function)) Model1_pte_t Model1_ptep_get_and_clear(struct Model1_mm_struct *Model1_mm, unsigned long Model1_addr,
           Model1_pte_t *Model1_ptep)
{
 Model1_pte_t Model1_pte = Model1_native_ptep_get_and_clear(Model1_ptep);
 do { } while (0);
 return Model1_pte;
}


static inline __attribute__((no_instrument_function)) Model1_pte_t Model1_ptep_get_and_clear_full(struct Model1_mm_struct *Model1_mm,
         unsigned long Model1_addr, Model1_pte_t *Model1_ptep,
         int Model1_full)
{
 Model1_pte_t Model1_pte;
 if (Model1_full) {
  /*
		 * Full address destruction in progress; paravirt does not
		 * care about updates and native needs no locking
		 */
  Model1_pte = Model1_native_local_ptep_get_and_clear(Model1_ptep);
 } else {
  Model1_pte = Model1_ptep_get_and_clear(Model1_mm, Model1_addr, Model1_ptep);
 }
 return Model1_pte;
}


static inline __attribute__((no_instrument_function)) void Model1_ptep_set_wrprotect(struct Model1_mm_struct *Model1_mm,
          unsigned long Model1_addr, Model1_pte_t *Model1_ptep)
{
 Model1_clear_bit(1, (unsigned long *)&Model1_ptep->Model1_pte);
 do { } while (0);
}






extern int Model1_pmdp_set_access_flags(struct Model1_vm_area_struct *Model1_vma,
     unsigned long Model1_address, Model1_pmd_t *Model1_pmdp,
     Model1_pmd_t Model1_entry, int Model1_dirty);


extern int Model1_pmdp_test_and_clear_young(struct Model1_vm_area_struct *Model1_vma,
         unsigned long Model1_addr, Model1_pmd_t *Model1_pmdp);


extern int Model1_pmdp_clear_flush_young(struct Model1_vm_area_struct *Model1_vma,
      unsigned long Model1_address, Model1_pmd_t *Model1_pmdp);



static inline __attribute__((no_instrument_function)) int Model1_pmd_write(Model1_pmd_t Model1_pmd)
{
 return Model1_pmd_flags(Model1_pmd) & (((Model1_pteval_t)(1)) << 1);
}


static inline __attribute__((no_instrument_function)) Model1_pmd_t Model1_pmdp_huge_get_and_clear(struct Model1_mm_struct *Model1_mm, unsigned long Model1_addr,
           Model1_pmd_t *Model1_pmdp)
{
 return Model1_native_pmdp_get_and_clear(Model1_pmdp);
}


static inline __attribute__((no_instrument_function)) void Model1_pmdp_set_wrprotect(struct Model1_mm_struct *Model1_mm,
          unsigned long Model1_addr, Model1_pmd_t *Model1_pmdp)
{
 Model1_clear_bit(1, (unsigned long *)Model1_pmdp);
}

/*
 * clone_pgd_range(pgd_t *dst, pgd_t *src, int count);
 *
 *  dst - pointer to pgd range anwhere on a pgd page
 *  src - ""
 *  count - the number of pgds to copy.
 *
 * dst and src can be on the same page, but the range must not overlap,
 * and must not cross a page boundary.
 */
static inline __attribute__((no_instrument_function)) void Model1_clone_pgd_range(Model1_pgd_t *Model1_dst, Model1_pgd_t *Model1_src, int Model1_count)
{
       ({ Model1_size_t Model1___len = (Model1_count * sizeof(Model1_pgd_t)); void *Model1___ret; if (__builtin_constant_p(Model1_count * sizeof(Model1_pgd_t)) && Model1___len >= 64) Model1___ret = Model1___memcpy((Model1_dst), (Model1_src), Model1___len); else Model1___ret = __builtin_memcpy((Model1_dst), (Model1_src), Model1___len); Model1___ret; });
}


static inline __attribute__((no_instrument_function)) int Model1_page_level_shift(enum Model1_pg_level Model1_level)
{
 return (12 - ( __builtin_constant_p(512) ? ( (512) < 1 ? Model1_____ilog2_NaN() : (512) & (1ULL << 63) ? 63 : (512) & (1ULL << 62) ? 62 : (512) & (1ULL << 61) ? 61 : (512) & (1ULL << 60) ? 60 : (512) & (1ULL << 59) ? 59 : (512) & (1ULL << 58) ? 58 : (512) & (1ULL << 57) ? 57 : (512) & (1ULL << 56) ? 56 : (512) & (1ULL << 55) ? 55 : (512) & (1ULL << 54) ? 54 : (512) & (1ULL << 53) ? 53 : (512) & (1ULL << 52) ? 52 : (512) & (1ULL << 51) ? 51 : (512) & (1ULL << 50) ? 50 : (512) & (1ULL << 49) ? 49 : (512) & (1ULL << 48) ? 48 : (512) & (1ULL << 47) ? 47 : (512) & (1ULL << 46) ? 46 : (512) & (1ULL << 45) ? 45 : (512) & (1ULL << 44) ? 44 : (512) & (1ULL << 43) ? 43 : (512) & (1ULL << 42) ? 42 : (512) & (1ULL << 41) ? 41 : (512) & (1ULL << 40) ? 40 : (512) & (1ULL << 39) ? 39 : (512) & (1ULL << 38) ? 38 : (512) & (1ULL << 37) ? 37 : (512) & (1ULL << 36) ? 36 : (512) & (1ULL << 35) ? 35 : (512) & (1ULL << 34) ? 34 : (512) & (1ULL << 33) ? 33 : (512) & (1ULL << 32) ? 32 : (512) & (1ULL << 31) ? 31 : (512) & (1ULL << 30) ? 30 : (512) & (1ULL << 29) ? 29 : (512) & (1ULL << 28) ? 28 : (512) & (1ULL << 27) ? 27 : (512) & (1ULL << 26) ? 26 : (512) & (1ULL << 25) ? 25 : (512) & (1ULL << 24) ? 24 : (512) & (1ULL << 23) ? 23 : (512) & (1ULL << 22) ? 22 : (512) & (1ULL << 21) ? 21 : (512) & (1ULL << 20) ? 20 : (512) & (1ULL << 19) ? 19 : (512) & (1ULL << 18) ? 18 : (512) & (1ULL << 17) ? 17 : (512) & (1ULL << 16) ? 16 : (512) & (1ULL << 15) ? 15 : (512) & (1ULL << 14) ? 14 : (512) & (1ULL << 13) ? 13 : (512) & (1ULL << 12) ? 12 : (512) & (1ULL << 11) ? 11 : (512) & (1ULL << 10) ? 10 : (512) & (1ULL << 9) ? 9 : (512) & (1ULL << 8) ? 8 : (512) & (1ULL << 7) ? 7 : (512) & (1ULL << 6) ? 6 : (512) & (1ULL << 5) ? 5 : (512) & (1ULL << 4) ? 4 : (512) & (1ULL << 3) ? 3 : (512) & (1ULL << 2) ? 2 : (512) & (1ULL << 1) ? 1 : (512) & (1ULL << 0) ? 0 : Model1_____ilog2_NaN() ) : (sizeof(512) <= 4) ? Model1___ilog2_u32(512) : Model1___ilog2_u64(512) )) + Model1_level * ( __builtin_constant_p(512) ? ( (512) < 1 ? Model1_____ilog2_NaN() : (512) & (1ULL << 63) ? 63 : (512) & (1ULL << 62) ? 62 : (512) & (1ULL << 61) ? 61 : (512) & (1ULL << 60) ? 60 : (512) & (1ULL << 59) ? 59 : (512) & (1ULL << 58) ? 58 : (512) & (1ULL << 57) ? 57 : (512) & (1ULL << 56) ? 56 : (512) & (1ULL << 55) ? 55 : (512) & (1ULL << 54) ? 54 : (512) & (1ULL << 53) ? 53 : (512) & (1ULL << 52) ? 52 : (512) & (1ULL << 51) ? 51 : (512) & (1ULL << 50) ? 50 : (512) & (1ULL << 49) ? 49 : (512) & (1ULL << 48) ? 48 : (512) & (1ULL << 47) ? 47 : (512) & (1ULL << 46) ? 46 : (512) & (1ULL << 45) ? 45 : (512) & (1ULL << 44) ? 44 : (512) & (1ULL << 43) ? 43 : (512) & (1ULL << 42) ? 42 : (512) & (1ULL << 41) ? 41 : (512) & (1ULL << 40) ? 40 : (512) & (1ULL << 39) ? 39 : (512) & (1ULL << 38) ? 38 : (512) & (1ULL << 37) ? 37 : (512) & (1ULL << 36) ? 36 : (512) & (1ULL << 35) ? 35 : (512) & (1ULL << 34) ? 34 : (512) & (1ULL << 33) ? 33 : (512) & (1ULL << 32) ? 32 : (512) & (1ULL << 31) ? 31 : (512) & (1ULL << 30) ? 30 : (512) & (1ULL << 29) ? 29 : (512) & (1ULL << 28) ? 28 : (512) & (1ULL << 27) ? 27 : (512) & (1ULL << 26) ? 26 : (512) & (1ULL << 25) ? 25 : (512) & (1ULL << 24) ? 24 : (512) & (1ULL << 23) ? 23 : (512) & (1ULL << 22) ? 22 : (512) & (1ULL << 21) ? 21 : (512) & (1ULL << 20) ? 20 : (512) & (1ULL << 19) ? 19 : (512) & (1ULL << 18) ? 18 : (512) & (1ULL << 17) ? 17 : (512) & (1ULL << 16) ? 16 : (512) & (1ULL << 15) ? 15 : (512) & (1ULL << 14) ? 14 : (512) & (1ULL << 13) ? 13 : (512) & (1ULL << 12) ? 12 : (512) & (1ULL << 11) ? 11 : (512) & (1ULL << 10) ? 10 : (512) & (1ULL << 9) ? 9 : (512) & (1ULL << 8) ? 8 : (512) & (1ULL << 7) ? 7 : (512) & (1ULL << 6) ? 6 : (512) & (1ULL << 5) ? 5 : (512) & (1ULL << 4) ? 4 : (512) & (1ULL << 3) ? 3 : (512) & (1ULL << 2) ? 2 : (512) & (1ULL << 1) ? 1 : (512) & (1ULL << 0) ? 0 : Model1_____ilog2_NaN() ) : (sizeof(512) <= 4) ? Model1___ilog2_u32(512) : Model1___ilog2_u64(512) );
}
static inline __attribute__((no_instrument_function)) unsigned long Model1_page_level_size(enum Model1_pg_level Model1_level)
{
 return 1UL << Model1_page_level_shift(Model1_level);
}
static inline __attribute__((no_instrument_function)) unsigned long Model1_page_level_mask(enum Model1_pg_level Model1_level)
{
 return ~(Model1_page_level_size(Model1_level) - 1);
}

/*
 * The x86 doesn't have any external MMU info: the kernel page
 * tables contain all the necessary information.
 */
static inline __attribute__((no_instrument_function)) void Model1_update_mmu_cache(struct Model1_vm_area_struct *Model1_vma,
  unsigned long Model1_addr, Model1_pte_t *Model1_ptep)
{
}
static inline __attribute__((no_instrument_function)) void Model1_update_mmu_cache_pmd(struct Model1_vm_area_struct *Model1_vma,
  unsigned long Model1_addr, Model1_pmd_t *Model1_pmd)
{
}


static inline __attribute__((no_instrument_function)) Model1_pte_t Model1_pte_swp_mksoft_dirty(Model1_pte_t Model1_pte)
{
 return Model1_pte_set_flags(Model1_pte, (((Model1_pteval_t)(0))));
}

static inline __attribute__((no_instrument_function)) int Model1_pte_swp_soft_dirty(Model1_pte_t Model1_pte)
{
 return Model1_pte_flags(Model1_pte) & (((Model1_pteval_t)(0)));
}

static inline __attribute__((no_instrument_function)) Model1_pte_t Model1_pte_swp_clear_soft_dirty(Model1_pte_t Model1_pte)
{
 return Model1_pte_clear_flags(Model1_pte, (((Model1_pteval_t)(0))));
}






static inline __attribute__((no_instrument_function)) bool Model1___pkru_allows_read(Model1_u32 Model1_pkru, Model1_u16 Model1_pkey)
{
 int Model1_pkru_pkey_bits = Model1_pkey * 2;
 return !(Model1_pkru & (0x1 << Model1_pkru_pkey_bits));
}

static inline __attribute__((no_instrument_function)) bool Model1___pkru_allows_write(Model1_u32 Model1_pkru, Model1_u16 Model1_pkey)
{
 int Model1_pkru_pkey_bits = Model1_pkey * 2;
 /*
	 * Access-disable disables writes too so we need to check
	 * both bits here.
	 */
 return !(Model1_pkru & ((0x1|0x2) << Model1_pkru_pkey_bits));
}

static inline __attribute__((no_instrument_function)) Model1_u16 Model1_pte_flags_pkey(unsigned long Model1_pte_flags)
{

 /* ifdef to avoid doing 59-bit shift on 32-bit values */
 return (Model1_pte_flags & ((((Model1_pteval_t)(1)) << 59) | (((Model1_pteval_t)(1)) << 60) | (((Model1_pteval_t)(1)) << 61) | (((Model1_pteval_t)(1)) << 62))) >> 59;



}


/*
 * On almost all architectures and configurations, 0 can be used as the
 * upper ceiling to free_pgtables(): on many architectures it has the same
 * effect as using TASK_SIZE.  However, there is one configuration which
 * must impose a more careful limit, to avoid freeing kernel pgtables.
 */
/*
 * Some architectures may be able to avoid expensive synchronization
 * primitives when modifications are made to PTE's which are already
 * not present, or in the process of an address space destruction.
 */

static inline __attribute__((no_instrument_function)) void Model1_pte_clear_not_present_full(struct Model1_mm_struct *Model1_mm,
           unsigned long Model1_address,
           Model1_pte_t *Model1_ptep,
           int Model1_full)
{
 Model1_native_pte_clear(Model1_mm, Model1_address, Model1_ptep);
}



extern Model1_pte_t Model1_ptep_clear_flush(struct Model1_vm_area_struct *Model1_vma,
         unsigned long Model1_address,
         Model1_pte_t *Model1_ptep);



extern Model1_pmd_t Model1_pmdp_huge_clear_flush(struct Model1_vm_area_struct *Model1_vma,
         unsigned long Model1_address,
         Model1_pmd_t *Model1_pmdp);
static inline __attribute__((no_instrument_function)) Model1_pmd_t Model1_pmdp_collapse_flush(struct Model1_vm_area_struct *Model1_vma,
     unsigned long Model1_address,
     Model1_pmd_t *Model1_pmdp)
{
 do { bool Model1___cond = !(!(1)); extern void Model1___compiletime_assert_221(void) ; if (Model1___cond) Model1___compiletime_assert_221(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
 return *Model1_pmdp;
}





extern void Model1_pgtable_trans_huge_deposit(struct Model1_mm_struct *Model1_mm, Model1_pmd_t *Model1_pmdp,
           Model1_pgtable_t Model1_pgtable);



extern Model1_pgtable_t Model1_pgtable_trans_huge_withdraw(struct Model1_mm_struct *Model1_mm, Model1_pmd_t *Model1_pmdp);



extern void Model1_pmdp_invalidate(struct Model1_vm_area_struct *Model1_vma, unsigned long Model1_address,
       Model1_pmd_t *Model1_pmdp);



static inline __attribute__((no_instrument_function)) void Model1_pmdp_huge_split_prepare(struct Model1_vm_area_struct *Model1_vma,
        unsigned long Model1_address, Model1_pmd_t *Model1_pmdp)
{

}
/*
 * Some architectures provide facilities to virtualization guests
 * so that they can flag allocated pages as unused. This allows the
 * host to transparently reclaim unused pages. This function returns
 * whether the pte's page is unused.
 */
static inline __attribute__((no_instrument_function)) int Model1_pte_unused(Model1_pte_t Model1_pte)
{
 return 0;
}
static inline __attribute__((no_instrument_function)) int Model1_pmd_same(Model1_pmd_t Model1_pmd_a, Model1_pmd_t Model1_pmd_b)
{
 do { bool Model1___cond = !(!(1)); extern void Model1___compiletime_assert_279(void) ; if (Model1___cond) Model1___compiletime_assert_279(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
 return 0;
}
/*
 * When walking page tables, get the address of the next boundary,
 * or the end address of the range if that comes earlier.  Although no
 * vma end wraps to 0, rounded up __boundary may wrap to 0 throughout.
 */
/*
 * When walking page tables, we usually want to skip any p?d_none entries;
 * and any p?d_bad entries - reporting the error before resetting to none.
 * Do the tests inline, but report and clear the bad entry in mm/memory.c.
 */
void Model1_pgd_clear_bad(Model1_pgd_t *);
void Model1_pud_clear_bad(Model1_pud_t *);
void Model1_pmd_clear_bad(Model1_pmd_t *);

static inline __attribute__((no_instrument_function)) int Model1_pgd_none_or_clear_bad(Model1_pgd_t *Model1_pgd)
{
 if (Model1_pgd_none(*Model1_pgd))
  return 1;
 if (__builtin_expect(!!(Model1_pgd_bad(*Model1_pgd)), 0)) {
  Model1_pgd_clear_bad(Model1_pgd);
  return 1;
 }
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_pud_none_or_clear_bad(Model1_pud_t *Model1_pud)
{
 if (Model1_pud_none(*Model1_pud))
  return 1;
 if (__builtin_expect(!!(Model1_pud_bad(*Model1_pud)), 0)) {
  Model1_pud_clear_bad(Model1_pud);
  return 1;
 }
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_pmd_none_or_clear_bad(Model1_pmd_t *Model1_pmd)
{
 if (Model1_pmd_none(*Model1_pmd))
  return 1;
 if (__builtin_expect(!!(Model1_pmd_bad(*Model1_pmd)), 0)) {
  Model1_pmd_clear_bad(Model1_pmd);
  return 1;
 }
 return 0;
}

static inline __attribute__((no_instrument_function)) Model1_pte_t Model1___ptep_modify_prot_start(struct Model1_mm_struct *Model1_mm,
          unsigned long Model1_addr,
          Model1_pte_t *Model1_ptep)
{
 /*
	 * Get the current pte state, but zero it out to make it
	 * non-present, preventing the hardware from asynchronously
	 * updating it.
	 */
 return Model1_ptep_get_and_clear(Model1_mm, Model1_addr, Model1_ptep);
}

static inline __attribute__((no_instrument_function)) void Model1___ptep_modify_prot_commit(struct Model1_mm_struct *Model1_mm,
          unsigned long Model1_addr,
          Model1_pte_t *Model1_ptep, Model1_pte_t Model1_pte)
{
 /*
	 * The pte is non-present, so there's no hardware state to
	 * preserve.
	 */
 Model1_native_set_pte_at(Model1_mm, Model1_addr, Model1_ptep, Model1_pte);
}


/*
 * Start a pte protection read-modify-write transaction, which
 * protects against asynchronous hardware modifications to the pte.
 * The intention is not to prevent the hardware from making pte
 * updates, but to prevent any updates it may make from being lost.
 *
 * This does not protect against other software modifications of the
 * pte; the appropriate pte lock must be held over the transation.
 *
 * Note that this interface is intended to be batchable, meaning that
 * ptep_modify_prot_commit may not actually update the pte, but merely
 * queue the update to be done at some later time.  The update must be
 * actually committed before the pte lock is released, however.
 */
static inline __attribute__((no_instrument_function)) Model1_pte_t Model1_ptep_modify_prot_start(struct Model1_mm_struct *Model1_mm,
        unsigned long Model1_addr,
        Model1_pte_t *Model1_ptep)
{
 return Model1___ptep_modify_prot_start(Model1_mm, Model1_addr, Model1_ptep);
}

/*
 * Commit an update to a pte, leaving any hardware-controlled bits in
 * the PTE unmodified.
 */
static inline __attribute__((no_instrument_function)) void Model1_ptep_modify_prot_commit(struct Model1_mm_struct *Model1_mm,
        unsigned long Model1_addr,
        Model1_pte_t *Model1_ptep, Model1_pte_t Model1_pte)
{
 Model1___ptep_modify_prot_commit(Model1_mm, Model1_addr, Model1_ptep, Model1_pte);
}



/*
 * A facility to provide lazy MMU batching.  This allows PTE updates and
 * page invalidations to be delayed until a call to leave lazy MMU mode
 * is issued.  Some architectures may benefit from doing this, and it is
 * beneficial for both shadow and direct mode hypervisors, which may batch
 * the PTE updates which happen during this window.  Note that using this
 * interface requires that read hazards be removed from the code.  A read
 * hazard could result in the direct mode hypervisor case, since the actual
 * write to the page tables may not yet have taken place, so reads though
 * a raw PTE pointer after it has been modified are not guaranteed to be
 * up to date.  This mode can only be entered and left under the protection of
 * the page table locks for all page tables which may be modified.  In the UP
 * case, this is required so that preemption is disabled, and in the SMP case,
 * it must synchronize the delayed page table writes properly on other CPUs.
 */






/*
 * A facility to provide batching of the reload of page tables and
 * other process state with the actual context switch code for
 * paravirtualized guests.  By convention, only one of the batched
 * update (lazy) modes (CPU, MMU) should be active at any given time,
 * entry should never be nested, and entry and exits should always be
 * paired.  This is for sanity of maintaining and reasoning about the
 * kernel code.  In this case, the exit (end of the context switch) is
 * in architecture-specific code, and so doesn't need a generic
 * definition.
 */
extern int Model1_track_pfn_remap(struct Model1_vm_area_struct *Model1_vma, Model1_pgprot_t *Model1_prot,
      unsigned long Model1_pfn, unsigned long Model1_addr,
      unsigned long Model1_size);
extern int Model1_track_pfn_insert(struct Model1_vm_area_struct *Model1_vma, Model1_pgprot_t *Model1_prot,
       Model1_pfn_t Model1_pfn);
extern int Model1_track_pfn_copy(struct Model1_vm_area_struct *Model1_vma);
extern void Model1_untrack_pfn(struct Model1_vm_area_struct *Model1_vma, unsigned long Model1_pfn,
   unsigned long Model1_size);
extern void Model1_untrack_pfn_moved(struct Model1_vm_area_struct *Model1_vma);
static inline __attribute__((no_instrument_function)) int Model1_is_zero_pfn(unsigned long Model1_pfn)
{
 extern unsigned long Model1_zero_pfn;
 return Model1_pfn == Model1_zero_pfn;
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_my_zero_pfn(unsigned long Model1_addr)
{
 extern unsigned long Model1_zero_pfn;
 return Model1_zero_pfn;
}





static inline __attribute__((no_instrument_function)) int Model1_pmd_trans_huge(Model1_pmd_t Model1_pmd)
{
 return 0;
}
static inline __attribute__((no_instrument_function)) Model1_pmd_t Model1_pmd_read_atomic(Model1_pmd_t *Model1_pmdp)
{
 /*
	 * Depend on compiler for an atomic pmd read. NOTE: this is
	 * only going to work, if the pmdval_t isn't larger than
	 * an unsigned long.
	 */
 return *Model1_pmdp;
}



static inline __attribute__((no_instrument_function)) int Model1_pmd_move_must_withdraw(Model1_spinlock_t *Model1_new_pmd_ptl,
      Model1_spinlock_t *Model1_old_pmd_ptl)
{
 /*
	 * With split pmd lock we also need to move preallocated
	 * PTE page table if new_pmd is on different PMD page table.
	 */
 return Model1_new_pmd_ptl != Model1_old_pmd_ptl;
}


/*
 * This function is meant to be used by sites walking pagetables with
 * the mmap_sem hold in read mode to protect against MADV_DONTNEED and
 * transhuge page faults. MADV_DONTNEED can convert a transhuge pmd
 * into a null pmd and the transhuge page fault can convert a null pmd
 * into an hugepmd or into a regular pmd (if the hugepage allocation
 * fails). While holding the mmap_sem in read mode the pmd becomes
 * stable and stops changing under us only if it's not null and not a
 * transhuge pmd. When those races occurs and this function makes a
 * difference vs the standard pmd_none_or_clear_bad, the result is
 * undefined so behaving like if the pmd was none is safe (because it
 * can return none anyway). The compiler level barrier() is critically
 * important to compute the two checks atomically on the same pmdval.
 *
 * For 32bit kernels with a 64bit large pmd_t this automatically takes
 * care of reading the pmd atomically to avoid SMP race conditions
 * against pmd_populate() when the mmap_sem is hold for reading by the
 * caller (a special atomic read not done by "gcc" as in the generic
 * version above, is also needed when THP is disabled because the page
 * fault can populate the pmd from under us).
 */
static inline __attribute__((no_instrument_function)) int Model1_pmd_none_or_trans_huge_or_clear_bad(Model1_pmd_t *Model1_pmd)
{
 Model1_pmd_t Model1_pmdval = Model1_pmd_read_atomic(Model1_pmd);
 /*
	 * The barrier will stabilize the pmdval in a register or on
	 * the stack so that it will stop changing under the code.
	 *
	 * When CONFIG_TRANSPARENT_HUGEPAGE=y on x86 32bit PAE,
	 * pmd_read_atomic is allowed to return a not atomic pmdval
	 * (for example pointing to an hugepage that has never been
	 * mapped in the pmd). The below checks will only care about
	 * the low part of the pmd with 32bit PAE x86 anyway, with the
	 * exception of pmd_none(). So the important thing is that if
	 * the low part of the pmd is found null, the high part will
	 * be also null or the pmd_none() check below would be
	 * confused.
	 */



 if (Model1_pmd_none(Model1_pmdval) || Model1_pmd_trans_huge(Model1_pmdval))
  return 1;
 if (__builtin_expect(!!(Model1_pmd_bad(Model1_pmdval)), 0)) {
  Model1_pmd_clear_bad(Model1_pmd);
  return 1;
 }
 return 0;
}

/*
 * This is a noop if Transparent Hugepage Support is not built into
 * the kernel. Otherwise it is equivalent to
 * pmd_none_or_trans_huge_or_clear_bad(), and shall only be called in
 * places that already verified the pmd is not none and they want to
 * walk ptes while holding the mmap sem in read mode (write mode don't
 * need this). If THP is not enabled, the pmd can't go away under the
 * code even if MADV_DONTNEED runs, but if THP is enabled we need to
 * run a pmd_trans_unstable before walking the ptes after
 * split_huge_page_pmd returns (because it may have run when the pmd
 * become null, but then a page fault can map in a THP and not a
 * regular page).
 */
static inline __attribute__((no_instrument_function)) int Model1_pmd_trans_unstable(Model1_pmd_t *Model1_pmd)
{



 return 0;

}


/*
 * Technically a PTE can be PROTNONE even when not doing NUMA balancing but
 * the only case the kernel cares is for NUMA balancing and is only ever set
 * when the VMA is accessible. For PROT_NONE VMAs, the PTEs are not marked
 * _PAGE_PROTNONE so by by default, implement the helper as "always no". It
 * is the responsibility of the caller to distinguish between PROT_NONE
 * protections and NUMA hinting fault protections.
 */
static inline __attribute__((no_instrument_function)) int Model1_pte_protnone(Model1_pte_t Model1_pte)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_pmd_protnone(Model1_pmd_t Model1_pmd)
{
 return 0;
}





int Model1_pud_set_huge(Model1_pud_t *Model1_pud, Model1_phys_addr_t Model1_addr, Model1_pgprot_t Model1_prot);
int Model1_pmd_set_huge(Model1_pmd_t *Model1_pmd, Model1_phys_addr_t Model1_addr, Model1_pgprot_t Model1_prot);
int Model1_pud_clear_huge(Model1_pud_t *Model1_pud);
int Model1_pmd_clear_huge(Model1_pmd_t *Model1_pmd);
/*
 * To prevent common memory management code establishing
 * a zero page mapping on a read fault.
 * This macro should be defined within <asm/pgtable.h>.
 * s390 does this to prevent multiplexing of hardware bits
 * related to the physical page in case of virtualization.
 */




/*
 * Default maximum number of active map areas, this limits the number of vmas
 * per mm struct. Users can overwrite this number by sysctl but there is a
 * problem.
 *
 * When a program's coredump is generated as ELF format, a section is created
 * per a vma. In ELF, the number of sections is represented in unsigned short.
 * This means the number of sections should be smaller than 65535 at coredump.
 * Because the kernel adds some informative sections to a image of program at
 * generating coredump, we need some margin. The number of extra sections is
 * 1-3 now and depends on arch. We use "5" as safe margin, here.
 *
 * ELF extended numbering allows more than 65535 sections, so 16-bit bound is
 * not a hard limit any more. Although some userspace tools can be surprised by
 * that.
 */



extern int Model1_sysctl_max_map_count;

extern unsigned long Model1_sysctl_user_reserve_kbytes;
extern unsigned long Model1_sysctl_admin_reserve_kbytes;

extern int Model1_sysctl_overcommit_memory;
extern int Model1_sysctl_overcommit_ratio;
extern unsigned long Model1_sysctl_overcommit_kbytes;

extern int Model1_overcommit_ratio_handler(struct Model1_ctl_table *, int, void *,
        Model1_size_t *, Model1_loff_t *);
extern int Model1_overcommit_kbytes_handler(struct Model1_ctl_table *, int, void *,
        Model1_size_t *, Model1_loff_t *);



/* to align the pointer to the (next) page boundary */


/* test whether an address (unsigned long or pointer) is aligned to PAGE_SIZE */


/*
 * Linux kernel virtual memory manager primitives.
 * The idea being to have a "virtual" mm in the same way
 * we have a virtual fs - giving a cleaner interface to the
 * mm details, and allowing different kinds of memory mappings
 * (from shared memory to executable loading to arbitrary
 * mmap() functions).
 */

extern struct Model1_kmem_cache *Model1_vm_area_cachep;
/*
 * vm_flags in vm_area_struct, see mm_types.h.
 * When changing, update also include/trace/events/mmflags.h
 */







/* mprotect() hardcodes VM_MAYREAD >> 4 == VM_READ, and so for r/w/x bits. */
     /* Used by sys_madvise() */
/* MPX specific bounds table or bounds directory */







/* Bits set in the VMA until the stack is in its final location */
/*
 * Special vmas that are non-mergable, non-mlock()able.
 * Note: mm/huge_memory.c VM_NO_THP depends on this definition.
 */


/* This mask defines which mm->def_flags a process can inherit its parent */


/* This mask is used to clear all the VMA flags used by mlock */


/*
 * mapping from the currently active vm_flags protection bits (the
 * low four bits) to a page protection mask..
 */
extern Model1_pgprot_t Model1_protection_map[16];
/*
 * vm_fault is filled by the the pagefault handler and passed to the vma's
 * ->fault function. The vma's ->fault is responsible for returning a bitmask
 * of VM_FAULT_xxx flags that give details about how the fault was handled.
 *
 * MM layer fills up gfp_mask for page allocations but fault handler might
 * alter it if its implementation requires a different allocation context.
 *
 * pgoff should be used in favour of virtual_address, if possible.
 */
struct Model1_vm_fault {
 unsigned int Model1_flags; /* FAULT_FLAG_xxx flags */
 Model1_gfp_t Model1_gfp_mask; /* gfp mask to be used for allocations */
 unsigned long Model1_pgoff; /* Logical page offset based on vma */
 void *Model1_virtual_address; /* Faulting virtual address */

 struct Model1_page *Model1_cow_page; /* Handler may choose to COW */
 struct Model1_page *Model1_page; /* ->fault handlers should return a
					 * page here, unless VM_FAULT_NOPAGE
					 * is set (which is also implied by
					 * VM_FAULT_ERROR).
					 */
 void *Model1_entry; /* ->fault handler can alternatively
					 * return locked DAX entry. In that
					 * case handler should return
					 * VM_FAULT_DAX_LOCKED and fill in
					 * entry here.
					 */
};

/*
 * Page fault context: passes though page fault handler instead of endless list
 * of function arguments.
 */
struct Model1_fault_env {
 struct Model1_vm_area_struct *Model1_vma; /* Target VMA */
 unsigned long Model1_address; /* Faulting virtual address */
 unsigned int Model1_flags; /* FAULT_FLAG_xxx flags */
 Model1_pmd_t *Model1_pmd; /* Pointer to pmd entry matching
					 * the 'address'
					 */
 Model1_pte_t *Model1_pte; /* Pointer to pte entry matching
					 * the 'address'. NULL if the page
					 * table hasn't been allocated.
					 */
 Model1_spinlock_t *Model1_ptl; /* Page table lock.
					 * Protects pte page table if 'pte'
					 * is not NULL, otherwise pmd.
					 */
 Model1_pgtable_t Model1_prealloc_pte; /* Pre-allocated pte page table.
					 * vm_ops->map_pages() calls
					 * alloc_set_pte() from atomic context.
					 * do_fault_around() pre-allocates
					 * page table to avoid allocation from
					 * atomic context.
					 */
};

/*
 * These are the virtual MM functions - opening of an area, closing and
 * unmapping it (needed to keep files on disk up-to-date etc), pointer
 * to the functions called when a no-page or a wp-page exception occurs. 
 */
struct Model1_vm_operations_struct {
 void (*Model1_open)(struct Model1_vm_area_struct * Model1_area);
 void (*Model1_close)(struct Model1_vm_area_struct * Model1_area);
 int (*Model1_mremap)(struct Model1_vm_area_struct * Model1_area);
 int (*fault)(struct Model1_vm_area_struct *Model1_vma, struct Model1_vm_fault *Model1_vmf);
 int (*Model1_pmd_fault)(struct Model1_vm_area_struct *, unsigned long Model1_address,
      Model1_pmd_t *, unsigned int Model1_flags);
 void (*Model1_map_pages)(struct Model1_fault_env *Model1_fe,
   unsigned long Model1_start_pgoff, unsigned long Model1_end_pgoff);

 /* notification that a previously read-only page is about to become
	 * writable, if an error is returned it will cause a SIGBUS */
 int (*Model1_page_mkwrite)(struct Model1_vm_area_struct *Model1_vma, struct Model1_vm_fault *Model1_vmf);

 /* same as page_mkwrite when using VM_PFNMAP|VM_MIXEDMAP */
 int (*Model1_pfn_mkwrite)(struct Model1_vm_area_struct *Model1_vma, struct Model1_vm_fault *Model1_vmf);

 /* called by access_process_vm when get_user_pages() fails, typically
	 * for use by special VMAs that can switch between memory and hardware
	 */
 int (*Model1_access)(struct Model1_vm_area_struct *Model1_vma, unsigned long Model1_addr,
        void *Model1_buf, int Model1_len, int Model1_write);

 /* Called by the /proc/PID/maps code to ask the vma whether it
	 * has a special name.  Returning non-NULL will also cause this
	 * vma to be dumped unconditionally. */
 const char *(*Model1_name)(struct Model1_vm_area_struct *Model1_vma);


 /*
	 * set_policy() op must add a reference to any non-NULL @new mempolicy
	 * to hold the policy upon return.  Caller should pass NULL @new to
	 * remove a policy and fall back to surrounding context--i.e. do not
	 * install a MPOL_DEFAULT policy, nor the task or system default
	 * mempolicy.
	 */
 int (*Model1_set_policy)(struct Model1_vm_area_struct *Model1_vma, struct Model1_mempolicy *Model1_new);

 /*
	 * get_policy() op must add reference [mpol_get()] to any policy at
	 * (vma,addr) marked as MPOL_SHARED.  The shared policy infrastructure
	 * in mm/mempolicy.c will do this automatically.
	 * get_policy() must NOT add a ref if the policy at (vma,addr) is not
	 * marked as MPOL_SHARED. vma policies are protected by the mmap_sem.
	 * If no [shared/vma] mempolicy exists at the addr, get_policy() op
	 * must return NULL--i.e., do not "fallback" to task or system default
	 * policy.
	 */
 struct Model1_mempolicy *(*Model1_get_policy)(struct Model1_vm_area_struct *Model1_vma,
     unsigned long Model1_addr);

 /*
	 * Called by vm_normal_page() for special PTEs to find the
	 * page for @addr.  This is useful if the default behavior
	 * (using pte_page()) would not find the correct page.
	 */
 struct Model1_page *(*Model1_find_special_page)(struct Model1_vm_area_struct *Model1_vma,
       unsigned long Model1_addr);
};

struct Model1_mmu_gather;
struct Model1_inode;





static inline __attribute__((no_instrument_function)) int Model1_pmd_devmap(Model1_pmd_t Model1_pmd)
{
 return 0;
}


/*
 * FIXME: take this include out, include page-flags.h in
 * files which need it (119 of them)
 */





extern int Model1_do_huge_pmd_anonymous_page(struct Model1_fault_env *Model1_fe);
extern int Model1_copy_huge_pmd(struct Model1_mm_struct *Model1_dst_mm, struct Model1_mm_struct *Model1_src_mm,
    Model1_pmd_t *Model1_dst_pmd, Model1_pmd_t *Model1_src_pmd, unsigned long Model1_addr,
    struct Model1_vm_area_struct *Model1_vma);
extern void Model1_huge_pmd_set_accessed(struct Model1_fault_env *Model1_fe, Model1_pmd_t Model1_orig_pmd);
extern int Model1_do_huge_pmd_wp_page(struct Model1_fault_env *Model1_fe, Model1_pmd_t Model1_orig_pmd);
extern struct Model1_page *Model1_follow_trans_huge_pmd(struct Model1_vm_area_struct *Model1_vma,
       unsigned long Model1_addr,
       Model1_pmd_t *Model1_pmd,
       unsigned int Model1_flags);
extern bool Model1_madvise_free_huge_pmd(struct Model1_mmu_gather *Model1_tlb,
   struct Model1_vm_area_struct *Model1_vma,
   Model1_pmd_t *Model1_pmd, unsigned long Model1_addr, unsigned long Model1_next);
extern int Model1_zap_huge_pmd(struct Model1_mmu_gather *Model1_tlb,
   struct Model1_vm_area_struct *Model1_vma,
   Model1_pmd_t *Model1_pmd, unsigned long Model1_addr);
extern int Model1_mincore_huge_pmd(struct Model1_vm_area_struct *Model1_vma, Model1_pmd_t *Model1_pmd,
   unsigned long Model1_addr, unsigned long Model1_end,
   unsigned char *Model1_vec);
extern bool Model1_move_huge_pmd(struct Model1_vm_area_struct *Model1_vma, unsigned long Model1_old_addr,
    unsigned long Model1_new_addr, unsigned long Model1_old_end,
    Model1_pmd_t *Model1_old_pmd, Model1_pmd_t *Model1_new_pmd);
extern int Model1_change_huge_pmd(struct Model1_vm_area_struct *Model1_vma, Model1_pmd_t *Model1_pmd,
   unsigned long Model1_addr, Model1_pgprot_t Model1_newprot,
   int Model1_prot_numa);
int Model1_vmf_insert_pfn_pmd(struct Model1_vm_area_struct *, unsigned long Model1_addr, Model1_pmd_t *,
   Model1_pfn_t Model1_pfn, bool Model1_write);
enum Model1_transparent_hugepage_flag {
 Model1_TRANSPARENT_HUGEPAGE_FLAG,
 Model1_TRANSPARENT_HUGEPAGE_REQ_MADV_FLAG,
 Model1_TRANSPARENT_HUGEPAGE_DEFRAG_DIRECT_FLAG,
 Model1_TRANSPARENT_HUGEPAGE_DEFRAG_KSWAPD_FLAG,
 Model1_TRANSPARENT_HUGEPAGE_DEFRAG_REQ_MADV_FLAG,
 Model1_TRANSPARENT_HUGEPAGE_DEFRAG_KHUGEPAGED_FLAG,
 Model1_TRANSPARENT_HUGEPAGE_USE_ZERO_PAGE_FLAG,



};

struct Model1_kobject;
struct Model1_kobj_attribute;

extern Model1_ssize_t Model1_single_hugepage_flag_store(struct Model1_kobject *Model1_kobj,
     struct Model1_kobj_attribute *Model1_attr,
     const char *Model1_buf, Model1_size_t Model1_count,
     enum Model1_transparent_hugepage_flag Model1_flag);
extern Model1_ssize_t Model1_single_hugepage_flag_show(struct Model1_kobject *Model1_kobj,
    struct Model1_kobj_attribute *Model1_attr, char *Model1_buf,
    enum Model1_transparent_hugepage_flag Model1_flag);
extern struct Model1_kobj_attribute Model1_shmem_enabled_attr;
static inline __attribute__((no_instrument_function)) void Model1_prep_transhuge_page(struct Model1_page *Model1_page) {}


static inline __attribute__((no_instrument_function)) int
Model1_split_huge_page_to_list(struct Model1_page *Model1_page, struct Model1_list_head *Model1_list)
{
 return 0;
}
static inline __attribute__((no_instrument_function)) int Model1_split_huge_page(struct Model1_page *Model1_page)
{
 return 0;
}
static inline __attribute__((no_instrument_function)) void Model1_deferred_split_huge_page(struct Model1_page *Model1_page) {}



static inline __attribute__((no_instrument_function)) void Model1_split_huge_pmd_address(struct Model1_vm_area_struct *Model1_vma,
  unsigned long Model1_address, bool Model1_freeze, struct Model1_page *Model1_page) {}

static inline __attribute__((no_instrument_function)) int Model1_hugepage_madvise(struct Model1_vm_area_struct *Model1_vma,
       unsigned long *Model1_vm_flags, int Model1_advice)
{
 do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/huge_mm.h"), "i" (191), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0);
 return 0;
}
static inline __attribute__((no_instrument_function)) void Model1_vma_adjust_trans_huge(struct Model1_vm_area_struct *Model1_vma,
      unsigned long Model1_start,
      unsigned long Model1_end,
      long Model1_adjust_next)
{
}
static inline __attribute__((no_instrument_function)) Model1_spinlock_t *Model1_pmd_trans_huge_lock(Model1_pmd_t *Model1_pmd,
  struct Model1_vm_area_struct *Model1_vma)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) int Model1_do_huge_pmd_numa_page(struct Model1_fault_env *Model1_fe, Model1_pmd_t Model1_orig_pmd)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) bool Model1_is_huge_zero_page(struct Model1_page *Model1_page)
{
 return false;
}

static inline __attribute__((no_instrument_function)) void Model1_put_huge_zero_page(void)
{
 do { bool Model1___cond = !(!(1)); extern void Model1___compiletime_assert_218(void) ; if (Model1___cond) Model1___compiletime_assert_218(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
}

static inline __attribute__((no_instrument_function)) struct Model1_page *Model1_follow_devmap_pmd(struct Model1_vm_area_struct *Model1_vma,
  unsigned long Model1_addr, Model1_pmd_t *Model1_pmd, int Model1_flags)
{
 return ((void *)0);
}

/*
 * Methods to modify the page usage count.
 *
 * What counts for a page usage:
 * - cache mapping   (page->mapping)
 * - private data    (page->private)
 * - page mapped in a task's page tables, each mapping
 *   is counted separately
 *
 * Also, many kernel routines increase the page count before a critical
 * routine so they can be sure the page doesn't go away from under them.
 */

/*
 * Drop a ref, return true if the refcount fell to zero (the page has no users)
 */
static inline __attribute__((no_instrument_function)) int Model1_put_page_testzero(struct Model1_page *Model1_page)
{
 ((void)(sizeof(( long)(Model1_page_ref_count(Model1_page) == 0))));
 return Model1_page_ref_dec_and_test(Model1_page);
}

/*
 * Try to grab a ref unless the page has a refcount of zero, return false if
 * that is the case.
 * This can be called when MMU is off so it must not access
 * any of the virtual mappings.
 */
static inline __attribute__((no_instrument_function)) int Model1_get_page_unless_zero(struct Model1_page *Model1_page)
{
 return Model1_page_ref_add_unless(Model1_page, 1, 0);
}

extern int Model1_page_is_ram(unsigned long Model1_pfn);

enum {
 Model1_REGION_INTERSECTS,
 Model1_REGION_DISJOINT,
 Model1_REGION_MIXED,
};

int Model1_region_intersects(Model1_resource_size_t Model1_offset, Model1_size_t Model1_size, unsigned long Model1_flags,
        unsigned long Model1_desc);

/* Support for virtually mapped pages */
struct Model1_page *Model1_vmalloc_to_page(const void *Model1_addr);
unsigned long Model1_vmalloc_to_pfn(const void *Model1_addr);

/*
 * Determine if an address is within the vmalloc range
 *
 * On nommu, vmalloc/vfree wrap through kmalloc/kfree directly, so there
 * is no special casing required.
 */
static inline __attribute__((no_instrument_function)) bool Model1_is_vmalloc_addr(const void *Model1_x)
{

 unsigned long Model1_addr = (unsigned long)Model1_x;

 return Model1_addr >= (0xffffc90000000000UL) && Model1_addr < ((0xffffc90000000000UL) + (((32UL) << 40) - 1UL));



}

extern int Model1_is_vmalloc_or_module_addr(const void *Model1_x);







extern void Model1_kvfree(const void *Model1_addr);

static inline __attribute__((no_instrument_function)) Model1_atomic_t *Model1_compound_mapcount_ptr(struct Model1_page *Model1_page)
{
 return &Model1_page[1].Model1_compound_mapcount;
}

static inline __attribute__((no_instrument_function)) int Model1_compound_mapcount(struct Model1_page *Model1_page)
{
 ((void)(sizeof(( long)(!Model1_PageCompound(Model1_page)))));
 Model1_page = Model1_compound_head(Model1_page);
 return Model1_atomic_read(Model1_compound_mapcount_ptr(Model1_page)) + 1;
}

/*
 * The atomic page->_mapcount, starts from -1: so that transitions
 * both from it and to it can be tracked, using atomic_inc_and_test
 * and atomic_add_negative(-1).
 */
static inline __attribute__((no_instrument_function)) void Model1_page_mapcount_reset(struct Model1_page *Model1_page)
{
 Model1_atomic_set(&(Model1_page)->Model1__mapcount, -1);
}

int Model1___page_mapcount(struct Model1_page *Model1_page);

static inline __attribute__((no_instrument_function)) int Model1_page_mapcount(struct Model1_page *Model1_page)
{
 ((void)(sizeof(( long)(Model1_PageSlab(Model1_page)))));

 if (__builtin_expect(!!(Model1_PageCompound(Model1_page)), 0))
  return Model1___page_mapcount(Model1_page);
 return Model1_atomic_read(&Model1_page->Model1__mapcount) + 1;
}





static inline __attribute__((no_instrument_function)) int Model1_total_mapcount(struct Model1_page *Model1_page)
{
 return Model1_page_mapcount(Model1_page);
}
static inline __attribute__((no_instrument_function)) int Model1_page_trans_huge_mapcount(struct Model1_page *Model1_page,
        int *Model1_total_mapcount)
{
 int Model1_mapcount = Model1_page_mapcount(Model1_page);
 if (Model1_total_mapcount)
  *Model1_total_mapcount = Model1_mapcount;
 return Model1_mapcount;
}


static inline __attribute__((no_instrument_function)) struct Model1_page *Model1_virt_to_head_page(const void *Model1_x)
{
 struct Model1_page *Model1_page = (((struct Model1_page *)(0xffffea0000000000UL)) + (Model1___phys_addr_nodebug((unsigned long)(Model1_x)) >> 12));

 return Model1_compound_head(Model1_page);
}

void Model1___put_page(struct Model1_page *Model1_page);

void Model1_put_pages_list(struct Model1_list_head *Model1_pages);

void Model1_split_page(struct Model1_page *Model1_page, unsigned int Model1_order);

/*
 * Compound pages have a destructor function.  Provide a
 * prototype for that function and accessor functions.
 * These are _only_ valid on the head of a compound page.
 */
typedef void Model1_compound_page_dtor(struct Model1_page *);

/* Keep the enum in sync with compound_page_dtors array in mm/page_alloc.c */
enum Model1_compound_dtor_id {
 Model1_NULL_COMPOUND_DTOR,
 Model1_COMPOUND_PAGE_DTOR,

 Model1_HUGETLB_PAGE_DTOR,




 Model1_NR_COMPOUND_DTORS,
};
extern Model1_compound_page_dtor * const Model1_compound_page_dtors[];

static inline __attribute__((no_instrument_function)) void Model1_set_compound_page_dtor(struct Model1_page *Model1_page,
  enum Model1_compound_dtor_id Model1_compound_dtor)
{
 ((void)(sizeof(( long)(Model1_compound_dtor >= Model1_NR_COMPOUND_DTORS))));
 Model1_page[1].Model1_compound_dtor = Model1_compound_dtor;
}

static inline __attribute__((no_instrument_function)) Model1_compound_page_dtor *Model1_get_compound_page_dtor(struct Model1_page *Model1_page)
{
 ((void)(sizeof(( long)(Model1_page[1].Model1_compound_dtor >= Model1_NR_COMPOUND_DTORS))));
 return Model1_compound_page_dtors[Model1_page[1].Model1_compound_dtor];
}

static inline __attribute__((no_instrument_function)) unsigned int Model1_compound_order(struct Model1_page *Model1_page)
{
 if (!Model1_PageHead(Model1_page))
  return 0;
 return Model1_page[1].Model1_compound_order;
}

static inline __attribute__((no_instrument_function)) void Model1_set_compound_order(struct Model1_page *Model1_page, unsigned int Model1_order)
{
 Model1_page[1].Model1_compound_order = Model1_order;
}

void Model1_free_compound_page(struct Model1_page *Model1_page);


/*
 * Do pte_mkwrite, but only if the vma says VM_WRITE.  We do this when
 * servicing faults for write access.  In the normal case, do always want
 * pte_mkwrite.  But get_user_pages can cause write faults for mappings
 * that do not have writing enabled, when used by access_process_vm.
 */
static inline __attribute__((no_instrument_function)) Model1_pte_t Model1_maybe_mkwrite(Model1_pte_t Model1_pte, struct Model1_vm_area_struct *Model1_vma)
{
 if (__builtin_expect(!!(Model1_vma->Model1_vm_flags & 0x00000002), 1))
  Model1_pte = Model1_pte_mkwrite(Model1_pte);
 return Model1_pte;
}

int Model1_alloc_set_pte(struct Model1_fault_env *Model1_fe, struct Model1_mem_cgroup *Model1_memcg,
  struct Model1_page *Model1_page);


/*
 * Multiple processes may "see" the same page. E.g. for untouched
 * mappings of /dev/null, all processes see the same page full of
 * zeroes, and text pages of executables and shared libraries have
 * only one copy in memory, at most, normally.
 *
 * For the non-reserved pages, page_count(page) denotes a reference count.
 *   page_count() == 0 means the page is free. page->lru is then used for
 *   freelist management in the buddy allocator.
 *   page_count() > 0  means the page has been allocated.
 *
 * Pages are allocated by the slab allocator in order to provide memory
 * to kmalloc and kmem_cache_alloc. In this case, the management of the
 * page, and the fields in 'struct page' are the responsibility of mm/slab.c
 * unless a particular usage is carefully commented. (the responsibility of
 * freeing the kmalloc memory is the caller's, of course).
 *
 * A page may be used by anyone else who does a __get_free_page().
 * In this case, page_count still tracks the references, and should only
 * be used through the normal accessor functions. The top bits of page->flags
 * and page->virtual store page management information, but all other fields
 * are unused and could be used privately, carefully. The management of this
 * page is the responsibility of the one who allocated it, and those who have
 * subsequently been given references to it.
 *
 * The other pages (we may call them "pagecache pages") are completely
 * managed by the Linux memory manager: I/O, buffers, swapping etc.
 * The following discussion applies only to them.
 *
 * A pagecache page contains an opaque `private' member, which belongs to the
 * page's address_space. Usually, this is the address of a circular list of
 * the page's disk buffers. PG_private must be set to tell the VM to call
 * into the filesystem to release these pages.
 *
 * A page may belong to an inode's memory mapping. In this case, page->mapping
 * is the pointer to the inode, and page->index is the file offset of the page,
 * in units of PAGE_SIZE.
 *
 * If pagecache pages are not associated with an inode, they are said to be
 * anonymous pages. These may become associated with the swapcache, and in that
 * case PG_swapcache is set, and page->private is an offset into the swapcache.
 *
 * In either case (swapcache or inode backed), the pagecache itself holds one
 * reference to the page. Setting PG_private should also increment the
 * refcount. The each user mapping also has a reference to the page.
 *
 * The pagecache pages are stored in a per-mapping radix tree, which is
 * rooted at mapping->page_tree, and indexed by offset.
 * Where 2.4 and early 2.6 kernels kept dirty/clean pages in per-address_space
 * lists, we instead now tag pages as dirty/writeback in the radix tree.
 *
 * All pagecache pages may be subject to I/O:
 * - inode pages may need to be read from disk,
 * - inode pages which have been modified and are MAP_SHARED may need
 *   to be written back to the inode on disk,
 * - anonymous pages (including MAP_PRIVATE file mappings) which have been
 *   modified may need to be swapped out to swap space and (later) to be read
 *   back into memory.
 */

/*
 * The zone field is never updated after free_area_init_core()
 * sets it, so none of the operations on it need to be atomic.
 */

/* Page flags: | [SECTION] | [NODE] | ZONE | [LAST_CPUPID] | ... | FLAGS | */





/*
 * Define the bit shifts to access each section.  For non-existent
 * sections we define the shift as 0; that plus a 0 mask ensures
 * the compiler will optimise away reference to them.
 */





/* NODE:ZONE or SECTION:ZONE is used to ID a zone for the buddy allocator */
static inline __attribute__((no_instrument_function)) enum Model1_zone_type Model1_page_zonenum(const struct Model1_page *Model1_page)
{
 return (Model1_page->Model1_flags >> (((((sizeof(unsigned long)*8) - 0) - 6) - 2) * (2 != 0))) & ((1UL << 2) - 1);
}
static inline __attribute__((no_instrument_function)) void Model1_get_zone_device_page(struct Model1_page *Model1_page)
{
}
static inline __attribute__((no_instrument_function)) void Model1_put_zone_device_page(struct Model1_page *Model1_page)
{
}
static inline __attribute__((no_instrument_function)) bool Model1_is_zone_device_page(const struct Model1_page *Model1_page)
{
 return false;
}


static inline __attribute__((no_instrument_function)) void Model1_get_page(struct Model1_page *Model1_page)
{
 Model1_page = Model1_compound_head(Model1_page);
 /*
	 * Getting a normal page or the head of a compound page
	 * requires to already have an elevated page->_refcount.
	 */
 ((void)(sizeof(( long)(Model1_page_ref_count(Model1_page) <= 0))));
 Model1_page_ref_inc(Model1_page);

 if (__builtin_expect(!!(Model1_is_zone_device_page(Model1_page)), 0))
  Model1_get_zone_device_page(Model1_page);
}

static inline __attribute__((no_instrument_function)) void Model1_put_page(struct Model1_page *Model1_page)
{
 Model1_page = Model1_compound_head(Model1_page);

 if (Model1_put_page_testzero(Model1_page))
  Model1___put_page(Model1_page);

 if (__builtin_expect(!!(Model1_is_zone_device_page(Model1_page)), 0))
  Model1_put_zone_device_page(Model1_page);
}





/*
 * The identification function is mainly used by the buddy allocator for
 * determining if two pages could be buddies. We are not really identifying
 * the zone since we could be using the section number id if we do not have
 * node id available in page flags.
 * We only guarantee that it will return the same value for two combinable
 * pages in a zone.
 */
static inline __attribute__((no_instrument_function)) int Model1_page_zone_id(struct Model1_page *Model1_page)
{
 return (Model1_page->Model1_flags >> ((((((sizeof(unsigned long)*8) - 0) - 6) < ((((sizeof(unsigned long)*8) - 0) - 6) - 2))? (((sizeof(unsigned long)*8) - 0) - 6) : ((((sizeof(unsigned long)*8) - 0) - 6) - 2)) * ((6 + 2) != 0))) & ((1UL << (6 + 2)) - 1);
}

static inline __attribute__((no_instrument_function)) int Model1_zone_to_nid(struct Model1_zone *Model1_zone)
{

 return Model1_zone->Model1_node;



}




static inline __attribute__((no_instrument_function)) int Model1_page_to_nid(const struct Model1_page *Model1_page)
{
 return (Model1_page->Model1_flags >> ((((sizeof(unsigned long)*8) - 0) - 6) * (6 != 0))) & ((1UL << 6) - 1);
}
static inline __attribute__((no_instrument_function)) int Model1_page_cpupid_xchg_last(struct Model1_page *Model1_page, int Model1_cpupid)
{
 return Model1_page_to_nid(Model1_page); /* XXX */
}

static inline __attribute__((no_instrument_function)) int Model1_page_cpupid_last(struct Model1_page *Model1_page)
{
 return Model1_page_to_nid(Model1_page); /* XXX */
}

static inline __attribute__((no_instrument_function)) int Model1_cpupid_to_nid(int Model1_cpupid)
{
 return -1;
}

static inline __attribute__((no_instrument_function)) int Model1_cpupid_to_pid(int Model1_cpupid)
{
 return -1;
}

static inline __attribute__((no_instrument_function)) int Model1_cpupid_to_cpu(int Model1_cpupid)
{
 return -1;
}

static inline __attribute__((no_instrument_function)) int Model1_cpu_pid_to_cpupid(int Model1_nid, int Model1_pid)
{
 return -1;
}

static inline __attribute__((no_instrument_function)) bool Model1_cpupid_pid_unset(int Model1_cpupid)
{
 return 1;
}

static inline __attribute__((no_instrument_function)) void Model1_page_cpupid_reset_last(struct Model1_page *Model1_page)
{
}

static inline __attribute__((no_instrument_function)) bool Model1_cpupid_match_pid(struct Model1_task_struct *Model1_task, int Model1_cpupid)
{
 return false;
}


static inline __attribute__((no_instrument_function)) struct Model1_zone *Model1_page_zone(const struct Model1_page *Model1_page)
{
 return &(Model1_node_data[Model1_page_to_nid(Model1_page)])->Model1_node_zones[Model1_page_zonenum(Model1_page)];
}

static inline __attribute__((no_instrument_function)) Model1_pg_data_t *Model1_page_pgdat(const struct Model1_page *Model1_page)
{
 return (Model1_node_data[Model1_page_to_nid(Model1_page)]);
}
static inline __attribute__((no_instrument_function)) void Model1_set_page_zone(struct Model1_page *Model1_page, enum Model1_zone_type Model1_zone)
{
 Model1_page->Model1_flags &= ~(((1UL << 2) - 1) << (((((sizeof(unsigned long)*8) - 0) - 6) - 2) * (2 != 0)));
 Model1_page->Model1_flags |= (Model1_zone & ((1UL << 2) - 1)) << (((((sizeof(unsigned long)*8) - 0) - 6) - 2) * (2 != 0));
}

static inline __attribute__((no_instrument_function)) void Model1_set_page_node(struct Model1_page *Model1_page, unsigned long Model1_node)
{
 Model1_page->Model1_flags &= ~(((1UL << 6) - 1) << ((((sizeof(unsigned long)*8) - 0) - 6) * (6 != 0)));
 Model1_page->Model1_flags |= (Model1_node & ((1UL << 6) - 1)) << ((((sizeof(unsigned long)*8) - 0) - 6) * (6 != 0));
}

static inline __attribute__((no_instrument_function)) void Model1_set_page_links(struct Model1_page *Model1_page, enum Model1_zone_type Model1_zone,
 unsigned long Model1_node, unsigned long Model1_pfn)
{
 Model1_set_page_zone(Model1_page, Model1_zone);
 Model1_set_page_node(Model1_page, Model1_node);



}
static inline __attribute__((no_instrument_function)) struct Model1_mem_cgroup *Model1_page_memcg(struct Model1_page *Model1_page)
{
 return ((void *)0);
}
static inline __attribute__((no_instrument_function)) struct Model1_mem_cgroup *Model1_page_memcg_rcu(struct Model1_page *Model1_page)
{
 ({ static bool __attribute__ ((__section__(".data.unlikely"))) Model1___warned; int Model1___ret_warn_once = !!(!Model1_rcu_read_lock_held()); if (__builtin_expect(!!(Model1___ret_warn_once && !Model1___warned), 0)) { Model1___warned = true; ({ int Model1___ret_warn_on = !!(1); if (__builtin_expect(!!(Model1___ret_warn_on), 0)) Model1_warn_slowpath_null("./include/linux/mm.h", 993); __builtin_expect(!!(Model1___ret_warn_on), 0); }); } __builtin_expect(!!(Model1___ret_warn_once), 0); });
 return ((void *)0);
}


/*
 * Some inline functions in vmstat.h depend on page_zone()
 */







enum Model1_vm_event_item { Model1_PGPGIN, Model1_PGPGOUT, Model1_PSWPIN, Model1_PSWPOUT,
  Model1_PGALLOC_DMA, Model1_PGALLOC_DMA32, Model1_PGALLOC_NORMAL, Model1_PGALLOC_MOVABLE,
  Model1_ALLOCSTALL_DMA, Model1_ALLOCSTALL_DMA32, Model1_ALLOCSTALL_NORMAL, Model1_ALLOCSTALL_MOVABLE,
  Model1_PGSCAN_SKIP_DMA, Model1_PGSCAN_SKIP_DMA32, Model1_PGSCAN_SKIP_NORMAL, Model1_PGSCAN_SKIP_MOVABLE,
  Model1_PGFREE, Model1_PGACTIVATE, Model1_PGDEACTIVATE,
  Model1_PGFAULT, Model1_PGMAJFAULT,
  Model1_PGLAZYFREED,
  Model1_PGREFILL,
  Model1_PGSTEAL_KSWAPD,
  Model1_PGSTEAL_DIRECT,
  Model1_PGSCAN_KSWAPD,
  Model1_PGSCAN_DIRECT,
  Model1_PGSCAN_DIRECT_THROTTLE,

  Model1_PGSCAN_ZONE_RECLAIM_FAILED,

  Model1_PGINODESTEAL, Model1_SLABS_SCANNED, Model1_KSWAPD_INODESTEAL,
  Model1_KSWAPD_LOW_WMARK_HIT_QUICKLY, Model1_KSWAPD_HIGH_WMARK_HIT_QUICKLY,
  Model1_PAGEOUTRUN, Model1_PGROTATED,
  Model1_DROP_PAGECACHE, Model1_DROP_SLAB,
  Model1_PGMIGRATE_SUCCESS, Model1_PGMIGRATE_FAIL,


  Model1_COMPACTMIGRATE_SCANNED, Model1_COMPACTFREE_SCANNED,
  Model1_COMPACTISOLATED,
  Model1_COMPACTSTALL, Model1_COMPACTFAIL, Model1_COMPACTSUCCESS,
  Model1_KCOMPACTD_WAKE,


  Model1_HTLB_BUDDY_PGALLOC, Model1_HTLB_BUDDY_PGALLOC_FAIL,

  Model1_UNEVICTABLE_PGCULLED, /* culled to noreclaim list */
  Model1_UNEVICTABLE_PGSCANNED, /* scanned for reclaimability */
  Model1_UNEVICTABLE_PGRESCUED, /* rescued from noreclaim list */
  Model1_UNEVICTABLE_PGMLOCKED,
  Model1_UNEVICTABLE_PGMUNLOCKED,
  Model1_UNEVICTABLE_PGCLEARED, /* on COW, page truncate */
  Model1_UNEVICTABLE_PGSTRANDED, /* unable to isolate on unlock */
  Model1_NR_VM_EVENT_ITEMS
};


extern int Model1_sysctl_stat_interval;


/*
 * Light weight per cpu counter implementation.
 *
 * Counters should only be incremented and no critical kernel component
 * should rely on the counter values.
 *
 * Counters are handled completely inline. On many platforms the code
 * generated will simply be the increment of a global address.
 */

struct Model1_vm_event_state {
 unsigned long Model1_event[Model1_NR_VM_EVENT_ITEMS];
};

extern __attribute__((section(".data..percpu" ""))) __typeof__(struct Model1_vm_event_state) Model1_vm_event_states;

/*
 * vm counters are allowed to be racy. Use raw_cpu_ops to avoid the
 * local_irq_disable overhead.
 */
static inline __attribute__((no_instrument_function)) void Model1___count_vm_event(enum Model1_vm_event_item Model1_item)
{
 do { do { const void *Model1___vpp_verify = (typeof((&(Model1_vm_event_states.Model1_event[Model1_item])) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); switch(sizeof(Model1_vm_event_states.Model1_event[Model1_item])) { case 1: do { typedef typeof((Model1_vm_event_states.Model1_event[Model1_item])) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (1); (void)Model1_pao_tmp__; } switch (sizeof((Model1_vm_event_states.Model1_event[Model1_item]))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "qi" ((Model1_pao_T__)(1))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(1))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(1))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "re" ((Model1_pao_T__)(1))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 2: do { typedef typeof((Model1_vm_event_states.Model1_event[Model1_item])) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (1); (void)Model1_pao_tmp__; } switch (sizeof((Model1_vm_event_states.Model1_event[Model1_item]))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "qi" ((Model1_pao_T__)(1))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(1))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(1))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "re" ((Model1_pao_T__)(1))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 4: do { typedef typeof((Model1_vm_event_states.Model1_event[Model1_item])) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (1); (void)Model1_pao_tmp__; } switch (sizeof((Model1_vm_event_states.Model1_event[Model1_item]))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "qi" ((Model1_pao_T__)(1))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(1))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(1))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "re" ((Model1_pao_T__)(1))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 8: do { typedef typeof((Model1_vm_event_states.Model1_event[Model1_item])) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (1); (void)Model1_pao_tmp__; } switch (sizeof((Model1_vm_event_states.Model1_event[Model1_item]))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "qi" ((Model1_pao_T__)(1))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(1))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(1))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "re" ((Model1_pao_T__)(1))); break; default: Model1___bad_percpu_size(); } } while (0);break; default: Model1___bad_size_call_parameter();break; } } while (0);
}

static inline __attribute__((no_instrument_function)) void Model1_count_vm_event(enum Model1_vm_event_item Model1_item)
{
 do { do { const void *Model1___vpp_verify = (typeof((&(Model1_vm_event_states.Model1_event[Model1_item])) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); switch(sizeof(Model1_vm_event_states.Model1_event[Model1_item])) { case 1: do { typedef typeof((Model1_vm_event_states.Model1_event[Model1_item])) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (1); (void)Model1_pao_tmp__; } switch (sizeof((Model1_vm_event_states.Model1_event[Model1_item]))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "qi" ((Model1_pao_T__)(1))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(1))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(1))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "re" ((Model1_pao_T__)(1))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 2: do { typedef typeof((Model1_vm_event_states.Model1_event[Model1_item])) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (1); (void)Model1_pao_tmp__; } switch (sizeof((Model1_vm_event_states.Model1_event[Model1_item]))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "qi" ((Model1_pao_T__)(1))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(1))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(1))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "re" ((Model1_pao_T__)(1))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 4: do { typedef typeof((Model1_vm_event_states.Model1_event[Model1_item])) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (1); (void)Model1_pao_tmp__; } switch (sizeof((Model1_vm_event_states.Model1_event[Model1_item]))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "qi" ((Model1_pao_T__)(1))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(1))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(1))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "re" ((Model1_pao_T__)(1))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 8: do { typedef typeof((Model1_vm_event_states.Model1_event[Model1_item])) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (1); (void)Model1_pao_tmp__; } switch (sizeof((Model1_vm_event_states.Model1_event[Model1_item]))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "qi" ((Model1_pao_T__)(1))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(1))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(1))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "re" ((Model1_pao_T__)(1))); break; default: Model1___bad_percpu_size(); } } while (0);break; default: Model1___bad_size_call_parameter();break; } } while (0);
}

static inline __attribute__((no_instrument_function)) void Model1___count_vm_events(enum Model1_vm_event_item Model1_item, long Model1_delta)
{
 do { do { const void *Model1___vpp_verify = (typeof((&(Model1_vm_event_states.Model1_event[Model1_item])) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); switch(sizeof(Model1_vm_event_states.Model1_event[Model1_item])) { case 1: do { typedef typeof((Model1_vm_event_states.Model1_event[Model1_item])) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(Model1_delta) && ((Model1_delta) == 1 || (Model1_delta) == -1)) ? (int)(Model1_delta) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (Model1_delta); (void)Model1_pao_tmp__; } switch (sizeof((Model1_vm_event_states.Model1_event[Model1_item]))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "qi" ((Model1_pao_T__)(Model1_delta))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(Model1_delta))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(Model1_delta))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "re" ((Model1_pao_T__)(Model1_delta))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 2: do { typedef typeof((Model1_vm_event_states.Model1_event[Model1_item])) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(Model1_delta) && ((Model1_delta) == 1 || (Model1_delta) == -1)) ? (int)(Model1_delta) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (Model1_delta); (void)Model1_pao_tmp__; } switch (sizeof((Model1_vm_event_states.Model1_event[Model1_item]))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "qi" ((Model1_pao_T__)(Model1_delta))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(Model1_delta))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(Model1_delta))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "re" ((Model1_pao_T__)(Model1_delta))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 4: do { typedef typeof((Model1_vm_event_states.Model1_event[Model1_item])) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(Model1_delta) && ((Model1_delta) == 1 || (Model1_delta) == -1)) ? (int)(Model1_delta) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (Model1_delta); (void)Model1_pao_tmp__; } switch (sizeof((Model1_vm_event_states.Model1_event[Model1_item]))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "qi" ((Model1_pao_T__)(Model1_delta))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(Model1_delta))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(Model1_delta))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "re" ((Model1_pao_T__)(Model1_delta))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 8: do { typedef typeof((Model1_vm_event_states.Model1_event[Model1_item])) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(Model1_delta) && ((Model1_delta) == 1 || (Model1_delta) == -1)) ? (int)(Model1_delta) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (Model1_delta); (void)Model1_pao_tmp__; } switch (sizeof((Model1_vm_event_states.Model1_event[Model1_item]))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "qi" ((Model1_pao_T__)(Model1_delta))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(Model1_delta))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(Model1_delta))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "re" ((Model1_pao_T__)(Model1_delta))); break; default: Model1___bad_percpu_size(); } } while (0);break; default: Model1___bad_size_call_parameter();break; } } while (0);
}

static inline __attribute__((no_instrument_function)) void Model1_count_vm_events(enum Model1_vm_event_item Model1_item, long Model1_delta)
{
 do { do { const void *Model1___vpp_verify = (typeof((&(Model1_vm_event_states.Model1_event[Model1_item])) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); switch(sizeof(Model1_vm_event_states.Model1_event[Model1_item])) { case 1: do { typedef typeof((Model1_vm_event_states.Model1_event[Model1_item])) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(Model1_delta) && ((Model1_delta) == 1 || (Model1_delta) == -1)) ? (int)(Model1_delta) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (Model1_delta); (void)Model1_pao_tmp__; } switch (sizeof((Model1_vm_event_states.Model1_event[Model1_item]))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "qi" ((Model1_pao_T__)(Model1_delta))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(Model1_delta))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(Model1_delta))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "re" ((Model1_pao_T__)(Model1_delta))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 2: do { typedef typeof((Model1_vm_event_states.Model1_event[Model1_item])) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(Model1_delta) && ((Model1_delta) == 1 || (Model1_delta) == -1)) ? (int)(Model1_delta) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (Model1_delta); (void)Model1_pao_tmp__; } switch (sizeof((Model1_vm_event_states.Model1_event[Model1_item]))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "qi" ((Model1_pao_T__)(Model1_delta))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(Model1_delta))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(Model1_delta))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "re" ((Model1_pao_T__)(Model1_delta))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 4: do { typedef typeof((Model1_vm_event_states.Model1_event[Model1_item])) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(Model1_delta) && ((Model1_delta) == 1 || (Model1_delta) == -1)) ? (int)(Model1_delta) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (Model1_delta); (void)Model1_pao_tmp__; } switch (sizeof((Model1_vm_event_states.Model1_event[Model1_item]))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "qi" ((Model1_pao_T__)(Model1_delta))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(Model1_delta))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(Model1_delta))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "re" ((Model1_pao_T__)(Model1_delta))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 8: do { typedef typeof((Model1_vm_event_states.Model1_event[Model1_item])) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(Model1_delta) && ((Model1_delta) == 1 || (Model1_delta) == -1)) ? (int)(Model1_delta) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (Model1_delta); (void)Model1_pao_tmp__; } switch (sizeof((Model1_vm_event_states.Model1_event[Model1_item]))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "qi" ((Model1_pao_T__)(Model1_delta))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(Model1_delta))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "ri" ((Model1_pao_T__)(Model1_delta))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item]))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((Model1_vm_event_states.Model1_event[Model1_item])) : "re" ((Model1_pao_T__)(Model1_delta))); break; default: Model1___bad_percpu_size(); } } while (0);break; default: Model1___bad_size_call_parameter();break; } } while (0);
}

extern void Model1_all_vm_events(unsigned long *);

extern void Model1_vm_events_fold_cpu(int Model1_cpu);
/*
 * Zone and node-based page accounting with per cpu differentials.
 */
extern Model1_atomic_long_t Model1_vm_zone_stat[Model1_NR_VM_ZONE_STAT_ITEMS];
extern Model1_atomic_long_t Model1_vm_node_stat[Model1_NR_VM_NODE_STAT_ITEMS];

static inline __attribute__((no_instrument_function)) void Model1_zone_page_state_add(long Model1_x, struct Model1_zone *Model1_zone,
     enum Model1_zone_stat_item Model1_item)
{
 Model1_atomic_long_add(Model1_x, &Model1_zone->Model1_vm_stat[Model1_item]);
 Model1_atomic_long_add(Model1_x, &Model1_vm_zone_stat[Model1_item]);
}

static inline __attribute__((no_instrument_function)) void Model1_node_page_state_add(long Model1_x, struct Model1_pglist_data *Model1_pgdat,
     enum Model1_node_stat_item Model1_item)
{
 Model1_atomic_long_add(Model1_x, &Model1_pgdat->Model1_vm_stat[Model1_item]);
 Model1_atomic_long_add(Model1_x, &Model1_vm_node_stat[Model1_item]);
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_global_page_state(enum Model1_zone_stat_item Model1_item)
{
 long Model1_x = Model1_atomic_long_read(&Model1_vm_zone_stat[Model1_item]);

 if (Model1_x < 0)
  Model1_x = 0;

 return Model1_x;
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_global_node_page_state(enum Model1_node_stat_item Model1_item)
{
 long Model1_x = Model1_atomic_long_read(&Model1_vm_node_stat[Model1_item]);

 if (Model1_x < 0)
  Model1_x = 0;

 return Model1_x;
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_zone_page_state(struct Model1_zone *Model1_zone,
     enum Model1_zone_stat_item Model1_item)
{
 long Model1_x = Model1_atomic_long_read(&Model1_zone->Model1_vm_stat[Model1_item]);

 if (Model1_x < 0)
  Model1_x = 0;

 return Model1_x;
}

/*
 * More accurate version that also considers the currently pending
 * deltas. For that we need to loop over all cpus to find the current
 * deltas. There is no synchronization so the result cannot be
 * exactly accurate either.
 */
static inline __attribute__((no_instrument_function)) unsigned long Model1_zone_page_state_snapshot(struct Model1_zone *Model1_zone,
     enum Model1_zone_stat_item Model1_item)
{
 long Model1_x = Model1_atomic_long_read(&Model1_zone->Model1_vm_stat[Model1_item]);


 int Model1_cpu;
 for (((Model1_cpu)) = -1; ((Model1_cpu)) = Model1_cpumask_next(((Model1_cpu)), (((const struct Model1_cpumask *)&Model1___cpu_online_mask))), ((Model1_cpu)) < Model1_nr_cpu_ids;)
  Model1_x += ({ do { const void *Model1___vpp_verify = (typeof((Model1_zone->Model1_pageset) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); ({ unsigned long Model1___ptr; __asm__ ("" : "=r"(Model1___ptr) : "0"((typeof(*((Model1_zone->Model1_pageset))) *)((Model1_zone->Model1_pageset)))); (typeof((typeof(*((Model1_zone->Model1_pageset))) *)((Model1_zone->Model1_pageset)))) (Model1___ptr + (((Model1___per_cpu_offset[(Model1_cpu)])))); }); })->Model1_vm_stat_diff[Model1_item];

 if (Model1_x < 0)
  Model1_x = 0;

 return Model1_x;
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_node_page_state_snapshot(Model1_pg_data_t *Model1_pgdat,
     enum Model1_node_stat_item Model1_item)
{
 long Model1_x = Model1_atomic_long_read(&Model1_pgdat->Model1_vm_stat[Model1_item]);


 int Model1_cpu;
 for (((Model1_cpu)) = -1; ((Model1_cpu)) = Model1_cpumask_next(((Model1_cpu)), (((const struct Model1_cpumask *)&Model1___cpu_online_mask))), ((Model1_cpu)) < Model1_nr_cpu_ids;)
  Model1_x += ({ do { const void *Model1___vpp_verify = (typeof((Model1_pgdat->Model1_per_cpu_nodestats) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); ({ unsigned long Model1___ptr; __asm__ ("" : "=r"(Model1___ptr) : "0"((typeof(*((Model1_pgdat->Model1_per_cpu_nodestats))) *)((Model1_pgdat->Model1_per_cpu_nodestats)))); (typeof((typeof(*((Model1_pgdat->Model1_per_cpu_nodestats))) *)((Model1_pgdat->Model1_per_cpu_nodestats)))) (Model1___ptr + (((Model1___per_cpu_offset[(Model1_cpu)])))); }); })->Model1_vm_node_stat_diff[Model1_item];

 if (Model1_x < 0)
  Model1_x = 0;

 return Model1_x;
}



extern unsigned long Model1_sum_zone_node_page_state(int Model1_node,
      enum Model1_zone_stat_item Model1_item);
extern unsigned long Model1_node_page_state(struct Model1_pglist_data *Model1_pgdat,
      enum Model1_node_stat_item Model1_item);
void Model1___mod_zone_page_state(struct Model1_zone *, enum Model1_zone_stat_item Model1_item, long);
void Model1___inc_zone_page_state(struct Model1_page *, enum Model1_zone_stat_item);
void Model1___dec_zone_page_state(struct Model1_page *, enum Model1_zone_stat_item);

void Model1___mod_node_page_state(struct Model1_pglist_data *, enum Model1_node_stat_item Model1_item, long);
void Model1___inc_node_page_state(struct Model1_page *, enum Model1_node_stat_item);
void Model1___dec_node_page_state(struct Model1_page *, enum Model1_node_stat_item);

void Model1_mod_zone_page_state(struct Model1_zone *, enum Model1_zone_stat_item, long);
void Model1_inc_zone_page_state(struct Model1_page *, enum Model1_zone_stat_item);
void Model1_dec_zone_page_state(struct Model1_page *, enum Model1_zone_stat_item);

void Model1_mod_node_page_state(struct Model1_pglist_data *, enum Model1_node_stat_item, long);
void Model1_inc_node_page_state(struct Model1_page *, enum Model1_node_stat_item);
void Model1_dec_node_page_state(struct Model1_page *, enum Model1_node_stat_item);

extern void Model1_inc_node_state(struct Model1_pglist_data *, enum Model1_node_stat_item);
extern void Model1___inc_zone_state(struct Model1_zone *, enum Model1_zone_stat_item);
extern void Model1___inc_node_state(struct Model1_pglist_data *, enum Model1_node_stat_item);
extern void Model1_dec_zone_state(struct Model1_zone *, enum Model1_zone_stat_item);
extern void Model1___dec_zone_state(struct Model1_zone *, enum Model1_zone_stat_item);
extern void Model1___dec_node_state(struct Model1_pglist_data *, enum Model1_node_stat_item);

void Model1_quiet_vmstat(void);
void Model1_cpu_vm_stats_fold(int Model1_cpu);
void Model1_refresh_zone_stat_thresholds(void);

struct Model1_ctl_table;
int Model1_vmstat_refresh(struct Model1_ctl_table *, int Model1_write,
     void *Model1_buffer, Model1_size_t *Model1_lenp, Model1_loff_t *Model1_ppos);

void Model1_drain_zonestat(struct Model1_zone *Model1_zone, struct Model1_per_cpu_pageset *);

int Model1_calculate_pressure_threshold(struct Model1_zone *Model1_zone);
int Model1_calculate_normal_threshold(struct Model1_zone *Model1_zone);
void Model1_set_pgdat_percpu_threshold(Model1_pg_data_t *Model1_pgdat,
    int (*Model1_calculate_pressure)(struct Model1_zone *));
static inline __attribute__((no_instrument_function)) void Model1___mod_zone_freepage_state(struct Model1_zone *Model1_zone, int Model1_nr_pages,
          int Model1_migratetype)
{
 Model1___mod_zone_page_state(Model1_zone, Model1_NR_FREE_PAGES, Model1_nr_pages);
 if (false)
  Model1___mod_zone_page_state(Model1_zone, Model1_NR_FREE_CMA_PAGES, Model1_nr_pages);
}

extern const char * const Model1_vmstat_text[];

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void *Model1_lowmem_page_address(const struct Model1_page *Model1_page)
{
 return ((void *)((unsigned long)(((Model1_phys_addr_t)((unsigned long)((Model1_page) - ((struct Model1_page *)(0xffffea0000000000UL)))) << 12))+((unsigned long)(0xffff880000000000UL))));
}
extern void *Model1_page_rmapping(struct Model1_page *Model1_page);
extern struct Model1_anon_vma *Model1_page_anon_vma(struct Model1_page *Model1_page);
extern struct Model1_address_space *Model1_page_mapping(struct Model1_page *Model1_page);

extern struct Model1_address_space *Model1___page_file_mapping(struct Model1_page *);

static inline __attribute__((no_instrument_function))
struct Model1_address_space *Model1_page_file_mapping(struct Model1_page *Model1_page)
{
 if (__builtin_expect(!!(Model1_PageSwapCache(Model1_page)), 0))
  return Model1___page_file_mapping(Model1_page);

 return Model1_page->Model1_mapping;
}

/*
 * Return the pagecache index of the passed page.  Regular pagecache pages
 * use ->index whereas swapcache pages use ->private
 */
static inline __attribute__((no_instrument_function)) unsigned long Model1_page_index(struct Model1_page *Model1_page)
{
 if (__builtin_expect(!!(Model1_PageSwapCache(Model1_page)), 0))
  return ((Model1_page)->Model1_private);
 return Model1_page->Model1_index;
}

extern unsigned long Model1___page_file_index(struct Model1_page *Model1_page);

/*
 * Return the file index of the page. Regular pagecache pages use ->index
 * whereas swapcache pages use swp_offset(->private)
 */
static inline __attribute__((no_instrument_function)) unsigned long Model1_page_file_index(struct Model1_page *Model1_page)
{
 if (__builtin_expect(!!(Model1_PageSwapCache(Model1_page)), 0))
  return Model1___page_file_index(Model1_page);

 return Model1_page->Model1_index;
}

bool Model1_page_mapped(struct Model1_page *Model1_page);
struct Model1_address_space *Model1_page_mapping(struct Model1_page *Model1_page);

/*
 * Return true only if the page has been allocated with
 * ALLOC_NO_WATERMARKS and the low watermark was not
 * met implying that the system is under some pressure.
 */
static inline __attribute__((no_instrument_function)) bool Model1_page_is_pfmemalloc(struct Model1_page *Model1_page)
{
 /*
	 * Page index cannot be this large so this must be
	 * a pfmemalloc page.
	 */
 return Model1_page->Model1_index == -1UL;
}

/*
 * Only to be called by the page allocator on a freshly allocated
 * page.
 */
static inline __attribute__((no_instrument_function)) void Model1_set_page_pfmemalloc(struct Model1_page *Model1_page)
{
 Model1_page->Model1_index = -1UL;
}

static inline __attribute__((no_instrument_function)) void Model1_clear_page_pfmemalloc(struct Model1_page *Model1_page)
{
 Model1_page->Model1_index = 0;
}

/*
 * Different kinds of faults, as returned by handle_mm_fault().
 * Used to decide whether a process gets delivered SIGBUS or
 * just gets major/minor fault counters bumped up.
 */
/* Encode hstate index for a hwpoisoned large page */



/*
 * Can be called by the pagefault handler when it gets a VM_FAULT_OOM.
 */
extern void Model1_pagefault_out_of_memory(void);



/*
 * Flags passed to show_mem() and show_free_areas() to suppress output in
 * various contexts.
 */


extern void Model1_show_free_areas(unsigned int Model1_flags);
extern bool Model1_skip_free_areas_node(unsigned int Model1_flags, int Model1_nid);

int Model1_shmem_zero_setup(struct Model1_vm_area_struct *);

bool Model1_shmem_mapping(struct Model1_address_space *Model1_mapping);







extern bool Model1_can_do_mlock(void);
extern int Model1_user_shm_lock(Model1_size_t, struct Model1_user_struct *);
extern void Model1_user_shm_unlock(Model1_size_t, struct Model1_user_struct *);

/*
 * Parameter block passed down to zap_pte_range in exceptional cases.
 */
struct Model1_zap_details {
 struct Model1_address_space *Model1_check_mapping; /* Check page->mapping if set */
 unsigned long Model1_first_index; /* Lowest page->index to unmap */
 unsigned long Model1_last_index; /* Highest page->index to unmap */
 bool Model1_ignore_dirty; /* Ignore dirty pages */
 bool Model1_check_swap_entries; /* Check also swap entries */
};

struct Model1_page *Model1_vm_normal_page(struct Model1_vm_area_struct *Model1_vma, unsigned long Model1_addr,
  Model1_pte_t Model1_pte);
struct Model1_page *Model1_vm_normal_page_pmd(struct Model1_vm_area_struct *Model1_vma, unsigned long Model1_addr,
    Model1_pmd_t Model1_pmd);

int Model1_zap_vma_ptes(struct Model1_vm_area_struct *Model1_vma, unsigned long Model1_address,
  unsigned long Model1_size);
void Model1_zap_page_range(struct Model1_vm_area_struct *Model1_vma, unsigned long Model1_address,
  unsigned long Model1_size, struct Model1_zap_details *);
void Model1_unmap_vmas(struct Model1_mmu_gather *Model1_tlb, struct Model1_vm_area_struct *Model1_start_vma,
  unsigned long Model1_start, unsigned long Model1_end);

/**
 * mm_walk - callbacks for walk_page_range
 * @pmd_entry: if set, called for each non-empty PMD (3rd-level) entry
 *	       this handler is required to be able to handle
 *	       pmd_trans_huge() pmds.  They may simply choose to
 *	       split_huge_page() instead of handling it explicitly.
 * @pte_entry: if set, called for each non-empty PTE (4th-level) entry
 * @pte_hole: if set, called for each hole at all levels
 * @hugetlb_entry: if set, called for each hugetlb entry
 * @test_walk: caller specific callback function to determine whether
 *             we walk over the current vma or not. A positive returned
 *             value means "do page table walk over the current vma,"
 *             and a negative one means "abort current page table walk
 *             right now." 0 means "skip the current vma."
 * @mm:        mm_struct representing the target process of page table walk
 * @vma:       vma currently walked (NULL if walking outside vmas)
 * @private:   private data for callbacks' usage
 *
 * (see the comment on walk_page_range() for more details)
 */
struct Model1_mm_walk {
 int (*Model1_pmd_entry)(Model1_pmd_t *Model1_pmd, unsigned long Model1_addr,
    unsigned long Model1_next, struct Model1_mm_walk *Model1_walk);
 int (*Model1_pte_entry)(Model1_pte_t *Model1_pte, unsigned long Model1_addr,
    unsigned long Model1_next, struct Model1_mm_walk *Model1_walk);
 int (*Model1_pte_hole)(unsigned long Model1_addr, unsigned long Model1_next,
   struct Model1_mm_walk *Model1_walk);
 int (*Model1_hugetlb_entry)(Model1_pte_t *Model1_pte, unsigned long Model1_hmask,
        unsigned long Model1_addr, unsigned long Model1_next,
        struct Model1_mm_walk *Model1_walk);
 int (*Model1_test_walk)(unsigned long Model1_addr, unsigned long Model1_next,
   struct Model1_mm_walk *Model1_walk);
 struct Model1_mm_struct *Model1_mm;
 struct Model1_vm_area_struct *Model1_vma;
 void *Model1_private;
};

int Model1_walk_page_range(unsigned long Model1_addr, unsigned long Model1_end,
  struct Model1_mm_walk *Model1_walk);
int Model1_walk_page_vma(struct Model1_vm_area_struct *Model1_vma, struct Model1_mm_walk *Model1_walk);
void Model1_free_pgd_range(struct Model1_mmu_gather *Model1_tlb, unsigned long Model1_addr,
  unsigned long Model1_end, unsigned long Model1_floor, unsigned long Model1_ceiling);
int Model1_copy_page_range(struct Model1_mm_struct *Model1_dst, struct Model1_mm_struct *Model1_src,
   struct Model1_vm_area_struct *Model1_vma);
void Model1_unmap_mapping_range(struct Model1_address_space *Model1_mapping,
  Model1_loff_t const Model1_holebegin, Model1_loff_t const Model1_holelen, int Model1_even_cows);
int Model1_follow_pfn(struct Model1_vm_area_struct *Model1_vma, unsigned long Model1_address,
 unsigned long *Model1_pfn);
int Model1_follow_phys(struct Model1_vm_area_struct *Model1_vma, unsigned long Model1_address,
  unsigned int Model1_flags, unsigned long *Model1_prot, Model1_resource_size_t *Model1_phys);
int Model1_generic_access_phys(struct Model1_vm_area_struct *Model1_vma, unsigned long Model1_addr,
   void *Model1_buf, int Model1_len, int Model1_write);

static inline __attribute__((no_instrument_function)) void Model1_unmap_shared_mapping_range(struct Model1_address_space *Model1_mapping,
  Model1_loff_t const Model1_holebegin, Model1_loff_t const Model1_holelen)
{
 Model1_unmap_mapping_range(Model1_mapping, Model1_holebegin, Model1_holelen, 0);
}

extern void Model1_truncate_pagecache(struct Model1_inode *Model1_inode, Model1_loff_t Model1_new);
extern void Model1_truncate_setsize(struct Model1_inode *Model1_inode, Model1_loff_t Model1_newsize);
void Model1_pagecache_isize_extended(struct Model1_inode *Model1_inode, Model1_loff_t Model1_from, Model1_loff_t Model1_to);
void Model1_truncate_pagecache_range(struct Model1_inode *Model1_inode, Model1_loff_t Model1_offset, Model1_loff_t Model1_end);
int Model1_truncate_inode_page(struct Model1_address_space *Model1_mapping, struct Model1_page *Model1_page);
int Model1_generic_error_remove_page(struct Model1_address_space *Model1_mapping, struct Model1_page *Model1_page);
int Model1_invalidate_inode_page(struct Model1_page *Model1_page);


extern int Model1_handle_mm_fault(struct Model1_vm_area_struct *Model1_vma, unsigned long Model1_address,
  unsigned int Model1_flags);
extern int Model1_fixup_user_fault(struct Model1_task_struct *Model1_tsk, struct Model1_mm_struct *Model1_mm,
       unsigned long Model1_address, unsigned int Model1_fault_flags,
       bool *Model1_unlocked);
extern int Model1_access_process_vm(struct Model1_task_struct *Model1_tsk, unsigned long Model1_addr, void *Model1_buf, int Model1_len, int Model1_write);
extern int Model1_access_remote_vm(struct Model1_mm_struct *Model1_mm, unsigned long Model1_addr,
  void *Model1_buf, int Model1_len, int Model1_write);

long Model1___get_user_pages(struct Model1_task_struct *Model1_tsk, struct Model1_mm_struct *Model1_mm,
        unsigned long Model1_start, unsigned long Model1_nr_pages,
        unsigned int Model1_foll_flags, struct Model1_page **Model1_pages,
        struct Model1_vm_area_struct **Model1_vmas, int *Model1_nonblocking);
long Model1_get_user_pages_remote(struct Model1_task_struct *Model1_tsk, struct Model1_mm_struct *Model1_mm,
       unsigned long Model1_start, unsigned long Model1_nr_pages,
       int Model1_write, int Model1_force, struct Model1_page **Model1_pages,
       struct Model1_vm_area_struct **Model1_vmas);
long Model1_get_user_pages(unsigned long Model1_start, unsigned long Model1_nr_pages,
       int Model1_write, int Model1_force, struct Model1_page **Model1_pages,
       struct Model1_vm_area_struct **Model1_vmas);
long Model1_get_user_pages_locked(unsigned long Model1_start, unsigned long Model1_nr_pages,
      int Model1_write, int Model1_force, struct Model1_page **Model1_pages, int *Model1_locked);
long Model1___get_user_pages_unlocked(struct Model1_task_struct *Model1_tsk, struct Model1_mm_struct *Model1_mm,
          unsigned long Model1_start, unsigned long Model1_nr_pages,
          int Model1_write, int Model1_force, struct Model1_page **Model1_pages,
          unsigned int Model1_gup_flags);
long Model1_get_user_pages_unlocked(unsigned long Model1_start, unsigned long Model1_nr_pages,
      int Model1_write, int Model1_force, struct Model1_page **Model1_pages);
int Model1_get_user_pages_fast(unsigned long Model1_start, int Model1_nr_pages, int Model1_write,
   struct Model1_page **Model1_pages);

/* Container for pinned pfns / pages */
struct Model1_frame_vector {
 unsigned int Model1_nr_allocated; /* Number of frames we have space for */
 unsigned int Model1_nr_frames; /* Number of frames stored in ptrs array */
 bool Model1_got_ref; /* Did we pin pages by getting page ref? */
 bool Model1_is_pfns; /* Does array contain pages or pfns? */
 void *Model1_ptrs[0]; /* Array of pinned pfns / pages. Use
				 * pfns_vector_pages() or pfns_vector_pfns()
				 * for access */
};

struct Model1_frame_vector *Model1_frame_vector_create(unsigned int Model1_nr_frames);
void Model1_frame_vector_destroy(struct Model1_frame_vector *Model1_vec);
int Model1_get_vaddr_frames(unsigned long Model1_start, unsigned int Model1_nr_pfns,
       bool Model1_write, bool Model1_force, struct Model1_frame_vector *Model1_vec);
void Model1_put_vaddr_frames(struct Model1_frame_vector *Model1_vec);
int Model1_frame_vector_to_pages(struct Model1_frame_vector *Model1_vec);
void Model1_frame_vector_to_pfns(struct Model1_frame_vector *Model1_vec);

static inline __attribute__((no_instrument_function)) unsigned int Model1_frame_vector_count(struct Model1_frame_vector *Model1_vec)
{
 return Model1_vec->Model1_nr_frames;
}

static inline __attribute__((no_instrument_function)) struct Model1_page **Model1_frame_vector_pages(struct Model1_frame_vector *Model1_vec)
{
 if (Model1_vec->Model1_is_pfns) {
  int err = Model1_frame_vector_to_pages(Model1_vec);

  if (err)
   return Model1_ERR_PTR(err);
 }
 return (struct Model1_page **)(Model1_vec->Model1_ptrs);
}

static inline __attribute__((no_instrument_function)) unsigned long *Model1_frame_vector_pfns(struct Model1_frame_vector *Model1_vec)
{
 if (!Model1_vec->Model1_is_pfns)
  Model1_frame_vector_to_pfns(Model1_vec);
 return (unsigned long *)(Model1_vec->Model1_ptrs);
}

struct Model1_kvec;
int Model1_get_kernel_pages(const struct Model1_kvec *Model1_iov, int Model1_nr_pages, int Model1_write,
   struct Model1_page **Model1_pages);
int Model1_get_kernel_page(unsigned long Model1_start, int Model1_write, struct Model1_page **Model1_pages);
struct Model1_page *Model1_get_dump_page(unsigned long Model1_addr);

extern int Model1_try_to_release_page(struct Model1_page * Model1_page, Model1_gfp_t Model1_gfp_mask);
extern void Model1_do_invalidatepage(struct Model1_page *Model1_page, unsigned int Model1_offset,
         unsigned int Model1_length);

int Model1___set_page_dirty_nobuffers(struct Model1_page *Model1_page);
int Model1___set_page_dirty_no_writeback(struct Model1_page *Model1_page);
int Model1_redirty_page_for_writepage(struct Model1_writeback_control *Model1_wbc,
    struct Model1_page *Model1_page);
void Model1_account_page_dirtied(struct Model1_page *Model1_page, struct Model1_address_space *Model1_mapping);
void Model1_account_page_cleaned(struct Model1_page *Model1_page, struct Model1_address_space *Model1_mapping,
     struct Model1_bdi_writeback *Model1_wb);
int Model1_set_page_dirty(struct Model1_page *Model1_page);
int Model1_set_page_dirty_lock(struct Model1_page *Model1_page);
void Model1_cancel_dirty_page(struct Model1_page *Model1_page);
int Model1_clear_page_dirty_for_io(struct Model1_page *Model1_page);

int Model1_get_cmdline(struct Model1_task_struct *Model1_task, char *Model1_buffer, int Model1_buflen);

/* Is the vma a continuation of the stack vma above it? */
static inline __attribute__((no_instrument_function)) int Model1_vma_growsdown(struct Model1_vm_area_struct *Model1_vma, unsigned long Model1_addr)
{
 return Model1_vma && (Model1_vma->Model1_vm_end == Model1_addr) && (Model1_vma->Model1_vm_flags & 0x00000100);
}

static inline __attribute__((no_instrument_function)) bool Model1_vma_is_anonymous(struct Model1_vm_area_struct *Model1_vma)
{
 return !Model1_vma->Model1_vm_ops;
}

static inline __attribute__((no_instrument_function)) int Model1_stack_guard_page_start(struct Model1_vm_area_struct *Model1_vma,
          unsigned long Model1_addr)
{
 return (Model1_vma->Model1_vm_flags & 0x00000100) &&
  (Model1_vma->Model1_vm_start == Model1_addr) &&
  !Model1_vma_growsdown(Model1_vma->Model1_vm_prev, Model1_addr);
}

/* Is the vma a continuation of the stack vma below it? */
static inline __attribute__((no_instrument_function)) int Model1_vma_growsup(struct Model1_vm_area_struct *Model1_vma, unsigned long Model1_addr)
{
 return Model1_vma && (Model1_vma->Model1_vm_start == Model1_addr) && (Model1_vma->Model1_vm_flags & 0x00000000);
}

static inline __attribute__((no_instrument_function)) int Model1_stack_guard_page_end(struct Model1_vm_area_struct *Model1_vma,
        unsigned long Model1_addr)
{
 return (Model1_vma->Model1_vm_flags & 0x00000000) &&
  (Model1_vma->Model1_vm_end == Model1_addr) &&
  !Model1_vma_growsup(Model1_vma->Model1_vm_next, Model1_addr);
}

int Model1_vma_is_stack_for_task(struct Model1_vm_area_struct *Model1_vma, struct Model1_task_struct *Model1_t);

extern unsigned long Model1_move_page_tables(struct Model1_vm_area_struct *Model1_vma,
  unsigned long Model1_old_addr, struct Model1_vm_area_struct *Model1_new_vma,
  unsigned long Model1_new_addr, unsigned long Model1_len,
  bool Model1_need_rmap_locks);
extern unsigned long Model1_change_protection(struct Model1_vm_area_struct *Model1_vma, unsigned long Model1_start,
         unsigned long Model1_end, Model1_pgprot_t Model1_newprot,
         int Model1_dirty_accountable, int Model1_prot_numa);
extern int Model1_mprotect_fixup(struct Model1_vm_area_struct *Model1_vma,
     struct Model1_vm_area_struct **Model1_pprev, unsigned long Model1_start,
     unsigned long Model1_end, unsigned long Model1_newflags);

/*
 * doesn't attempt to fault and will return short.
 */
int Model1___get_user_pages_fast(unsigned long Model1_start, int Model1_nr_pages, int Model1_write,
     struct Model1_page **Model1_pages);
/*
 * per-process(per-mm_struct) statistics.
 */
static inline __attribute__((no_instrument_function)) unsigned long Model1_get_mm_counter(struct Model1_mm_struct *Model1_mm, int Model1_member)
{
 long Model1_val = Model1_atomic_long_read(&Model1_mm->Model1_rss_stat.Model1_count[Model1_member]);


 /*
	 * counter is updated in asynchronous manner and may go to minus.
	 * But it's never be expected number for users.
	 */
 if (Model1_val < 0)
  Model1_val = 0;

 return (unsigned long)Model1_val;
}

static inline __attribute__((no_instrument_function)) void Model1_add_mm_counter(struct Model1_mm_struct *Model1_mm, int Model1_member, long Model1_value)
{
 Model1_atomic_long_add(Model1_value, &Model1_mm->Model1_rss_stat.Model1_count[Model1_member]);
}

static inline __attribute__((no_instrument_function)) void Model1_inc_mm_counter(struct Model1_mm_struct *Model1_mm, int Model1_member)
{
 Model1_atomic_long_inc(&Model1_mm->Model1_rss_stat.Model1_count[Model1_member]);
}

static inline __attribute__((no_instrument_function)) void Model1_dec_mm_counter(struct Model1_mm_struct *Model1_mm, int Model1_member)
{
 Model1_atomic_long_dec(&Model1_mm->Model1_rss_stat.Model1_count[Model1_member]);
}

/* Optimized variant when page is already known not to be PageAnon */
static inline __attribute__((no_instrument_function)) int Model1_mm_counter_file(struct Model1_page *Model1_page)
{
 if (Model1_PageSwapBacked(Model1_page))
  return Model1_MM_SHMEMPAGES;
 return Model1_MM_FILEPAGES;
}

static inline __attribute__((no_instrument_function)) int Model1_mm_counter(struct Model1_page *Model1_page)
{
 if (Model1_PageAnon(Model1_page))
  return Model1_MM_ANONPAGES;
 return Model1_mm_counter_file(Model1_page);
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_get_mm_rss(struct Model1_mm_struct *Model1_mm)
{
 return Model1_get_mm_counter(Model1_mm, Model1_MM_FILEPAGES) +
  Model1_get_mm_counter(Model1_mm, Model1_MM_ANONPAGES) +
  Model1_get_mm_counter(Model1_mm, Model1_MM_SHMEMPAGES);
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_get_mm_hiwater_rss(struct Model1_mm_struct *Model1_mm)
{
 return ({ typeof(Model1_mm->Model1_hiwater_rss) Model1__max1 = (Model1_mm->Model1_hiwater_rss); typeof(Model1_get_mm_rss(Model1_mm)) Model1__max2 = (Model1_get_mm_rss(Model1_mm)); (void) (&Model1__max1 == &Model1__max2); Model1__max1 > Model1__max2 ? Model1__max1 : Model1__max2; });
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_get_mm_hiwater_vm(struct Model1_mm_struct *Model1_mm)
{
 return ({ typeof(Model1_mm->Model1_hiwater_vm) Model1__max1 = (Model1_mm->Model1_hiwater_vm); typeof(Model1_mm->Model1_total_vm) Model1__max2 = (Model1_mm->Model1_total_vm); (void) (&Model1__max1 == &Model1__max2); Model1__max1 > Model1__max2 ? Model1__max1 : Model1__max2; });
}

static inline __attribute__((no_instrument_function)) void Model1_update_hiwater_rss(struct Model1_mm_struct *Model1_mm)
{
 unsigned long Model1__rss = Model1_get_mm_rss(Model1_mm);

 if ((Model1_mm)->Model1_hiwater_rss < Model1__rss)
  (Model1_mm)->Model1_hiwater_rss = Model1__rss;
}

static inline __attribute__((no_instrument_function)) void Model1_update_hiwater_vm(struct Model1_mm_struct *Model1_mm)
{
 if (Model1_mm->Model1_hiwater_vm < Model1_mm->Model1_total_vm)
  Model1_mm->Model1_hiwater_vm = Model1_mm->Model1_total_vm;
}

static inline __attribute__((no_instrument_function)) void Model1_reset_mm_hiwater_rss(struct Model1_mm_struct *Model1_mm)
{
 Model1_mm->Model1_hiwater_rss = Model1_get_mm_rss(Model1_mm);
}

static inline __attribute__((no_instrument_function)) void Model1_setmax_mm_hiwater_rss(unsigned long *Model1_maxrss,
      struct Model1_mm_struct *Model1_mm)
{
 unsigned long Model1_hiwater_rss = Model1_get_mm_hiwater_rss(Model1_mm);

 if (*Model1_maxrss < Model1_hiwater_rss)
  *Model1_maxrss = Model1_hiwater_rss;
}


void Model1_sync_mm_rss(struct Model1_mm_struct *Model1_mm);
int Model1_vma_wants_writenotify(struct Model1_vm_area_struct *Model1_vma);

extern Model1_pte_t *Model1___get_locked_pte(struct Model1_mm_struct *Model1_mm, unsigned long Model1_addr,
          Model1_spinlock_t **Model1_ptl);
static inline __attribute__((no_instrument_function)) Model1_pte_t *Model1_get_locked_pte(struct Model1_mm_struct *Model1_mm, unsigned long Model1_addr,
        Model1_spinlock_t **Model1_ptl)
{
 Model1_pte_t *Model1_ptep;
 (Model1_ptep = Model1___get_locked_pte(Model1_mm, Model1_addr, Model1_ptl));
 return Model1_ptep;
}
int Model1___pud_alloc(struct Model1_mm_struct *Model1_mm, Model1_pgd_t *Model1_pgd, unsigned long Model1_address);
int Model1___pmd_alloc(struct Model1_mm_struct *Model1_mm, Model1_pud_t *Model1_pud, unsigned long Model1_address);

static inline __attribute__((no_instrument_function)) void Model1_mm_nr_pmds_init(struct Model1_mm_struct *Model1_mm)
{
 Model1_atomic_long_set(&Model1_mm->Model1_nr_pmds, 0);
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_mm_nr_pmds(struct Model1_mm_struct *Model1_mm)
{
 return Model1_atomic_long_read(&Model1_mm->Model1_nr_pmds);
}

static inline __attribute__((no_instrument_function)) void Model1_mm_inc_nr_pmds(struct Model1_mm_struct *Model1_mm)
{
 Model1_atomic_long_inc(&Model1_mm->Model1_nr_pmds);
}

static inline __attribute__((no_instrument_function)) void Model1_mm_dec_nr_pmds(struct Model1_mm_struct *Model1_mm)
{
 Model1_atomic_long_dec(&Model1_mm->Model1_nr_pmds);
}


int Model1___pte_alloc(struct Model1_mm_struct *Model1_mm, Model1_pmd_t *Model1_pmd, unsigned long Model1_address);
int Model1___pte_alloc_kernel(Model1_pmd_t *Model1_pmd, unsigned long Model1_address);

/*
 * The following ifdef needed to get the 4level-fixup.h header to work.
 * Remove it when 4level-fixup.h has been removed.
 */

static inline __attribute__((no_instrument_function)) Model1_pud_t *Model1_pud_alloc(struct Model1_mm_struct *Model1_mm, Model1_pgd_t *Model1_pgd, unsigned long Model1_address)
{
 return (__builtin_expect(!!(Model1_pgd_none(*Model1_pgd)), 0) && Model1___pud_alloc(Model1_mm, Model1_pgd, Model1_address))?
  ((void *)0): Model1_pud_offset(Model1_pgd, Model1_address);
}

static inline __attribute__((no_instrument_function)) Model1_pmd_t *Model1_pmd_alloc(struct Model1_mm_struct *Model1_mm, Model1_pud_t *Model1_pud, unsigned long Model1_address)
{
 return (__builtin_expect(!!(Model1_pud_none(*Model1_pud)), 0) && Model1___pmd_alloc(Model1_mm, Model1_pud, Model1_address))?
  ((void *)0): Model1_pmd_offset(Model1_pud, Model1_address);
}
static inline __attribute__((no_instrument_function)) void Model1_ptlock_cache_init(void)
{
}

static inline __attribute__((no_instrument_function)) bool Model1_ptlock_alloc(struct Model1_page *Model1_page)
{
 return true;
}

static inline __attribute__((no_instrument_function)) void Model1_ptlock_free(struct Model1_page *Model1_page)
{
}

static inline __attribute__((no_instrument_function)) Model1_spinlock_t *Model1_ptlock_ptr(struct Model1_page *Model1_page)
{
 return &Model1_page->Model1_ptl;
}


static inline __attribute__((no_instrument_function)) Model1_spinlock_t *Model1_pte_lockptr(struct Model1_mm_struct *Model1_mm, Model1_pmd_t *Model1_pmd)
{
 return Model1_ptlock_ptr((((struct Model1_page *)(0xffffea0000000000UL)) + ((Model1_native_pmd_val(*Model1_pmd) & Model1_pmd_pfn_mask(*Model1_pmd)) >> 12)));
}

static inline __attribute__((no_instrument_function)) bool Model1_ptlock_init(struct Model1_page *Model1_page)
{
 /*
	 * prep_new_page() initialize page->private (and therefore page->ptl)
	 * with 0. Make sure nobody took it in use in between.
	 *
	 * It can happen if arch try to use slab for page table allocation:
	 * slab code uses page->slab_cache, which share storage with page->ptl.
	 */
 ((void)(sizeof(( long)(*(unsigned long *)&Model1_page->Model1_ptl))));
 if (!Model1_ptlock_alloc(Model1_page))
  return false;
 do { Model1_spinlock_check(Model1_ptlock_ptr(Model1_page)); do { *(&(Model1_ptlock_ptr(Model1_page))->Model1_rlock) = (Model1_raw_spinlock_t) { .Model1_raw_lock = { { (0) } }, }; } while (0); } while (0);
 return true;
}

/* Reset page->mapping so free_pages_check won't complain. */
static inline __attribute__((no_instrument_function)) void Model1_pte_lock_deinit(struct Model1_page *Model1_page)
{
 Model1_page->Model1_mapping = ((void *)0);
 Model1_ptlock_free(Model1_page);
}
static inline __attribute__((no_instrument_function)) void Model1_pgtable_init(void)
{
 Model1_ptlock_cache_init();
 do { } while (0);
}

static inline __attribute__((no_instrument_function)) bool Model1_pgtable_page_ctor(struct Model1_page *Model1_page)
{
 if (!Model1_ptlock_init(Model1_page))
  return false;
 Model1_inc_zone_page_state(Model1_page, Model1_NR_PAGETABLE);
 return true;
}

static inline __attribute__((no_instrument_function)) void Model1_pgtable_page_dtor(struct Model1_page *Model1_page)
{
 Model1_pte_lock_deinit(Model1_page);
 Model1_dec_zone_page_state(Model1_page, Model1_NR_PAGETABLE);
}
static struct Model1_page *Model1_pmd_to_page(Model1_pmd_t *Model1_pmd)
{
 unsigned long Model1_mask = ~(512 * sizeof(Model1_pmd_t) - 1);
 return (((struct Model1_page *)(0xffffea0000000000UL)) + (Model1___phys_addr_nodebug((unsigned long)((void *)((unsigned long) Model1_pmd & Model1_mask))) >> 12));
}

static inline __attribute__((no_instrument_function)) Model1_spinlock_t *Model1_pmd_lockptr(struct Model1_mm_struct *Model1_mm, Model1_pmd_t *Model1_pmd)
{
 return Model1_ptlock_ptr(Model1_pmd_to_page(Model1_pmd));
}

static inline __attribute__((no_instrument_function)) bool Model1_pgtable_pmd_page_ctor(struct Model1_page *Model1_page)
{



 return Model1_ptlock_init(Model1_page);
}

static inline __attribute__((no_instrument_function)) void Model1_pgtable_pmd_page_dtor(struct Model1_page *Model1_page)
{



 Model1_ptlock_free(Model1_page);
}
static inline __attribute__((no_instrument_function)) Model1_spinlock_t *Model1_pmd_lock(struct Model1_mm_struct *Model1_mm, Model1_pmd_t *Model1_pmd)
{
 Model1_spinlock_t *Model1_ptl = Model1_pmd_lockptr(Model1_mm, Model1_pmd);
 Model1_spin_lock(Model1_ptl);
 return Model1_ptl;
}

extern void Model1_free_area_init(unsigned long * Model1_zones_size);
extern void Model1_free_area_init_node(int Model1_nid, unsigned long * Model1_zones_size,
  unsigned long Model1_zone_start_pfn, unsigned long *Model1_zholes_size);
extern void Model1_free_initmem(void);

/*
 * Free reserved pages within range [PAGE_ALIGN(start), end & PAGE_MASK)
 * into the buddy system. The freed pages will be poisoned with pattern
 * "poison" if it's within range [0, UCHAR_MAX].
 * Return pages freed into the buddy system.
 */
extern unsigned long Model1_free_reserved_area(void *Model1_start, void *Model1_end,
     int Model1_poison, char *Model1_s);
extern void Model1_adjust_managed_page_count(struct Model1_page *Model1_page, long Model1_count);
extern void Model1_mem_init_print_info(const char *Model1_str);

extern void Model1_reserve_bootmem_region(Model1_phys_addr_t Model1_start, Model1_phys_addr_t Model1_end);

/* Free the reserved page into the buddy system, so it gets managed. */
static inline __attribute__((no_instrument_function)) void Model1___free_reserved_page(struct Model1_page *Model1_page)
{
 Model1_ClearPageReserved(Model1_page);
 Model1_init_page_count(Model1_page);
 Model1___free_pages((Model1_page), 0);
}

static inline __attribute__((no_instrument_function)) void Model1_free_reserved_page(struct Model1_page *Model1_page)
{
 Model1___free_reserved_page(Model1_page);
 Model1_adjust_managed_page_count(Model1_page, 1);
}

static inline __attribute__((no_instrument_function)) void Model1_mark_page_reserved(struct Model1_page *Model1_page)
{
 Model1_SetPageReserved(Model1_page);
 Model1_adjust_managed_page_count(Model1_page, -1);
}

/*
 * Default method to free all the __init memory into the buddy system.
 * The freed pages will be poisoned with pattern "poison" if it's within
 * range [0, UCHAR_MAX].
 * Return pages freed into the buddy system.
 */
static inline __attribute__((no_instrument_function)) unsigned long Model1_free_initmem_default(int Model1_poison)
{
 extern char Model1___init_begin[], Model1___init_end[];

 return Model1_free_reserved_area(&Model1___init_begin, &Model1___init_end,
      Model1_poison, "unused kernel");
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_get_num_physpages(void)
{
 int Model1_nid;
 unsigned long Model1_phys_pages = 0;

 for (((Model1_nid)) = Model1___first_node(&(Model1_node_states[Model1_N_ONLINE])); ((Model1_nid)) < (1 << 6); ((Model1_nid)) = Model1___next_node((((Model1_nid))), &((Model1_node_states[Model1_N_ONLINE]))))
  Model1_phys_pages += ((Model1_node_data[Model1_nid])->Model1_node_present_pages);

 return Model1_phys_pages;
}


/*
 * With CONFIG_HAVE_MEMBLOCK_NODE_MAP set, an architecture may initialise its
 * zones, allocate the backing mem_map and account for memory holes in a more
 * architecture independent manner. This is a substitute for creating the
 * zone_sizes[] and zholes_size[] arrays and passing them to
 * free_area_init_node()
 *
 * An architecture is expected to register range of page frames backed by
 * physical memory with memblock_add[_node]() before calling
 * free_area_init_nodes() passing in the PFN each zone ends at. At a basic
 * usage, an architecture is expected to do something like
 *
 * unsigned long max_zone_pfns[MAX_NR_ZONES] = {max_dma, max_normal_pfn,
 * 							 max_highmem_pfn};
 * for_each_valid_physical_page_range()
 * 	memblock_add_node(base, size, nid)
 * free_area_init_nodes(max_zone_pfns);
 *
 * free_bootmem_with_active_regions() calls free_bootmem_node() for each
 * registered physical page range.  Similarly
 * sparse_memory_present_with_active_regions() calls memory_present() for
 * each range when SPARSEMEM is enabled.
 *
 * See mm/page_alloc.c for more information on each function exposed by
 * CONFIG_HAVE_MEMBLOCK_NODE_MAP.
 */
extern void Model1_free_area_init_nodes(unsigned long *Model1_max_zone_pfn);
unsigned long Model1_node_map_pfn_alignment(void);
unsigned long Model1___absent_pages_in_range(int Model1_nid, unsigned long Model1_start_pfn,
      unsigned long Model1_end_pfn);
extern unsigned long Model1_absent_pages_in_range(unsigned long Model1_start_pfn,
      unsigned long Model1_end_pfn);
extern void Model1_get_pfn_range_for_nid(unsigned int Model1_nid,
   unsigned long *Model1_start_pfn, unsigned long *Model1_end_pfn);
extern unsigned long Model1_find_min_pfn_with_active_regions(void);
extern void Model1_free_bootmem_with_active_regions(int Model1_nid,
      unsigned long Model1_max_low_pfn);
extern void Model1_sparse_memory_present_with_active_regions(int Model1_nid);
/* please see mm/page_alloc.c */
extern int __attribute__ ((__section__(".meminit.text"))) __attribute__((no_instrument_function)) Model1_early_pfn_to_nid(unsigned long Model1_pfn);
/* there is a per-arch backend function. */
extern int __attribute__ ((__section__(".meminit.text"))) __attribute__((no_instrument_function)) Model1___early_pfn_to_nid(unsigned long Model1_pfn,
     struct Model1_mminit_pfnnid_cache *Model1_state);


extern void Model1_set_dma_reserve(unsigned long Model1_new_dma_reserve);
extern void Model1_memmap_init_zone(unsigned long, int, unsigned long,
    unsigned long, enum Model1_memmap_context);
extern void Model1_setup_per_zone_wmarks(void);
extern int __attribute__ ((__section__(".meminit.text"))) __attribute__((no_instrument_function)) Model1_init_per_zone_wmark_min(void);
extern void Model1_mem_init(void);
extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_mmap_init(void);
extern void Model1_show_mem(unsigned int Model1_flags);
extern long Model1_si_mem_available(void);
extern void Model1_si_meminfo(struct Model1_sysinfo * Model1_val);
extern void Model1_si_meminfo_node(struct Model1_sysinfo *Model1_val, int Model1_nid);

extern __attribute__((format(printf, 3, 4)))
void Model1_warn_alloc_failed(Model1_gfp_t Model1_gfp_mask, unsigned int Model1_order,
  const char *Model1_fmt, ...);

extern void Model1_setup_per_cpu_pageset(void);

extern void Model1_zone_pcp_update(struct Model1_zone *Model1_zone);
extern void Model1_zone_pcp_reset(struct Model1_zone *Model1_zone);

/* page_alloc.c */
extern int Model1_min_free_kbytes;
extern int Model1_watermark_scale_factor;

/* nommu.c */
extern Model1_atomic_long_t Model1_mmap_pages_allocated;
extern int Model1_nommu_shrink_inode_mappings(struct Model1_inode *, Model1_size_t, Model1_size_t);

/* interval_tree.c */
void Model1_vma_interval_tree_insert(struct Model1_vm_area_struct *Model1_node,
         struct Model1_rb_root *Model1_root);
void Model1_vma_interval_tree_insert_after(struct Model1_vm_area_struct *Model1_node,
        struct Model1_vm_area_struct *Model1_prev,
        struct Model1_rb_root *Model1_root);
void Model1_vma_interval_tree_remove(struct Model1_vm_area_struct *Model1_node,
         struct Model1_rb_root *Model1_root);
struct Model1_vm_area_struct *Model1_vma_interval_tree_iter_first(struct Model1_rb_root *Model1_root,
    unsigned long Model1_start, unsigned long Model1_last);
struct Model1_vm_area_struct *Model1_vma_interval_tree_iter_next(struct Model1_vm_area_struct *Model1_node,
    unsigned long Model1_start, unsigned long Model1_last);





void Model1_anon_vma_interval_tree_insert(struct Model1_anon_vma_chain *Model1_node,
       struct Model1_rb_root *Model1_root);
void Model1_anon_vma_interval_tree_remove(struct Model1_anon_vma_chain *Model1_node,
       struct Model1_rb_root *Model1_root);
struct Model1_anon_vma_chain *Model1_anon_vma_interval_tree_iter_first(
 struct Model1_rb_root *Model1_root, unsigned long Model1_start, unsigned long Model1_last);
struct Model1_anon_vma_chain *Model1_anon_vma_interval_tree_iter_next(
 struct Model1_anon_vma_chain *Model1_node, unsigned long Model1_start, unsigned long Model1_last);
/* mmap.c */
extern int Model1___vm_enough_memory(struct Model1_mm_struct *Model1_mm, long Model1_pages, int Model1_cap_sys_admin);
extern int Model1_vma_adjust(struct Model1_vm_area_struct *Model1_vma, unsigned long Model1_start,
 unsigned long Model1_end, unsigned long Model1_pgoff, struct Model1_vm_area_struct *Model1_insert);
extern struct Model1_vm_area_struct *Model1_vma_merge(struct Model1_mm_struct *,
 struct Model1_vm_area_struct *Model1_prev, unsigned long Model1_addr, unsigned long Model1_end,
 unsigned long Model1_vm_flags, struct Model1_anon_vma *, struct Model1_file *, unsigned long,
 struct Model1_mempolicy *, struct Model1_vm_userfaultfd_ctx);
extern struct Model1_anon_vma *Model1_find_mergeable_anon_vma(struct Model1_vm_area_struct *);
extern int Model1_split_vma(struct Model1_mm_struct *,
 struct Model1_vm_area_struct *, unsigned long Model1_addr, int Model1_new_below);
extern int Model1_insert_vm_struct(struct Model1_mm_struct *, struct Model1_vm_area_struct *);
extern void Model1___vma_link_rb(struct Model1_mm_struct *, struct Model1_vm_area_struct *,
 struct Model1_rb_node **, struct Model1_rb_node *);
extern void Model1_unlink_file_vma(struct Model1_vm_area_struct *);
extern struct Model1_vm_area_struct *Model1_copy_vma(struct Model1_vm_area_struct **,
 unsigned long Model1_addr, unsigned long Model1_len, unsigned long Model1_pgoff,
 bool *Model1_need_rmap_locks);
extern void Model1_exit_mmap(struct Model1_mm_struct *);

static inline __attribute__((no_instrument_function)) int Model1_check_data_rlimit(unsigned long Model1_rlim,
        unsigned long Model1_new,
        unsigned long Model1_start,
        unsigned long Model1_end_data,
        unsigned long Model1_start_data)
{
 if (Model1_rlim < (~0UL)) {
  if (((Model1_new - Model1_start) + (Model1_end_data - Model1_start_data)) > Model1_rlim)
   return -28;
 }

 return 0;
}

extern int Model1_mm_take_all_locks(struct Model1_mm_struct *Model1_mm);
extern void Model1_mm_drop_all_locks(struct Model1_mm_struct *Model1_mm);

extern void Model1_set_mm_exe_file(struct Model1_mm_struct *Model1_mm, struct Model1_file *Model1_new_exe_file);
extern struct Model1_file *Model1_get_mm_exe_file(struct Model1_mm_struct *Model1_mm);
extern struct Model1_file *Model1_get_task_exe_file(struct Model1_task_struct *Model1_task);

extern bool Model1_may_expand_vm(struct Model1_mm_struct *, Model1_vm_flags_t, unsigned long Model1_npages);
extern void Model1_vm_stat_account(struct Model1_mm_struct *, Model1_vm_flags_t, long Model1_npages);

extern struct Model1_vm_area_struct *Model1__install_special_mapping(struct Model1_mm_struct *Model1_mm,
       unsigned long Model1_addr, unsigned long Model1_len,
       unsigned long Model1_flags,
       const struct Model1_vm_special_mapping *Model1_spec);
/* This is an obsolete alternative to _install_special_mapping. */
extern int Model1_install_special_mapping(struct Model1_mm_struct *Model1_mm,
       unsigned long Model1_addr, unsigned long Model1_len,
       unsigned long Model1_flags, struct Model1_page **Model1_pages);

extern unsigned long Model1_get_unmapped_area(struct Model1_file *, unsigned long, unsigned long, unsigned long, unsigned long);

extern unsigned long Model1_mmap_region(struct Model1_file *Model1_file, unsigned long Model1_addr,
 unsigned long Model1_len, Model1_vm_flags_t Model1_vm_flags, unsigned long Model1_pgoff);
extern unsigned long Model1_do_mmap(struct Model1_file *Model1_file, unsigned long Model1_addr,
 unsigned long Model1_len, unsigned long Model1_prot, unsigned long Model1_flags,
 Model1_vm_flags_t Model1_vm_flags, unsigned long Model1_pgoff, unsigned long *Model1_populate);
extern int Model1_do_munmap(struct Model1_mm_struct *, unsigned long, Model1_size_t);

static inline __attribute__((no_instrument_function)) unsigned long
Model1_do_mmap_pgoff(struct Model1_file *Model1_file, unsigned long Model1_addr,
 unsigned long Model1_len, unsigned long Model1_prot, unsigned long Model1_flags,
 unsigned long Model1_pgoff, unsigned long *Model1_populate)
{
 return Model1_do_mmap(Model1_file, Model1_addr, Model1_len, Model1_prot, Model1_flags, 0, Model1_pgoff, Model1_populate);
}


extern int Model1___mm_populate(unsigned long Model1_addr, unsigned long Model1_len,
    int Model1_ignore_errors);
static inline __attribute__((no_instrument_function)) void Model1_mm_populate(unsigned long Model1_addr, unsigned long Model1_len)
{
 /* Ignore errors */
 (void) Model1___mm_populate(Model1_addr, Model1_len, 1);
}




/* These take the mm semaphore themselves */
extern int __attribute__((warn_unused_result)) Model1_vm_brk(unsigned long, unsigned long);
extern int Model1_vm_munmap(unsigned long, Model1_size_t);
extern unsigned long __attribute__((warn_unused_result)) Model1_vm_mmap(struct Model1_file *, unsigned long,
        unsigned long, unsigned long,
        unsigned long, unsigned long);

struct Model1_vm_unmapped_area_info {

 unsigned long Model1_flags;
 unsigned long Model1_length;
 unsigned long Model1_low_limit;
 unsigned long Model1_high_limit;
 unsigned long Model1_align_mask;
 unsigned long Model1_align_offset;
};

extern unsigned long Model1_unmapped_area(struct Model1_vm_unmapped_area_info *Model1_info);
extern unsigned long Model1_unmapped_area_topdown(struct Model1_vm_unmapped_area_info *Model1_info);

/*
 * Search for an unmapped address range.
 *
 * We are looking for a range that:
 * - does not intersect with any VMA;
 * - is contained within the [low_limit, high_limit) interval;
 * - is at least the desired size.
 * - satisfies (begin_addr & align_mask) == (align_offset & align_mask)
 */
static inline __attribute__((no_instrument_function)) unsigned long
Model1_vm_unmapped_area(struct Model1_vm_unmapped_area_info *Model1_info)
{
 if (Model1_info->Model1_flags & 1)
  return Model1_unmapped_area_topdown(Model1_info);
 else
  return Model1_unmapped_area(Model1_info);
}

/* truncate.c */
extern void Model1_truncate_inode_pages(struct Model1_address_space *, Model1_loff_t);
extern void Model1_truncate_inode_pages_range(struct Model1_address_space *,
           Model1_loff_t Model1_lstart, Model1_loff_t Model1_lend);
extern void Model1_truncate_inode_pages_final(struct Model1_address_space *);

/* generic vm_area_ops exported for stackable file systems */
extern int Model1_filemap_fault(struct Model1_vm_area_struct *, struct Model1_vm_fault *);
extern void Model1_filemap_map_pages(struct Model1_fault_env *Model1_fe,
  unsigned long Model1_start_pgoff, unsigned long Model1_end_pgoff);
extern int Model1_filemap_page_mkwrite(struct Model1_vm_area_struct *Model1_vma, struct Model1_vm_fault *Model1_vmf);

/* mm/page-writeback.c */
int Model1_write_one_page(struct Model1_page *Model1_page, int Model1_wait);
void Model1_task_dirty_inc(struct Model1_task_struct *Model1_tsk);

/* readahead.c */



int Model1_force_page_cache_readahead(struct Model1_address_space *Model1_mapping, struct Model1_file *Model1_filp,
   unsigned long Model1_offset, unsigned long Model1_nr_to_read);

void Model1_page_cache_sync_readahead(struct Model1_address_space *Model1_mapping,
          struct Model1_file_ra_state *Model1_ra,
          struct Model1_file *Model1_filp,
          unsigned long Model1_offset,
          unsigned long Model1_size);

void Model1_page_cache_async_readahead(struct Model1_address_space *Model1_mapping,
    struct Model1_file_ra_state *Model1_ra,
    struct Model1_file *Model1_filp,
    struct Model1_page *Model1_pg,
    unsigned long Model1_offset,
    unsigned long Model1_size);

/* Generic expand stack which grows the stack according to GROWS{UP,DOWN} */
extern int Model1_expand_stack(struct Model1_vm_area_struct *Model1_vma, unsigned long Model1_address);

/* CONFIG_STACK_GROWSUP still needs to to grow downwards at some places */
extern int Model1_expand_downwards(struct Model1_vm_area_struct *Model1_vma,
  unsigned long Model1_address);






/* Look up the first VMA which satisfies  addr < vm_end,  NULL if none. */
extern struct Model1_vm_area_struct * Model1_find_vma(struct Model1_mm_struct * Model1_mm, unsigned long Model1_addr);
extern struct Model1_vm_area_struct * Model1_find_vma_prev(struct Model1_mm_struct * Model1_mm, unsigned long Model1_addr,
          struct Model1_vm_area_struct **Model1_pprev);

/* Look up the first VMA which intersects the interval start_addr..end_addr-1,
   NULL if none.  Assume start_addr < end_addr. */
static inline __attribute__((no_instrument_function)) struct Model1_vm_area_struct * Model1_find_vma_intersection(struct Model1_mm_struct * Model1_mm, unsigned long Model1_start_addr, unsigned long Model1_end_addr)
{
 struct Model1_vm_area_struct * Model1_vma = Model1_find_vma(Model1_mm,Model1_start_addr);

 if (Model1_vma && Model1_end_addr <= Model1_vma->Model1_vm_start)
  Model1_vma = ((void *)0);
 return Model1_vma;
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_vma_pages(struct Model1_vm_area_struct *Model1_vma)
{
 return (Model1_vma->Model1_vm_end - Model1_vma->Model1_vm_start) >> 12;
}

/* Look up the first VMA which exactly match the interval vm_start ... vm_end */
static inline __attribute__((no_instrument_function)) struct Model1_vm_area_struct *Model1_find_exact_vma(struct Model1_mm_struct *Model1_mm,
    unsigned long Model1_vm_start, unsigned long Model1_vm_end)
{
 struct Model1_vm_area_struct *Model1_vma = Model1_find_vma(Model1_mm, Model1_vm_start);

 if (Model1_vma && (Model1_vma->Model1_vm_start != Model1_vm_start || Model1_vma->Model1_vm_end != Model1_vm_end))
  Model1_vma = ((void *)0);

 return Model1_vma;
}


Model1_pgprot_t Model1_vm_get_page_prot(unsigned long Model1_vm_flags);
void Model1_vma_set_page_prot(struct Model1_vm_area_struct *Model1_vma);
struct Model1_vm_area_struct *Model1_find_extend_vma(struct Model1_mm_struct *, unsigned long Model1_addr);
int Model1_remap_pfn_range(struct Model1_vm_area_struct *, unsigned long Model1_addr,
   unsigned long Model1_pfn, unsigned long Model1_size, Model1_pgprot_t);
int Model1_vm_insert_page(struct Model1_vm_area_struct *, unsigned long Model1_addr, struct Model1_page *);
int Model1_vm_insert_pfn(struct Model1_vm_area_struct *Model1_vma, unsigned long Model1_addr,
   unsigned long Model1_pfn);
int Model1_vm_insert_pfn_prot(struct Model1_vm_area_struct *Model1_vma, unsigned long Model1_addr,
   unsigned long Model1_pfn, Model1_pgprot_t Model1_pgprot);
int Model1_vm_insert_mixed(struct Model1_vm_area_struct *Model1_vma, unsigned long Model1_addr,
   Model1_pfn_t Model1_pfn);
int Model1_vm_iomap_memory(struct Model1_vm_area_struct *Model1_vma, Model1_phys_addr_t Model1_start, unsigned long Model1_len);


struct Model1_page *Model1_follow_page_mask(struct Model1_vm_area_struct *Model1_vma,
         unsigned long Model1_address, unsigned int Model1_foll_flags,
         unsigned int *Model1_page_mask);

static inline __attribute__((no_instrument_function)) struct Model1_page *Model1_follow_page(struct Model1_vm_area_struct *Model1_vma,
  unsigned long Model1_address, unsigned int Model1_foll_flags)
{
 unsigned int Model1_unused_page_mask;
 return Model1_follow_page_mask(Model1_vma, Model1_address, Model1_foll_flags, &Model1_unused_page_mask);
}
typedef int (*Model1_pte_fn_t)(Model1_pte_t *Model1_pte, Model1_pgtable_t Model1_token, unsigned long Model1_addr,
   void *Model1_data);
extern int Model1_apply_to_page_range(struct Model1_mm_struct *Model1_mm, unsigned long Model1_address,
          unsigned long Model1_size, Model1_pte_fn_t Model1_fn, void *Model1_data);







static inline __attribute__((no_instrument_function)) bool Model1_page_poisoning_enabled(void) { return false; }
static inline __attribute__((no_instrument_function)) void Model1_kernel_poison_pages(struct Model1_page *Model1_page, int Model1_numpages,
     int Model1_enable) { }
static inline __attribute__((no_instrument_function)) bool Model1_page_is_poisoned(struct Model1_page *Model1_page) { return false; }
static inline __attribute__((no_instrument_function)) void
Model1_kernel_map_pages(struct Model1_page *Model1_page, int Model1_numpages, int Model1_enable) {}

static inline __attribute__((no_instrument_function)) bool Model1_kernel_page_present(struct Model1_page *Model1_page) { return true; }

static inline __attribute__((no_instrument_function)) bool Model1_debug_pagealloc_enabled(void)
{
 return false;
}



extern struct Model1_vm_area_struct *Model1_get_gate_vma(struct Model1_mm_struct *Model1_mm);
extern int Model1_in_gate_area_no_mm(unsigned long Model1_addr);
extern int Model1_in_gate_area(struct Model1_mm_struct *Model1_mm, unsigned long Model1_addr);
extern bool Model1_process_shares_mm(struct Model1_task_struct *Model1_p, struct Model1_mm_struct *Model1_mm);


extern int Model1_sysctl_drop_caches;
int Model1_drop_caches_sysctl_handler(struct Model1_ctl_table *, int,
     void *, Model1_size_t *, Model1_loff_t *);


void Model1_drop_slab(void);
void Model1_drop_slab_node(int Model1_nid);




extern int Model1_randomize_va_space;


const char * Model1_arch_vma_name(struct Model1_vm_area_struct *Model1_vma);
void Model1_print_vma_addr(char *Model1_prefix, unsigned long Model1_rip);

void Model1_sparse_mem_maps_populate_node(struct Model1_page **Model1_map_map,
       unsigned long Model1_pnum_begin,
       unsigned long Model1_pnum_end,
       unsigned long Model1_map_count,
       int Model1_nodeid);

struct Model1_page *Model1_sparse_mem_map_populate(unsigned long Model1_pnum, int Model1_nid);
Model1_pgd_t *Model1_vmemmap_pgd_populate(unsigned long Model1_addr, int Model1_node);
Model1_pud_t *Model1_vmemmap_pud_populate(Model1_pgd_t *Model1_pgd, unsigned long Model1_addr, int Model1_node);
Model1_pmd_t *Model1_vmemmap_pmd_populate(Model1_pud_t *Model1_pud, unsigned long Model1_addr, int Model1_node);
Model1_pte_t *Model1_vmemmap_pte_populate(Model1_pmd_t *Model1_pmd, unsigned long Model1_addr, int Model1_node);
void *Model1_vmemmap_alloc_block(unsigned long Model1_size, int Model1_node);
struct Model1_vmem_altmap;
void *Model1___vmemmap_alloc_block_buf(unsigned long Model1_size, int Model1_node,
  struct Model1_vmem_altmap *Model1_altmap);
static inline __attribute__((no_instrument_function)) void *Model1_vmemmap_alloc_block_buf(unsigned long Model1_size, int Model1_node)
{
 return Model1___vmemmap_alloc_block_buf(Model1_size, Model1_node, ((void *)0));
}

void Model1_vmemmap_verify(Model1_pte_t *, int, unsigned long, unsigned long);
int Model1_vmemmap_populate_basepages(unsigned long Model1_start, unsigned long Model1_end,
          int Model1_node);
int Model1_vmemmap_populate(unsigned long Model1_start, unsigned long Model1_end, int Model1_node);
void Model1_vmemmap_populate_print_last(void);



void Model1_register_page_bootmem_memmap(unsigned long Model1_section_nr, struct Model1_page *Model1_map,
      unsigned long Model1_size);

enum Model1_mf_flags {
 Model1_MF_COUNT_INCREASED = 1 << 0,
 Model1_MF_ACTION_REQUIRED = 1 << 1,
 Model1_MF_MUST_KILL = 1 << 2,
 Model1_MF_SOFT_OFFLINE = 1 << 3,
};
extern int Model1_memory_failure(unsigned long Model1_pfn, int Model1_trapno, int Model1_flags);
extern void Model1_memory_failure_queue(unsigned long Model1_pfn, int Model1_trapno, int Model1_flags);
extern int Model1_unpoison_memory(unsigned long Model1_pfn);
extern int Model1_get_hwpoison_page(struct Model1_page *Model1_page);

extern int Model1_sysctl_memory_failure_early_kill;
extern int Model1_sysctl_memory_failure_recovery;
extern void Model1_shake_page(struct Model1_page *Model1_p, int Model1_access);
extern Model1_atomic_long_t Model1_num_poisoned_pages;
extern int Model1_soft_offline_page(struct Model1_page *Model1_page, int Model1_flags);


/*
 * Error handlers for various types of pages.
 */
enum Model1_mf_result {
 Model1_MF_IGNORED, /* Error: cannot be handled */
 Model1_MF_FAILED, /* Error: handling failed */
 Model1_MF_DELAYED, /* Will be handled later */
 Model1_MF_RECOVERED, /* Successfully recovered */
};

enum Model1_mf_action_page_type {
 Model1_MF_MSG_KERNEL,
 Model1_MF_MSG_KERNEL_HIGH_ORDER,
 Model1_MF_MSG_SLAB,
 Model1_MF_MSG_DIFFERENT_COMPOUND,
 Model1_MF_MSG_POISONED_HUGE,
 Model1_MF_MSG_HUGE,
 Model1_MF_MSG_FREE_HUGE,
 Model1_MF_MSG_UNMAP_FAILED,
 Model1_MF_MSG_DIRTY_SWAPCACHE,
 Model1_MF_MSG_CLEAN_SWAPCACHE,
 Model1_MF_MSG_DIRTY_MLOCKED_LRU,
 Model1_MF_MSG_CLEAN_MLOCKED_LRU,
 Model1_MF_MSG_DIRTY_UNEVICTABLE_LRU,
 Model1_MF_MSG_CLEAN_UNEVICTABLE_LRU,
 Model1_MF_MSG_DIRTY_LRU,
 Model1_MF_MSG_CLEAN_LRU,
 Model1_MF_MSG_TRUNCATED_LRU,
 Model1_MF_MSG_BUDDY,
 Model1_MF_MSG_BUDDY_2ND,
 Model1_MF_MSG_UNKNOWN,
};


extern void Model1_clear_huge_page(struct Model1_page *Model1_page,
       unsigned long Model1_addr,
       unsigned int Model1_pages_per_huge_page);
extern void Model1_copy_user_huge_page(struct Model1_page *Model1_dst, struct Model1_page *Model1_src,
    unsigned long Model1_addr, struct Model1_vm_area_struct *Model1_vma,
    unsigned int Model1_pages_per_huge_page);


extern struct Model1_page_ext_operations Model1_debug_guardpage_ops;
extern struct Model1_page_ext_operations Model1_page_poisoning_ops;
static inline __attribute__((no_instrument_function)) unsigned int Model1_debug_guardpage_minorder(void) { return 0; }
static inline __attribute__((no_instrument_function)) bool Model1_debug_guardpage_enabled(void) { return false; }
static inline __attribute__((no_instrument_function)) bool Model1_page_is_guard(struct Model1_page *Model1_page) { return false; }



void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_setup_nr_node_ids(void);
/*
 * Written by Mark Hemment, 1996 (markhe@nextd.demon.co.uk).
 *
 * (C) SGI 2006, Christoph Lameter
 * 	Cleaned up and restructured to ease the addition of alternative
 * 	implementations of SLAB allocators.
 * (C) Linux Foundation 2008-2013
 *      Unified interface for all slab allocators
 */
/*
 * Flags to pass to kmem_cache_create().
 * The ones marked DEBUG are only valid if CONFIG_DEBUG_SLAB is set.
 */







/*
 * SLAB_DESTROY_BY_RCU - **WARNING** READ THIS!
 *
 * This delays freeing the SLAB page by a grace period, it does _NOT_
 * delay object freeing. This means that if you do kmem_cache_free()
 * that memory location is free to be reused at any time. Thus it may
 * be possible to see another object there in the same RCU grace period.
 *
 * This feature only ensures the memory location backing the object
 * stays valid, the trick to using this is relying on an independent
 * object validation pass. Something like:
 *
 *  rcu_read_lock()
 * again:
 *  obj = lockless_lookup(key);
 *  if (obj) {
 *    if (!try_get_ref(obj)) // might fail for free objects
 *      goto again;
 *
 *    if (obj->key != key) { // not the object we expected
 *      put_ref(obj);
 *      goto again;
 *    }
 *  }
 *  rcu_read_unlock();
 *
 * This is useful if we need to approach a kernel structure obliquely,
 * from its address obtained without the usual locking. We can lock
 * the structure to stabilize it and check it's still at the given address,
 * only if we can be sure that the memory has not been meanwhile reused
 * for some other kind of object (which our subsystem's lock might corrupt).
 *
 * rcu_read_lock before reading the address, then rcu_read_unlock after
 * taking the spinlock within the structure expected at that address.
 */




/* Flag to prevent checks on free */
/* Don't track use of uninitialized memory */
/* The following flags affect the page allocator grouping pages by mobility */


/*
 * ZERO_SIZE_PTR will be returned for zero sized kmalloc requests.
 *
 * Dereferencing ZERO_SIZE_PTR will lead to a distinct access fault.
 *
 * ZERO_SIZE_PTR can be passed to kfree though in the same way that NULL can.
 * Both make kfree a no-op.
 */






/*
 * include/linux/kmemleak.h
 *
 * Copyright (C) 2008 ARM Limited
 * Written by Catalin Marinas <catalin.marinas@arm.com>
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2 as
 * published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
 */





/*
 * Written by Mark Hemment, 1996 (markhe@nextd.demon.co.uk).
 *
 * (C) SGI 2006, Christoph Lameter
 * 	Cleaned up and restructured to ease the addition of alternative
 * 	implementations of SLAB allocators.
 * (C) Linux Foundation 2008-2013
 *      Unified interface for all slab allocators
 */
static inline __attribute__((no_instrument_function)) void Model1_kmemleak_init(void)
{
}
static inline __attribute__((no_instrument_function)) void Model1_kmemleak_alloc(const void *Model1_ptr, Model1_size_t Model1_size, int Model1_min_count,
      Model1_gfp_t Model1_gfp)
{
}
static inline __attribute__((no_instrument_function)) void Model1_kmemleak_alloc_recursive(const void *Model1_ptr, Model1_size_t Model1_size,
         int Model1_min_count, unsigned long Model1_flags,
         Model1_gfp_t Model1_gfp)
{
}
static inline __attribute__((no_instrument_function)) void Model1_kmemleak_alloc_percpu(const void *Model1_ptr, Model1_size_t Model1_size,
      Model1_gfp_t Model1_gfp)
{
}
static inline __attribute__((no_instrument_function)) void Model1_kmemleak_free(const void *Model1_ptr)
{
}
static inline __attribute__((no_instrument_function)) void Model1_kmemleak_free_part(const void *Model1_ptr, Model1_size_t Model1_size)
{
}
static inline __attribute__((no_instrument_function)) void Model1_kmemleak_free_recursive(const void *Model1_ptr, unsigned long Model1_flags)
{
}
static inline __attribute__((no_instrument_function)) void Model1_kmemleak_free_percpu(const void *Model1_ptr)
{
}
static inline __attribute__((no_instrument_function)) void Model1_kmemleak_update_trace(const void *Model1_ptr)
{
}
static inline __attribute__((no_instrument_function)) void Model1_kmemleak_not_leak(const void *Model1_ptr)
{
}
static inline __attribute__((no_instrument_function)) void Model1_kmemleak_ignore(const void *Model1_ptr)
{
}
static inline __attribute__((no_instrument_function)) void Model1_kmemleak_scan_area(const void *Model1_ptr, Model1_size_t Model1_size, Model1_gfp_t Model1_gfp)
{
}
static inline __attribute__((no_instrument_function)) void Model1_kmemleak_erase(void **Model1_ptr)
{
}
static inline __attribute__((no_instrument_function)) void Model1_kmemleak_no_scan(const void *Model1_ptr)
{
}









/*
 * cloning flags:
 */
/*
 * Scheduling policies
 */




/* SCHED_ISO: reserved but not implemented yet */



/* Can be ORed in to make sure the process is reverted back to SCHED_NORMAL on fork */


/*
 * For the sched_{set,get}attr() calls
 */








/*
 * Priority of a process goes from 0..MAX_PRIO-1, valid RT
 * priority is 0..MAX_RT_PRIO-1, and SCHED_NORMAL/SCHED_BATCH
 * tasks are in the range MAX_RT_PRIO..MAX_PRIO-1. Priority
 * values are inverted: lower p->prio value means higher priority.
 *
 * The MAX_USER_RT_PRIO value allows the actual maximum
 * RT priority to be separate from the value exported to
 * user-space.  This allows kernel threads to set their
 * priority to a value higher than any user task. Note:
 * MAX_RT_PRIO must not be smaller than MAX_USER_RT_PRIO.
 */







/*
 * Convert user-nice values [ -20 ... 0 ... 19 ]
 * to static priority [ MAX_RT_PRIO..MAX_PRIO-1 ],
 * and back.
 */



/*
 * 'User priority' is the nice value converted to something we
 * can work with better when scaling various scheduler parameters,
 * it's a [ 0 ... 39 ] range.
 */




/*
 * Convert nice value [19,-20] to rlimit style value [1,40].
 */
static inline __attribute__((no_instrument_function)) long Model1_nice_to_rlimit(long Model1_nice)
{
 return (19 - Model1_nice + 1);
}

/*
 * Convert rlimit style value [1,40] to nice value [-20, 19].
 */
static inline __attribute__((no_instrument_function)) long Model1_rlimit_to_nice(long Model1_prio)
{
 return (19 - Model1_prio + 1);
}


struct Model1_sched_param {
 int Model1_sched_priority;
};



/*
 * This is <linux/capability.h>
 *
 * Andrew G. Morgan <morgan@kernel.org>
 * Alexander Kjeldaas <astor@guardian.no>
 * with help from Aleph1, Roland Buresund and Andrew Main.
 *
 * See here for the libcap library ("POSIX draft" compliance):
 *
 * ftp://www.kernel.org/pub/linux/libs/security/linux-privs/kernel-2.6/
 */




/*
 * This is <linux/capability.h>
 *
 * Andrew G. Morgan <morgan@kernel.org>
 * Alexander Kjeldaas <astor@guardian.no>
 * with help from Aleph1, Roland Buresund and Andrew Main.
 *
 * See here for the libcap library ("POSIX draft" compliance):
 *
 * ftp://www.kernel.org/pub/linux/libs/security/linux-privs/kernel-2.6/
 */






/* User-level do most of the mapping between kernel and user
   capabilities based on the version tag given by the kernel. The
   kernel might be somewhat backwards compatible, but don't bet on
   it. */

/* Note, cap_t, is defined by POSIX (draft) to be an "opaque" pointer to
   a set of three capability sets.  The transposition of 3*the
   following structure to such a composite is better handled in a user
   library since the draft standard requires the use of malloc/free
   etc.. */
typedef struct Model1___user_cap_header_struct {
 __u32 Model1_version;
 int Model1_pid;
} *Model1_cap_user_header_t;

typedef struct Model1___user_cap_data_struct {
        __u32 Model1_effective;
        __u32 Model1_permitted;
        __u32 Model1_inheritable;
} *Model1_cap_user_data_t;
struct Model1_vfs_cap_data {
 Model1___le32 Model1_magic_etc; /* Little endian */
 struct {
  Model1___le32 Model1_permitted; /* Little endian */
  Model1___le32 Model1_inheritable; /* Little endian */
 } Model1_data[2];
};
/**
 ** POSIX-draft defined capabilities.
 **/

/* In a system with the [_POSIX_CHOWN_RESTRICTED] option defined, this
   overrides the restriction of changing file ownership and group
   ownership. */



/* Override all DAC access, including ACL execute access if
   [_POSIX_ACL] is defined. Excluding DAC access covered by
   CAP_LINUX_IMMUTABLE. */



/* Overrides all DAC restrictions regarding read and search on files
   and directories, including ACL restrictions if [_POSIX_ACL] is
   defined. Excluding DAC access covered by CAP_LINUX_IMMUTABLE. */



/* Overrides all restrictions about allowed operations on files, where
   file owner ID must be equal to the user ID, except where CAP_FSETID
   is applicable. It doesn't override MAC and DAC restrictions. */



/* Overrides the following restrictions that the effective user ID
   shall match the file owner ID when setting the S_ISUID and S_ISGID
   bits on that file; that the effective group ID (or one of the
   supplementary group IDs) shall match the file owner ID when setting
   the S_ISGID bit on that file; that the S_ISUID and S_ISGID bits are
   cleared on successful return from chown(2) (not implemented). */



/* Overrides the restriction that the real or effective user ID of a
   process sending a signal must match the real or effective user ID
   of the process receiving the signal. */



/* Allows setgid(2) manipulation */
/* Allows setgroups(2) */
/* Allows forged gids on socket credentials passing. */



/* Allows set*uid(2) manipulation (including fsuid). */
/* Allows forged pids on socket credentials passing. */




/**
 ** Linux-specific capabilities
 **/

/* Without VFS support for capabilities:
 *   Transfer any capability in your permitted set to any pid,
 *   remove any capability in your permitted set from any pid
 * With VFS support for capabilities (neither of above, but)
 *   Add any capability from current's capability bounding set
 *       to the current process' inheritable set
 *   Allow taking bits out of capability bounding set
 *   Allow modification of the securebits for a process
 */



/* Allow modification of S_IMMUTABLE and S_APPEND file attributes */



/* Allows binding to TCP/UDP sockets below 1024 */
/* Allows binding to ATM VCIs below 32 */



/* Allow broadcasting, listen to multicast */



/* Allow interface configuration */
/* Allow administration of IP firewall, masquerading and accounting */
/* Allow setting debug option on sockets */
/* Allow modification of routing tables */
/* Allow setting arbitrary process / process group ownership on
   sockets */
/* Allow binding to any address for transparent proxying (also via NET_RAW) */
/* Allow setting TOS (type of service) */
/* Allow setting promiscuous mode */
/* Allow clearing driver statistics */
/* Allow multicasting */
/* Allow read/write of device-specific registers */
/* Allow activation of ATM control sockets */



/* Allow use of RAW sockets */
/* Allow use of PACKET sockets */
/* Allow binding to any address for transparent proxying (also via NET_ADMIN) */



/* Allow locking of shared memory segments */
/* Allow mlock and mlockall (which doesn't really have anything to do
   with IPC) */



/* Override IPC ownership checks */



/* Insert and remove kernel modules - modify kernel without limit */


/* Allow ioperm/iopl access */
/* Allow sending USB messages to any device via /proc/bus/usb */



/* Allow use of chroot() */



/* Allow ptrace() of any process */



/* Allow configuration of process accounting */



/* Allow configuration of the secure attention key */
/* Allow administration of the random device */
/* Allow examination and configuration of disk quotas */
/* Allow setting the domainname */
/* Allow setting the hostname */
/* Allow calling bdflush() */
/* Allow mount() and umount(), setting up new smb connection */
/* Allow some autofs root ioctls */
/* Allow nfsservctl */
/* Allow VM86_REQUEST_IRQ */
/* Allow to read/write pci config on alpha */
/* Allow irix_prctl on mips (setstacksize) */
/* Allow flushing all cache on m68k (sys_cacheflush) */
/* Allow removing semaphores */
/* Used instead of CAP_CHOWN to "chown" IPC message queues, semaphores
   and shared memory */
/* Allow locking/unlocking of shared memory segment */
/* Allow turning swap on/off */
/* Allow forged pids on socket credentials passing */
/* Allow setting readahead and flushing buffers on block devices */
/* Allow setting geometry in floppy driver */
/* Allow turning DMA on/off in xd driver */
/* Allow administration of md devices (mostly the above, but some
   extra ioctls) */
/* Allow tuning the ide driver */
/* Allow access to the nvram device */
/* Allow administration of apm_bios, serial and bttv (TV) device */
/* Allow manufacturer commands in isdn CAPI support driver */
/* Allow reading non-standardized portions of pci configuration space */
/* Allow DDI debug ioctl on sbpcd driver */
/* Allow setting up serial ports */
/* Allow sending raw qic-117 commands */
/* Allow enabling/disabling tagged queuing on SCSI controllers and sending
   arbitrary SCSI commands */
/* Allow setting encryption key on loopback filesystem */
/* Allow setting zone reclaim policy */



/* Allow use of reboot() */



/* Allow raising priority and setting priority on other (different
   UID) processes */
/* Allow use of FIFO and round-robin (realtime) scheduling on own
   processes and setting the scheduling algorithm used by another
   process. */
/* Allow setting cpu affinity on other processes */



/* Override resource limits. Set resource limits. */
/* Override quota limits. */
/* Override reserved space on ext2 filesystem */
/* Modify data journaling mode on ext3 filesystem (uses journaling
   resources) */
/* NOTE: ext2 honors fsuid when checking for resource overrides, so
   you can override using fsuid too */
/* Override size restrictions on IPC message queues */
/* Allow more than 64hz interrupts from the real-time clock */
/* Override max number of consoles on console allocation */
/* Override max number of keymaps */



/* Allow manipulation of system clock */
/* Allow irix_stime on mips */
/* Allow setting the real-time clock */



/* Allow configuration of tty devices */
/* Allow vhangup() of tty */



/* Allow the privileged aspects of mknod() */



/* Allow taking of leases on files */



/* Allow writing the audit log via unicast netlink socket */



/* Allow configuration of audit via unicast netlink socket */





/* Override MAC access.
   The base kernel enforces no MAC policy.
   An LSM may enforce a MAC policy, and if it does and it chooses
   to implement capability based overrides of that policy, this is
   the capability it should use to do so. */



/* Allow MAC configuration or state changes.
   The base kernel requires no MAC configuration.
   An LSM may enforce a MAC policy, and if it does and it chooses
   to implement capability based checks on modifications to that
   policy or the data required to maintain it, this is the
   capability it should use to do so. */



/* Allow configuring the kernel's syslog (printk behaviour) */



/* Allow triggering something that will wake the system */



/* Allow preventing system suspends */



/* Allow reading the audit log via multicast netlink socket */
/*
 * Bit location of each capability (used by user-space library and kernel)
 */





extern int Model1_file_caps_enabled;

typedef struct Model1_kernel_cap_struct {
 __u32 Model1_cap[2];
} Model1_kernel_cap_t;

/* exact same as vfs_cap_data but in cpu endian and always filled completely */
struct Model1_cpu_vfs_cap_data {
 __u32 Model1_magic_etc;
 Model1_kernel_cap_t Model1_permitted;
 Model1_kernel_cap_t Model1_inheritable;
};





struct Model1_file;
struct Model1_inode;
struct Model1_dentry;
struct Model1_task_struct;
struct Model1_user_namespace;

extern const Model1_kernel_cap_t Model1___cap_empty_set;
extern const Model1_kernel_cap_t Model1___cap_init_eff_set;

/*
 * Internal kernel functions only
 */




/*
 * CAP_FS_MASK and CAP_NFSD_MASKS:
 *
 * The fs mask is all the privileges that fsuid==0 historically meant.
 * At one time in the past, that included CAP_MKNOD and CAP_LINUX_IMMUTABLE.
 *
 * It has never meant setting security.* and trusted.* xattrs.
 *
 * We could also define fsmask as follows:
 *   1. CAP_FS_MASK is the privilege to bypass all fs-related DAC permissions
 *   2. The security.* and trusted.* xattrs are fs-related MAC permissions
 */
static inline __attribute__((no_instrument_function)) Model1_kernel_cap_t Model1_cap_combine(const Model1_kernel_cap_t Model1_a,
           const Model1_kernel_cap_t Model1_b)
{
 Model1_kernel_cap_t Model1_dest;
 do { unsigned Model1___capi; for (Model1___capi = 0; Model1___capi < 2; ++Model1___capi) { Model1_dest.Model1_cap[Model1___capi] = Model1_a.Model1_cap[Model1___capi] | Model1_b.Model1_cap[Model1___capi]; } } while (0);
 return Model1_dest;
}

static inline __attribute__((no_instrument_function)) Model1_kernel_cap_t Model1_cap_intersect(const Model1_kernel_cap_t Model1_a,
      const Model1_kernel_cap_t Model1_b)
{
 Model1_kernel_cap_t Model1_dest;
 do { unsigned Model1___capi; for (Model1___capi = 0; Model1___capi < 2; ++Model1___capi) { Model1_dest.Model1_cap[Model1___capi] = Model1_a.Model1_cap[Model1___capi] & Model1_b.Model1_cap[Model1___capi]; } } while (0);
 return Model1_dest;
}

static inline __attribute__((no_instrument_function)) Model1_kernel_cap_t Model1_cap_drop(const Model1_kernel_cap_t Model1_a,
        const Model1_kernel_cap_t Model1_drop)
{
 Model1_kernel_cap_t Model1_dest;
 do { unsigned Model1___capi; for (Model1___capi = 0; Model1___capi < 2; ++Model1___capi) { Model1_dest.Model1_cap[Model1___capi] = Model1_a.Model1_cap[Model1___capi] &~ Model1_drop.Model1_cap[Model1___capi]; } } while (0);
 return Model1_dest;
}

static inline __attribute__((no_instrument_function)) Model1_kernel_cap_t Model1_cap_invert(const Model1_kernel_cap_t Model1_c)
{
 Model1_kernel_cap_t Model1_dest;
 do { unsigned Model1___capi; for (Model1___capi = 0; Model1___capi < 2; ++Model1___capi) { Model1_dest.Model1_cap[Model1___capi] = ~ Model1_c.Model1_cap[Model1___capi]; } } while (0);
 return Model1_dest;
}

static inline __attribute__((no_instrument_function)) bool Model1_cap_isclear(const Model1_kernel_cap_t Model1_a)
{
 unsigned Model1___capi;
 for (Model1___capi = 0; Model1___capi < 2; ++Model1___capi) {
  if (Model1_a.Model1_cap[Model1___capi] != 0)
   return false;
 }
 return true;
}

/*
 * Check if "a" is a subset of "set".
 * return true if ALL of the capabilities in "a" are also in "set"
 *	cap_issubset(0101, 1111) will return true
 * return false if ANY of the capabilities in "a" are not in "set"
 *	cap_issubset(1111, 0101) will return false
 */
static inline __attribute__((no_instrument_function)) bool Model1_cap_issubset(const Model1_kernel_cap_t Model1_a, const Model1_kernel_cap_t Model1_set)
{
 Model1_kernel_cap_t Model1_dest;
 Model1_dest = Model1_cap_drop(Model1_a, Model1_set);
 return Model1_cap_isclear(Model1_dest);
}

/* Used to decide between falling back on the old suser() or fsuser(). */

static inline __attribute__((no_instrument_function)) Model1_kernel_cap_t Model1_cap_drop_fs_set(const Model1_kernel_cap_t Model1_a)
{
 const Model1_kernel_cap_t Model1___cap_fs_set = ((Model1_kernel_cap_t){{ ((1 << ((0) & 31)) | (1 << ((27) & 31)) | (1 << ((1) & 31)) | (1 << ((2) & 31)) | (1 << ((3) & 31)) | (1 << ((4) & 31))) | (1 << ((9) & 31)), ((1 << ((32) & 31))) } });
 return Model1_cap_drop(Model1_a, Model1___cap_fs_set);
}

static inline __attribute__((no_instrument_function)) Model1_kernel_cap_t Model1_cap_raise_fs_set(const Model1_kernel_cap_t Model1_a,
         const Model1_kernel_cap_t Model1_permitted)
{
 const Model1_kernel_cap_t Model1___cap_fs_set = ((Model1_kernel_cap_t){{ ((1 << ((0) & 31)) | (1 << ((27) & 31)) | (1 << ((1) & 31)) | (1 << ((2) & 31)) | (1 << ((3) & 31)) | (1 << ((4) & 31))) | (1 << ((9) & 31)), ((1 << ((32) & 31))) } });
 return Model1_cap_combine(Model1_a,
      Model1_cap_intersect(Model1_permitted, Model1___cap_fs_set));
}

static inline __attribute__((no_instrument_function)) Model1_kernel_cap_t Model1_cap_drop_nfsd_set(const Model1_kernel_cap_t Model1_a)
{
 const Model1_kernel_cap_t Model1___cap_fs_set = ((Model1_kernel_cap_t){{ ((1 << ((0) & 31)) | (1 << ((27) & 31)) | (1 << ((1) & 31)) | (1 << ((2) & 31)) | (1 << ((3) & 31)) | (1 << ((4) & 31))) | (1 << ((24) & 31)), ((1 << ((32) & 31))) } });
 return Model1_cap_drop(Model1_a, Model1___cap_fs_set);
}

static inline __attribute__((no_instrument_function)) Model1_kernel_cap_t Model1_cap_raise_nfsd_set(const Model1_kernel_cap_t Model1_a,
           const Model1_kernel_cap_t Model1_permitted)
{
 const Model1_kernel_cap_t Model1___cap_nfsd_set = ((Model1_kernel_cap_t){{ ((1 << ((0) & 31)) | (1 << ((27) & 31)) | (1 << ((1) & 31)) | (1 << ((2) & 31)) | (1 << ((3) & 31)) | (1 << ((4) & 31))) | (1 << ((24) & 31)), ((1 << ((32) & 31))) } });
 return Model1_cap_combine(Model1_a,
      Model1_cap_intersect(Model1_permitted, Model1___cap_nfsd_set));
}


extern bool Model1_has_capability(struct Model1_task_struct *Model1_t, int Model1_cap);
extern bool Model1_has_ns_capability(struct Model1_task_struct *Model1_t,
         struct Model1_user_namespace *Model1_ns, int Model1_cap);
extern bool Model1_has_capability_noaudit(struct Model1_task_struct *Model1_t, int Model1_cap);
extern bool Model1_has_ns_capability_noaudit(struct Model1_task_struct *Model1_t,
          struct Model1_user_namespace *Model1_ns, int Model1_cap);
extern bool Model1_capable(int Model1_cap);
extern bool Model1_ns_capable(struct Model1_user_namespace *Model1_ns, int Model1_cap);
extern bool Model1_ns_capable_noaudit(struct Model1_user_namespace *Model1_ns, int Model1_cap);
extern bool Model1_capable_wrt_inode_uidgid(const struct Model1_inode *Model1_inode, int Model1_cap);
extern bool Model1_file_ns_capable(const struct Model1_file *Model1_file, struct Model1_user_namespace *Model1_ns, int Model1_cap);

/* audit system wants to get cap info from files as well */
extern int Model1_get_vfs_caps_from_disk(const struct Model1_dentry *Model1_dentry, struct Model1_cpu_vfs_cap_data *Model1_cpu_caps);





/*
 * Descending-priority-sorted double-linked list
 *
 * (C) 2002-2003 Intel Corp
 * Inaky Perez-Gonzalez <inaky.perez-gonzalez@intel.com>.
 *
 * 2001-2005 (c) MontaVista Software, Inc.
 * Daniel Walker <dwalker@mvista.com>
 *
 * (C) 2005 Thomas Gleixner <tglx@linutronix.de>
 *
 * Simplifications of the original code by
 * Oleg Nesterov <oleg@tv-sign.ru>
 *
 * Licensed under the FSF's GNU Public License v2 or later.
 *
 * Based on simple lists (include/linux/list.h).
 *
 * This is a priority-sorted list of nodes; each node has a
 * priority from INT_MIN (highest) to INT_MAX (lowest).
 *
 * Addition is O(K), removal is O(1), change of priority of a node is
 * O(K) and K is the number of RT priority levels used in the system.
 * (1 <= K <= 99)
 *
 * This list is really a list of lists:
 *
 *  - The tier 1 list is the prio_list, different priority nodes.
 *
 *  - The tier 2 list is the node_list, serialized nodes.
 *
 * Simple ASCII art explanation:
 *
 * pl:prio_list (only for plist_node)
 * nl:node_list
 *   HEAD|             NODE(S)
 *       |
 *       ||------------------------------------|
 *       ||->|pl|<->|pl|<--------------->|pl|<-|
 *       |   |10|   |21|   |21|   |21|   |40|   (prio)
 *       |   |  |   |  |   |  |   |  |   |  |
 *       |   |  |   |  |   |  |   |  |   |  |
 * |->|nl|<->|nl|<->|nl|<->|nl|<->|nl|<->|nl|<-|
 * |-------------------------------------------|
 *
 * The nodes on the prio_list list are sorted by priority to simplify
 * the insertion of new nodes. There are no nodes with duplicate
 * priorites on the list.
 *
 * The nodes on the node_list are ordered by priority and can contain
 * entries which have the same priority. Those entries are ordered
 * FIFO
 *
 * Addition means: look for the prio_list node in the prio_list
 * for the priority of the node and insert it before the node_list
 * entry of the next prio_list node. If it is the first node of
 * that priority, add it to the prio_list in the right position and
 * insert it into the serialized node_list list
 *
 * Removal means remove it from the node_list and remove it from
 * the prio_list if the node_list list_head is non empty. In case
 * of removal from the prio_list it must be checked whether other
 * entries of the same priority are on the list or not. If there
 * is another entry of the same priority then this entry has to
 * replace the removed entry on the prio_list. If the entry which
 * is removed is the only entry of this priority then a simple
 * remove from both list is sufficient.
 *
 * INT_MIN is the highest priority, 0 is the medium highest, INT_MAX
 * is lowest priority.
 *
 * No locking is done, up to the caller.
 *
 */






struct Model1_plist_head {
 struct Model1_list_head Model1_node_list;
};

struct Model1_plist_node {
 int Model1_prio;
 struct Model1_list_head Model1_prio_list;
 struct Model1_list_head Model1_node_list;
};

/**
 * PLIST_HEAD_INIT - static struct plist_head initializer
 * @head:	struct plist_head variable name
 */





/**
 * PLIST_HEAD - declare and init plist_head
 * @head:	name for struct plist_head variable
 */



/**
 * PLIST_NODE_INIT - static struct plist_node initializer
 * @node:	struct plist_node variable name
 * @__prio:	initial node priority
 */







/**
 * plist_head_init - dynamic struct plist_head initializer
 * @head:	&struct plist_head pointer
 */
static inline __attribute__((no_instrument_function)) void
Model1_plist_head_init(struct Model1_plist_head *Model1_head)
{
 Model1_INIT_LIST_HEAD(&Model1_head->Model1_node_list);
}

/**
 * plist_node_init - Dynamic struct plist_node initializer
 * @node:	&struct plist_node pointer
 * @prio:	initial node priority
 */
static inline __attribute__((no_instrument_function)) void Model1_plist_node_init(struct Model1_plist_node *Model1_node, int Model1_prio)
{
 Model1_node->Model1_prio = Model1_prio;
 Model1_INIT_LIST_HEAD(&Model1_node->Model1_prio_list);
 Model1_INIT_LIST_HEAD(&Model1_node->Model1_node_list);
}

extern void Model1_plist_add(struct Model1_plist_node *Model1_node, struct Model1_plist_head *Model1_head);
extern void Model1_plist_del(struct Model1_plist_node *Model1_node, struct Model1_plist_head *Model1_head);

extern void Model1_plist_requeue(struct Model1_plist_node *Model1_node, struct Model1_plist_head *Model1_head);

/**
 * plist_for_each - iterate over the plist
 * @pos:	the type * to use as a loop counter
 * @head:	the head for your list
 */



/**
 * plist_for_each_continue - continue iteration over the plist
 * @pos:	the type * to use as a loop cursor
 * @head:	the head for your list
 *
 * Continue to iterate over plist, continuing after the current position.
 */



/**
 * plist_for_each_safe - iterate safely over a plist of given type
 * @pos:	the type * to use as a loop counter
 * @n:	another type * to use as temporary storage
 * @head:	the head for your list
 *
 * Iterate over a plist of given type, safe against removal of list entry.
 */



/**
 * plist_for_each_entry	- iterate over list of given type
 * @pos:	the type * to use as a loop counter
 * @head:	the head for your list
 * @mem:	the name of the list_head within the struct
 */



/**
 * plist_for_each_entry_continue - continue iteration over list of given type
 * @pos:	the type * to use as a loop cursor
 * @head:	the head for your list
 * @m:		the name of the list_head within the struct
 *
 * Continue to iterate over list of given type, continuing after
 * the current position.
 */



/**
 * plist_for_each_entry_safe - iterate safely over list of given type
 * @pos:	the type * to use as a loop counter
 * @n:		another type * to use as temporary storage
 * @head:	the head for your list
 * @m:		the name of the list_head within the struct
 *
 * Iterate over list of given type, safe against removal of list entry.
 */



/**
 * plist_head_empty - return !0 if a plist_head is empty
 * @head:	&struct plist_head pointer
 */
static inline __attribute__((no_instrument_function)) int Model1_plist_head_empty(const struct Model1_plist_head *Model1_head)
{
 return Model1_list_empty(&Model1_head->Model1_node_list);
}

/**
 * plist_node_empty - return !0 if plist_node is not on a list
 * @node:	&struct plist_node pointer
 */
static inline __attribute__((no_instrument_function)) int Model1_plist_node_empty(const struct Model1_plist_node *Model1_node)
{
 return Model1_list_empty(&Model1_node->Model1_node_list);
}

/* All functions below assume the plist_head is not empty. */

/**
 * plist_first_entry - get the struct for the first entry
 * @head:	the &struct plist_head pointer
 * @type:	the type of the struct this is embedded in
 * @member:	the name of the list_head within the struct
 */
/**
 * plist_last_entry - get the struct for the last entry
 * @head:	the &struct plist_head pointer
 * @type:	the type of the struct this is embedded in
 * @member:	the name of the list_head within the struct
 */
/**
 * plist_next - get the next entry in list
 * @pos:	the type * to cursor
 */



/**
 * plist_prev - get the prev entry in list
 * @pos:	the type * to cursor
 */



/**
 * plist_first - return the first node (and thus, highest priority)
 * @head:	the &struct plist_head pointer
 *
 * Assumes the plist is _not_ empty.
 */
static inline __attribute__((no_instrument_function)) struct Model1_plist_node *Model1_plist_first(const struct Model1_plist_head *Model1_head)
{
 return ({ const typeof( ((struct Model1_plist_node *)0)->Model1_node_list ) *Model1___mptr = (Model1_head->Model1_node_list.Model1_next); (struct Model1_plist_node *)( (char *)Model1___mptr - __builtin_offsetof(struct Model1_plist_node, Model1_node_list) );});

}

/**
 * plist_last - return the last node (and thus, lowest priority)
 * @head:	the &struct plist_head pointer
 *
 * Assumes the plist is _not_ empty.
 */
static inline __attribute__((no_instrument_function)) struct Model1_plist_node *Model1_plist_last(const struct Model1_plist_head *Model1_head)
{
 return ({ const typeof( ((struct Model1_plist_node *)0)->Model1_node_list ) *Model1___mptr = (Model1_head->Model1_node_list.Model1_prev); (struct Model1_plist_node *)( (char *)Model1___mptr - __builtin_offsetof(struct Model1_plist_node, Model1_node_list) );});

}













typedef unsigned long Model1_cputime_t;
typedef Model1_u64 Model1_cputime64_t;





/*
 * Convert nanoseconds <-> cputime
 */
/*
 * Convert cputime to microseconds and back.
 */







/*
 * Convert cputime to seconds and back.
 */



/*
 * Convert cputime to timespec and back.
 */





/*
 * Convert cputime to timeval and back.
 */





/*
 * Convert cputime to clock and back.
 */





/*
 * Convert cputime64 to clock.
 */


















/*
 * A set of types for the internal kernel types representing uids and gids.
 *
 * The types defined in this header allow distinguishing which uids and gids in
 * the kernel are values used by userspace and which uid and gid values are
 * the internal kernel values.  With the addition of user namespaces the values
 * can be different.  Using the type system makes it possible for the compiler
 * to detect when we overlook these differences.
 *
 */







/*
 * general notes:
 *
 * CONFIG_UID16 is defined if the given architecture needs to
 * support backwards compatibility for old system calls.
 *
 * kernel code should use uid_t and gid_t at all times when dealing with
 * kernel-private data.
 *
 * old_uid_t and old_gid_t should only be different if CONFIG_UID16 is
 * defined, else the platform should provide dummy typedefs for them
 * such that they are equivalent to __kernel_{u,g}id_t.
 *
 * uid16_t and gid16_t are used on all architectures. (when dealing
 * with structures hard coded to 16 bits, such as in filesystems)
 */


/*
 * This is the "overflow" UID and GID. They are used to signify uid/gid
 * overflow to old programs when they request uid/gid information but are
 * using the old 16 bit interfaces.
 * When you run a libc5 program, it will think that all highuid files or
 * processes are owned by this uid/gid.
 * The idea is that it's better to do so than possibly return 0 in lieu of
 * 65536, etc.
 */

extern int Model1_overflowuid;
extern int Model1_overflowgid;

extern void Model1___bad_uid(void);
extern void Model1___bad_gid(void);






/* prevent uid mod 65536 effect by returning a default value for high UIDs */


/*
 * -1 is different in 16 bits than it is in 32 bits
 * these macros are used by chown(), setreuid(), ...,
 */
/* uid/gid input should be always 32bit uid_t */



/*
 * Everything below this line is needed on all architectures, to deal with
 * filesystems that only store 16 bits of the UID/GID, etc.
 */

/*
 * This is the UID and GID that will get written to disk if a filesystem
 * only supports 16-bit UIDs and the kernel has a high UID/GID to write
 */
extern int Model1_fs_overflowuid;
extern int Model1_fs_overflowgid;




/*
 * Since these macros are used in architectures that only need limited
 * 16-bit UID back compatibility, we won't use old_uid_t and old_gid_t
 */

struct Model1_user_namespace;
extern struct Model1_user_namespace Model1_init_user_ns;

typedef struct {
 Model1_uid_t Model1_val;
} Model1_kuid_t;


typedef struct {
 Model1_gid_t Model1_val;
} Model1_kgid_t;





static inline __attribute__((no_instrument_function)) Model1_uid_t Model1___kuid_val(Model1_kuid_t Model1_uid)
{
 return Model1_uid.Model1_val;
}

static inline __attribute__((no_instrument_function)) Model1_gid_t Model1___kgid_val(Model1_kgid_t Model1_gid)
{
 return Model1_gid.Model1_val;
}
static inline __attribute__((no_instrument_function)) bool Model1_uid_eq(Model1_kuid_t Model1_left, Model1_kuid_t Model1_right)
{
 return Model1___kuid_val(Model1_left) == Model1___kuid_val(Model1_right);
}

static inline __attribute__((no_instrument_function)) bool Model1_gid_eq(Model1_kgid_t Model1_left, Model1_kgid_t Model1_right)
{
 return Model1___kgid_val(Model1_left) == Model1___kgid_val(Model1_right);
}

static inline __attribute__((no_instrument_function)) bool Model1_uid_gt(Model1_kuid_t Model1_left, Model1_kuid_t Model1_right)
{
 return Model1___kuid_val(Model1_left) > Model1___kuid_val(Model1_right);
}

static inline __attribute__((no_instrument_function)) bool Model1_gid_gt(Model1_kgid_t Model1_left, Model1_kgid_t Model1_right)
{
 return Model1___kgid_val(Model1_left) > Model1___kgid_val(Model1_right);
}

static inline __attribute__((no_instrument_function)) bool Model1_uid_gte(Model1_kuid_t Model1_left, Model1_kuid_t Model1_right)
{
 return Model1___kuid_val(Model1_left) >= Model1___kuid_val(Model1_right);
}

static inline __attribute__((no_instrument_function)) bool Model1_gid_gte(Model1_kgid_t Model1_left, Model1_kgid_t Model1_right)
{
 return Model1___kgid_val(Model1_left) >= Model1___kgid_val(Model1_right);
}

static inline __attribute__((no_instrument_function)) bool Model1_uid_lt(Model1_kuid_t Model1_left, Model1_kuid_t Model1_right)
{
 return Model1___kuid_val(Model1_left) < Model1___kuid_val(Model1_right);
}

static inline __attribute__((no_instrument_function)) bool Model1_gid_lt(Model1_kgid_t Model1_left, Model1_kgid_t Model1_right)
{
 return Model1___kgid_val(Model1_left) < Model1___kgid_val(Model1_right);
}

static inline __attribute__((no_instrument_function)) bool Model1_uid_lte(Model1_kuid_t Model1_left, Model1_kuid_t Model1_right)
{
 return Model1___kuid_val(Model1_left) <= Model1___kuid_val(Model1_right);
}

static inline __attribute__((no_instrument_function)) bool Model1_gid_lte(Model1_kgid_t Model1_left, Model1_kgid_t Model1_right)
{
 return Model1___kgid_val(Model1_left) <= Model1___kgid_val(Model1_right);
}

static inline __attribute__((no_instrument_function)) bool Model1_uid_valid(Model1_kuid_t Model1_uid)
{
 return Model1___kuid_val(Model1_uid) != (Model1_uid_t) -1;
}

static inline __attribute__((no_instrument_function)) bool Model1_gid_valid(Model1_kgid_t Model1_gid)
{
 return Model1___kgid_val(Model1_gid) != (Model1_gid_t) -1;
}
static inline __attribute__((no_instrument_function)) Model1_kuid_t Model1_make_kuid(struct Model1_user_namespace *Model1_from, Model1_uid_t Model1_uid)
{
 return (Model1_kuid_t){ Model1_uid };
}

static inline __attribute__((no_instrument_function)) Model1_kgid_t Model1_make_kgid(struct Model1_user_namespace *Model1_from, Model1_gid_t Model1_gid)
{
 return (Model1_kgid_t){ Model1_gid };
}

static inline __attribute__((no_instrument_function)) Model1_uid_t Model1_from_kuid(struct Model1_user_namespace *Model1_to, Model1_kuid_t Model1_kuid)
{
 return Model1___kuid_val(Model1_kuid);
}

static inline __attribute__((no_instrument_function)) Model1_gid_t Model1_from_kgid(struct Model1_user_namespace *Model1_to, Model1_kgid_t Model1_kgid)
{
 return Model1___kgid_val(Model1_kgid);
}

static inline __attribute__((no_instrument_function)) Model1_uid_t Model1_from_kuid_munged(struct Model1_user_namespace *Model1_to, Model1_kuid_t Model1_kuid)
{
 Model1_uid_t Model1_uid = Model1_from_kuid(Model1_to, Model1_kuid);
 if (Model1_uid == (Model1_uid_t)-1)
  Model1_uid = Model1_overflowuid;
 return Model1_uid;
}

static inline __attribute__((no_instrument_function)) Model1_gid_t Model1_from_kgid_munged(struct Model1_user_namespace *Model1_to, Model1_kgid_t Model1_kgid)
{
 Model1_gid_t Model1_gid = Model1_from_kgid(Model1_to, Model1_kgid);
 if (Model1_gid == (Model1_gid_t)-1)
  Model1_gid = Model1_overflowgid;
 return Model1_gid;
}

static inline __attribute__((no_instrument_function)) bool Model1_kuid_has_mapping(struct Model1_user_namespace *Model1_ns, Model1_kuid_t Model1_uid)
{
 return Model1_uid_valid(Model1_uid);
}

static inline __attribute__((no_instrument_function)) bool Model1_kgid_has_mapping(struct Model1_user_namespace *Model1_ns, Model1_kgid_t Model1_gid)
{
 return Model1_gid_valid(Model1_gid);
}







/* Obsolete, used only for backwards compatibility and libc5 compiles */
struct Model1_ipc_perm
{
 Model1___kernel_key_t Model1_key;
 Model1___kernel_uid_t Model1_uid;
 Model1___kernel_gid_t Model1_gid;
 Model1___kernel_uid_t Model1_cuid;
 Model1___kernel_gid_t Model1_cgid;
 Model1___kernel_mode_t Model1_mode;
 unsigned short Model1_seq;
};

/* Include the definition of ipc64_perm */




/*
 * The generic ipc64_perm structure:
 * Note extra padding because this structure is passed back and forth
 * between kernel and user space.
 *
 * ipc64_perm was originally meant to be architecture specific, but
 * everyone just ended up making identical copies without specific
 * optimizations, so we may just as well all use the same one.
 *
 * Pad space is left for:
 * - 32-bit mode_t on architectures that only had 16 bit
 * - 32-bit seq
 * - 2 miscellaneous 32-bit values
 */

struct Model1_ipc64_perm {
 Model1___kernel_key_t Model1_key;
 Model1___kernel_uid32_t Model1_uid;
 Model1___kernel_gid32_t Model1_gid;
 Model1___kernel_uid32_t Model1_cuid;
 Model1___kernel_gid32_t Model1_cgid;
 Model1___kernel_mode_t Model1_mode;
    /* pad if mode_t is u16: */
 unsigned char Model1___pad1[4 - sizeof(Model1___kernel_mode_t)];
 unsigned short Model1_seq;
 unsigned short Model1___pad2;
 Model1___kernel_ulong_t Model1___unused1;
 Model1___kernel_ulong_t Model1___unused2;
};

/* resource get request flags */




/* these fields are used by the DIPC package so the kernel as standard
   should avoid using them if possible */




/* 
 * Control commands used with semctl, msgctl and shmctl 
 * see also specific commands in sem.h, msg.h and shm.h
 */





/*
 * Version flags for semctl, msgctl, and shmctl commands
 * These are passed as bitflags or-ed with the actual command
 */





/*
 * These are used to wrap system calls.
 *
 * See architecture code for ugly details..
 */
struct Model1_ipc_kludge {
 struct Model1_msgbuf *Model1_msgp;
 long Model1_msgtyp;
};
/* Used by the DIPC package, try and avoid reusing it */



/* used by in-kernel data structures */
struct Model1_kern_ipc_perm
{
 Model1_spinlock_t Model1_lock;
 bool Model1_deleted;
 int Model1_id;
 Model1_key_t Model1_key;
 Model1_kuid_t Model1_uid;
 Model1_kgid_t Model1_gid;
 Model1_kuid_t Model1_cuid;
 Model1_kgid_t Model1_cgid;
 Model1_umode_t Model1_mode;
 unsigned long Model1_seq;
 void *Model1_security;
};

/* semop flags */


/* semctl Command Definitions. */
/* ipcs ctl cmds */



/* Obsolete, used only for backwards compatibility and libc5 compiles */
struct Model1_semid_ds {
 struct Model1_ipc_perm Model1_sem_perm; /* permissions .. see ipc.h */
 Model1___kernel_time_t Model1_sem_otime; /* last semop time */
 Model1___kernel_time_t Model1_sem_ctime; /* last change time */
 struct Model1_sem *Model1_sem_base; /* ptr to first semaphore in array */
 struct Model1_sem_queue *Model1_sem_pending; /* pending operations to be processed */
 struct Model1_sem_queue **Model1_sem_pending_last; /* last pending operation */
 struct Model1_sem_undo *Model1_undo; /* undo requests on this array */
 unsigned short Model1_sem_nsems; /* no. of semaphores in array */
};

/* Include the definition of semid64_ds */




/*
 * The semid64_ds structure for x86 architecture.
 * Note extra padding because this structure is passed back and forth
 * between kernel and user space.
 *
 * Pad space is left for:
 * - 64-bit time_t to solve y2038 problem
 * - 2 miscellaneous 32-bit values
 */
struct Model1_semid64_ds {
 struct Model1_ipc64_perm Model1_sem_perm; /* permissions .. see ipc.h */
 Model1___kernel_time_t Model1_sem_otime; /* last semop time */
 Model1___kernel_ulong_t Model1___unused1;
 Model1___kernel_time_t Model1_sem_ctime; /* last change time */
 Model1___kernel_ulong_t Model1___unused2;
 Model1___kernel_ulong_t Model1_sem_nsems; /* no. of semaphores in array */
 Model1___kernel_ulong_t Model1___unused3;
 Model1___kernel_ulong_t Model1___unused4;
};

/* semop system calls takes an array of these. */
struct Model1_sembuf {
 unsigned short Model1_sem_num; /* semaphore index in array */
 short Model1_sem_op; /* semaphore operation */
 short Model1_sem_flg; /* operation flags */
};

/* arg for semctl system calls. */
union Model1_semun {
 int Model1_val; /* value for SETVAL */
 struct Model1_semid_ds *Model1_buf; /* buffer for IPC_STAT & IPC_SET */
 unsigned short *Model1_array; /* array for GETALL & SETALL */
 struct Model1_seminfo *Model1___buf; /* buffer for IPC_INFO */
 void *Model1___pad;
};

struct Model1_seminfo {
 int Model1_semmap;
 int Model1_semmni;
 int Model1_semmns;
 int Model1_semmnu;
 int Model1_semmsl;
 int Model1_semopm;
 int Model1_semume;
 int Model1_semusz;
 int Model1_semvmx;
 int Model1_semaem;
};

/*
 * SEMMNI, SEMMSL and SEMMNS are default values which can be
 * modified by sysctl.
 * The values has been chosen to be larger than necessary for any
 * known configuration.
 *
 * SEMOPM should not be increased beyond 1000, otherwise there is the
 * risk that semop()/semtimedop() fails due to kernel memory fragmentation when
 * allocating the sop array.
 */
/* unused */

struct Model1_task_struct;

/* One sem_array data structure for each set of semaphores in the system. */
struct Model1_sem_array {
 struct Model1_kern_ipc_perm __attribute__((__aligned__((1 << (6)))))
    Model1_sem_perm; /* permissions .. see ipc.h */
 Model1_time_t Model1_sem_ctime; /* last change time */
 struct Model1_sem *Model1_sem_base; /* ptr to first semaphore in array */
 struct Model1_list_head Model1_pending_alter; /* pending operations */
      /* that alter the array */
 struct Model1_list_head Model1_pending_const; /* pending complex operations */
      /* that do not alter semvals */
 struct Model1_list_head Model1_list_id; /* undo requests on this array */
 int Model1_sem_nsems; /* no. of semaphores in array */
 int Model1_complex_count; /* pending complex operations */
};



struct Model1_sysv_sem {
 struct Model1_sem_undo_list *Model1_undo_list;
};

extern int Model1_copy_semundo(unsigned long Model1_clone_flags, struct Model1_task_struct *Model1_tsk);
extern void Model1_exit_sem(struct Model1_task_struct *Model1_tsk);





/*
 * SHMMNI, SHMMAX and SHMALL are default upper limits which can be
 * modified by sysctl. The SHMMAX and SHMALL values have been chosen to
 * be as large possible without facilitating scenarios where userspace
 * causes overflows when adjusting the limits via operations of the form
 * "retrieve current limit; add X; update limit". It is therefore not
 * advised to make SHMMAX and SHMALL any larger. These limits are
 * suitable for both 32 and 64-bit systems.
 */






/* Obsolete, used only for backwards compatibility and libc5 compiles */
struct Model1_shmid_ds {
 struct Model1_ipc_perm Model1_shm_perm; /* operation perms */
 int Model1_shm_segsz; /* size of segment (bytes) */
 Model1___kernel_time_t Model1_shm_atime; /* last attach time */
 Model1___kernel_time_t Model1_shm_dtime; /* last detach time */
 Model1___kernel_time_t Model1_shm_ctime; /* last change time */
 Model1___kernel_ipc_pid_t Model1_shm_cpid; /* pid of creator */
 Model1___kernel_ipc_pid_t Model1_shm_lpid; /* pid of last operator */
 unsigned short Model1_shm_nattch; /* no. of current attaches */
 unsigned short Model1_shm_unused; /* compatibility */
 void *Model1_shm_unused2; /* ditto - used by DIPC */
 void *Model1_shm_unused3; /* unused */
};

/* Include the definition of shmid64_ds and shminfo64 */






/*
 * The shmid64_ds structure for x86 architecture.
 * Note extra padding because this structure is passed back and forth
 * between kernel and user space.
 *
 * shmid64_ds was originally meant to be architecture specific, but
 * everyone just ended up making identical copies without specific
 * optimizations, so we may just as well all use the same one.
 *
 * 64 bit architectures typically define a 64 bit __kernel_time_t,
 * so they do not need the first two padding words.
 * On big-endian systems, the padding is in the wrong place.
 *
 *
 * Pad space is left for:
 * - 64-bit time_t to solve y2038 problem
 * - 2 miscellaneous 32-bit values
 */

struct Model1_shmid64_ds {
 struct Model1_ipc64_perm Model1_shm_perm; /* operation perms */
 Model1_size_t Model1_shm_segsz; /* size of segment (bytes) */
 Model1___kernel_time_t Model1_shm_atime; /* last attach time */



 Model1___kernel_time_t Model1_shm_dtime; /* last detach time */



 Model1___kernel_time_t Model1_shm_ctime; /* last change time */



 Model1___kernel_pid_t Model1_shm_cpid; /* pid of creator */
 Model1___kernel_pid_t Model1_shm_lpid; /* pid of last operator */
 Model1___kernel_ulong_t Model1_shm_nattch; /* no. of current attaches */
 Model1___kernel_ulong_t Model1___unused4;
 Model1___kernel_ulong_t Model1___unused5;
};

struct Model1_shminfo64 {
 Model1___kernel_ulong_t Model1_shmmax;
 Model1___kernel_ulong_t Model1_shmmin;
 Model1___kernel_ulong_t Model1_shmmni;
 Model1___kernel_ulong_t Model1_shmseg;
 Model1___kernel_ulong_t Model1_shmall;
 Model1___kernel_ulong_t Model1___unused1;
 Model1___kernel_ulong_t Model1___unused2;
 Model1___kernel_ulong_t Model1___unused3;
 Model1___kernel_ulong_t Model1___unused4;
};

/* permission flag for shmget */



/* mode for attach */





/* super user shmctl commands */



/* ipcs ctl commands */



/* Obsolete, used only for backwards compatibility */
struct Model1_shminfo {
 int Model1_shmmax;
 int Model1_shmmin;
 int Model1_shmmni;
 int Model1_shmseg;
 int Model1_shmall;
};

struct Model1_shm_info {
 int Model1_used_ids;
 Model1___kernel_ulong_t Model1_shm_tot; /* total allocated shm */
 Model1___kernel_ulong_t Model1_shm_rss; /* total resident shm */
 Model1___kernel_ulong_t Model1_shm_swp; /* total swapped shm */
 Model1___kernel_ulong_t Model1_swap_attempts;
 Model1___kernel_ulong_t Model1_swap_successes;
};

struct Model1_shmid_kernel /* private to the kernel */
{
 struct Model1_kern_ipc_perm Model1_shm_perm;
 struct Model1_file *Model1_shm_file;
 unsigned long Model1_shm_nattch;
 unsigned long Model1_shm_segsz;
 Model1_time_t Model1_shm_atim;
 Model1_time_t Model1_shm_dtim;
 Model1_time_t Model1_shm_ctim;
 Model1_pid_t Model1_shm_cprid;
 Model1_pid_t Model1_shm_lprid;
 struct Model1_user_struct *Model1_mlock_user;

 /* The task created the shm object.  NULL if the task is dead. */
 struct Model1_task_struct *Model1_shm_creator;
 struct Model1_list_head Model1_shm_clist; /* list by creator */
};

/* shm_mode upper byte flags */





/* Bits [26:31] are reserved */

/*
 * When SHM_HUGETLB is set bits [26:31] encode the log2 of the huge page size.
 * This gives us 6 bits, which is enough until someone invents 128 bit address
 * spaces.
 *
 * Assume these are all power of twos.
 * When 0 use the default page size.
 */






struct Model1_sysv_shm {
 struct Model1_list_head Model1_shm_clist;
};

long Model1_do_shmat(int Model1_shmid, char *Model1_shmaddr, int Model1_shmflg, unsigned long *Model1_addr,
       unsigned long Model1_shmlba);
bool Model1_is_file_shm_hugepages(struct Model1_file *Model1_file);
void Model1_exit_shm(struct Model1_task_struct *Model1_task);














/* Most things should be clean enough to redefine this at will, if care
   is taken to make libc match.  */
typedef unsigned long Model1_old_sigset_t; /* at least 32 bits */

typedef struct {
 unsigned long Model1_sig[(64 / 64)];
} Model1_sigset_t;















/* Avoid too many header ordering problems.  */
struct Model1_siginfo;
/*
#define SIGLOST		29
*/




/* These should not be considered constants from userland.  */



/*
 * SA_FLAGS values:
 *
 * SA_ONSTACK indicates that a registered stack_t will be used.
 * SA_RESTART flag to get restarting signals (which were the default long ago)
 * SA_NOCLDSTOP flag to turn off SIGCHLD when children stop.
 * SA_RESETHAND clears the handler when the signal is delivered.
 * SA_NOCLDWAIT flag on SIGCHLD to inhibit zombies.
 * SA_NODEFER prevents the current signal from being masked in the handler.
 *
 * SA_ONESHOT and SA_NOMASK are the historical Linux names for the Single
 * Unix names RESETHAND and NODEFER respectively.
 */
typedef void Model1___signalfn_t(int);
typedef Model1___signalfn_t *Model1___sighandler_t;

typedef void Model1___restorefn_t(void);
typedef Model1___restorefn_t *Model1___sigrestore_t;
typedef struct Model1_sigaltstack {
 void *Model1_ss_sp;
 int Model1_ss_flags;
 Model1_size_t Model1_ss_size;
} Model1_stack_t;

extern void Model1_do_signal(struct Model1_pt_regs *Model1_regs);









typedef union Model1_sigval {
 int Model1_sival_int;
 void *Model1_sival_ptr;
} Model1_sigval_t;

/*
 * This is the size (including padding) of the part of the
 * struct siginfo that is before the union.
 */
/*
 * The default "si_band" type is "long", as specified by POSIX.
 * However, some architectures want to override this to "int"
 * for historical compatibility reasons, so we allow that.
 */
typedef struct Model1_siginfo {
 int Model1_si_signo;
 int Model1_si_errno;
 int Model1_si_code;

 union {
  int Model1__pad[((128 - (4 * sizeof(int))) / sizeof(int))];

  /* kill() */
  struct {
   Model1___kernel_pid_t Model1__pid; /* sender's pid */
   Model1___kernel_uid32_t Model1__uid; /* sender's uid */
  } Model1__kill;

  /* POSIX.1b timers */
  struct {
   Model1___kernel_timer_t Model1__tid; /* timer id */
   int Model1__overrun; /* overrun count */
   char Model1__pad[sizeof( Model1___kernel_uid32_t) - sizeof(int)];
   Model1_sigval_t Model1__sigval; /* same as below */
   int Model1__sys_private; /* not to be passed to user */
  } Model1__timer;

  /* POSIX.1b signals */
  struct {
   Model1___kernel_pid_t Model1__pid; /* sender's pid */
   Model1___kernel_uid32_t Model1__uid; /* sender's uid */
   Model1_sigval_t Model1__sigval;
  } Model1__rt;

  /* SIGCHLD */
  struct {
   Model1___kernel_pid_t Model1__pid; /* which child */
   Model1___kernel_uid32_t Model1__uid; /* sender's uid */
   int Model1__status; /* exit code */
   Model1___kernel_clock_t Model1__utime;
   Model1___kernel_clock_t Model1__stime;
  } Model1__sigchld;

  /* SIGILL, SIGFPE, SIGSEGV, SIGBUS */
  struct {
   void *Model1__addr; /* faulting insn/memory ref. */



   short Model1__addr_lsb; /* LSB of the reported address */
   union {
    /* used when si_code=SEGV_BNDERR */
    struct {
     void *Model1__lower;
     void *Model1__upper;
    } Model1__addr_bnd;
    /* used when si_code=SEGV_PKUERR */
    __u32 Model1__pkey;
   };
  } Model1__sigfault;

  /* SIGPOLL */
  struct {
   long Model1__band; /* POLL_IN, POLL_OUT, POLL_MSG */
   int Model1__fd;
  } Model1__sigpoll;

  /* SIGSYS */
  struct {
   void *Model1__call_addr; /* calling user insn */
   int Model1__syscall; /* triggering system call number */
   unsigned int Model1__arch; /* AUDIT_ARCH_* of syscall */
  } Model1__sigsys;
 } Model1__sifields;
} Model1_siginfo_t;

/* If the arch shares siginfo, then it has SIGSYS. */



/*
 * How these fields are to be accessed.
 */
/*
 * si_code values
 * Digital reserves positive values for kernel-generated signals.
 */
/*
 * SIGILL si_codes
 */
/*
 * SIGFPE si_codes
 */
/*
 * SIGSEGV si_codes
 */






/*
 * SIGBUS si_codes
 */



/* hardware memory error consumed on a machine check: action required */

/* hardware memory error detected in process but not consumed: action optional*/



/*
 * SIGTRAP si_codes
 */






/*
 * SIGCHLD si_codes
 */
/*
 * SIGPOLL si_codes
 */
/*
 * SIGSYS si_codes
 */



/*
 * sigevent definitions
 * 
 * It seems likely that SIGEV_THREAD will have to be handled from 
 * userspace, libpthread transmuting it to SIGEV_SIGNAL, which the
 * thread manager then catches and does the appropriate nonsense.
 * However, everything is written out here so as to not get lost.
 */





/*
 * This works because the alignment is ok on all current architectures
 * but we leave open this being overridden in the future
 */
typedef struct Model1_sigevent {
 Model1_sigval_t Model1_sigev_value;
 int Model1_sigev_signo;
 int Model1_sigev_notify;
 union {
  int Model1__pad[((64 - (sizeof(int) * 2 + sizeof(Model1_sigval_t))) / sizeof(int))];
   int Model1__tid;

  struct {
   void (*Model1__function)(Model1_sigval_t);
   void *Model1__attribute; /* really pthread_attr_t */
  } Model1__sigev_thread;
 } Model1__sigev_un;
} Model1_sigevent_t;
struct Model1_siginfo;
void Model1_do_schedule_next_timer(struct Model1_siginfo *Model1_info);

extern int Model1_copy_siginfo_to_user(struct Model1_siginfo *Model1_to, const struct Model1_siginfo *Model1_from);




/* bit-flags */

/* mask for all SS_xxx flags */

struct Model1_task_struct;

/* for sysctl */
extern int Model1_print_fatal_signals;
/*
 * Real Time signals may be queued.
 */

struct Model1_sigqueue {
 struct Model1_list_head Model1_list;
 int Model1_flags;
 Model1_siginfo_t Model1_info;
 struct Model1_user_struct *Model1_user;
};

/* flags values. */


struct Model1_sigpending {
 struct Model1_list_head Model1_list;
 Model1_sigset_t Model1_signal;
};





static inline __attribute__((no_instrument_function)) void Model1_copy_siginfo(struct Model1_siginfo *Model1_to, struct Model1_siginfo *Model1_from)
{
 if (Model1_from->Model1_si_code < 0)
  ({ Model1_size_t Model1___len = (sizeof(*Model1_to)); void *Model1___ret; if (__builtin_constant_p(sizeof(*Model1_to)) && Model1___len >= 64) Model1___ret = Model1___memcpy((Model1_to), (Model1_from), Model1___len); else Model1___ret = __builtin_memcpy((Model1_to), (Model1_from), Model1___len); Model1___ret; });
 else
  /* _sigchld is currently the largest know union member */
  ({ Model1_size_t Model1___len = ((4 * sizeof(int)) + sizeof(Model1_from->Model1__sifields.Model1__sigchld)); void *Model1___ret; if (__builtin_constant_p((4 * sizeof(int)) + sizeof(Model1_from->Model1__sifields.Model1__sigchld)) && Model1___len >= 64) Model1___ret = Model1___memcpy((Model1_to), (Model1_from), Model1___len); else Model1___ret = __builtin_memcpy((Model1_to), (Model1_from), Model1___len); Model1___ret; });
}



/*
 * Define some primitives to manipulate sigset_t.
 */




/* We don't use <linux/bitops.h> for these because there is no need to
   be atomic.  */
static inline __attribute__((no_instrument_function)) void Model1_sigaddset(Model1_sigset_t *Model1_set, int Model1__sig)
{
 unsigned long Model1_sig = Model1__sig - 1;
 if ((64 / 64) == 1)
  Model1_set->Model1_sig[0] |= 1UL << Model1_sig;
 else
  Model1_set->Model1_sig[Model1_sig / 64] |= 1UL << (Model1_sig % 64);
}

static inline __attribute__((no_instrument_function)) void Model1_sigdelset(Model1_sigset_t *Model1_set, int Model1__sig)
{
 unsigned long Model1_sig = Model1__sig - 1;
 if ((64 / 64) == 1)
  Model1_set->Model1_sig[0] &= ~(1UL << Model1_sig);
 else
  Model1_set->Model1_sig[Model1_sig / 64] &= ~(1UL << (Model1_sig % 64));
}

static inline __attribute__((no_instrument_function)) int Model1_sigismember(Model1_sigset_t *Model1_set, int Model1__sig)
{
 unsigned long Model1_sig = Model1__sig - 1;
 if ((64 / 64) == 1)
  return 1 & (Model1_set->Model1_sig[0] >> Model1_sig);
 else
  return 1 & (Model1_set->Model1_sig[Model1_sig / 64] >> (Model1_sig % 64));
}



static inline __attribute__((no_instrument_function)) int Model1_sigisemptyset(Model1_sigset_t *Model1_set)
{
 switch ((64 / 64)) {
 case 4:
  return (Model1_set->Model1_sig[3] | Model1_set->Model1_sig[2] |
   Model1_set->Model1_sig[1] | Model1_set->Model1_sig[0]) == 0;
 case 2:
  return (Model1_set->Model1_sig[1] | Model1_set->Model1_sig[0]) == 0;
 case 1:
  return Model1_set->Model1_sig[0] == 0;
 default:
  do { bool Model1___cond = !(!(1)); extern void Model1___compiletime_assert_95(void) ; if (Model1___cond) Model1___compiletime_assert_95(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
  return 0;
 }
}
static inline __attribute__((no_instrument_function)) void Model1_sigorsets(Model1_sigset_t *Model1_r, const Model1_sigset_t *Model1_a, const Model1_sigset_t *Model1_b) { unsigned long Model1_a0, Model1_a1, Model1_a2, Model1_a3, Model1_b0, Model1_b1, Model1_b2, Model1_b3; switch ((64 / 64)) { case 4: Model1_a3 = Model1_a->Model1_sig[3]; Model1_a2 = Model1_a->Model1_sig[2]; Model1_b3 = Model1_b->Model1_sig[3]; Model1_b2 = Model1_b->Model1_sig[2]; Model1_r->Model1_sig[3] = ((Model1_a3) | (Model1_b3)); Model1_r->Model1_sig[2] = ((Model1_a2) | (Model1_b2)); case 2: Model1_a1 = Model1_a->Model1_sig[1]; Model1_b1 = Model1_b->Model1_sig[1]; Model1_r->Model1_sig[1] = ((Model1_a1) | (Model1_b1)); case 1: Model1_a0 = Model1_a->Model1_sig[0]; Model1_b0 = Model1_b->Model1_sig[0]; Model1_r->Model1_sig[0] = ((Model1_a0) | (Model1_b0)); break; default: do { bool Model1___cond = !(!(1)); extern void Model1___compiletime_assert_129(void) ; if (Model1___cond) Model1___compiletime_assert_129(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0); } }


static inline __attribute__((no_instrument_function)) void Model1_sigandsets(Model1_sigset_t *Model1_r, const Model1_sigset_t *Model1_a, const Model1_sigset_t *Model1_b) { unsigned long Model1_a0, Model1_a1, Model1_a2, Model1_a3, Model1_b0, Model1_b1, Model1_b2, Model1_b3; switch ((64 / 64)) { case 4: Model1_a3 = Model1_a->Model1_sig[3]; Model1_a2 = Model1_a->Model1_sig[2]; Model1_b3 = Model1_b->Model1_sig[3]; Model1_b2 = Model1_b->Model1_sig[2]; Model1_r->Model1_sig[3] = ((Model1_a3) & (Model1_b3)); Model1_r->Model1_sig[2] = ((Model1_a2) & (Model1_b2)); case 2: Model1_a1 = Model1_a->Model1_sig[1]; Model1_b1 = Model1_b->Model1_sig[1]; Model1_r->Model1_sig[1] = ((Model1_a1) & (Model1_b1)); case 1: Model1_a0 = Model1_a->Model1_sig[0]; Model1_b0 = Model1_b->Model1_sig[0]; Model1_r->Model1_sig[0] = ((Model1_a0) & (Model1_b0)); break; default: do { bool Model1___cond = !(!(1)); extern void Model1___compiletime_assert_132(void) ; if (Model1___cond) Model1___compiletime_assert_132(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0); } }


static inline __attribute__((no_instrument_function)) void Model1_sigandnsets(Model1_sigset_t *Model1_r, const Model1_sigset_t *Model1_a, const Model1_sigset_t *Model1_b) { unsigned long Model1_a0, Model1_a1, Model1_a2, Model1_a3, Model1_b0, Model1_b1, Model1_b2, Model1_b3; switch ((64 / 64)) { case 4: Model1_a3 = Model1_a->Model1_sig[3]; Model1_a2 = Model1_a->Model1_sig[2]; Model1_b3 = Model1_b->Model1_sig[3]; Model1_b2 = Model1_b->Model1_sig[2]; Model1_r->Model1_sig[3] = ((Model1_a3) & ~(Model1_b3)); Model1_r->Model1_sig[2] = ((Model1_a2) & ~(Model1_b2)); case 2: Model1_a1 = Model1_a->Model1_sig[1]; Model1_b1 = Model1_b->Model1_sig[1]; Model1_r->Model1_sig[1] = ((Model1_a1) & ~(Model1_b1)); case 1: Model1_a0 = Model1_a->Model1_sig[0]; Model1_b0 = Model1_b->Model1_sig[0]; Model1_r->Model1_sig[0] = ((Model1_a0) & ~(Model1_b0)); break; default: do { bool Model1___cond = !(!(1)); extern void Model1___compiletime_assert_135(void) ; if (Model1___cond) Model1___compiletime_assert_135(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0); } }
static inline __attribute__((no_instrument_function)) void Model1_signotset(Model1_sigset_t *Model1_set) { switch ((64 / 64)) { case 4: Model1_set->Model1_sig[3] = (~(Model1_set->Model1_sig[3])); Model1_set->Model1_sig[2] = (~(Model1_set->Model1_sig[2])); case 2: Model1_set->Model1_sig[1] = (~(Model1_set->Model1_sig[1])); case 1: Model1_set->Model1_sig[0] = (~(Model1_set->Model1_sig[0])); break; default: do { bool Model1___cond = !(!(1)); extern void Model1___compiletime_assert_157(void) ; if (Model1___cond) Model1___compiletime_assert_157(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0); } }




static inline __attribute__((no_instrument_function)) void Model1_sigemptyset(Model1_sigset_t *Model1_set)
{
 switch ((64 / 64)) {
 default:
  memset(Model1_set, 0, sizeof(Model1_sigset_t));
  break;
 case 2: Model1_set->Model1_sig[1] = 0;
 case 1: Model1_set->Model1_sig[0] = 0;
  break;
 }
}

static inline __attribute__((no_instrument_function)) void Model1_sigfillset(Model1_sigset_t *Model1_set)
{
 switch ((64 / 64)) {
 default:
  memset(Model1_set, -1, sizeof(Model1_sigset_t));
  break;
 case 2: Model1_set->Model1_sig[1] = -1;
 case 1: Model1_set->Model1_sig[0] = -1;
  break;
 }
}

/* Some extensions for manipulating the low 32 signals in particular.  */

static inline __attribute__((no_instrument_function)) void Model1_sigaddsetmask(Model1_sigset_t *Model1_set, unsigned long Model1_mask)
{
 Model1_set->Model1_sig[0] |= Model1_mask;
}

static inline __attribute__((no_instrument_function)) void Model1_sigdelsetmask(Model1_sigset_t *Model1_set, unsigned long Model1_mask)
{
 Model1_set->Model1_sig[0] &= ~Model1_mask;
}

static inline __attribute__((no_instrument_function)) int Model1_sigtestsetmask(Model1_sigset_t *Model1_set, unsigned long Model1_mask)
{
 return (Model1_set->Model1_sig[0] & Model1_mask) != 0;
}

static inline __attribute__((no_instrument_function)) void Model1_siginitset(Model1_sigset_t *Model1_set, unsigned long Model1_mask)
{
 Model1_set->Model1_sig[0] = Model1_mask;
 switch ((64 / 64)) {
 default:
  memset(&Model1_set->Model1_sig[1], 0, sizeof(long)*((64 / 64)-1));
  break;
 case 2: Model1_set->Model1_sig[1] = 0;
 case 1: ;
 }
}

static inline __attribute__((no_instrument_function)) void Model1_siginitsetinv(Model1_sigset_t *Model1_set, unsigned long Model1_mask)
{
 Model1_set->Model1_sig[0] = ~Model1_mask;
 switch ((64 / 64)) {
 default:
  memset(&Model1_set->Model1_sig[1], -1, sizeof(long)*((64 / 64)-1));
  break;
 case 2: Model1_set->Model1_sig[1] = -1;
 case 1: ;
 }
}



static inline __attribute__((no_instrument_function)) void Model1_init_sigpending(struct Model1_sigpending *Model1_sig)
{
 Model1_sigemptyset(&Model1_sig->Model1_signal);
 Model1_INIT_LIST_HEAD(&Model1_sig->Model1_list);
}

extern void Model1_flush_sigqueue(struct Model1_sigpending *Model1_queue);

/* Test if 'sig' is valid signal. Use this instead of testing _NSIG directly */
static inline __attribute__((no_instrument_function)) int Model1_valid_signal(unsigned long Model1_sig)
{
 return Model1_sig <= 64 ? 1 : 0;
}

struct Model1_timespec;
struct Model1_pt_regs;

extern int Model1_next_signal(struct Model1_sigpending *Model1_pending, Model1_sigset_t *Model1_mask);
extern int Model1_do_send_sig_info(int Model1_sig, struct Model1_siginfo *Model1_info,
    struct Model1_task_struct *Model1_p, bool Model1_group);
extern int Model1_group_send_sig_info(int Model1_sig, struct Model1_siginfo *Model1_info, struct Model1_task_struct *Model1_p);
extern int Model1___group_send_sig_info(int, struct Model1_siginfo *, struct Model1_task_struct *);
extern int Model1_do_sigtimedwait(const Model1_sigset_t *, Model1_siginfo_t *,
    const struct Model1_timespec *);
extern int Model1_sigprocmask(int, Model1_sigset_t *, Model1_sigset_t *);
extern void Model1_set_current_blocked(Model1_sigset_t *);
extern void Model1___set_current_blocked(const Model1_sigset_t *);
extern int Model1_show_unhandled_signals;

struct Model1_sigaction {

 Model1___sighandler_t Model1_sa_handler;
 unsigned long Model1_sa_flags;





 Model1___sigrestore_t Model1_sa_restorer;

 Model1_sigset_t Model1_sa_mask; /* mask last for extensibility */
};

struct Model1_k_sigaction {
 struct Model1_sigaction Model1_sa;



};
struct Model1_ksignal {
 struct Model1_k_sigaction Model1_ka;
 Model1_siginfo_t Model1_info;
 int Model1_sig;
};

extern int Model1_get_signal(struct Model1_ksignal *Model1_ksig);
extern void Model1_signal_setup_done(int Model1_failed, struct Model1_ksignal *Model1_ksig, int Model1_stepping);
extern void Model1_exit_signals(struct Model1_task_struct *Model1_tsk);
extern void Model1_kernel_sigaction(int, Model1___sighandler_t);

static inline __attribute__((no_instrument_function)) void Model1_allow_signal(int Model1_sig)
{
 /*
	 * Kernel threads handle their own signals. Let the signal code
	 * know it'll be handled, so that they don't get converted to
	 * SIGKILL or just silently dropped.
	 */
 Model1_kernel_sigaction(Model1_sig, ( Model1___sighandler_t)2);
}

static inline __attribute__((no_instrument_function)) void Model1_disallow_signal(int Model1_sig)
{
 Model1_kernel_sigaction(Model1_sig, (( Model1___sighandler_t)1));
}

extern struct Model1_kmem_cache *Model1_sighand_cachep;

int Model1_unhandled_signal(struct Model1_task_struct *Model1_tsk, int Model1_sig);

/*
 * In POSIX a signal is sent either to a specific thread (Linux task)
 * or to the process as a whole (Linux thread group).  How the signal
 * is sent determines whether it's to one thread or the whole group,
 * which determines which signal mask(s) are involved in blocking it
 * from being delivered until later.  When the signal is delivered,
 * either it's caught or ignored by a user handler or it has a default
 * effect that applies to the whole thread group (POSIX process).
 *
 * The possible effects an unblocked signal set to SIG_DFL can have are:
 *   ignore	- Nothing Happens
 *   terminate	- kill the process, i.e. all threads in the group,
 * 		  similar to exit_group.  The group leader (only) reports
 *		  WIFSIGNALED status to its parent.
 *   coredump	- write a core dump file describing all threads using
 *		  the same mm and then kill all those threads
 *   stop 	- stop all the threads in the group, i.e. TASK_STOPPED state
 *
 * SIGKILL and SIGSTOP cannot be caught, blocked, or ignored.
 * Other signals when not blocked and set to SIG_DFL behaves as follows.
 * The job control signals also have other special effects.
 *
 *	+--------------------+------------------+
 *	|  POSIX signal      |  default action  |
 *	+--------------------+------------------+
 *	|  SIGHUP            |  terminate	|
 *	|  SIGINT            |	terminate	|
 *	|  SIGQUIT           |	coredump 	|
 *	|  SIGILL            |	coredump 	|
 *	|  SIGTRAP           |	coredump 	|
 *	|  SIGABRT/SIGIOT    |	coredump 	|
 *	|  SIGBUS            |	coredump 	|
 *	|  SIGFPE            |	coredump 	|
 *	|  SIGKILL           |	terminate(+)	|
 *	|  SIGUSR1           |	terminate	|
 *	|  SIGSEGV           |	coredump 	|
 *	|  SIGUSR2           |	terminate	|
 *	|  SIGPIPE           |	terminate	|
 *	|  SIGALRM           |	terminate	|
 *	|  SIGTERM           |	terminate	|
 *	|  SIGCHLD           |	ignore   	|
 *	|  SIGCONT           |	ignore(*)	|
 *	|  SIGSTOP           |	stop(*)(+)  	|
 *	|  SIGTSTP           |	stop(*)  	|
 *	|  SIGTTIN           |	stop(*)  	|
 *	|  SIGTTOU           |	stop(*)  	|
 *	|  SIGURG            |	ignore   	|
 *	|  SIGXCPU           |	coredump 	|
 *	|  SIGXFSZ           |	coredump 	|
 *	|  SIGVTALRM         |	terminate	|
 *	|  SIGPROF           |	terminate	|
 *	|  SIGPOLL/SIGIO     |	terminate	|
 *	|  SIGSYS/SIGUNUSED  |	coredump 	|
 *	|  SIGSTKFLT         |	terminate	|
 *	|  SIGWINCH          |	ignore   	|
 *	|  SIGPWR            |	terminate	|
 *	|  SIGRTMIN-SIGRTMAX |	terminate       |
 *	+--------------------+------------------+
 *	|  non-POSIX signal  |  default action  |
 *	+--------------------+------------------+
 *	|  SIGEMT            |  coredump	|
 *	+--------------------+------------------+
 *
 * (+) For SIGKILL and SIGSTOP the action is "always", not just "default".
 * (*) Special job control effects:
 * When SIGCONT is sent, it resumes the process (all threads in the group)
 * from TASK_STOPPED state and also clears any pending/queued stop signals
 * (any of those marked with "stop(*)").  This happens regardless of blocking,
 * catching, or ignoring SIGCONT.  When any stop signal is sent, it clears
 * any pending/queued SIGCONT signals; this happens regardless of blocking,
 * catching, or ignored the stop signal, though (except for SIGSTOP) the
 * default action of stopping the process may happen later or never.
 */
void Model1_signals_init(void);

int Model1_restore_altstack(const Model1_stack_t *);
int Model1___save_altstack(Model1_stack_t *, unsigned long);
struct Model1_seq_file;
extern void Model1_render_sigset_t(struct Model1_seq_file *, const char *, Model1_sigset_t *);







enum Model1_pid_type
{
 Model1_PIDTYPE_PID,
 Model1_PIDTYPE_PGID,
 Model1_PIDTYPE_SID,
 Model1_PIDTYPE_MAX
};

/*
 * What is struct pid?
 *
 * A struct pid is the kernel's internal notion of a process identifier.
 * It refers to individual tasks, process groups, and sessions.  While
 * there are processes attached to it the struct pid lives in a hash
 * table, so it and then the processes that it refers to can be found
 * quickly from the numeric pid value.  The attached processes may be
 * quickly accessed by following pointers from struct pid.
 *
 * Storing pid_t values in the kernel and referring to them later has a
 * problem.  The process originally with that pid may have exited and the
 * pid allocator wrapped, and another process could have come along
 * and been assigned that pid.
 *
 * Referring to user space processes by holding a reference to struct
 * task_struct has a problem.  When the user space process exits
 * the now useless task_struct is still kept.  A task_struct plus a
 * stack consumes around 10K of low kernel memory.  More precisely
 * this is THREAD_SIZE + sizeof(struct task_struct).  By comparison
 * a struct pid is about 64 bytes.
 *
 * Holding a reference to struct pid solves both of these problems.
 * It is small so holding a reference does not consume a lot of
 * resources, and since a new struct pid is allocated when the numeric pid
 * value is reused (when pids wrap around) we don't mistakenly refer to new
 * processes.
 */


/*
 * struct upid is used to get the id of the struct pid, as it is
 * seen in particular namespace. Later the struct pid is found with
 * find_pid_ns() using the int nr and struct pid_namespace *ns.
 */

struct Model1_upid {
 /* Try to keep pid_chain in the same cacheline as nr for find_vpid */
 int Model1_nr;
 struct Model1_pid_namespace *Model1_ns;
 struct Model1_hlist_node Model1_pid_chain;
};

struct Model1_pid
{
 Model1_atomic_t Model1_count;
 unsigned int Model1_level;
 /* lists of tasks that use this pid */
 struct Model1_hlist_head Model1_tasks[Model1_PIDTYPE_MAX];
 struct Model1_callback_head Model1_rcu;
 struct Model1_upid Model1_numbers[1];
};

extern struct Model1_pid Model1_init_struct_pid;

struct Model1_pid_link
{
 struct Model1_hlist_node Model1_node;
 struct Model1_pid *Model1_pid;
};

static inline __attribute__((no_instrument_function)) struct Model1_pid *Model1_get_pid(struct Model1_pid *Model1_pid)
{
 if (Model1_pid)
  Model1_atomic_inc(&Model1_pid->Model1_count);
 return Model1_pid;
}

extern void Model1_put_pid(struct Model1_pid *Model1_pid);
extern struct Model1_task_struct *Model1_pid_task(struct Model1_pid *Model1_pid, enum Model1_pid_type);
extern struct Model1_task_struct *Model1_get_pid_task(struct Model1_pid *Model1_pid, enum Model1_pid_type);

extern struct Model1_pid *Model1_get_task_pid(struct Model1_task_struct *Model1_task, enum Model1_pid_type Model1_type);

/*
 * these helpers must be called with the tasklist_lock write-held.
 */
extern void Model1_attach_pid(struct Model1_task_struct *Model1_task, enum Model1_pid_type);
extern void Model1_detach_pid(struct Model1_task_struct *Model1_task, enum Model1_pid_type);
extern void Model1_change_pid(struct Model1_task_struct *Model1_task, enum Model1_pid_type,
   struct Model1_pid *Model1_pid);
extern void Model1_transfer_pid(struct Model1_task_struct *old, struct Model1_task_struct *Model1_new,
    enum Model1_pid_type);

struct Model1_pid_namespace;
extern struct Model1_pid_namespace Model1_init_pid_ns;

/*
 * look up a PID in the hash table. Must be called with the tasklist_lock
 * or rcu_read_lock() held.
 *
 * find_pid_ns() finds the pid in the namespace specified
 * find_vpid() finds the pid by its virtual id, i.e. in the current namespace
 *
 * see also find_task_by_vpid() set in include/linux/sched.h
 */
extern struct Model1_pid *Model1_find_pid_ns(int Model1_nr, struct Model1_pid_namespace *Model1_ns);
extern struct Model1_pid *Model1_find_vpid(int Model1_nr);

/*
 * Lookup a PID in the hash table, and return with it's count elevated.
 */
extern struct Model1_pid *Model1_find_get_pid(int Model1_nr);
extern struct Model1_pid *Model1_find_ge_pid(int Model1_nr, struct Model1_pid_namespace *);
int Model1_next_pidmap(struct Model1_pid_namespace *Model1_pid_ns, unsigned int Model1_last);

extern struct Model1_pid *Model1_alloc_pid(struct Model1_pid_namespace *Model1_ns);
extern void Model1_free_pid(struct Model1_pid *Model1_pid);
extern void Model1_disable_pid_allocation(struct Model1_pid_namespace *Model1_ns);

/*
 * ns_of_pid() returns the pid namespace in which the specified pid was
 * allocated.
 *
 * NOTE:
 * 	ns_of_pid() is expected to be called for a process (task) that has
 * 	an attached 'struct pid' (see attach_pid(), detach_pid()) i.e @pid
 * 	is expected to be non-NULL. If @pid is NULL, caller should handle
 * 	the resulting NULL pid-ns.
 */
static inline __attribute__((no_instrument_function)) struct Model1_pid_namespace *Model1_ns_of_pid(struct Model1_pid *Model1_pid)
{
 struct Model1_pid_namespace *Model1_ns = ((void *)0);
 if (Model1_pid)
  Model1_ns = Model1_pid->Model1_numbers[Model1_pid->Model1_level].Model1_ns;
 return Model1_ns;
}

/*
 * is_child_reaper returns true if the pid is the init process
 * of the current namespace. As this one could be checked before
 * pid_ns->child_reaper is assigned in copy_process, we check
 * with the pid number.
 */
static inline __attribute__((no_instrument_function)) bool Model1_is_child_reaper(struct Model1_pid *Model1_pid)
{
 return Model1_pid->Model1_numbers[Model1_pid->Model1_level].Model1_nr == 1;
}

/*
 * the helpers to get the pid's id seen from different namespaces
 *
 * pid_nr()    : global id, i.e. the id seen from the init namespace;
 * pid_vnr()   : virtual id, i.e. the id seen from the pid namespace of
 *               current.
 * pid_nr_ns() : id seen from the ns specified.
 *
 * see also task_xid_nr() etc in include/linux/sched.h
 */

static inline __attribute__((no_instrument_function)) Model1_pid_t Model1_pid_nr(struct Model1_pid *Model1_pid)
{
 Model1_pid_t Model1_nr = 0;
 if (Model1_pid)
  Model1_nr = Model1_pid->Model1_numbers[0].Model1_nr;
 return Model1_nr;
}

Model1_pid_t Model1_pid_nr_ns(struct Model1_pid *Model1_pid, struct Model1_pid_namespace *Model1_ns);
Model1_pid_t Model1_pid_vnr(struct Model1_pid *Model1_pid);







   /*
			 * Both old and new leaders may be attached to
			 * the same pid in the middle of de_thread().
			 */












/* Valid values for seccomp.mode and prctl(PR_SET_SECCOMP, <mode>) */




/* Valid operations for seccomp syscall. */



/* Valid flags for SECCOMP_SET_MODE_FILTER */


/*
 * All BPF programs must return a 32-bit value.
 * The bottom 16-bits are for optional return data.
 * The upper 16-bits are ordered from least permissive values to most.
 *
 * The ordering ensures that a min_t() over composed return values always
 * selects the least permissive choice.
 */






/* Masks for the return value sections. */



/**
 * struct seccomp_data - the format the BPF program executes over.
 * @nr: the system call number
 * @arch: indicates system call convention as an AUDIT_ARCH_* value
 *        as defined in <linux/audit.h>.
 * @instruction_pointer: at the time of the system call.
 * @args: up to 6 system call arguments always stored as 64-bit values
 *        regardless of the architecture.
 */
struct Model1_seccomp_data {
 int Model1_nr;
 __u32 Model1_arch;
 __u64 Model1_instruction_pointer;
 __u64 Model1_args[6];
};















/* x32 syscall flag bit */









/*
 * This file contains the system call numbers of the ia32 compat ABI,
 * this is for the kernel only.
 */








/*
 * include/asm-generic/seccomp.h
 *
 * Copyright (C) 2014 Linaro Limited
 * Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2 as
 * published by the Free Software Foundation.
 */







/*
 * Include machine specific syscall numbers
 */
static inline __attribute__((no_instrument_function)) const int *Model1_get_compat_mode1_syscalls(void)
{
 static const int Model1_mode1_syscalls_32[] = {
  3, 4,
  1, 119,
  0, /* null terminated */
 };
 return Model1_mode1_syscalls_32;
}

struct Model1_seccomp_filter;
/**
 * struct seccomp - the state of a seccomp'ed process
 *
 * @mode:  indicates one of the valid values above for controlled
 *         system calls available to a process.
 * @filter: must always point to a valid seccomp-filter or NULL as it is
 *          accessed without locking during system call entry.
 *
 *          @filter must only be accessed from the context of current as there
 *          is no read locking.
 */
struct Model1_seccomp {
 int Model1_mode;
 struct Model1_seccomp_filter *Model1_filter;
};


extern int Model1___secure_computing(const struct Model1_seccomp_data *Model1_sd);
static inline __attribute__((no_instrument_function)) int Model1_secure_computing(const struct Model1_seccomp_data *Model1_sd)
{
 if (__builtin_expect(!!(Model1_test_ti_thread_flag(Model1_current_thread_info(), 8)), 0))
  return Model1___secure_computing(Model1_sd);
 return 0;
}




extern long Model1_prctl_get_seccomp(void);
extern long Model1_prctl_set_seccomp(unsigned long, char *);

static inline __attribute__((no_instrument_function)) int Model1_seccomp_mode(struct Model1_seccomp *Model1_s)
{
 return Model1_s->Model1_mode;
}
extern void Model1_put_seccomp_filter(struct Model1_task_struct *Model1_tsk);
extern void Model1_get_seccomp_filter(struct Model1_task_struct *Model1_tsk);
static inline __attribute__((no_instrument_function)) long Model1_seccomp_get_filter(struct Model1_task_struct *Model1_task,
          unsigned long Model1_n, void *Model1_data)
{
 return -22;
}






/*
 * RCU-protected list version
 */



/*
 * Why is there no list_empty_rcu()?  Because list_empty() serves this
 * purpose.  The list_empty() function fetches the RCU-protected pointer
 * and compares it to the address of the list head, but neither dereferences
 * this pointer itself nor provides this pointer to the caller.  Therefore,
 * it is not necessary to use rcu_dereference(), so that list_empty() can
 * be used anywhere you would want to use a list_empty_rcu().
 */

/*
 * INIT_LIST_HEAD_RCU - Initialize a list_head visible to RCU readers
 * @list: list to be initialized
 *
 * You should instead use INIT_LIST_HEAD() for normal initialization and
 * cleanup tasks, when readers have no access to the list being initialized.
 * However, if the list being initialized is visible to readers, you
 * need to keep the compiler from being too mischievous.
 */
static inline __attribute__((no_instrument_function)) void Model1_INIT_LIST_HEAD_RCU(struct Model1_list_head *Model1_list)
{
 ({ union { typeof(Model1_list->Model1_next) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(Model1_list->Model1_next)) (Model1_list) }; Model1___write_once_size(&(Model1_list->Model1_next), Model1___u.Model1___c, sizeof(Model1_list->Model1_next)); Model1___u.Model1___val; });
 ({ union { typeof(Model1_list->Model1_prev) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(Model1_list->Model1_prev)) (Model1_list) }; Model1___write_once_size(&(Model1_list->Model1_prev), Model1___u.Model1___c, sizeof(Model1_list->Model1_prev)); Model1___u.Model1___val; });
}

/*
 * return the ->next pointer of a list_head in an rcu safe
 * way, we must not access it directly
 */


/*
 * Insert a new entry between two known consecutive entries.
 *
 * This is only for internal list manipulation where we know
 * the prev/next entries already!
 */

static inline __attribute__((no_instrument_function)) void Model1___list_add_rcu(struct Model1_list_head *Model1_new,
  struct Model1_list_head *Model1_prev, struct Model1_list_head *Model1_next)
{
 Model1_new->Model1_next = Model1_next;
 Model1_new->Model1_prev = Model1_prev;
 ({ Model1_uintptr_t Model1__r_a_p__v = (Model1_uintptr_t)(Model1_new); if (__builtin_constant_p(Model1_new) && (Model1__r_a_p__v) == (Model1_uintptr_t)((void *)0)) ({ union { typeof(((*((struct Model1_list_head **)(&(Model1_prev)->Model1_next))))) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(((*((struct Model1_list_head **)(&(Model1_prev)->Model1_next)))))) ((typeof((*((struct Model1_list_head **)(&(Model1_prev)->Model1_next)))))(Model1__r_a_p__v)) }; Model1___write_once_size(&(((*((struct Model1_list_head **)(&(Model1_prev)->Model1_next))))), Model1___u.Model1___c, sizeof(((*((struct Model1_list_head **)(&(Model1_prev)->Model1_next)))))); Model1___u.Model1___val; }); else do { do { bool Model1___cond = !((sizeof(*&(*((struct Model1_list_head **)(&(Model1_prev)->Model1_next)))) == sizeof(char) || sizeof(*&(*((struct Model1_list_head **)(&(Model1_prev)->Model1_next)))) == sizeof(short) || sizeof(*&(*((struct Model1_list_head **)(&(Model1_prev)->Model1_next)))) == sizeof(int) || sizeof(*&(*((struct Model1_list_head **)(&(Model1_prev)->Model1_next)))) == sizeof(long))); extern void Model1___compiletime_assert_54(void) ; if (Model1___cond) Model1___compiletime_assert_54(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&(*((struct Model1_list_head **)(&(Model1_prev)->Model1_next)))) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(*&(*((struct Model1_list_head **)(&(Model1_prev)->Model1_next))))) ((typeof(*((typeof((*((struct Model1_list_head **)(&(Model1_prev)->Model1_next)))))Model1__r_a_p__v)) *)((typeof((*((struct Model1_list_head **)(&(Model1_prev)->Model1_next)))))Model1__r_a_p__v)) }; Model1___write_once_size(&(*&(*((struct Model1_list_head **)(&(Model1_prev)->Model1_next)))), Model1___u.Model1___c, sizeof(*&(*((struct Model1_list_head **)(&(Model1_prev)->Model1_next))))); Model1___u.Model1___val; }); } while (0); Model1__r_a_p__v; });
 Model1_next->Model1_prev = Model1_new;
}





/**
 * list_add_rcu - add a new entry to rcu-protected list
 * @new: new entry to be added
 * @head: list head to add it after
 *
 * Insert a new entry after the specified head.
 * This is good for implementing stacks.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as list_add_rcu()
 * or list_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * list_for_each_entry_rcu().
 */
static inline __attribute__((no_instrument_function)) void Model1_list_add_rcu(struct Model1_list_head *Model1_new, struct Model1_list_head *Model1_head)
{
 Model1___list_add_rcu(Model1_new, Model1_head, Model1_head->Model1_next);
}

/**
 * list_add_tail_rcu - add a new entry to rcu-protected list
 * @new: new entry to be added
 * @head: list head to add it before
 *
 * Insert a new entry before the specified head.
 * This is useful for implementing queues.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as list_add_tail_rcu()
 * or list_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * list_for_each_entry_rcu().
 */
static inline __attribute__((no_instrument_function)) void Model1_list_add_tail_rcu(struct Model1_list_head *Model1_new,
     struct Model1_list_head *Model1_head)
{
 Model1___list_add_rcu(Model1_new, Model1_head->Model1_prev, Model1_head);
}

/**
 * list_del_rcu - deletes entry from list without re-initialization
 * @entry: the element to delete from the list.
 *
 * Note: list_empty() on entry does not return true after this,
 * the entry is in an undefined state. It is useful for RCU based
 * lockfree traversal.
 *
 * In particular, it means that we can not poison the forward
 * pointers that may still be used for walking the list.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as list_del_rcu()
 * or list_add_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * list_for_each_entry_rcu().
 *
 * Note that the caller is not permitted to immediately free
 * the newly deleted entry.  Instead, either synchronize_rcu()
 * or call_rcu() must be used to defer freeing until an RCU
 * grace period has elapsed.
 */
static inline __attribute__((no_instrument_function)) void Model1_list_del_rcu(struct Model1_list_head *Model1_entry)
{
 Model1___list_del_entry(Model1_entry);
 Model1_entry->Model1_prev = ((void *) 0x200 + (0xdead000000000000UL));
}

/**
 * hlist_del_init_rcu - deletes entry from hash list with re-initialization
 * @n: the element to delete from the hash list.
 *
 * Note: list_unhashed() on the node return true after this. It is
 * useful for RCU based read lockfree traversal if the writer side
 * must know if the list entry is still hashed or already unhashed.
 *
 * In particular, it means that we can not poison the forward pointers
 * that may still be used for walking the hash list and we can only
 * zero the pprev pointer so list_unhashed() will return true after
 * this.
 *
 * The caller must take whatever precautions are necessary (such as
 * holding appropriate locks) to avoid racing with another
 * list-mutation primitive, such as hlist_add_head_rcu() or
 * hlist_del_rcu(), running on this same list.  However, it is
 * perfectly legal to run concurrently with the _rcu list-traversal
 * primitives, such as hlist_for_each_entry_rcu().
 */
static inline __attribute__((no_instrument_function)) void Model1_hlist_del_init_rcu(struct Model1_hlist_node *Model1_n)
{
 if (!Model1_hlist_unhashed(Model1_n)) {
  Model1___hlist_del(Model1_n);
  Model1_n->Model1_pprev = ((void *)0);
 }
}

/**
 * list_replace_rcu - replace old entry by new one
 * @old : the element to be replaced
 * @new : the new element to insert
 *
 * The @old entry will be replaced with the @new entry atomically.
 * Note: @old should not be empty.
 */
static inline __attribute__((no_instrument_function)) void Model1_list_replace_rcu(struct Model1_list_head *old,
    struct Model1_list_head *Model1_new)
{
 Model1_new->Model1_next = old->Model1_next;
 Model1_new->Model1_prev = old->Model1_prev;
 ({ Model1_uintptr_t Model1__r_a_p__v = (Model1_uintptr_t)(Model1_new); if (__builtin_constant_p(Model1_new) && (Model1__r_a_p__v) == (Model1_uintptr_t)((void *)0)) ({ union { typeof(((*((struct Model1_list_head **)(&(Model1_new->Model1_prev)->Model1_next))))) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(((*((struct Model1_list_head **)(&(Model1_new->Model1_prev)->Model1_next)))))) ((typeof((*((struct Model1_list_head **)(&(Model1_new->Model1_prev)->Model1_next)))))(Model1__r_a_p__v)) }; Model1___write_once_size(&(((*((struct Model1_list_head **)(&(Model1_new->Model1_prev)->Model1_next))))), Model1___u.Model1___c, sizeof(((*((struct Model1_list_head **)(&(Model1_new->Model1_prev)->Model1_next)))))); Model1___u.Model1___val; }); else do { do { bool Model1___cond = !((sizeof(*&(*((struct Model1_list_head **)(&(Model1_new->Model1_prev)->Model1_next)))) == sizeof(char) || sizeof(*&(*((struct Model1_list_head **)(&(Model1_new->Model1_prev)->Model1_next)))) == sizeof(short) || sizeof(*&(*((struct Model1_list_head **)(&(Model1_new->Model1_prev)->Model1_next)))) == sizeof(int) || sizeof(*&(*((struct Model1_list_head **)(&(Model1_new->Model1_prev)->Model1_next)))) == sizeof(long))); extern void Model1___compiletime_assert_176(void) ; if (Model1___cond) Model1___compiletime_assert_176(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&(*((struct Model1_list_head **)(&(Model1_new->Model1_prev)->Model1_next)))) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(*&(*((struct Model1_list_head **)(&(Model1_new->Model1_prev)->Model1_next))))) ((typeof(*((typeof((*((struct Model1_list_head **)(&(Model1_new->Model1_prev)->Model1_next)))))Model1__r_a_p__v)) *)((typeof((*((struct Model1_list_head **)(&(Model1_new->Model1_prev)->Model1_next)))))Model1__r_a_p__v)) }; Model1___write_once_size(&(*&(*((struct Model1_list_head **)(&(Model1_new->Model1_prev)->Model1_next)))), Model1___u.Model1___c, sizeof(*&(*((struct Model1_list_head **)(&(Model1_new->Model1_prev)->Model1_next))))); Model1___u.Model1___val; }); } while (0); Model1__r_a_p__v; });
 Model1_new->Model1_next->Model1_prev = Model1_new;
 old->Model1_prev = ((void *) 0x200 + (0xdead000000000000UL));
}

/**
 * __list_splice_init_rcu - join an RCU-protected list into an existing list.
 * @list:	the RCU-protected list to splice
 * @prev:	points to the last element of the existing list
 * @next:	points to the first element of the existing list
 * @sync:	function to sync: synchronize_rcu(), synchronize_sched(), ...
 *
 * The list pointed to by @prev and @next can be RCU-read traversed
 * concurrently with this function.
 *
 * Note that this function blocks.
 *
 * Important note: the caller must take whatever action is necessary to prevent
 * any other updates to the existing list.  In principle, it is possible to
 * modify the list as soon as sync() begins execution. If this sort of thing
 * becomes necessary, an alternative version based on call_rcu() could be
 * created.  But only if -really- needed -- there is no shortage of RCU API
 * members.
 */
static inline __attribute__((no_instrument_function)) void Model1___list_splice_init_rcu(struct Model1_list_head *Model1_list,
       struct Model1_list_head *Model1_prev,
       struct Model1_list_head *Model1_next,
       void (*Model1_sync)(void))
{
 struct Model1_list_head *Model1_first = Model1_list->Model1_next;
 struct Model1_list_head *Model1_last = Model1_list->Model1_prev;

 /*
	 * "first" and "last" tracking list, so initialize it.  RCU readers
	 * have access to this list, so we must use INIT_LIST_HEAD_RCU()
	 * instead of INIT_LIST_HEAD().
	 */

 Model1_INIT_LIST_HEAD_RCU(Model1_list);

 /*
	 * At this point, the list body still points to the source list.
	 * Wait for any readers to finish using the list before splicing
	 * the list body into the new list.  Any new readers will see
	 * an empty list.
	 */

 Model1_sync();

 /*
	 * Readers are finished with the source list, so perform splice.
	 * The order is important if the new list is global and accessible
	 * to concurrent RCU readers.  Note that RCU readers are not
	 * permitted to traverse the prev pointers without excluding
	 * this function.
	 */

 Model1_last->Model1_next = Model1_next;
 ({ Model1_uintptr_t Model1__r_a_p__v = (Model1_uintptr_t)(Model1_first); if (__builtin_constant_p(Model1_first) && (Model1__r_a_p__v) == (Model1_uintptr_t)((void *)0)) ({ union { typeof(((*((struct Model1_list_head **)(&(Model1_prev)->Model1_next))))) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(((*((struct Model1_list_head **)(&(Model1_prev)->Model1_next)))))) ((typeof((*((struct Model1_list_head **)(&(Model1_prev)->Model1_next)))))(Model1__r_a_p__v)) }; Model1___write_once_size(&(((*((struct Model1_list_head **)(&(Model1_prev)->Model1_next))))), Model1___u.Model1___c, sizeof(((*((struct Model1_list_head **)(&(Model1_prev)->Model1_next)))))); Model1___u.Model1___val; }); else do { do { bool Model1___cond = !((sizeof(*&(*((struct Model1_list_head **)(&(Model1_prev)->Model1_next)))) == sizeof(char) || sizeof(*&(*((struct Model1_list_head **)(&(Model1_prev)->Model1_next)))) == sizeof(short) || sizeof(*&(*((struct Model1_list_head **)(&(Model1_prev)->Model1_next)))) == sizeof(int) || sizeof(*&(*((struct Model1_list_head **)(&(Model1_prev)->Model1_next)))) == sizeof(long))); extern void Model1___compiletime_assert_234(void) ; if (Model1___cond) Model1___compiletime_assert_234(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&(*((struct Model1_list_head **)(&(Model1_prev)->Model1_next)))) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(*&(*((struct Model1_list_head **)(&(Model1_prev)->Model1_next))))) ((typeof(*((typeof((*((struct Model1_list_head **)(&(Model1_prev)->Model1_next)))))Model1__r_a_p__v)) *)((typeof((*((struct Model1_list_head **)(&(Model1_prev)->Model1_next)))))Model1__r_a_p__v)) }; Model1___write_once_size(&(*&(*((struct Model1_list_head **)(&(Model1_prev)->Model1_next)))), Model1___u.Model1___c, sizeof(*&(*((struct Model1_list_head **)(&(Model1_prev)->Model1_next))))); Model1___u.Model1___val; }); } while (0); Model1__r_a_p__v; });
 Model1_first->Model1_prev = Model1_prev;
 Model1_next->Model1_prev = Model1_last;
}

/**
 * list_splice_init_rcu - splice an RCU-protected list into an existing list,
 *                        designed for stacks.
 * @list:	the RCU-protected list to splice
 * @head:	the place in the existing list to splice the first list into
 * @sync:	function to sync: synchronize_rcu(), synchronize_sched(), ...
 */
static inline __attribute__((no_instrument_function)) void Model1_list_splice_init_rcu(struct Model1_list_head *Model1_list,
     struct Model1_list_head *Model1_head,
     void (*Model1_sync)(void))
{
 if (!Model1_list_empty(Model1_list))
  Model1___list_splice_init_rcu(Model1_list, Model1_head, Model1_head->Model1_next, Model1_sync);
}

/**
 * list_splice_tail_init_rcu - splice an RCU-protected list into an existing
 *                             list, designed for queues.
 * @list:	the RCU-protected list to splice
 * @head:	the place in the existing list to splice the first list into
 * @sync:	function to sync: synchronize_rcu(), synchronize_sched(), ...
 */
static inline __attribute__((no_instrument_function)) void Model1_list_splice_tail_init_rcu(struct Model1_list_head *Model1_list,
          struct Model1_list_head *Model1_head,
          void (*Model1_sync)(void))
{
 if (!Model1_list_empty(Model1_list))
  Model1___list_splice_init_rcu(Model1_list, Model1_head->Model1_prev, Model1_head, Model1_sync);
}

/**
 * list_entry_rcu - get the struct for this entry
 * @ptr:        the &struct list_head pointer.
 * @type:       the type of the struct this is embedded in.
 * @member:     the name of the list_head within the struct.
 *
 * This primitive may safely run concurrently with the _rcu list-mutation
 * primitives such as list_add_rcu() as long as it's guarded by rcu_read_lock().
 */



/**
 * Where are list_empty_rcu() and list_first_entry_rcu()?
 *
 * Implementing those functions following their counterparts list_empty() and
 * list_first_entry() is not advisable because they lead to subtle race
 * conditions as the following snippet shows:
 *
 * if (!list_empty_rcu(mylist)) {
 *	struct foo *bar = list_first_entry_rcu(mylist, struct foo, list_member);
 *	do_something(bar);
 * }
 *
 * The list may not be empty when list_empty_rcu checks it, but it may be when
 * list_first_entry_rcu rereads the ->next pointer.
 *
 * Rereading the ->next pointer is not a problem for list_empty() and
 * list_first_entry() because they would be protected by a lock that blocks
 * writers.
 *
 * See list_first_or_null_rcu for an alternative.
 */

/**
 * list_first_or_null_rcu - get the first element from a list
 * @ptr:        the list head to take the element from.
 * @type:       the type of the struct this is embedded in.
 * @member:     the name of the list_head within the struct.
 *
 * Note that if the list is empty, it returns NULL.
 *
 * This primitive may safely run concurrently with the _rcu list-mutation
 * primitives such as list_add_rcu() as long as it's guarded by rcu_read_lock().
 */







/**
 * list_next_or_null_rcu - get the first element from a list
 * @head:	the head for the list.
 * @ptr:        the list head to take the next element from.
 * @type:       the type of the struct this is embedded in.
 * @member:     the name of the list_head within the struct.
 *
 * Note that if the ptr is at the end of the list, NULL is returned.
 *
 * This primitive may safely run concurrently with the _rcu list-mutation
 * primitives such as list_add_rcu() as long as it's guarded by rcu_read_lock().
 */
/**
 * list_for_each_entry_rcu	-	iterate over rcu list of given type
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 *
 * This list-traversal primitive may safely run concurrently with
 * the _rcu list-mutation primitives such as list_add_rcu()
 * as long as the traversal is guarded by rcu_read_lock().
 */





/**
 * list_entry_lockless - get the struct for this entry
 * @ptr:        the &struct list_head pointer.
 * @type:       the type of the struct this is embedded in.
 * @member:     the name of the list_head within the struct.
 *
 * This primitive may safely run concurrently with the _rcu list-mutation
 * primitives such as list_add_rcu(), but requires some implicit RCU
 * read-side guarding.  One example is running within a special
 * exception-time environment where preemption is disabled and where
 * lockdep cannot be invoked (in which case updaters must use RCU-sched,
 * as in synchronize_sched(), call_rcu_sched(), and friends).  Another
 * example is when items are added to the list, but never deleted.
 */



/**
 * list_for_each_entry_lockless - iterate over rcu list of given type
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_struct within the struct.
 *
 * This primitive may safely run concurrently with the _rcu list-mutation
 * primitives such as list_add_rcu(), but requires some implicit RCU
 * read-side guarding.  One example is running within a special
 * exception-time environment where preemption is disabled and where
 * lockdep cannot be invoked (in which case updaters must use RCU-sched,
 * as in synchronize_sched(), call_rcu_sched(), and friends).  Another
 * example is when items are added to the list, but never deleted.
 */





/**
 * list_for_each_entry_continue_rcu - continue iteration over list of given type
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_head within the struct.
 *
 * Continue to iterate over list of given type, continuing after
 * the current position.
 */





/**
 * hlist_del_rcu - deletes entry from hash list without re-initialization
 * @n: the element to delete from the hash list.
 *
 * Note: list_unhashed() on entry does not return true after this,
 * the entry is in an undefined state. It is useful for RCU based
 * lockfree traversal.
 *
 * In particular, it means that we can not poison the forward
 * pointers that may still be used for walking the hash list.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as hlist_add_head_rcu()
 * or hlist_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * hlist_for_each_entry().
 */
static inline __attribute__((no_instrument_function)) void Model1_hlist_del_rcu(struct Model1_hlist_node *Model1_n)
{
 Model1___hlist_del(Model1_n);
 Model1_n->Model1_pprev = ((void *) 0x200 + (0xdead000000000000UL));
}

/**
 * hlist_replace_rcu - replace old entry by new one
 * @old : the element to be replaced
 * @new : the new element to insert
 *
 * The @old entry will be replaced with the @new entry atomically.
 */
static inline __attribute__((no_instrument_function)) void Model1_hlist_replace_rcu(struct Model1_hlist_node *old,
     struct Model1_hlist_node *Model1_new)
{
 struct Model1_hlist_node *Model1_next = old->Model1_next;

 Model1_new->Model1_next = Model1_next;
 Model1_new->Model1_pprev = old->Model1_pprev;
 ({ Model1_uintptr_t Model1__r_a_p__v = (Model1_uintptr_t)(Model1_new); if (__builtin_constant_p(Model1_new) && (Model1__r_a_p__v) == (Model1_uintptr_t)((void *)0)) ({ union { typeof((*(struct Model1_hlist_node **)Model1_new->Model1_pprev)) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof((*(struct Model1_hlist_node **)Model1_new->Model1_pprev))) ((typeof(*(struct Model1_hlist_node **)Model1_new->Model1_pprev))(Model1__r_a_p__v)) }; Model1___write_once_size(&((*(struct Model1_hlist_node **)Model1_new->Model1_pprev)), Model1___u.Model1___c, sizeof((*(struct Model1_hlist_node **)Model1_new->Model1_pprev))); Model1___u.Model1___val; }); else do { do { bool Model1___cond = !((sizeof(*&*(struct Model1_hlist_node **)Model1_new->Model1_pprev) == sizeof(char) || sizeof(*&*(struct Model1_hlist_node **)Model1_new->Model1_pprev) == sizeof(short) || sizeof(*&*(struct Model1_hlist_node **)Model1_new->Model1_pprev) == sizeof(int) || sizeof(*&*(struct Model1_hlist_node **)Model1_new->Model1_pprev) == sizeof(long))); extern void Model1___compiletime_assert_446(void) ; if (Model1___cond) Model1___compiletime_assert_446(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&*(struct Model1_hlist_node **)Model1_new->Model1_pprev) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(*&*(struct Model1_hlist_node **)Model1_new->Model1_pprev)) ((typeof(*((typeof(*(struct Model1_hlist_node **)Model1_new->Model1_pprev))Model1__r_a_p__v)) *)((typeof(*(struct Model1_hlist_node **)Model1_new->Model1_pprev))Model1__r_a_p__v)) }; Model1___write_once_size(&(*&*(struct Model1_hlist_node **)Model1_new->Model1_pprev), Model1___u.Model1___c, sizeof(*&*(struct Model1_hlist_node **)Model1_new->Model1_pprev)); Model1___u.Model1___val; }); } while (0); Model1__r_a_p__v; });
 if (Model1_next)
  Model1_new->Model1_next->Model1_pprev = &Model1_new->Model1_next;
 old->Model1_pprev = ((void *) 0x200 + (0xdead000000000000UL));
}

/*
 * return the first or the next element in an RCU protected hlist
 */




/**
 * hlist_add_head_rcu
 * @n: the element to add to the hash list.
 * @h: the list to add to.
 *
 * Description:
 * Adds the specified element to the specified hlist,
 * while permitting racing traversals.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as hlist_add_head_rcu()
 * or hlist_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * hlist_for_each_entry_rcu(), used to prevent memory-consistency
 * problems on Alpha CPUs.  Regardless of the type of CPU, the
 * list-traversal primitive must be guarded by rcu_read_lock().
 */
static inline __attribute__((no_instrument_function)) void Model1_hlist_add_head_rcu(struct Model1_hlist_node *Model1_n,
     struct Model1_hlist_head *Model1_h)
{
 struct Model1_hlist_node *Model1_first = Model1_h->Model1_first;

 Model1_n->Model1_next = Model1_first;
 Model1_n->Model1_pprev = &Model1_h->Model1_first;
 ({ Model1_uintptr_t Model1__r_a_p__v = (Model1_uintptr_t)(Model1_n); if (__builtin_constant_p(Model1_n) && (Model1__r_a_p__v) == (Model1_uintptr_t)((void *)0)) ({ union { typeof(((*((struct Model1_hlist_node **)(&(Model1_h)->Model1_first))))) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(((*((struct Model1_hlist_node **)(&(Model1_h)->Model1_first)))))) ((typeof((*((struct Model1_hlist_node **)(&(Model1_h)->Model1_first)))))(Model1__r_a_p__v)) }; Model1___write_once_size(&(((*((struct Model1_hlist_node **)(&(Model1_h)->Model1_first))))), Model1___u.Model1___c, sizeof(((*((struct Model1_hlist_node **)(&(Model1_h)->Model1_first)))))); Model1___u.Model1___val; }); else do { do { bool Model1___cond = !((sizeof(*&(*((struct Model1_hlist_node **)(&(Model1_h)->Model1_first)))) == sizeof(char) || sizeof(*&(*((struct Model1_hlist_node **)(&(Model1_h)->Model1_first)))) == sizeof(short) || sizeof(*&(*((struct Model1_hlist_node **)(&(Model1_h)->Model1_first)))) == sizeof(int) || sizeof(*&(*((struct Model1_hlist_node **)(&(Model1_h)->Model1_first)))) == sizeof(long))); extern void Model1___compiletime_assert_485(void) ; if (Model1___cond) Model1___compiletime_assert_485(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&(*((struct Model1_hlist_node **)(&(Model1_h)->Model1_first)))) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(*&(*((struct Model1_hlist_node **)(&(Model1_h)->Model1_first))))) ((typeof(*((typeof((*((struct Model1_hlist_node **)(&(Model1_h)->Model1_first)))))Model1__r_a_p__v)) *)((typeof((*((struct Model1_hlist_node **)(&(Model1_h)->Model1_first)))))Model1__r_a_p__v)) }; Model1___write_once_size(&(*&(*((struct Model1_hlist_node **)(&(Model1_h)->Model1_first)))), Model1___u.Model1___c, sizeof(*&(*((struct Model1_hlist_node **)(&(Model1_h)->Model1_first))))); Model1___u.Model1___val; }); } while (0); Model1__r_a_p__v; });
 if (Model1_first)
  Model1_first->Model1_pprev = &Model1_n->Model1_next;
}

/**
 * hlist_add_tail_rcu
 * @n: the element to add to the hash list.
 * @h: the list to add to.
 *
 * Description:
 * Adds the specified element to the specified hlist,
 * while permitting racing traversals.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as hlist_add_head_rcu()
 * or hlist_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * hlist_for_each_entry_rcu(), used to prevent memory-consistency
 * problems on Alpha CPUs.  Regardless of the type of CPU, the
 * list-traversal primitive must be guarded by rcu_read_lock().
 */
static inline __attribute__((no_instrument_function)) void Model1_hlist_add_tail_rcu(struct Model1_hlist_node *Model1_n,
          struct Model1_hlist_head *Model1_h)
{
 struct Model1_hlist_node *Model1_i, *Model1_last = ((void *)0);

 for (Model1_i = (*((struct Model1_hlist_node **)(&(Model1_h)->Model1_first))); Model1_i; Model1_i = (*((struct Model1_hlist_node **)(&(Model1_i)->Model1_next))))
  Model1_last = Model1_i;

 if (Model1_last) {
  Model1_n->Model1_next = Model1_last->Model1_next;
  Model1_n->Model1_pprev = &Model1_last->Model1_next;
  ({ Model1_uintptr_t Model1__r_a_p__v = (Model1_uintptr_t)(Model1_n); if (__builtin_constant_p(Model1_n) && (Model1__r_a_p__v) == (Model1_uintptr_t)((void *)0)) ({ union { typeof(((*((struct Model1_hlist_node **)(&(Model1_last)->Model1_next))))) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(((*((struct Model1_hlist_node **)(&(Model1_last)->Model1_next)))))) ((typeof((*((struct Model1_hlist_node **)(&(Model1_last)->Model1_next)))))(Model1__r_a_p__v)) }; Model1___write_once_size(&(((*((struct Model1_hlist_node **)(&(Model1_last)->Model1_next))))), Model1___u.Model1___c, sizeof(((*((struct Model1_hlist_node **)(&(Model1_last)->Model1_next)))))); Model1___u.Model1___val; }); else do { do { bool Model1___cond = !((sizeof(*&(*((struct Model1_hlist_node **)(&(Model1_last)->Model1_next)))) == sizeof(char) || sizeof(*&(*((struct Model1_hlist_node **)(&(Model1_last)->Model1_next)))) == sizeof(short) || sizeof(*&(*((struct Model1_hlist_node **)(&(Model1_last)->Model1_next)))) == sizeof(int) || sizeof(*&(*((struct Model1_hlist_node **)(&(Model1_last)->Model1_next)))) == sizeof(long))); extern void Model1___compiletime_assert_520(void) ; if (Model1___cond) Model1___compiletime_assert_520(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&(*((struct Model1_hlist_node **)(&(Model1_last)->Model1_next)))) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(*&(*((struct Model1_hlist_node **)(&(Model1_last)->Model1_next))))) ((typeof(*((typeof((*((struct Model1_hlist_node **)(&(Model1_last)->Model1_next)))))Model1__r_a_p__v)) *)((typeof((*((struct Model1_hlist_node **)(&(Model1_last)->Model1_next)))))Model1__r_a_p__v)) }; Model1___write_once_size(&(*&(*((struct Model1_hlist_node **)(&(Model1_last)->Model1_next)))), Model1___u.Model1___c, sizeof(*&(*((struct Model1_hlist_node **)(&(Model1_last)->Model1_next))))); Model1___u.Model1___val; }); } while (0); Model1__r_a_p__v; });
 } else {
  Model1_hlist_add_head_rcu(Model1_n, Model1_h);
 }
}

/**
 * hlist_add_before_rcu
 * @n: the new element to add to the hash list.
 * @next: the existing element to add the new element before.
 *
 * Description:
 * Adds the specified element to the specified hlist
 * before the specified node while permitting racing traversals.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as hlist_add_head_rcu()
 * or hlist_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * hlist_for_each_entry_rcu(), used to prevent memory-consistency
 * problems on Alpha CPUs.
 */
static inline __attribute__((no_instrument_function)) void Model1_hlist_add_before_rcu(struct Model1_hlist_node *Model1_n,
     struct Model1_hlist_node *Model1_next)
{
 Model1_n->Model1_pprev = Model1_next->Model1_pprev;
 Model1_n->Model1_next = Model1_next;
 ({ Model1_uintptr_t Model1__r_a_p__v = (Model1_uintptr_t)(Model1_n); if (__builtin_constant_p(Model1_n) && (Model1__r_a_p__v) == (Model1_uintptr_t)((void *)0)) ({ union { typeof(((*((struct Model1_hlist_node **)((Model1_n)->Model1_pprev))))) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(((*((struct Model1_hlist_node **)((Model1_n)->Model1_pprev)))))) ((typeof((*((struct Model1_hlist_node **)((Model1_n)->Model1_pprev)))))(Model1__r_a_p__v)) }; Model1___write_once_size(&(((*((struct Model1_hlist_node **)((Model1_n)->Model1_pprev))))), Model1___u.Model1___c, sizeof(((*((struct Model1_hlist_node **)((Model1_n)->Model1_pprev)))))); Model1___u.Model1___val; }); else do { do { bool Model1___cond = !((sizeof(*&(*((struct Model1_hlist_node **)((Model1_n)->Model1_pprev)))) == sizeof(char) || sizeof(*&(*((struct Model1_hlist_node **)((Model1_n)->Model1_pprev)))) == sizeof(short) || sizeof(*&(*((struct Model1_hlist_node **)((Model1_n)->Model1_pprev)))) == sizeof(int) || sizeof(*&(*((struct Model1_hlist_node **)((Model1_n)->Model1_pprev)))) == sizeof(long))); extern void Model1___compiletime_assert_549(void) ; if (Model1___cond) Model1___compiletime_assert_549(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&(*((struct Model1_hlist_node **)((Model1_n)->Model1_pprev)))) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(*&(*((struct Model1_hlist_node **)((Model1_n)->Model1_pprev))))) ((typeof(*((typeof((*((struct Model1_hlist_node **)((Model1_n)->Model1_pprev)))))Model1__r_a_p__v)) *)((typeof((*((struct Model1_hlist_node **)((Model1_n)->Model1_pprev)))))Model1__r_a_p__v)) }; Model1___write_once_size(&(*&(*((struct Model1_hlist_node **)((Model1_n)->Model1_pprev)))), Model1___u.Model1___c, sizeof(*&(*((struct Model1_hlist_node **)((Model1_n)->Model1_pprev))))); Model1___u.Model1___val; }); } while (0); Model1__r_a_p__v; });
 Model1_next->Model1_pprev = &Model1_n->Model1_next;
}

/**
 * hlist_add_behind_rcu
 * @n: the new element to add to the hash list.
 * @prev: the existing element to add the new element after.
 *
 * Description:
 * Adds the specified element to the specified hlist
 * after the specified node while permitting racing traversals.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as hlist_add_head_rcu()
 * or hlist_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * hlist_for_each_entry_rcu(), used to prevent memory-consistency
 * problems on Alpha CPUs.
 */
static inline __attribute__((no_instrument_function)) void Model1_hlist_add_behind_rcu(struct Model1_hlist_node *Model1_n,
     struct Model1_hlist_node *Model1_prev)
{
 Model1_n->Model1_next = Model1_prev->Model1_next;
 Model1_n->Model1_pprev = &Model1_prev->Model1_next;
 ({ Model1_uintptr_t Model1__r_a_p__v = (Model1_uintptr_t)(Model1_n); if (__builtin_constant_p(Model1_n) && (Model1__r_a_p__v) == (Model1_uintptr_t)((void *)0)) ({ union { typeof(((*((struct Model1_hlist_node **)(&(Model1_prev)->Model1_next))))) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(((*((struct Model1_hlist_node **)(&(Model1_prev)->Model1_next)))))) ((typeof((*((struct Model1_hlist_node **)(&(Model1_prev)->Model1_next)))))(Model1__r_a_p__v)) }; Model1___write_once_size(&(((*((struct Model1_hlist_node **)(&(Model1_prev)->Model1_next))))), Model1___u.Model1___c, sizeof(((*((struct Model1_hlist_node **)(&(Model1_prev)->Model1_next)))))); Model1___u.Model1___val; }); else do { do { bool Model1___cond = !((sizeof(*&(*((struct Model1_hlist_node **)(&(Model1_prev)->Model1_next)))) == sizeof(char) || sizeof(*&(*((struct Model1_hlist_node **)(&(Model1_prev)->Model1_next)))) == sizeof(short) || sizeof(*&(*((struct Model1_hlist_node **)(&(Model1_prev)->Model1_next)))) == sizeof(int) || sizeof(*&(*((struct Model1_hlist_node **)(&(Model1_prev)->Model1_next)))) == sizeof(long))); extern void Model1___compiletime_assert_576(void) ; if (Model1___cond) Model1___compiletime_assert_576(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&(*((struct Model1_hlist_node **)(&(Model1_prev)->Model1_next)))) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(*&(*((struct Model1_hlist_node **)(&(Model1_prev)->Model1_next))))) ((typeof(*((typeof((*((struct Model1_hlist_node **)(&(Model1_prev)->Model1_next)))))Model1__r_a_p__v)) *)((typeof((*((struct Model1_hlist_node **)(&(Model1_prev)->Model1_next)))))Model1__r_a_p__v)) }; Model1___write_once_size(&(*&(*((struct Model1_hlist_node **)(&(Model1_prev)->Model1_next)))), Model1___u.Model1___c, sizeof(*&(*((struct Model1_hlist_node **)(&(Model1_prev)->Model1_next))))); Model1___u.Model1___val; }); } while (0); Model1__r_a_p__v; });
 if (Model1_n->Model1_next)
  Model1_n->Model1_next->Model1_pprev = &Model1_n->Model1_next;
}






/**
 * hlist_for_each_entry_rcu - iterate over rcu list of given type
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the hlist_node within the struct.
 *
 * This list-traversal primitive may safely run concurrently with
 * the _rcu list-mutation primitives such as hlist_add_head_rcu()
 * as long as the traversal is guarded by rcu_read_lock().
 */







/**
 * hlist_for_each_entry_rcu_notrace - iterate over rcu list of given type (for tracing)
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the hlist_node within the struct.
 *
 * This list-traversal primitive may safely run concurrently with
 * the _rcu list-mutation primitives such as hlist_add_head_rcu()
 * as long as the traversal is guarded by rcu_read_lock().
 *
 * This is the same as hlist_for_each_entry_rcu() except that it does
 * not do any RCU debugging or tracing.
 */







/**
 * hlist_for_each_entry_rcu_bh - iterate over rcu list of given type
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the hlist_node within the struct.
 *
 * This list-traversal primitive may safely run concurrently with
 * the _rcu list-mutation primitives such as hlist_add_head_rcu()
 * as long as the traversal is guarded by rcu_read_lock().
 */







/**
 * hlist_for_each_entry_continue_rcu - iterate over a hlist continuing after current point
 * @pos:	the type * to use as a loop cursor.
 * @member:	the name of the hlist_node within the struct.
 */







/**
 * hlist_for_each_entry_continue_rcu_bh - iterate over a hlist continuing after current point
 * @pos:	the type * to use as a loop cursor.
 * @member:	the name of the hlist_node within the struct.
 */







/**
 * hlist_for_each_entry_from_rcu - iterate over a hlist continuing from current point
 * @pos:	the type * to use as a loop cursor.
 * @member:	the name of the hlist_node within the struct.
 */
/*
 * RT Mutexes: blocking mutual exclusion locks with PI support
 *
 * started by Ingo Molnar and Thomas Gleixner:
 *
 *  Copyright (C) 2004-2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
 *  Copyright (C) 2006, Timesys Corp., Thomas Gleixner <tglx@timesys.com>
 *
 * This file contains the public data structure and API definitions.
 */
extern int Model1_max_lock_depth; /* for sysctl */

/**
 * The rt_mutex structure
 *
 * @wait_lock:	spinlock to protect the structure
 * @waiters:	rbtree root to enqueue waiters in priority order
 * @waiters_leftmost: top waiter
 * @owner:	the mutex owner
 */
struct Model1_rt_mutex {
 Model1_raw_spinlock_t Model1_wait_lock;
 struct Model1_rb_root Model1_waiters;
 struct Model1_rb_node *Model1_waiters_leftmost;
 struct Model1_task_struct *Model1_owner;






};

struct Model1_rt_mutex_waiter;
struct Model1_hrtimer_sleeper;






 static inline __attribute__((no_instrument_function)) int Model1_rt_mutex_debug_check_no_locks_freed(const void *Model1_from,
             unsigned long Model1_len)
 {
 return 0;
 }
/**
 * rt_mutex_is_locked - is the mutex locked
 * @lock: the mutex to be queried
 *
 * Returns 1 if the mutex is locked, 0 if unlocked.
 */
static inline __attribute__((no_instrument_function)) int Model1_rt_mutex_is_locked(struct Model1_rt_mutex *Model1_lock)
{
 return Model1_lock->Model1_owner != ((void *)0);
}

extern void Model1___rt_mutex_init(struct Model1_rt_mutex *Model1_lock, const char *Model1_name);
extern void Model1_rt_mutex_destroy(struct Model1_rt_mutex *Model1_lock);

extern void Model1_rt_mutex_lock(struct Model1_rt_mutex *Model1_lock);
extern int Model1_rt_mutex_lock_interruptible(struct Model1_rt_mutex *Model1_lock);
extern int Model1_rt_mutex_timed_lock(struct Model1_rt_mutex *Model1_lock,
          struct Model1_hrtimer_sleeper *Model1_timeout);

extern int Model1_rt_mutex_trylock(struct Model1_rt_mutex *Model1_lock);

extern void Model1_rt_mutex_unlock(struct Model1_rt_mutex *Model1_lock);





/*
 *  include/linux/hrtimer.h
 *
 *  hrtimers - High-resolution kernel timers
 *
 *   Copyright(C) 2005, Thomas Gleixner <tglx@linutronix.de>
 *   Copyright(C) 2005, Red Hat, Inc., Ingo Molnar
 *
 *  data type definitions, declarations, prototypes
 *
 *  Started by: Thomas Gleixner and Ingo Molnar
 *
 *  For licencing details see kernel-base/COPYING
 */







struct Model1_timerqueue_node {
 struct Model1_rb_node Model1_node;
 Model1_ktime_t Model1_expires;
};

struct Model1_timerqueue_head {
 struct Model1_rb_root Model1_head;
 struct Model1_timerqueue_node *Model1_next;
};


extern bool Model1_timerqueue_add(struct Model1_timerqueue_head *Model1_head,
      struct Model1_timerqueue_node *Model1_node);
extern bool Model1_timerqueue_del(struct Model1_timerqueue_head *Model1_head,
      struct Model1_timerqueue_node *Model1_node);
extern struct Model1_timerqueue_node *Model1_timerqueue_iterate_next(
      struct Model1_timerqueue_node *Model1_node);

/**
 * timerqueue_getnext - Returns the timer with the earliest expiration time
 *
 * @head: head of timerqueue
 *
 * Returns a pointer to the timer node that has the
 * earliest expiration time.
 */
static inline __attribute__((no_instrument_function))
struct Model1_timerqueue_node *Model1_timerqueue_getnext(struct Model1_timerqueue_head *Model1_head)
{
 return Model1_head->Model1_next;
}

static inline __attribute__((no_instrument_function)) void Model1_timerqueue_init(struct Model1_timerqueue_node *Model1_node)
{
 ((&Model1_node->Model1_node)->Model1___rb_parent_color = (unsigned long)(&Model1_node->Model1_node));
}

static inline __attribute__((no_instrument_function)) void Model1_timerqueue_init_head(struct Model1_timerqueue_head *Model1_head)
{
 Model1_head->Model1_head = (struct Model1_rb_root) { ((void *)0), };
 Model1_head->Model1_next = ((void *)0);
}

struct Model1_hrtimer_clock_base;
struct Model1_hrtimer_cpu_base;

/*
 * Mode arguments of xxx_hrtimer functions:
 */
enum Model1_hrtimer_mode {
 Model1_HRTIMER_MODE_ABS = 0x0, /* Time value is absolute */
 Model1_HRTIMER_MODE_REL = 0x1, /* Time value is relative to now */
 Model1_HRTIMER_MODE_PINNED = 0x02, /* Timer is bound to CPU */
 Model1_HRTIMER_MODE_ABS_PINNED = 0x02,
 Model1_HRTIMER_MODE_REL_PINNED = 0x03,
};

/*
 * Return values for the callback function
 */
enum Model1_hrtimer_restart {
 Model1_HRTIMER_NORESTART, /* Timer is not restarted */
 Model1_HRTIMER_RESTART, /* Timer must be restarted */
};

/*
 * Values to track state of the timer
 *
 * Possible states:
 *
 * 0x00		inactive
 * 0x01		enqueued into rbtree
 *
 * The callback state is not part of the timer->state because clearing it would
 * mean touching the timer after the callback, this makes it impossible to free
 * the timer from the callback function.
 *
 * Therefore we track the callback state in:
 *
 *	timer->base->cpu_base->running == timer
 *
 * On SMP it is possible to have a "callback function running and enqueued"
 * status. It happens for example when a posix timer expired and the callback
 * queued a signal. Between dropping the lock which protects the posix timer
 * and reacquiring the base lock of the hrtimer, another CPU can deliver the
 * signal and rearm the timer.
 *
 * All state transitions are protected by cpu_base->lock.
 */



/**
 * struct hrtimer - the basic hrtimer structure
 * @node:	timerqueue node, which also manages node.expires,
 *		the absolute expiry time in the hrtimers internal
 *		representation. The time is related to the clock on
 *		which the timer is based. Is setup by adding
 *		slack to the _softexpires value. For non range timers
 *		identical to _softexpires.
 * @_softexpires: the absolute earliest expiry time of the hrtimer.
 *		The time which was given as expiry time when the timer
 *		was armed.
 * @function:	timer expiry callback function
 * @base:	pointer to the timer base (per cpu and per clock)
 * @state:	state information (See bit values above)
 * @is_rel:	Set if the timer was armed relative
 * @start_pid:  timer statistics field to store the pid of the task which
 *		started the timer
 * @start_site:	timer statistics field to store the site where the timer
 *		was started
 * @start_comm: timer statistics field to store the name of the process which
 *		started the timer
 *
 * The hrtimer structure must be initialized by hrtimer_init()
 */
struct Model1_hrtimer {
 struct Model1_timerqueue_node Model1_node;
 Model1_ktime_t Model1__softexpires;
 enum Model1_hrtimer_restart (*Model1_function)(struct Model1_hrtimer *);
 struct Model1_hrtimer_clock_base *Model1_base;
 Model1_u8 Model1_state;
 Model1_u8 Model1_is_rel;

 int Model1_start_pid;
 void *Model1_start_site;
 char Model1_start_comm[16];

};

/**
 * struct hrtimer_sleeper - simple sleeper structure
 * @timer:	embedded timer structure
 * @task:	task to wake up
 *
 * task is set to NULL, when the timer expires.
 */
struct Model1_hrtimer_sleeper {
 struct Model1_hrtimer Model1_timer;
 struct Model1_task_struct *Model1_task;
};







/**
 * struct hrtimer_clock_base - the timer base for a specific clock
 * @cpu_base:		per cpu clock base
 * @index:		clock type index for per_cpu support when moving a
 *			timer to a base on another cpu.
 * @clockid:		clock id for per_cpu support
 * @active:		red black tree root node for the active timers
 * @get_time:		function to retrieve the current time of the clock
 * @offset:		offset of this clock to the monotonic base
 */
struct Model1_hrtimer_clock_base {
 struct Model1_hrtimer_cpu_base *Model1_cpu_base;
 int Model1_index;
 Model1_clockid_t Model1_clockid;
 struct Model1_timerqueue_head Model1_active;
 Model1_ktime_t (*Model1_get_time)(void);
 Model1_ktime_t Model1_offset;
} __attribute__((__aligned__(64)));

enum Model1_hrtimer_base_type {
 Model1_HRTIMER_BASE_MONOTONIC,
 Model1_HRTIMER_BASE_REALTIME,
 Model1_HRTIMER_BASE_BOOTTIME,
 Model1_HRTIMER_BASE_TAI,
 Model1_HRTIMER_MAX_CLOCK_BASES,
};

/*
 * struct hrtimer_cpu_base - the per cpu clock bases
 * @lock:		lock protecting the base and associated clock bases
 *			and timers
 * @seq:		seqcount around __run_hrtimer
 * @running:		pointer to the currently running hrtimer
 * @cpu:		cpu number
 * @active_bases:	Bitfield to mark bases with active timers
 * @clock_was_set_seq:	Sequence counter of clock was set events
 * @migration_enabled:	The migration of hrtimers to other cpus is enabled
 * @nohz_active:	The nohz functionality is enabled
 * @expires_next:	absolute time of the next event which was scheduled
 *			via clock_set_next_event()
 * @next_timer:		Pointer to the first expiring timer
 * @in_hrtirq:		hrtimer_interrupt() is currently executing
 * @hres_active:	State of high resolution mode
 * @hang_detected:	The last hrtimer interrupt detected a hang
 * @nr_events:		Total number of hrtimer interrupt events
 * @nr_retries:		Total number of hrtimer interrupt retries
 * @nr_hangs:		Total number of hrtimer interrupt hangs
 * @max_hang_time:	Maximum time spent in hrtimer_interrupt
 * @clock_base:		array of clock bases for this cpu
 *
 * Note: next_timer is just an optimization for __remove_hrtimer().
 *	 Do not dereference the pointer because it is not reliable on
 *	 cross cpu removals.
 */
struct Model1_hrtimer_cpu_base {
 Model1_raw_spinlock_t Model1_lock;
 Model1_seqcount_t Model1_seq;
 struct Model1_hrtimer *Model1_running;
 unsigned int Model1_cpu;
 unsigned int Model1_active_bases;
 unsigned int Model1_clock_was_set_seq;
 bool Model1_migration_enabled;
 bool Model1_nohz_active;

 unsigned int Model1_in_hrtirq : 1,
     Model1_hres_active : 1,
     Model1_hang_detected : 1;
 Model1_ktime_t Model1_expires_next;
 struct Model1_hrtimer *Model1_next_timer;
 unsigned int Model1_nr_events;
 unsigned int Model1_nr_retries;
 unsigned int Model1_nr_hangs;
 unsigned int Model1_max_hang_time;

 struct Model1_hrtimer_clock_base Model1_clock_base[Model1_HRTIMER_MAX_CLOCK_BASES];
} __attribute__((__aligned__((1 << (6)))));

static inline __attribute__((no_instrument_function)) void Model1_hrtimer_set_expires(struct Model1_hrtimer *Model1_timer, Model1_ktime_t Model1_time)
{
 do { bool Model1___cond = !(!(sizeof(struct Model1_hrtimer_clock_base) > 64)); extern void Model1___compiletime_assert_211(void) ; if (Model1___cond) Model1___compiletime_assert_211(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);

 Model1_timer->Model1_node.Model1_expires = Model1_time;
 Model1_timer->Model1__softexpires = Model1_time;
}

static inline __attribute__((no_instrument_function)) void Model1_hrtimer_set_expires_range(struct Model1_hrtimer *Model1_timer, Model1_ktime_t Model1_time, Model1_ktime_t Model1_delta)
{
 Model1_timer->Model1__softexpires = Model1_time;
 Model1_timer->Model1_node.Model1_expires = Model1_ktime_add_safe(Model1_time, Model1_delta);
}

static inline __attribute__((no_instrument_function)) void Model1_hrtimer_set_expires_range_ns(struct Model1_hrtimer *Model1_timer, Model1_ktime_t Model1_time, Model1_u64 Model1_delta)
{
 Model1_timer->Model1__softexpires = Model1_time;
 Model1_timer->Model1_node.Model1_expires = Model1_ktime_add_safe(Model1_time, Model1_ns_to_ktime(Model1_delta));
}

static inline __attribute__((no_instrument_function)) void Model1_hrtimer_set_expires_tv64(struct Model1_hrtimer *Model1_timer, Model1_s64 Model1_tv64)
{
 Model1_timer->Model1_node.Model1_expires.Model1_tv64 = Model1_tv64;
 Model1_timer->Model1__softexpires.Model1_tv64 = Model1_tv64;
}

static inline __attribute__((no_instrument_function)) void Model1_hrtimer_add_expires(struct Model1_hrtimer *Model1_timer, Model1_ktime_t Model1_time)
{
 Model1_timer->Model1_node.Model1_expires = Model1_ktime_add_safe(Model1_timer->Model1_node.Model1_expires, Model1_time);
 Model1_timer->Model1__softexpires = Model1_ktime_add_safe(Model1_timer->Model1__softexpires, Model1_time);
}

static inline __attribute__((no_instrument_function)) void Model1_hrtimer_add_expires_ns(struct Model1_hrtimer *Model1_timer, Model1_u64 Model1_ns)
{
 Model1_timer->Model1_node.Model1_expires = ({ (Model1_ktime_t){ .Model1_tv64 = (Model1_timer->Model1_node.Model1_expires).Model1_tv64 + (Model1_ns) }; });
 Model1_timer->Model1__softexpires = ({ (Model1_ktime_t){ .Model1_tv64 = (Model1_timer->Model1__softexpires).Model1_tv64 + (Model1_ns) }; });
}

static inline __attribute__((no_instrument_function)) Model1_ktime_t Model1_hrtimer_get_expires(const struct Model1_hrtimer *Model1_timer)
{
 return Model1_timer->Model1_node.Model1_expires;
}

static inline __attribute__((no_instrument_function)) Model1_ktime_t Model1_hrtimer_get_softexpires(const struct Model1_hrtimer *Model1_timer)
{
 return Model1_timer->Model1__softexpires;
}

static inline __attribute__((no_instrument_function)) Model1_s64 Model1_hrtimer_get_expires_tv64(const struct Model1_hrtimer *Model1_timer)
{
 return Model1_timer->Model1_node.Model1_expires.Model1_tv64;
}
static inline __attribute__((no_instrument_function)) Model1_s64 Model1_hrtimer_get_softexpires_tv64(const struct Model1_hrtimer *Model1_timer)
{
 return Model1_timer->Model1__softexpires.Model1_tv64;
}

static inline __attribute__((no_instrument_function)) Model1_s64 Model1_hrtimer_get_expires_ns(const struct Model1_hrtimer *Model1_timer)
{
 return ((Model1_timer->Model1_node.Model1_expires).Model1_tv64);
}

static inline __attribute__((no_instrument_function)) Model1_ktime_t Model1_hrtimer_expires_remaining(const struct Model1_hrtimer *Model1_timer)
{
 return ({ (Model1_ktime_t){ .Model1_tv64 = (Model1_timer->Model1_node.Model1_expires).Model1_tv64 - (Model1_timer->Model1_base->Model1_get_time()).Model1_tv64 }; });
}

static inline __attribute__((no_instrument_function)) Model1_ktime_t Model1_hrtimer_cb_get_time(struct Model1_hrtimer *Model1_timer)
{
 return Model1_timer->Model1_base->Model1_get_time();
}


struct Model1_clock_event_device;

extern void Model1_hrtimer_interrupt(struct Model1_clock_event_device *Model1_dev);

static inline __attribute__((no_instrument_function)) int Model1_hrtimer_is_hres_active(struct Model1_hrtimer *Model1_timer)
{
 return Model1_timer->Model1_base->Model1_cpu_base->Model1_hres_active;
}

extern void Model1_hrtimer_peek_ahead_timers(void);

/*
 * The resolution of the clocks. The resolution value is returned in
 * the clock_getres() system call to give application programmers an
 * idea of the (in)accuracy of timers. Timer values are rounded up to
 * this resolution values.
 */





extern void Model1_clock_was_set_delayed(void);

extern unsigned int Model1_hrtimer_resolution;
static inline __attribute__((no_instrument_function)) Model1_ktime_t
Model1___hrtimer_expires_remaining_adjusted(const struct Model1_hrtimer *Model1_timer, Model1_ktime_t Model1_now)
{
 Model1_ktime_t Model1_rem = ({ (Model1_ktime_t){ .Model1_tv64 = (Model1_timer->Model1_node.Model1_expires).Model1_tv64 - (Model1_now).Model1_tv64 }; });

 /*
	 * Adjust relative timers for the extra we added in
	 * hrtimer_start_range_ns() to prevent short timeouts.
	 */
 if (0 && Model1_timer->Model1_is_rel)
  Model1_rem.Model1_tv64 -= Model1_hrtimer_resolution;
 return Model1_rem;
}

static inline __attribute__((no_instrument_function)) Model1_ktime_t
Model1_hrtimer_expires_remaining_adjusted(const struct Model1_hrtimer *Model1_timer)
{
 return Model1___hrtimer_expires_remaining_adjusted(Model1_timer,
          Model1_timer->Model1_base->Model1_get_time());
}

extern void Model1_clock_was_set(void);

extern void Model1_timerfd_clock_was_set(void);



extern void Model1_hrtimers_resume(void);

extern __attribute__((section(".data..percpu" ""))) __typeof__(struct Model1_tick_device) Model1_tick_cpu_device;


/* Exported timer functions: */

/* Initialize timers: */
extern void Model1_hrtimer_init(struct Model1_hrtimer *Model1_timer, Model1_clockid_t Model1_which_clock,
    enum Model1_hrtimer_mode Model1_mode);







static inline __attribute__((no_instrument_function)) void Model1_hrtimer_init_on_stack(struct Model1_hrtimer *Model1_timer,
      Model1_clockid_t Model1_which_clock,
      enum Model1_hrtimer_mode Model1_mode)
{
 Model1_hrtimer_init(Model1_timer, Model1_which_clock, Model1_mode);
}
static inline __attribute__((no_instrument_function)) void Model1_destroy_hrtimer_on_stack(struct Model1_hrtimer *Model1_timer) { }


/* Basic timer operations: */
extern void Model1_hrtimer_start_range_ns(struct Model1_hrtimer *Model1_timer, Model1_ktime_t Model1_tim,
       Model1_u64 Model1_range_ns, const enum Model1_hrtimer_mode Model1_mode);

/**
 * hrtimer_start - (re)start an hrtimer on the current CPU
 * @timer:	the timer to be added
 * @tim:	expiry time
 * @mode:	expiry mode: absolute (HRTIMER_MODE_ABS) or
 *		relative (HRTIMER_MODE_REL)
 */
static inline __attribute__((no_instrument_function)) void Model1_hrtimer_start(struct Model1_hrtimer *Model1_timer, Model1_ktime_t Model1_tim,
     const enum Model1_hrtimer_mode Model1_mode)
{
 Model1_hrtimer_start_range_ns(Model1_timer, Model1_tim, 0, Model1_mode);
}

extern int Model1_hrtimer_cancel(struct Model1_hrtimer *Model1_timer);
extern int Model1_hrtimer_try_to_cancel(struct Model1_hrtimer *Model1_timer);

static inline __attribute__((no_instrument_function)) void Model1_hrtimer_start_expires(struct Model1_hrtimer *Model1_timer,
      enum Model1_hrtimer_mode Model1_mode)
{
 Model1_u64 Model1_delta;
 Model1_ktime_t Model1_soft, Model1_hard;
 Model1_soft = Model1_hrtimer_get_softexpires(Model1_timer);
 Model1_hard = Model1_hrtimer_get_expires(Model1_timer);
 Model1_delta = ((({ (Model1_ktime_t){ .Model1_tv64 = (Model1_hard).Model1_tv64 - (Model1_soft).Model1_tv64 }; })).Model1_tv64);
 Model1_hrtimer_start_range_ns(Model1_timer, Model1_soft, Model1_delta, Model1_mode);
}

static inline __attribute__((no_instrument_function)) void Model1_hrtimer_restart(struct Model1_hrtimer *Model1_timer)
{
 Model1_hrtimer_start_expires(Model1_timer, Model1_HRTIMER_MODE_ABS);
}

/* Query timers: */
extern Model1_ktime_t Model1___hrtimer_get_remaining(const struct Model1_hrtimer *Model1_timer, bool Model1_adjust);

static inline __attribute__((no_instrument_function)) Model1_ktime_t Model1_hrtimer_get_remaining(const struct Model1_hrtimer *Model1_timer)
{
 return Model1___hrtimer_get_remaining(Model1_timer, false);
}

extern Model1_u64 Model1_hrtimer_get_next_event(void);

extern bool Model1_hrtimer_active(const struct Model1_hrtimer *Model1_timer);

/*
 * Helper function to check, whether the timer is on one of the queues
 */
static inline __attribute__((no_instrument_function)) int Model1_hrtimer_is_queued(struct Model1_hrtimer *Model1_timer)
{
 return Model1_timer->Model1_state & 0x01;
}

/*
 * Helper function to check, whether the timer is running the callback
 * function
 */
static inline __attribute__((no_instrument_function)) int Model1_hrtimer_callback_running(struct Model1_hrtimer *Model1_timer)
{
 return Model1_timer->Model1_base->Model1_cpu_base->Model1_running == Model1_timer;
}

/* Forward a hrtimer so it expires after now: */
extern Model1_u64
Model1_hrtimer_forward(struct Model1_hrtimer *Model1_timer, Model1_ktime_t Model1_now, Model1_ktime_t Model1_interval);

/**
 * hrtimer_forward_now - forward the timer expiry so it expires after now
 * @timer:	hrtimer to forward
 * @interval:	the interval to forward
 *
 * Forward the timer expiry so it will expire after the current time
 * of the hrtimer clock base. Returns the number of overruns.
 *
 * Can be safely called from the callback function of @timer. If
 * called from other contexts @timer must neither be enqueued nor
 * running the callback and the caller needs to take care of
 * serialization.
 *
 * Note: This only updates the timer expiry value and does not requeue
 * the timer.
 */
static inline __attribute__((no_instrument_function)) Model1_u64 Model1_hrtimer_forward_now(struct Model1_hrtimer *Model1_timer,
          Model1_ktime_t Model1_interval)
{
 return Model1_hrtimer_forward(Model1_timer, Model1_timer->Model1_base->Model1_get_time(), Model1_interval);
}

/* Precise sleep: */
extern long Model1_hrtimer_nanosleep(struct Model1_timespec *Model1_rqtp,
         struct Model1_timespec *Model1_rmtp,
         const enum Model1_hrtimer_mode Model1_mode,
         const Model1_clockid_t Model1_clockid);
extern long Model1_hrtimer_nanosleep_restart(struct Model1_restart_block *Model1_restart_block);

extern void Model1_hrtimer_init_sleeper(struct Model1_hrtimer_sleeper *Model1_sl,
     struct Model1_task_struct *Model1_tsk);

extern int Model1_schedule_hrtimeout_range(Model1_ktime_t *Model1_expires, Model1_u64 Model1_delta,
      const enum Model1_hrtimer_mode Model1_mode);
extern int Model1_schedule_hrtimeout_range_clock(Model1_ktime_t *Model1_expires,
       Model1_u64 Model1_delta,
       const enum Model1_hrtimer_mode Model1_mode,
       int Model1_clock);
extern int Model1_schedule_hrtimeout(Model1_ktime_t *Model1_expires, const enum Model1_hrtimer_mode Model1_mode);

/* Soft interrupt function to run the hrtimer queues: */
extern void Model1_hrtimer_run_queues(void);

/* Bootup initialization: */
extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_hrtimers_init(void);

/* Show pending timers: */
extern void Model1_sysrq_timer_list_show(void);

int Model1_hrtimers_prepare_cpu(unsigned int Model1_cpu);

int Model1_hrtimers_dead_cpu(unsigned int Model1_cpu);




struct Model1_task_struct;
static inline __attribute__((no_instrument_function)) void Model1_kcov_task_init(struct Model1_task_struct *Model1_t) {}
static inline __attribute__((no_instrument_function)) void Model1_kcov_task_exit(struct Model1_task_struct *Model1_t) {}
/*
 * task_io_accounting: a structure which is used for recording a single task's
 * IO statistics.
 *
 * Don't include this header file directly - it is designed to be dragged in via
 * sched.h.
 *
 * Blame Andrew Morton for all this.
 */

struct Model1_task_io_accounting {

 /* bytes read */
 Model1_u64 Model1_rchar;
 /*  bytes written */
 Model1_u64 Model1_wchar;
 /* # of read syscalls */
 Model1_u64 Model1_syscr;
 /* # of write syscalls */
 Model1_u64 Model1_syscw;



 /*
	 * The number of bytes which this task has caused to be read from
	 * storage.
	 */
 Model1_u64 Model1_read_bytes;

 /*
	 * The number of bytes which this task has caused, or shall cause to be
	 * written to disk.
	 */
 Model1_u64 Model1_write_bytes;

 /*
	 * A task can cause "negative" IO too.  If this task truncates some
	 * dirty pagecache, some IO which another task has been accounted for
	 * (in its write_bytes) will not be happening.  We _could_ just
	 * subtract that from the truncating task's write_bytes, but there is
	 * information loss in doing that.
	 */
 Model1_u64 Model1_cancelled_write_bytes;

};
/*
 * latencytop.h: Infrastructure for displaying latency
 *
 * (C) Copyright 2008 Intel Corporation
 * Author: Arjan van de Ven <arjan@linux.intel.com>
 *
 */





struct Model1_task_struct;
static inline __attribute__((no_instrument_function)) void
Model1_account_scheduler_latency(struct Model1_task_struct *Model1_task, int Model1_usecs, int Model1_inter)
{
}

static inline __attribute__((no_instrument_function)) void Model1_clear_all_latency_tracing(struct Model1_task_struct *Model1_p)
{
}
/* Credentials management - see Documentation/security/credentials.txt
 *
 * Copyright (C) 2008 Red Hat, Inc. All Rights Reserved.
 * Written by David Howells (dhowells@redhat.com)
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public Licence
 * as published by the Free Software Foundation; either version
 * 2 of the Licence, or (at your option) any later version.
 */







/* Authentication token and access key management
 *
 * Copyright (C) 2004, 2007 Red Hat, Inc. All Rights Reserved.
 * Written by David Howells (dhowells@redhat.com)
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public License
 * as published by the Free Software Foundation; either version
 * 2 of the License, or (at your option) any later version.
 *
 *
 * See Documentation/security/keys.txt for information on keys/keyrings.
 */
/* Generic associative array implementation.
 *
 * See Documentation/assoc_array.txt for information.
 *
 * Copyright (C) 2013 Red Hat, Inc. All Rights Reserved.
 * Written by David Howells (dhowells@redhat.com)
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public Licence
 * as published by the Free Software Foundation; either version
 * 2 of the Licence, or (at your option) any later version.
 */
/*
 * Generic associative array.
 */
struct Model1_assoc_array {
 struct Model1_assoc_array_ptr *Model1_root; /* The node at the root of the tree */
 unsigned long Model1_nr_leaves_on_tree;
};

/*
 * Operations on objects and index keys for use by array manipulation routines.
 */
struct Model1_assoc_array_ops {
 /* Method to get a chunk of an index key from caller-supplied data */
 unsigned long (*Model1_get_key_chunk)(const void *Model1_index_key, int Model1_level);

 /* Method to get a piece of an object's index key */
 unsigned long (*Model1_get_object_key_chunk)(const void *Model1_object, int Model1_level);

 /* Is this the object we're looking for? */
 bool (*Model1_compare_object)(const void *Model1_object, const void *Model1_index_key);

 /* How different is an object from an index key, to a bit position in
	 * their keys? (or -1 if they're the same)
	 */
 int (*Model1_diff_objects)(const void *Model1_object, const void *Model1_index_key);

 /* Method to free an object. */
 void (*Model1_free_object)(void *Model1_object);
};

/*
 * Access and manipulation functions.
 */
struct Model1_assoc_array_edit;

static inline __attribute__((no_instrument_function)) void Model1_assoc_array_init(struct Model1_assoc_array *Model1_array)
{
 Model1_array->Model1_root = ((void *)0);
 Model1_array->Model1_nr_leaves_on_tree = 0;
}

extern int Model1_assoc_array_iterate(const struct Model1_assoc_array *Model1_array,
          int (*Model1_iterator)(const void *Model1_object,
            void *Model1_iterator_data),
          void *Model1_iterator_data);
extern void *Model1_assoc_array_find(const struct Model1_assoc_array *Model1_array,
         const struct Model1_assoc_array_ops *Model1_ops,
         const void *Model1_index_key);
extern void Model1_assoc_array_destroy(struct Model1_assoc_array *Model1_array,
    const struct Model1_assoc_array_ops *Model1_ops);
extern struct Model1_assoc_array_edit *Model1_assoc_array_insert(struct Model1_assoc_array *Model1_array,
         const struct Model1_assoc_array_ops *Model1_ops,
         const void *Model1_index_key,
         void *Model1_object);
extern void Model1_assoc_array_insert_set_object(struct Model1_assoc_array_edit *Model1_edit,
       void *Model1_object);
extern struct Model1_assoc_array_edit *Model1_assoc_array_delete(struct Model1_assoc_array *Model1_array,
         const struct Model1_assoc_array_ops *Model1_ops,
         const void *Model1_index_key);
extern struct Model1_assoc_array_edit *Model1_assoc_array_clear(struct Model1_assoc_array *Model1_array,
        const struct Model1_assoc_array_ops *Model1_ops);
extern void Model1_assoc_array_apply_edit(struct Model1_assoc_array_edit *Model1_edit);
extern void Model1_assoc_array_cancel_edit(struct Model1_assoc_array_edit *Model1_edit);
extern int Model1_assoc_array_gc(struct Model1_assoc_array *Model1_array,
     const struct Model1_assoc_array_ops *Model1_ops,
     bool (*Model1_iterator)(void *Model1_object, void *Model1_iterator_data),
     void *Model1_iterator_data);




/* key handle serial number */
typedef Model1_int32_t Model1_key_serial_t;

/* key handle permissions mask */
typedef Model1_uint32_t Model1_key_perm_t;

struct Model1_key;
struct Model1_seq_file;
struct Model1_user_struct;
struct Model1_signal_struct;
struct Model1_cred;

struct Model1_key_type;
struct Model1_key_owner;
struct Model1_keyring_list;
struct Model1_keyring_name;

struct Model1_keyring_index_key {
 struct Model1_key_type *Model1_type;
 const char *Model1_description;
 Model1_size_t Model1_desc_len;
};

union Model1_key_payload {
 void *Model1_rcu_data0;
 void *Model1_data[4];
};

/*****************************************************************************/
/*
 * key reference with possession attribute handling
 *
 * NOTE! key_ref_t is a typedef'd pointer to a type that is not actually
 * defined. This is because we abuse the bottom bit of the reference to carry a
 * flag to indicate whether the calling process possesses that key in one of
 * its keyrings.
 *
 * the key_ref_t has been made a separate type so that the compiler can reject
 * attempts to dereference it without proper conversion.
 *
 * the three functions are used to assemble and disassemble references
 */
typedef struct Model1___key_reference_with_attributes *Model1_key_ref_t;

static inline __attribute__((no_instrument_function)) Model1_key_ref_t Model1_make_key_ref(const struct Model1_key *Model1_key,
         bool Model1_possession)
{
 return (Model1_key_ref_t) ((unsigned long) Model1_key | Model1_possession);
}

static inline __attribute__((no_instrument_function)) struct Model1_key *Model1_key_ref_to_ptr(const Model1_key_ref_t Model1_key_ref)
{
 return (struct Model1_key *) ((unsigned long) Model1_key_ref & ~1UL);
}

static inline __attribute__((no_instrument_function)) bool Model1_is_key_possessed(const Model1_key_ref_t Model1_key_ref)
{
 return (unsigned long) Model1_key_ref & 1UL;
}

/*****************************************************************************/
/*
 * authentication token / access credential / keyring
 * - types of key include:
 *   - keyrings
 *   - disk encryption IDs
 *   - Kerberos TGTs and tickets
 */
struct Model1_key {
 Model1_atomic_t Model1_usage; /* number of references */
 Model1_key_serial_t Model1_serial; /* key serial number */
 union {
  struct Model1_list_head Model1_graveyard_link;
  struct Model1_rb_node Model1_serial_node;
 };
 struct Model1_rw_semaphore Model1_sem; /* change vs change sem */
 struct Model1_key_user *Model1_user; /* owner of this key */
 void *Model1_security; /* security data for this key */
 union {
  Model1_time_t Model1_expiry; /* time at which key expires (or 0) */
  Model1_time_t Model1_revoked_at; /* time at which key was revoked */
 };
 Model1_time_t Model1_last_used_at; /* last time used for LRU keyring discard */
 Model1_kuid_t Model1_uid;
 Model1_kgid_t Model1_gid;
 Model1_key_perm_t Model1_perm; /* access permissions */
 unsigned short Model1_quotalen; /* length added to quota */
 unsigned short Model1_datalen; /* payload data length
						 * - may not match RCU dereferenced payload
						 * - payload should contain own length
						 */







 unsigned long Model1_flags; /* status flags (change with bitops) */
 /* the key type and key description string
	 * - the desc is used to match a key against search criteria
	 * - it should be a printable string
	 * - eg: for krb5 AFS, this might be "afs@REDHAT.COM"
	 */
 union {
  struct Model1_keyring_index_key Model1_index_key;
  struct {
   struct Model1_key_type *Model1_type; /* type of key */
   char *Model1_description;
  };
 };

 /* key data
	 * - this is used to hold the data actually used in cryptography or
	 *   whatever
	 */
 union {
  union Model1_key_payload Model1_payload;
  struct {
   /* Keyring bits */
   struct Model1_list_head Model1_name_link;
   struct Model1_assoc_array Model1_keys;
  };
  int Model1_reject_error;
 };

 /* This is set on a keyring to restrict the addition of a link to a key
	 * to it.  If this method isn't provided then it is assumed that the
	 * keyring is open to any addition.  It is ignored for non-keyring
	 * keys.
	 *
	 * This is intended for use with rings of trusted keys whereby addition
	 * to the keyring needs to be controlled.  KEY_ALLOC_BYPASS_RESTRICTION
	 * overrides this, allowing the kernel to add extra keys without
	 * restriction.
	 */
 int (*Model1_restrict_link)(struct Model1_key *Model1_keyring,
        const struct Model1_key_type *Model1_type,
        const union Model1_key_payload *Model1_payload);
};

extern struct Model1_key *Model1_key_alloc(struct Model1_key_type *Model1_type,
        const char *Model1_desc,
        Model1_kuid_t Model1_uid, Model1_kgid_t Model1_gid,
        const struct Model1_cred *Model1_cred,
        Model1_key_perm_t Model1_perm,
        unsigned long Model1_flags,
        int (*Model1_restrict_link)(struct Model1_key *,
        const struct Model1_key_type *,
        const union Model1_key_payload *));
extern void Model1_key_revoke(struct Model1_key *Model1_key);
extern void Model1_key_invalidate(struct Model1_key *Model1_key);
extern void Model1_key_put(struct Model1_key *Model1_key);

static inline __attribute__((no_instrument_function)) struct Model1_key *Model1___key_get(struct Model1_key *Model1_key)
{
 Model1_atomic_inc(&Model1_key->Model1_usage);
 return Model1_key;
}

static inline __attribute__((no_instrument_function)) struct Model1_key *Model1_key_get(struct Model1_key *Model1_key)
{
 return Model1_key ? Model1___key_get(Model1_key) : Model1_key;
}

static inline __attribute__((no_instrument_function)) void Model1_key_ref_put(Model1_key_ref_t Model1_key_ref)
{
 Model1_key_put(Model1_key_ref_to_ptr(Model1_key_ref));
}

extern struct Model1_key *Model1_request_key(struct Model1_key_type *Model1_type,
          const char *Model1_description,
          const char *Model1_callout_info);

extern struct Model1_key *Model1_request_key_with_auxdata(struct Model1_key_type *Model1_type,
         const char *Model1_description,
         const void *Model1_callout_info,
         Model1_size_t Model1_callout_len,
         void *Model1_aux);

extern struct Model1_key *Model1_request_key_async(struct Model1_key_type *Model1_type,
         const char *Model1_description,
         const void *Model1_callout_info,
         Model1_size_t Model1_callout_len);

extern struct Model1_key *Model1_request_key_async_with_auxdata(struct Model1_key_type *Model1_type,
        const char *Model1_description,
        const void *Model1_callout_info,
        Model1_size_t Model1_callout_len,
        void *Model1_aux);

extern int Model1_wait_for_key_construction(struct Model1_key *Model1_key, bool Model1_intr);

extern int Model1_key_validate(const struct Model1_key *Model1_key);

extern Model1_key_ref_t Model1_key_create_or_update(Model1_key_ref_t Model1_keyring,
          const char *Model1_type,
          const char *Model1_description,
          const void *Model1_payload,
          Model1_size_t Model1_plen,
          Model1_key_perm_t Model1_perm,
          unsigned long Model1_flags);

extern int Model1_key_update(Model1_key_ref_t Model1_key,
        const void *Model1_payload,
        Model1_size_t Model1_plen);

extern int Model1_key_link(struct Model1_key *Model1_keyring,
      struct Model1_key *Model1_key);

extern int Model1_key_unlink(struct Model1_key *Model1_keyring,
        struct Model1_key *Model1_key);

extern struct Model1_key *Model1_keyring_alloc(const char *Model1_description, Model1_kuid_t Model1_uid, Model1_kgid_t Model1_gid,
     const struct Model1_cred *Model1_cred,
     Model1_key_perm_t Model1_perm,
     unsigned long Model1_flags,
     int (*Model1_restrict_link)(struct Model1_key *,
            const struct Model1_key_type *,
            const union Model1_key_payload *),
     struct Model1_key *Model1_dest);

extern int Model1_restrict_link_reject(struct Model1_key *Model1_keyring,
    const struct Model1_key_type *Model1_type,
    const union Model1_key_payload *Model1_payload);

extern int Model1_keyring_clear(struct Model1_key *Model1_keyring);

extern Model1_key_ref_t Model1_keyring_search(Model1_key_ref_t Model1_keyring,
    struct Model1_key_type *Model1_type,
    const char *Model1_description);

extern int Model1_keyring_add_key(struct Model1_key *Model1_keyring,
      struct Model1_key *Model1_key);

extern struct Model1_key *Model1_key_lookup(Model1_key_serial_t Model1_id);

static inline __attribute__((no_instrument_function)) Model1_key_serial_t Model1_key_serial(const struct Model1_key *Model1_key)
{
 return Model1_key ? Model1_key->Model1_serial : 0;
}

extern void Model1_key_set_timeout(struct Model1_key *, unsigned);

/*
 * The permissions required on a key that we're looking up.
 */
/**
 * key_is_instantiated - Determine if a key has been positively instantiated
 * @key: The key to check.
 *
 * Return true if the specified key has been positively instantiated, false
 * otherwise.
 */
static inline __attribute__((no_instrument_function)) bool Model1_key_is_instantiated(const struct Model1_key *Model1_key)
{
 return (__builtin_constant_p((0)) ? Model1_constant_test_bit((0), (&Model1_key->Model1_flags)) : Model1_variable_test_bit((0), (&Model1_key->Model1_flags))) &&
  !(__builtin_constant_p((5)) ? Model1_constant_test_bit((5), (&Model1_key->Model1_flags)) : Model1_variable_test_bit((5), (&Model1_key->Model1_flags)));
}
extern struct Model1_ctl_table Model1_key_sysctls[];

/*
 * the userspace interface
 */
extern int Model1_install_thread_keyring_to_cred(struct Model1_cred *Model1_cred);
extern void Model1_key_fsuid_changed(struct Model1_task_struct *Model1_tsk);
extern void Model1_key_fsgid_changed(struct Model1_task_struct *Model1_tsk);
extern void Model1_key_init(void);
/*
 * SELinux services exported to the rest of the kernel.
 *
 * Author: James Morris <jmorris@redhat.com>
 *
 * Copyright (C) 2005 Red Hat, Inc., James Morris <jmorris@redhat.com>
 * Copyright (C) 2006 Trusted Computer Solutions, Inc. <dgoeddel@trustedcs.com>
 * Copyright (C) 2006 IBM Corporation, Timothy R. Chavez <tinytim@us.ibm.com>
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2,
 * as published by the Free Software Foundation.
 */



struct Model1_selinux_audit_rule;
struct Model1_audit_context;
struct Model1_kern_ipc_perm;



/**
 * selinux_is_enabled - is SELinux enabled?
 */
bool Model1_selinux_is_enabled(void);



struct Model1_user_struct;
struct Model1_cred;
struct Model1_inode;

/*
 * COW Supplementary groups list
 */



struct Model1_group_info {
 Model1_atomic_t Model1_usage;
 int Model1_ngroups;
 int Model1_nblocks;
 Model1_kgid_t Model1_small_block[32];
 Model1_kgid_t *Model1_blocks[0];
};

/**
 * get_group_info - Get a reference to a group info structure
 * @group_info: The group info to reference
 *
 * This gets a reference to a set of supplementary groups.
 *
 * If the caller is accessing a task's credentials, they must hold the RCU read
 * lock when reading.
 */
static inline __attribute__((no_instrument_function)) struct Model1_group_info *Model1_get_group_info(struct Model1_group_info *Model1_gi)
{
 Model1_atomic_inc(&Model1_gi->Model1_usage);
 return Model1_gi;
}

/**
 * put_group_info - Release a reference to a group info structure
 * @group_info: The group info to release
 */






extern struct Model1_group_info Model1_init_groups;

extern struct Model1_group_info *Model1_groups_alloc(int);
extern void Model1_groups_free(struct Model1_group_info *);

extern int Model1_in_group_p(Model1_kgid_t);
extern int Model1_in_egroup_p(Model1_kgid_t);
extern int Model1_set_current_groups(struct Model1_group_info *);
extern void Model1_set_groups(struct Model1_cred *, struct Model1_group_info *);
extern int Model1_groups_search(const struct Model1_group_info *, Model1_kgid_t);
extern bool Model1_may_setgroups(void);

/* access the groups "array" with this macro */



/*
 * The security context of a task
 *
 * The parts of the context break down into two categories:
 *
 *  (1) The objective context of a task.  These parts are used when some other
 *	task is attempting to affect this one.
 *
 *  (2) The subjective context.  These details are used when the task is acting
 *	upon another object, be that a file, a task, a key or whatever.
 *
 * Note that some members of this structure belong to both categories - the
 * LSM security pointer for instance.
 *
 * A task has two security pointers.  task->real_cred points to the objective
 * context that defines that task's actual details.  The objective part of this
 * context is used whenever that task is acted upon.
 *
 * task->cred points to the subjective context that defines the details of how
 * that task is going to act upon another object.  This may be overridden
 * temporarily to point to another security context, but normally points to the
 * same context as task->real_cred.
 */
struct Model1_cred {
 Model1_atomic_t Model1_usage;







 Model1_kuid_t Model1_uid; /* real UID of the task */
 Model1_kgid_t Model1_gid; /* real GID of the task */
 Model1_kuid_t Model1_suid; /* saved UID of the task */
 Model1_kgid_t Model1_sgid; /* saved GID of the task */
 Model1_kuid_t Model1_euid; /* effective UID of the task */
 Model1_kgid_t Model1_egid; /* effective GID of the task */
 Model1_kuid_t Model1_fsuid; /* UID for VFS ops */
 Model1_kgid_t Model1_fsgid; /* GID for VFS ops */
 unsigned Model1_securebits; /* SUID-less security management */
 Model1_kernel_cap_t Model1_cap_inheritable; /* caps our children can inherit */
 Model1_kernel_cap_t Model1_cap_permitted; /* caps we're permitted */
 Model1_kernel_cap_t Model1_cap_effective; /* caps we can actually use */
 Model1_kernel_cap_t Model1_cap_bset; /* capability bounding set */
 Model1_kernel_cap_t Model1_cap_ambient; /* Ambient capability set */

 unsigned char Model1_jit_keyring; /* default keyring to attach requested
					 * keys to */
 struct Model1_key *Model1_session_keyring; /* keyring inherited over fork */
 struct Model1_key *Model1_process_keyring; /* keyring private to this process */
 struct Model1_key *Model1_thread_keyring; /* keyring private to this thread */
 struct Model1_key *Model1_request_key_auth; /* assumed request_key authority */


 void *Model1_security; /* subjective LSM security */

 struct Model1_user_struct *Model1_user; /* real user ID subscription */
 struct Model1_user_namespace *Model1_user_ns; /* user_ns the caps and keyrings are relative to. */
 struct Model1_group_info *Model1_group_info; /* supplementary groups for euid/fsgid */
 struct Model1_callback_head Model1_rcu; /* RCU deletion hook */
};

extern void Model1___put_cred(struct Model1_cred *);
extern void Model1_exit_creds(struct Model1_task_struct *);
extern int Model1_copy_creds(struct Model1_task_struct *, unsigned long);
extern const struct Model1_cred *Model1_get_task_cred(struct Model1_task_struct *);
extern struct Model1_cred *Model1_cred_alloc_blank(void);
extern struct Model1_cred *Model1_prepare_creds(void);
extern struct Model1_cred *Model1_prepare_exec_creds(void);
extern int Model1_commit_creds(struct Model1_cred *);
extern void Model1_abort_creds(struct Model1_cred *);
extern const struct Model1_cred *Model1_override_creds(const struct Model1_cred *);
extern void Model1_revert_creds(const struct Model1_cred *);
extern struct Model1_cred *Model1_prepare_kernel_cred(struct Model1_task_struct *);
extern int Model1_change_create_files_as(struct Model1_cred *, struct Model1_inode *);
extern int Model1_set_security_override(struct Model1_cred *, Model1_u32);
extern int Model1_set_security_override_from_ctx(struct Model1_cred *, const char *);
extern int Model1_set_create_files_as(struct Model1_cred *, struct Model1_inode *);
extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_cred_init(void);

/*
 * check for validity of credentials
 */
static inline __attribute__((no_instrument_function)) void Model1_validate_creds(const struct Model1_cred *Model1_cred)
{
}
static inline __attribute__((no_instrument_function)) void Model1_validate_creds_for_do_exit(struct Model1_task_struct *Model1_tsk)
{
}
static inline __attribute__((no_instrument_function)) void Model1_validate_process_creds(void)
{
}


static inline __attribute__((no_instrument_function)) bool Model1_cap_ambient_invariant_ok(const struct Model1_cred *Model1_cred)
{
 return Model1_cap_issubset(Model1_cred->Model1_cap_ambient,
       Model1_cap_intersect(Model1_cred->Model1_cap_permitted,
       Model1_cred->Model1_cap_inheritable));
}

/**
 * get_new_cred - Get a reference on a new set of credentials
 * @cred: The new credentials to reference
 *
 * Get a reference on the specified set of new credentials.  The caller must
 * release the reference.
 */
static inline __attribute__((no_instrument_function)) struct Model1_cred *Model1_get_new_cred(struct Model1_cred *Model1_cred)
{
 Model1_atomic_inc(&Model1_cred->Model1_usage);
 return Model1_cred;
}

/**
 * get_cred - Get a reference on a set of credentials
 * @cred: The credentials to reference
 *
 * Get a reference on the specified set of credentials.  The caller must
 * release the reference.
 *
 * This is used to deal with a committed set of credentials.  Although the
 * pointer is const, this will temporarily discard the const and increment the
 * usage count.  The purpose of this is to attempt to catch at compile time the
 * accidental alteration of a set of credentials that should be considered
 * immutable.
 */
static inline __attribute__((no_instrument_function)) const struct Model1_cred *Model1_get_cred(const struct Model1_cred *Model1_cred)
{
 struct Model1_cred *Model1_nonconst_cred = (struct Model1_cred *) Model1_cred;
 Model1_validate_creds(Model1_cred);
 return Model1_get_new_cred(Model1_nonconst_cred);
}

/**
 * put_cred - Release a reference to a set of credentials
 * @cred: The credentials to release
 *
 * Release a reference to a set of credentials, deleting them when the last ref
 * is released.
 *
 * This takes a const pointer to a set of credentials because the credentials
 * on task_struct are attached by const pointers to prevent accidental
 * alteration of otherwise immutable credential sets.
 */
static inline __attribute__((no_instrument_function)) void Model1_put_cred(const struct Model1_cred *Model1__cred)
{
 struct Model1_cred *Model1_cred = (struct Model1_cred *) Model1__cred;

 Model1_validate_creds(Model1_cred);
 if (Model1_atomic_dec_and_test(&(Model1_cred)->Model1_usage))
  Model1___put_cred(Model1_cred);
}

/**
 * current_cred - Access the current task's subjective credentials
 *
 * Access the subjective credentials of the current task.  RCU-safe,
 * since nobody else can modify it.
 */



/**
 * current_real_cred - Access the current task's objective credentials
 *
 * Access the objective credentials of the current task.  RCU-safe,
 * since nobody else can modify it.
 */



/**
 * __task_cred - Access a task's objective credentials
 * @task: The task to query
 *
 * Access the objective credentials of a task.  The caller must hold the RCU
 * readlock.
 *
 * The result of this function should not be passed directly to get_cred();
 * rather get_task_cred() should be used instead.
 */



/**
 * get_current_cred - Get the current task's subjective credentials
 *
 * Get the subjective credentials of the current task, pinning them so that
 * they can't go away.  Accessing the current task's credentials directly is
 * not permitted.
 */



/**
 * get_current_user - Get the current task's user_struct
 *
 * Get the user record of the current task, pinning it so that it can't go
 * away.
 */
/**
 * get_current_groups - Get the current task's supplementary group list
 *
 * Get the supplementary group list of the current task, pinning it so that it
 * can't go away.
 */
extern struct Model1_user_namespace Model1_init_user_ns;



static inline __attribute__((no_instrument_function)) struct Model1_user_namespace *Model1_current_user_ns(void)
{
 return &Model1_init_user_ns;
}



     /* used by file system utilities that
	                                   look at the superblock, etc.  */
/* Since UDF 2.01 is ISO 13346 based... */
/*
 * linux/cgroup-defs.h - basic definitions for cgroup
 *
 * This file provides basic type and interface.  Include this file directly
 * only if necessary to avoid cyclic dependencies.
 */





/*
 * include/linux/idr.h
 * 
 * 2002-10-18  written by Jim Houston jim.houston@ccur.com
 *	Copyright (C) 2002 by Concurrent Computer Corporation
 *	Distributed under the GNU GPL license version 2.
 *
 * Small id to pointer translation service avoiding fixed sized
 * tables.
 */
/*
 * We want shallower trees and thus more bits covered at each layer.  8
 * bits gives us large enough first layer for most use cases and maximum
 * tree depth of 4.  Each idr_layer is slightly larger than 2k on 64bit and
 * 1k on 32bit.
 */




struct Model1_idr_layer {
 int Model1_prefix; /* the ID prefix of this idr_layer */
 int Model1_layer; /* distance from leaf */
 struct Model1_idr_layer *Model1_ary[1<<8];
 int Model1_count; /* When zero, we can release it */
 union {
  /* A zero bit means "space here" */
  unsigned long Model1_bitmap[((((1 << 8)) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];
  struct Model1_callback_head Model1_callback_head;
 };
};

struct Model1_idr {
 struct Model1_idr_layer *Model1_hint; /* the last layer allocated from */
 struct Model1_idr_layer *Model1_top;
 int Model1_layers; /* only valid w/o concurrent changes */
 int Model1_cur; /* current pos for cyclic allocation */
 Model1_spinlock_t Model1_lock;
 int Model1_id_free_cnt;
 struct Model1_idr_layer *Model1_id_free;
};







/**
 * DOC: idr sync
 * idr synchronization (stolen from radix-tree.h)
 *
 * idr_find() is able to be called locklessly, using RCU. The caller must
 * ensure calls to this function are made within rcu_read_lock() regions.
 * Other readers (lock-free or otherwise) and modifications may be running
 * concurrently.
 *
 * It is still required that the caller manage the synchronization and
 * lifetimes of the items. So if RCU lock-free lookups are used, typically
 * this would mean that the items have their own locks, or are amenable to
 * lock-free access; and that the items are freed by RCU (or only freed after
 * having been deleted from the idr tree *and* a synchronize_rcu() grace
 * period).
 */

/*
 * This is what we export.
 */

void *Model1_idr_find_slowpath(struct Model1_idr *Model1_idp, int Model1_id);
void Model1_idr_preload(Model1_gfp_t Model1_gfp_mask);
int Model1_idr_alloc(struct Model1_idr *Model1_idp, void *Model1_ptr, int Model1_start, int Model1_end, Model1_gfp_t Model1_gfp_mask);
int Model1_idr_alloc_cyclic(struct Model1_idr *Model1_idr, void *Model1_ptr, int Model1_start, int Model1_end, Model1_gfp_t Model1_gfp_mask);
int Model1_idr_for_each(struct Model1_idr *Model1_idp,
   int (*Model1_fn)(int Model1_id, void *Model1_p, void *Model1_data), void *Model1_data);
void *Model1_idr_get_next(struct Model1_idr *Model1_idp, int *Model1_nextid);
void *Model1_idr_replace(struct Model1_idr *Model1_idp, void *Model1_ptr, int Model1_id);
void Model1_idr_remove(struct Model1_idr *Model1_idp, int Model1_id);
void Model1_idr_destroy(struct Model1_idr *Model1_idp);
void Model1_idr_init(struct Model1_idr *Model1_idp);
bool Model1_idr_is_empty(struct Model1_idr *Model1_idp);

/**
 * idr_preload_end - end preload section started with idr_preload()
 *
 * Each idr_preload() should be matched with an invocation of this
 * function.  See idr_preload() for details.
 */
static inline __attribute__((no_instrument_function)) void Model1_idr_preload_end(void)
{
 __asm__ __volatile__("": : :"memory");
}

/**
 * idr_find - return pointer for given id
 * @idr: idr handle
 * @id: lookup key
 *
 * Return the pointer given the id it has been registered with.  A %NULL
 * return indicates that @id is not valid or you passed %NULL in
 * idr_get_new().
 *
 * This function can be called under rcu_read_lock(), given that the leaf
 * pointers lifetimes are correctly managed.
 */
static inline __attribute__((no_instrument_function)) void *Model1_idr_find(struct Model1_idr *Model1_idr, int Model1_id)
{
 struct Model1_idr_layer *Model1_hint = ({ typeof(Model1_idr->Model1_hint) Model1_________p1 = ({ typeof(Model1_idr->Model1_hint) Model1__________p1 = ({ union { typeof(Model1_idr->Model1_hint) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&(Model1_idr->Model1_hint), Model1___u.Model1___c, sizeof(Model1_idr->Model1_hint)); else Model1___read_once_size_nocheck(&(Model1_idr->Model1_hint), Model1___u.Model1___c, sizeof(Model1_idr->Model1_hint)); Model1___u.Model1___val; }); typeof(*(Model1_idr->Model1_hint)) *Model1____typecheck_p __attribute__((unused)); do { } while (0); (Model1__________p1); }); ((typeof(*Model1_idr->Model1_hint) *)(Model1_________p1)); });

 if (Model1_hint && (Model1_id & ~((1 << 8)-1)) == Model1_hint->Model1_prefix)
  return ({ typeof(Model1_hint->Model1_ary[Model1_id & ((1 << 8)-1)]) Model1_________p1 = ({ typeof(Model1_hint->Model1_ary[Model1_id & ((1 << 8)-1)]) Model1__________p1 = ({ union { typeof(Model1_hint->Model1_ary[Model1_id & ((1 << 8)-1)]) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&(Model1_hint->Model1_ary[Model1_id & ((1 << 8)-1)]), Model1___u.Model1___c, sizeof(Model1_hint->Model1_ary[Model1_id & ((1 << 8)-1)])); else Model1___read_once_size_nocheck(&(Model1_hint->Model1_ary[Model1_id & ((1 << 8)-1)]), Model1___u.Model1___c, sizeof(Model1_hint->Model1_ary[Model1_id & ((1 << 8)-1)])); Model1___u.Model1___val; }); typeof(*(Model1_hint->Model1_ary[Model1_id & ((1 << 8)-1)])) *Model1____typecheck_p __attribute__((unused)); do { } while (0); (Model1__________p1); }); ((typeof(*Model1_hint->Model1_ary[Model1_id & ((1 << 8)-1)]) *)(Model1_________p1)); });

 return Model1_idr_find_slowpath(Model1_idr, Model1_id);
}

/**
 * idr_for_each_entry - iterate over an idr's elements of a given type
 * @idp:     idr handle
 * @entry:   the type * to use as cursor
 * @id:      id entry's key
 *
 * @entry and @id do not need to be initialized before the loop, and
 * after normal terminatinon @entry is left with the value NULL.  This
 * is convenient for a "not found" value.
 */



/**
 * idr_for_each_entry - continue iteration over an idr's elements of a given type
 * @idp:     idr handle
 * @entry:   the type * to use as cursor
 * @id:      id entry's key
 *
 * Continue to iterate over list of given type, continuing after
 * the current position.
 */





/*
 * IDA - IDR based id allocator, use when translation from id to
 * pointer isn't necessary.
 *
 * IDA_BITMAP_LONGS is calculated to be one less to accommodate
 * ida_bitmap->nr_busy so that the whole struct fits in 128 bytes.
 */




struct Model1_ida_bitmap {
 long Model1_nr_busy;
 unsigned long Model1_bitmap[(128 / sizeof(long) - 1)];
};

struct Model1_ida {
 struct Model1_idr Model1_idr;
 struct Model1_ida_bitmap *Model1_free_bitmap;
};




int Model1_ida_pre_get(struct Model1_ida *Model1_ida, Model1_gfp_t Model1_gfp_mask);
int Model1_ida_get_new_above(struct Model1_ida *Model1_ida, int Model1_starting_id, int *Model1_p_id);
void Model1_ida_remove(struct Model1_ida *Model1_ida, int Model1_id);
void Model1_ida_destroy(struct Model1_ida *Model1_ida);
void Model1_ida_init(struct Model1_ida *Model1_ida);

int Model1_ida_simple_get(struct Model1_ida *Model1_ida, unsigned int Model1_start, unsigned int Model1_end,
     Model1_gfp_t Model1_gfp_mask);
void Model1_ida_simple_remove(struct Model1_ida *Model1_ida, unsigned int Model1_id);

/**
 * ida_get_new - allocate new ID
 * @ida:	idr handle
 * @p_id:	pointer to the allocated handle
 *
 * Simple wrapper around ida_get_new_above() w/ @starting_id of zero.
 */
static inline __attribute__((no_instrument_function)) int Model1_ida_get_new(struct Model1_ida *Model1_ida, int *Model1_p_id)
{
 return Model1_ida_get_new_above(Model1_ida, 0, Model1_p_id);
}

void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_idr_init_cache(void);











/*
 * RCU-based infrastructure for lightweight reader-writer locking
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, you can access it online at
 * http://www.gnu.org/licenses/gpl-2.0.html.
 *
 * Copyright (c) 2015, Red Hat, Inc.
 *
 * Author: Oleg Nesterov <oleg@redhat.com>
 */







enum Model1_rcu_sync_type { Model1_RCU_SYNC, Model1_RCU_SCHED_SYNC, Model1_RCU_BH_SYNC };

/* Structure to mediate between updaters and fastpath-using readers.  */
struct Model1_rcu_sync {
 int Model1_gp_state;
 int Model1_gp_count;
 Model1_wait_queue_head_t Model1_gp_wait;

 int Model1_cb_state;
 struct Model1_callback_head Model1_cb_head;

 enum Model1_rcu_sync_type Model1_gp_type;
};

extern void Model1_rcu_sync_lockdep_assert(struct Model1_rcu_sync *);

/**
 * rcu_sync_is_idle() - Are readers permitted to use their fastpaths?
 * @rsp: Pointer to rcu_sync structure to use for synchronization
 *
 * Returns true if readers are permitted to use their fastpaths.
 * Must be invoked within an RCU read-side critical section whose
 * flavor matches that of the rcu_sync struture.
 */
static inline __attribute__((no_instrument_function)) bool Model1_rcu_sync_is_idle(struct Model1_rcu_sync *Model1_rsp)
{



 return !Model1_rsp->Model1_gp_state; /* GP_IDLE */
}

extern void Model1_rcu_sync_init(struct Model1_rcu_sync *, enum Model1_rcu_sync_type);
extern void Model1_rcu_sync_enter(struct Model1_rcu_sync *);
extern void Model1_rcu_sync_exit(struct Model1_rcu_sync *);
extern void Model1_rcu_sync_dtor(struct Model1_rcu_sync *);


struct Model1_percpu_rw_semaphore {
 struct Model1_rcu_sync Model1_rss;
 unsigned int *Model1_fast_read_ctr;
 struct Model1_rw_semaphore Model1_rw_sem;
 Model1_atomic_t Model1_slow_read_ctr;
 Model1_wait_queue_head_t Model1_write_waitq;
};

extern void Model1_percpu_down_read(struct Model1_percpu_rw_semaphore *);
extern int Model1_percpu_down_read_trylock(struct Model1_percpu_rw_semaphore *);
extern void Model1_percpu_up_read(struct Model1_percpu_rw_semaphore *);

extern void Model1_percpu_down_write(struct Model1_percpu_rw_semaphore *);
extern void Model1_percpu_up_write(struct Model1_percpu_rw_semaphore *);

extern int Model1___percpu_init_rwsem(struct Model1_percpu_rw_semaphore *,
    const char *, struct Model1_lock_class_key *);
extern void Model1_percpu_free_rwsem(struct Model1_percpu_rw_semaphore *);
static inline __attribute__((no_instrument_function)) void Model1_percpu_rwsem_release(struct Model1_percpu_rw_semaphore *Model1_sem,
     bool Model1_read, unsigned long Model1_ip)
{
 do { } while (0);

 if (!Model1_read)
  Model1_sem->Model1_rw_sem.Model1_owner = ((void *)0);

}

static inline __attribute__((no_instrument_function)) void Model1_percpu_rwsem_acquire(struct Model1_percpu_rw_semaphore *Model1_sem,
     bool Model1_read, unsigned long Model1_ip)
{
 do { } while (0);
}




struct Model1_cgroup;
struct Model1_cgroup_root;
struct Model1_cgroup_subsys;
struct Model1_cgroup_taskset;
struct Model1_kernfs_node;
struct Model1_kernfs_ops;
struct Model1_kernfs_open_file;
struct Model1_seq_file;





/* define the enumeration of all cgroup subsystems */

enum Model1_cgroup_subsys_id {

/*
 * List of cgroup subsystems.
 *
 * DO NOT ADD ANY SUBSYSTEM WITHOUT EXPLICIT ACKS FROM CGROUP MAINTAINERS.
 */

/*
 * This file *must* be included with SUBSYS() defined.
 */


Model1_cpuset_cgrp_id,



Model1_cpu_cgrp_id,



Model1_cpuacct_cgrp_id,
Model1_freezer_cgrp_id,
/*
 * The following subsystems are not supported on the default hierarchy.
 */




/*
 * DO NOT ADD ANY SUBSYSTEM WITHOUT EXPLICIT ACKS FROM CGROUP MAINTAINERS.
 */
 Model1_CGROUP_SUBSYS_COUNT,
};


/* bits in struct cgroup_subsys_state flags field */
enum {
 Model1_CSS_NO_REF = (1 << 0), /* no reference counting for this css */
 Model1_CSS_ONLINE = (1 << 1), /* between ->css_online() and ->css_offline() */
 Model1_CSS_RELEASED = (1 << 2), /* refcnt reached zero, released */
 Model1_CSS_VISIBLE = (1 << 3), /* css is visible to userland */
};

/* bits in struct cgroup flags field */
enum {
 /* Control Group requires release notifications to userspace */
 Model1_CGRP_NOTIFY_ON_RELEASE,
 /*
	 * Clone the parent's configuration when creating a new child
	 * cpuset cgroup.  For historical reasons, this option can be
	 * specified at mount time and thus is implemented here.
	 */
 Model1_CGRP_CPUSET_CLONE_CHILDREN,
};

/* cgroup_root->flags */
enum {
 Model1_CGRP_ROOT_NOPREFIX = (1 << 1), /* mounted subsystems have no named prefix */
 Model1_CGRP_ROOT_XATTR = (1 << 2), /* supports extended attributes */
};

/* cftype->flags */
enum {
 Model1_CFTYPE_ONLY_ON_ROOT = (1 << 0), /* only create on root cgrp */
 Model1_CFTYPE_NOT_ON_ROOT = (1 << 1), /* don't create on root cgrp */
 Model1_CFTYPE_NO_PREFIX = (1 << 3), /* (DON'T USE FOR NEW FILES) no subsys prefix */
 Model1_CFTYPE_WORLD_WRITABLE = (1 << 4), /* (DON'T USE FOR NEW FILES) S_IWUGO */

 /* internal flags, do not use outside cgroup core proper */
 Model1___CFTYPE_ONLY_ON_DFL = (1 << 16), /* only on default hierarchy */
 Model1___CFTYPE_NOT_ON_DFL = (1 << 17), /* not on default hierarchy */
};

/*
 * cgroup_file is the handle for a file instance created in a cgroup which
 * is used, for example, to generate file changed notifications.  This can
 * be obtained by setting cftype->file_offset.
 */
struct Model1_cgroup_file {
 /* do not access any fields from outside cgroup core */
 struct Model1_kernfs_node *Model1_kn;
};

/*
 * Per-subsystem/per-cgroup state maintained by the system.  This is the
 * fundamental structural building block that controllers deal with.
 *
 * Fields marked with "PI:" are public and immutable and may be accessed
 * directly without synchronization.
 */
struct Model1_cgroup_subsys_state {
 /* PI: the cgroup that this css is attached to */
 struct Model1_cgroup *Model1_cgroup;

 /* PI: the cgroup subsystem that this css is attached to */
 struct Model1_cgroup_subsys *Model1_ss;

 /* reference count - access via css_[try]get() and css_put() */
 struct Model1_percpu_ref Model1_refcnt;

 /* PI: the parent css */
 struct Model1_cgroup_subsys_state *Model1_parent;

 /* siblings list anchored at the parent's ->children */
 struct Model1_list_head Model1_sibling;
 struct Model1_list_head Model1_children;

 /*
	 * PI: Subsys-unique ID.  0 is unused and root is always 1.  The
	 * matching css can be looked up using css_from_id().
	 */
 int Model1_id;

 unsigned int Model1_flags;

 /*
	 * Monotonically increasing unique serial number which defines a
	 * uniform order among all csses.  It's guaranteed that all
	 * ->children lists are in the ascending order of ->serial_nr and
	 * used to allow interrupting and resuming iterations.
	 */
 Model1_u64 Model1_serial_nr;

 /*
	 * Incremented by online self and children.  Used to guarantee that
	 * parents are not offlined before their children.
	 */
 Model1_atomic_t Model1_online_cnt;

 /* percpu_ref killing and RCU release */
 struct Model1_callback_head Model1_callback_head;
 struct Model1_work_struct Model1_destroy_work;
};

/*
 * A css_set is a structure holding pointers to a set of
 * cgroup_subsys_state objects. This saves space in the task struct
 * object and speeds up fork()/exit(), since a single inc/dec and a
 * list_add()/del() can bump the reference count on the entire cgroup
 * set for a task.
 */
struct Model1_css_set {
 /* Reference count */
 Model1_atomic_t Model1_refcount;

 /*
	 * List running through all cgroup groups in the same hash
	 * slot. Protected by css_set_lock
	 */
 struct Model1_hlist_node Model1_hlist;

 /*
	 * Lists running through all tasks using this cgroup group.
	 * mg_tasks lists tasks which belong to this cset but are in the
	 * process of being migrated out or in.  Protected by
	 * css_set_rwsem, but, during migration, once tasks are moved to
	 * mg_tasks, it can be read safely while holding cgroup_mutex.
	 */
 struct Model1_list_head Model1_tasks;
 struct Model1_list_head Model1_mg_tasks;

 /*
	 * List of cgrp_cset_links pointing at cgroups referenced from this
	 * css_set.  Protected by css_set_lock.
	 */
 struct Model1_list_head Model1_cgrp_links;

 /* the default cgroup associated with this css_set */
 struct Model1_cgroup *Model1_dfl_cgrp;

 /*
	 * Set of subsystem states, one for each subsystem. This array is
	 * immutable after creation apart from the init_css_set during
	 * subsystem registration (at boot time).
	 */
 struct Model1_cgroup_subsys_state *Model1_subsys[Model1_CGROUP_SUBSYS_COUNT];

 /*
	 * List of csets participating in the on-going migration either as
	 * source or destination.  Protected by cgroup_mutex.
	 */
 struct Model1_list_head Model1_mg_preload_node;
 struct Model1_list_head Model1_mg_node;

 /*
	 * If this cset is acting as the source of migration the following
	 * two fields are set.  mg_src_cgrp and mg_dst_cgrp are
	 * respectively the source and destination cgroups of the on-going
	 * migration.  mg_dst_cset is the destination cset the target tasks
	 * on this cset should be migrated to.  Protected by cgroup_mutex.
	 */
 struct Model1_cgroup *Model1_mg_src_cgrp;
 struct Model1_cgroup *Model1_mg_dst_cgrp;
 struct Model1_css_set *Model1_mg_dst_cset;

 /*
	 * On the default hierarhcy, ->subsys[ssid] may point to a css
	 * attached to an ancestor instead of the cgroup this css_set is
	 * associated with.  The following node is anchored at
	 * ->subsys[ssid]->cgroup->e_csets[ssid] and provides a way to
	 * iterate through all css's attached to a given cgroup.
	 */
 struct Model1_list_head Model1_e_cset_node[Model1_CGROUP_SUBSYS_COUNT];

 /* all css_task_iters currently walking this cset */
 struct Model1_list_head Model1_task_iters;

 /* dead and being drained, ignore for migration */
 bool Model1_dead;

 /* For RCU-protected deletion */
 struct Model1_callback_head Model1_callback_head;
};

struct Model1_cgroup {
 /* self css with NULL ->ss, points back to this cgroup */
 struct Model1_cgroup_subsys_state Model1_self;

 unsigned long Model1_flags; /* "unsigned long" so bitops work */

 /*
	 * idr allocated in-hierarchy ID.
	 *
	 * ID 0 is not used, the ID of the root cgroup is always 1, and a
	 * new cgroup will be assigned with a smallest available ID.
	 *
	 * Allocating/Removing ID must be protected by cgroup_mutex.
	 */
 int Model1_id;

 /*
	 * The depth this cgroup is at.  The root is at depth zero and each
	 * step down the hierarchy increments the level.  This along with
	 * ancestor_ids[] can determine whether a given cgroup is a
	 * descendant of another without traversing the hierarchy.
	 */
 int Model1_level;

 /*
	 * Each non-empty css_set associated with this cgroup contributes
	 * one to populated_cnt.  All children with non-zero popuplated_cnt
	 * of their own contribute one.  The count is zero iff there's no
	 * task in this cgroup or its subtree.
	 */
 int Model1_populated_cnt;

 struct Model1_kernfs_node *Model1_kn; /* cgroup kernfs entry */
 struct Model1_cgroup_file Model1_procs_file; /* handle for "cgroup.procs" */
 struct Model1_cgroup_file Model1_events_file; /* handle for "cgroup.events" */

 /*
	 * The bitmask of subsystems enabled on the child cgroups.
	 * ->subtree_control is the one configured through
	 * "cgroup.subtree_control" while ->child_ss_mask is the effective
	 * one which may have more subsystems enabled.  Controller knobs
	 * are made available iff it's enabled in ->subtree_control.
	 */
 Model1_u16 Model1_subtree_control;
 Model1_u16 Model1_subtree_ss_mask;
 Model1_u16 Model1_old_subtree_control;
 Model1_u16 Model1_old_subtree_ss_mask;

 /* Private pointers for each registered subsystem */
 struct Model1_cgroup_subsys_state *Model1_subsys[Model1_CGROUP_SUBSYS_COUNT];

 struct Model1_cgroup_root *Model1_root;

 /*
	 * List of cgrp_cset_links pointing at css_sets with tasks in this
	 * cgroup.  Protected by css_set_lock.
	 */
 struct Model1_list_head Model1_cset_links;

 /*
	 * On the default hierarchy, a css_set for a cgroup with some
	 * susbsys disabled will point to css's which are associated with
	 * the closest ancestor which has the subsys enabled.  The
	 * following lists all css_sets which point to this cgroup's css
	 * for the given subsystem.
	 */
 struct Model1_list_head Model1_e_csets[Model1_CGROUP_SUBSYS_COUNT];

 /*
	 * list of pidlists, up to two for each namespace (one for procs, one
	 * for tasks); created on demand.
	 */
 struct Model1_list_head Model1_pidlists;
 struct Model1_mutex Model1_pidlist_mutex;

 /* used to wait for offlining of csses */
 Model1_wait_queue_head_t Model1_offline_waitq;

 /* used to schedule release agent */
 struct Model1_work_struct Model1_release_agent_work;

 /* ids of the ancestors at each level including self */
 int Model1_ancestor_ids[];
};

/*
 * A cgroup_root represents the root of a cgroup hierarchy, and may be
 * associated with a kernfs_root to form an active hierarchy.  This is
 * internal to cgroup core.  Don't access directly from controllers.
 */
struct Model1_cgroup_root {
 struct Model1_kernfs_root *Model1_kf_root;

 /* The bitmask of subsystems attached to this hierarchy */
 unsigned int Model1_subsys_mask;

 /* Unique id for this hierarchy. */
 int Model1_hierarchy_id;

 /* The root cgroup.  Root is destroyed on its release. */
 struct Model1_cgroup Model1_cgrp;

 /* for cgrp->ancestor_ids[0] */
 int Model1_cgrp_ancestor_id_storage;

 /* Number of cgroups in the hierarchy, used only for /proc/cgroups */
 Model1_atomic_t Model1_nr_cgrps;

 /* A list running through the active hierarchies */
 struct Model1_list_head Model1_root_list;

 /* Hierarchy-specific flags */
 unsigned int Model1_flags;

 /* IDs for cgroups in this hierarchy */
 struct Model1_idr Model1_cgroup_idr;

 /* The path to use for release notifications. */
 char Model1_release_agent_path[4096];

 /* The name for this hierarchy - may be empty */
 char Model1_name[64];
};

/*
 * struct cftype: handler definitions for cgroup control files
 *
 * When reading/writing to a file:
 *	- the cgroup to use is file->f_path.dentry->d_parent->d_fsdata
 *	- the 'cftype' of the file is file->f_path.dentry->d_fsdata
 */
struct Model1_cftype {
 /*
	 * By convention, the name should begin with the name of the
	 * subsystem, followed by a period.  Zero length string indicates
	 * end of cftype array.
	 */
 char Model1_name[64];
 unsigned long Model1_private;

 /*
	 * The maximum length of string, excluding trailing nul, that can
	 * be passed to write.  If < PAGE_SIZE-1, PAGE_SIZE-1 is assumed.
	 */
 Model1_size_t Model1_max_write_len;

 /* CFTYPE_* flags */
 unsigned int Model1_flags;

 /*
	 * If non-zero, should contain the offset from the start of css to
	 * a struct cgroup_file field.  cgroup will record the handle of
	 * the created file into it.  The recorded handle can be used as
	 * long as the containing css remains accessible.
	 */
 unsigned int Model1_file_offset;

 /*
	 * Fields used for internal bookkeeping.  Initialized automatically
	 * during registration.
	 */
 struct Model1_cgroup_subsys *Model1_ss; /* NULL for cgroup core files */
 struct Model1_list_head Model1_node; /* anchored at ss->cfts */
 struct Model1_kernfs_ops *Model1_kf_ops;

 /*
	 * read_u64() is a shortcut for the common case of returning a
	 * single integer. Use it in place of read()
	 */
 Model1_u64 (*Model1_read_u64)(struct Model1_cgroup_subsys_state *Model1_css, struct Model1_cftype *Model1_cft);
 /*
	 * read_s64() is a signed version of read_u64()
	 */
 Model1_s64 (*Model1_read_s64)(struct Model1_cgroup_subsys_state *Model1_css, struct Model1_cftype *Model1_cft);

 /* generic seq_file read interface */
 int (*Model1_seq_show)(struct Model1_seq_file *Model1_sf, void *Model1_v);

 /* optional ops, implement all or none */
 void *(*Model1_seq_start)(struct Model1_seq_file *Model1_sf, Model1_loff_t *Model1_ppos);
 void *(*Model1_seq_next)(struct Model1_seq_file *Model1_sf, void *Model1_v, Model1_loff_t *Model1_ppos);
 void (*Model1_seq_stop)(struct Model1_seq_file *Model1_sf, void *Model1_v);

 /*
	 * write_u64() is a shortcut for the common case of accepting
	 * a single integer (as parsed by simple_strtoull) from
	 * userspace. Use in place of write(); return 0 or error.
	 */
 int (*Model1_write_u64)(struct Model1_cgroup_subsys_state *Model1_css, struct Model1_cftype *Model1_cft,
    Model1_u64 Model1_val);
 /*
	 * write_s64() is a signed version of write_u64()
	 */
 int (*Model1_write_s64)(struct Model1_cgroup_subsys_state *Model1_css, struct Model1_cftype *Model1_cft,
    Model1_s64 Model1_val);

 /*
	 * write() is the generic write callback which maps directly to
	 * kernfs write operation and overrides all other operations.
	 * Maximum write size is determined by ->max_write_len.  Use
	 * of_css/cft() to access the associated css and cft.
	 */
 Model1_ssize_t (*Model1_write)(struct Model1_kernfs_open_file *Model1_of,
    char *Model1_buf, Model1_size_t Model1_nbytes, Model1_loff_t Model1_off);




};

/*
 * Control Group subsystem type.
 * See Documentation/cgroups/cgroups.txt for details
 */
struct Model1_cgroup_subsys {
 struct Model1_cgroup_subsys_state *(*Model1_css_alloc)(struct Model1_cgroup_subsys_state *Model1_parent_css);
 int (*Model1_css_online)(struct Model1_cgroup_subsys_state *Model1_css);
 void (*Model1_css_offline)(struct Model1_cgroup_subsys_state *Model1_css);
 void (*Model1_css_released)(struct Model1_cgroup_subsys_state *Model1_css);
 void (*Model1_css_free)(struct Model1_cgroup_subsys_state *Model1_css);
 void (*Model1_css_reset)(struct Model1_cgroup_subsys_state *Model1_css);

 int (*Model1_can_attach)(struct Model1_cgroup_taskset *Model1_tset);
 void (*Model1_cancel_attach)(struct Model1_cgroup_taskset *Model1_tset);
 void (*Model1_attach)(struct Model1_cgroup_taskset *Model1_tset);
 void (*Model1_post_attach)(void);
 int (*Model1_can_fork)(struct Model1_task_struct *Model1_task);
 void (*Model1_cancel_fork)(struct Model1_task_struct *Model1_task);
 void (*Model1_fork)(struct Model1_task_struct *Model1_task);
 void (*Model1_exit)(struct Model1_task_struct *Model1_task);
 void (*Model1_free)(struct Model1_task_struct *Model1_task);
 void (*Model1_bind)(struct Model1_cgroup_subsys_state *Model1_root_css);

 bool Model1_early_init:1;

 /*
	 * If %true, the controller, on the default hierarchy, doesn't show
	 * up in "cgroup.controllers" or "cgroup.subtree_control", is
	 * implicitly enabled on all cgroups on the default hierarchy, and
	 * bypasses the "no internal process" constraint.  This is for
	 * utility type controllers which is transparent to userland.
	 *
	 * An implicit controller can be stolen from the default hierarchy
	 * anytime and thus must be okay with offline csses from previous
	 * hierarchies coexisting with csses for the current one.
	 */
 bool Model1_implicit_on_dfl:1;

 /*
	 * If %false, this subsystem is properly hierarchical -
	 * configuration, resource accounting and restriction on a parent
	 * cgroup cover those of its children.  If %true, hierarchy support
	 * is broken in some ways - some subsystems ignore hierarchy
	 * completely while others are only implemented half-way.
	 *
	 * It's now disallowed to create nested cgroups if the subsystem is
	 * broken and cgroup core will emit a warning message on such
	 * cases.  Eventually, all subsystems will be made properly
	 * hierarchical and this will go away.
	 */
 bool Model1_broken_hierarchy:1;
 bool Model1_warned_broken_hierarchy:1;

 /* the following two fields are initialized automtically during boot */
 int Model1_id;
 const char *Model1_name;

 /* optional, initialized automatically during boot if not set */
 const char *Model1_legacy_name;

 /* link to parent, protected by cgroup_lock() */
 struct Model1_cgroup_root *Model1_root;

 /* idr for css->id */
 struct Model1_idr Model1_css_idr;

 /*
	 * List of cftypes.  Each entry is the first entry of an array
	 * terminated by zero length name.
	 */
 struct Model1_list_head Model1_cfts;

 /*
	 * Base cftypes which are automatically registered.  The two can
	 * point to the same array.
	 */
 struct Model1_cftype *Model1_dfl_cftypes; /* for the default hierarchy */
 struct Model1_cftype *Model1_legacy_cftypes; /* for the legacy hierarchies */

 /*
	 * A subsystem may depend on other subsystems.  When such subsystem
	 * is enabled on a cgroup, the depended-upon subsystems are enabled
	 * together if available.  Subsystems enabled due to dependency are
	 * not visible to userland until explicitly enabled.  The following
	 * specifies the mask of subsystems that this one depends on.
	 */
 unsigned int Model1_depends_on;
};

extern struct Model1_percpu_rw_semaphore Model1_cgroup_threadgroup_rwsem;

/**
 * cgroup_threadgroup_change_begin - threadgroup exclusion for cgroups
 * @tsk: target task
 *
 * Called from threadgroup_change_begin() and allows cgroup operations to
 * synchronize against threadgroup changes using a percpu_rw_semaphore.
 */
static inline __attribute__((no_instrument_function)) void Model1_cgroup_threadgroup_change_begin(struct Model1_task_struct *Model1_tsk)
{
 Model1_percpu_down_read(&Model1_cgroup_threadgroup_rwsem);
}

/**
 * cgroup_threadgroup_change_end - threadgroup exclusion for cgroups
 * @tsk: target task
 *
 * Called from threadgroup_change_end().  Counterpart of
 * cgroup_threadcgroup_change_begin().
 */
static inline __attribute__((no_instrument_function)) void Model1_cgroup_threadgroup_change_end(struct Model1_task_struct *Model1_tsk)
{
 Model1_percpu_up_read(&Model1_cgroup_threadgroup_rwsem);
}
struct Model1_sock_cgroup_data {
};





/*
 * Extended scheduling parameters data structure.
 *
 * This is needed because the original struct sched_param can not be
 * altered without introducing ABI issues with legacy applications
 * (e.g., in sched_getparam()).
 *
 * However, the possibility of specifying more than just a priority for
 * the tasks may be useful for a wide variety of application fields, e.g.,
 * multimedia, streaming, automation and control, and many others.
 *
 * This variant (sched_attr) is meant at describing a so-called
 * sporadic time-constrained task. In such model a task is specified by:
 *  - the activation period or minimum instance inter-arrival time;
 *  - the maximum (or average, depending on the actual scheduling
 *    discipline) computation time of all instances, a.k.a. runtime;
 *  - the deadline (relative to the actual activation time) of each
 *    instance.
 * Very briefly, a periodic (sporadic) task asks for the execution of
 * some specific computation --which is typically called an instance--
 * (at most) every period. Moreover, each instance typically lasts no more
 * than the runtime and must be completed by time instant t equal to
 * the instance activation time + the deadline.
 *
 * This is reflected by the actual fields of the sched_attr structure:
 *
 *  @size		size of the structure, for fwd/bwd compat.
 *
 *  @sched_policy	task's scheduling policy
 *  @sched_flags	for customizing the scheduler behaviour
 *  @sched_nice		task's nice value      (SCHED_NORMAL/BATCH)
 *  @sched_priority	task's static priority (SCHED_FIFO/RR)
 *  @sched_deadline	representative of the task's deadline
 *  @sched_runtime	representative of the task's runtime
 *  @sched_period	representative of the task's period
 *
 * Given this task model, there are a multiplicity of scheduling algorithms
 * and policies, that can be used to ensure all the tasks will make their
 * timing constraints.
 *
 * As of now, the SCHED_DEADLINE policy (sched_dl scheduling class) is the
 * only user of this new interface. More information about the algorithm
 * available in the scheduling class file or in Documentation/.
 */
struct Model1_sched_attr {
 Model1_u32 Model1_size;

 Model1_u32 Model1_sched_policy;
 Model1_u64 Model1_sched_flags;

 /* SCHED_NORMAL, SCHED_BATCH */
 Model1_s32 Model1_sched_nice;

 /* SCHED_FIFO, SCHED_RR */
 Model1_u32 Model1_sched_priority;

 /* SCHED_DEADLINE */
 Model1_u64 Model1_sched_runtime;
 Model1_u64 Model1_sched_deadline;
 Model1_u64 Model1_sched_period;
};

struct Model1_futex_pi_state;
struct Model1_robust_list_head;
struct Model1_bio_list;
struct Model1_fs_struct;
struct Model1_perf_event_context;
struct Model1_blk_plug;
struct Model1_filename;
struct Model1_nameidata;





/*
 * These are the constant used to fake the fixed-point load-average
 * counting. Some notes:
 *  - 11 bit fractions expand to 22 bits by the multiplies: this gives
 *    a load-average precision of 10 bits integer + 11 bits fractional
 *  - if you want to count load-averages more often, you need more
 *    precision, or rounding will get you. With 2-second counting freq,
 *    the EXP_n values would be 1981, 2034 and 2043 if still using only
 *    11 bit fractions.
 */
extern unsigned long Model1_avenrun[]; /* Load averages */
extern void Model1_get_avenrun(unsigned long *Model1_loads, unsigned long Model1_offset, int Model1_shift);
extern unsigned long Model1_total_forks;
extern int Model1_nr_threads;
extern __attribute__((section(".data..percpu" ""))) __typeof__(unsigned long) Model1_process_counts;
extern int Model1_nr_processes(void);
extern unsigned long Model1_nr_running(void);
extern bool Model1_single_task_running(void);
extern unsigned long Model1_nr_iowait(void);
extern unsigned long Model1_nr_iowait_cpu(int Model1_cpu);
extern void Model1_get_iowait_load(unsigned long *Model1_nr_waiters, unsigned long *Model1_load);

extern void Model1_calc_global_load(unsigned long Model1_ticks);


extern void Model1_cpu_load_update_nohz_start(void);
extern void Model1_cpu_load_update_nohz_stop(void);





extern void Model1_dump_cpu_task(int Model1_cpu);

struct Model1_seq_file;
struct Model1_cfs_rq;
struct Model1_task_group;





/*
 * Task state bitmask. NOTE! These bits are also
 * encoded in fs/proc/array.c: get_task_state().
 *
 * We have two separate sets of flags: task->state
 * is about runnability, while task->exit_state are
 * about the task exiting. Confusing, but this way
 * modifying one set can't modify the other one by
 * mistake.
 */





/* in tsk->exit_state */



/* in tsk->state again */
extern char Model1____assert_task_state[1 - 2*!!(
  sizeof("RSDTtXZxKWPNn")-1 != ( __builtin_constant_p(4096) ? ( (4096) < 1 ? Model1_____ilog2_NaN() : (4096) & (1ULL << 63) ? 63 : (4096) & (1ULL << 62) ? 62 : (4096) & (1ULL << 61) ? 61 : (4096) & (1ULL << 60) ? 60 : (4096) & (1ULL << 59) ? 59 : (4096) & (1ULL << 58) ? 58 : (4096) & (1ULL << 57) ? 57 : (4096) & (1ULL << 56) ? 56 : (4096) & (1ULL << 55) ? 55 : (4096) & (1ULL << 54) ? 54 : (4096) & (1ULL << 53) ? 53 : (4096) & (1ULL << 52) ? 52 : (4096) & (1ULL << 51) ? 51 : (4096) & (1ULL << 50) ? 50 : (4096) & (1ULL << 49) ? 49 : (4096) & (1ULL << 48) ? 48 : (4096) & (1ULL << 47) ? 47 : (4096) & (1ULL << 46) ? 46 : (4096) & (1ULL << 45) ? 45 : (4096) & (1ULL << 44) ? 44 : (4096) & (1ULL << 43) ? 43 : (4096) & (1ULL << 42) ? 42 : (4096) & (1ULL << 41) ? 41 : (4096) & (1ULL << 40) ? 40 : (4096) & (1ULL << 39) ? 39 : (4096) & (1ULL << 38) ? 38 : (4096) & (1ULL << 37) ? 37 : (4096) & (1ULL << 36) ? 36 : (4096) & (1ULL << 35) ? 35 : (4096) & (1ULL << 34) ? 34 : (4096) & (1ULL << 33) ? 33 : (4096) & (1ULL << 32) ? 32 : (4096) & (1ULL << 31) ? 31 : (4096) & (1ULL << 30) ? 30 : (4096) & (1ULL << 29) ? 29 : (4096) & (1ULL << 28) ? 28 : (4096) & (1ULL << 27) ? 27 : (4096) & (1ULL << 26) ? 26 : (4096) & (1ULL << 25) ? 25 : (4096) & (1ULL << 24) ? 24 : (4096) & (1ULL << 23) ? 23 : (4096) & (1ULL << 22) ? 22 : (4096) & (1ULL << 21) ? 21 : (4096) & (1ULL << 20) ? 20 : (4096) & (1ULL << 19) ? 19 : (4096) & (1ULL << 18) ? 18 : (4096) & (1ULL << 17) ? 17 : (4096) & (1ULL << 16) ? 16 : (4096) & (1ULL << 15) ? 15 : (4096) & (1ULL << 14) ? 14 : (4096) & (1ULL << 13) ? 13 : (4096) & (1ULL << 12) ? 12 : (4096) & (1ULL << 11) ? 11 : (4096) & (1ULL << 10) ? 10 : (4096) & (1ULL << 9) ? 9 : (4096) & (1ULL << 8) ? 8 : (4096) & (1ULL << 7) ? 7 : (4096) & (1ULL << 6) ? 6 : (4096) & (1ULL << 5) ? 5 : (4096) & (1ULL << 4) ? 4 : (4096) & (1ULL << 3) ? 3 : (4096) & (1ULL << 2) ? 2 : (4096) & (1ULL << 1) ? 1 : (4096) & (1ULL << 0) ? 0 : Model1_____ilog2_NaN() ) : (sizeof(4096) <= 4) ? Model1___ilog2_u32(4096) : Model1___ilog2_u64(4096) )+1)];

/* Convenience macros for the sake of set_task_state */






/* Convenience macros for the sake of wake_up */



/* get_task_state() */
/*
 * set_current_state() includes a barrier so that the write of current->state
 * is correctly serialised wrt the caller's subsequent test of whether to
 * actually sleep:
 *
 *	set_current_state(TASK_UNINTERRUPTIBLE);
 *	if (do_i_need_to_sleep())
 *		schedule();
 *
 * If the caller does not need such serialisation then use __set_current_state()
 */







/* Task command name length */




/*
 * This serializes "schedule()" and also protects
 * the run-queue from deletions/modifications (but
 * _adding_ to the beginning of the run-queue has
 * a separate lock).
 */
extern Model1_rwlock_t Model1_tasklist_lock;
extern Model1_spinlock_t Model1_mmlist_lock;

struct Model1_task_struct;





extern void Model1_sched_init(void);
extern void Model1_sched_init_smp(void);
extern void Model1_schedule_tail(struct Model1_task_struct *Model1_prev);
extern void Model1_init_idle(struct Model1_task_struct *Model1_idle, int Model1_cpu);
extern void Model1_init_idle_bootup_task(struct Model1_task_struct *Model1_idle);

extern Model1_cpumask_var_t Model1_cpu_isolated_map;

extern int Model1_runqueue_is_locked(int Model1_cpu);


extern void Model1_nohz_balance_enter_idle(int Model1_cpu);
extern void Model1_set_cpu_sd_state_idle(void);
extern int Model1_get_nohz_timer_target(void);





/*
 * Only dump TASK_* tasks. (0 for all tasks)
 */
extern void Model1_show_state_filter(unsigned long Model1_state_filter);

static inline __attribute__((no_instrument_function)) void Model1_show_state(void)
{
 Model1_show_state_filter(0);
}

extern void Model1_show_regs(struct Model1_pt_regs *);

/*
 * TASK is a pointer to the task whose backtrace we want to see (or NULL for current
 * task), SP is the stack pointer of the first frame that should be shown in the back
 * trace (or NULL if the entire call-chain of the task should be shown).
 */
extern void Model1_show_stack(struct Model1_task_struct *Model1_task, unsigned long *Model1_sp);

extern void Model1_cpu_init (void);
extern void Model1_trap_init(void);
extern void Model1_update_process_times(int Model1_user);
extern void Model1_scheduler_tick(void);
extern int Model1_sched_cpu_starting(unsigned int Model1_cpu);
extern int Model1_sched_cpu_activate(unsigned int Model1_cpu);
extern int Model1_sched_cpu_deactivate(unsigned int Model1_cpu);


extern int Model1_sched_cpu_dying(unsigned int Model1_cpu);




extern void Model1_sched_show_task(struct Model1_task_struct *Model1_p);
static inline __attribute__((no_instrument_function)) void Model1_touch_softlockup_watchdog_sched(void)
{
}
static inline __attribute__((no_instrument_function)) void Model1_touch_softlockup_watchdog(void)
{
}
static inline __attribute__((no_instrument_function)) void Model1_touch_softlockup_watchdog_sync(void)
{
}
static inline __attribute__((no_instrument_function)) void Model1_touch_all_softlockup_watchdogs(void)
{
}
static inline __attribute__((no_instrument_function)) void Model1_lockup_detector_init(void)
{
}





static inline __attribute__((no_instrument_function)) void Model1_reset_hung_task_detector(void)
{
}


/* Attach to any functions which should be ignored in wchan output. */


/* Linker adds these: start and end of __sched functions */
extern char Model1___sched_text_start[], Model1___sched_text_end[];

/* Is this address in the __sched functions? */
extern int Model1_in_sched_functions(unsigned long Model1_addr);


extern signed long Model1_schedule_timeout(signed long Model1_timeout);
extern signed long Model1_schedule_timeout_interruptible(signed long Model1_timeout);
extern signed long Model1_schedule_timeout_killable(signed long Model1_timeout);
extern signed long Model1_schedule_timeout_uninterruptible(signed long Model1_timeout);
extern signed long Model1_schedule_timeout_idle(signed long Model1_timeout);
           void Model1_schedule(void);
extern void Model1_schedule_preempt_disabled(void);

extern long Model1_io_schedule_timeout(long Model1_timeout);

static inline __attribute__((no_instrument_function)) void Model1_io_schedule(void)
{
 Model1_io_schedule_timeout(((long)(~0UL>>1)));
}

struct Model1_nsproxy;
struct Model1_user_namespace;


extern void Model1_arch_pick_mmap_layout(struct Model1_mm_struct *Model1_mm);
extern unsigned long
Model1_arch_get_unmapped_area(struct Model1_file *, unsigned long, unsigned long,
         unsigned long, unsigned long);
extern unsigned long
Model1_arch_get_unmapped_area_topdown(struct Model1_file *Model1_filp, unsigned long Model1_addr,
     unsigned long Model1_len, unsigned long Model1_pgoff,
     unsigned long Model1_flags);
/* mm flags */

/* for SUID_DUMP_* above */



extern void Model1_set_dumpable(struct Model1_mm_struct *Model1_mm, int Model1_value);
/*
 * This returns the actual value of the suid_dumpable flag. For things
 * that are using this for checking for privilege transitions, it must
 * test against SUID_DUMP_USER rather than treating it as a boolean
 * value.
 */
static inline __attribute__((no_instrument_function)) int Model1___get_dumpable(unsigned long Model1_mm_flags)
{
 return Model1_mm_flags & ((1 << 2) - 1);
}

static inline __attribute__((no_instrument_function)) int Model1_get_dumpable(struct Model1_mm_struct *Model1_mm)
{
 return Model1___get_dumpable(Model1_mm->Model1_flags);
}

/* coredump filter bits */
     /* leave room for more dump flags */
struct Model1_sighand_struct {
 Model1_atomic_t Model1_count;
 struct Model1_k_sigaction Model1_action[64];
 Model1_spinlock_t Model1_siglock;
 Model1_wait_queue_head_t Model1_signalfd_wqh;
};

struct Model1_pacct_struct {
 int Model1_ac_flag;
 long Model1_ac_exitcode;
 unsigned long Model1_ac_mem;
 Model1_cputime_t Model1_ac_utime, Model1_ac_stime;
 unsigned long Model1_ac_minflt, Model1_ac_majflt;
};

struct Model1_cpu_itimer {
 Model1_cputime_t Model1_expires;
 Model1_cputime_t Model1_incr;
 Model1_u32 error;
 Model1_u32 Model1_incr_error;
};

/**
 * struct prev_cputime - snaphsot of system and user cputime
 * @utime: time spent in user mode
 * @stime: time spent in system mode
 * @lock: protects the above two fields
 *
 * Stores previous user/system time values such that we can guarantee
 * monotonicity.
 */
struct Model1_prev_cputime {

 Model1_cputime_t Model1_utime;
 Model1_cputime_t Model1_stime;
 Model1_raw_spinlock_t Model1_lock;

};

static inline __attribute__((no_instrument_function)) void Model1_prev_cputime_init(struct Model1_prev_cputime *Model1_prev)
{

 Model1_prev->Model1_utime = Model1_prev->Model1_stime = 0;
 do { *(&Model1_prev->Model1_lock) = (Model1_raw_spinlock_t) { .Model1_raw_lock = { { (0) } }, }; } while (0);

}

/**
 * struct task_cputime - collected CPU time counts
 * @utime:		time spent in user mode, in &cputime_t units
 * @stime:		time spent in kernel mode, in &cputime_t units
 * @sum_exec_runtime:	total time spent on the CPU, in nanoseconds
 *
 * This structure groups together three kinds of CPU time that are tracked for
 * threads and thread groups.  Most things considering CPU time want to group
 * these counts together and treat all three of them in parallel.
 */
struct Model1_task_cputime {
 Model1_cputime_t Model1_utime;
 Model1_cputime_t Model1_stime;
 unsigned long long Model1_sum_exec_runtime;
};

/* Alternate field names when used to cache expirations. */
/*
 * This is the atomic variant of task_cputime, which can be used for
 * storing and updating task_cputime statistics without locking.
 */
struct Model1_task_cputime_atomic {
 Model1_atomic64_t Model1_utime;
 Model1_atomic64_t Model1_stime;
 Model1_atomic64_t Model1_sum_exec_runtime;
};
/*
 * Disable preemption until the scheduler is running -- use an unconditional
 * value so that it also works on !PREEMPT_COUNT kernels.
 *
 * Reset by start_kernel()->sched_init()->init_idle()->init_idle_preempt_count().
 */


/*
 * Initial preempt_count value; reflects the preempt_count schedule invariant
 * which states that during context switches:
 *
 *    preempt_count() == 2*PREEMPT_DISABLE_OFFSET
 *
 * Note: PREEMPT_DISABLE_OFFSET is 0 for !PREEMPT_COUNT kernels.
 * Note: See finish_task_switch().
 */


/**
 * struct thread_group_cputimer - thread group interval timer counts
 * @cputime_atomic:	atomic thread group interval timers.
 * @running:		true when there are timers running and
 *			@cputime_atomic receives updates.
 * @checking_timer:	true when a thread in the group is in the
 *			process of checking for thread group timers.
 *
 * This structure contains the version of task_cputime, above, that is
 * used for thread group CPU timer calculations.
 */
struct Model1_thread_group_cputimer {
 struct Model1_task_cputime_atomic Model1_cputime_atomic;
 bool Model1_running;
 bool Model1_checking_timer;
};


struct Model1_autogroup;

/*
 * NOTE! "signal_struct" does not have its own
 * locking, because a shared signal_struct always
 * implies a shared sighand_struct, so locking
 * sighand_struct is always a proper superset of
 * the locking of signal_struct.
 */
struct Model1_signal_struct {
 Model1_atomic_t Model1_sigcnt;
 Model1_atomic_t Model1_live;
 int Model1_nr_threads;
 Model1_atomic_t Model1_oom_victims; /* # of TIF_MEDIE threads in this thread group */
 struct Model1_list_head Model1_thread_head;

 Model1_wait_queue_head_t Model1_wait_chldexit; /* for wait4() */

 /* current thread group signal load-balancing target: */
 struct Model1_task_struct *Model1_curr_target;

 /* shared signal handling: */
 struct Model1_sigpending Model1_shared_pending;

 /* thread group exit support */
 int Model1_group_exit_code;
 /* overloaded:
	 * - notify group_exit_task when ->count is equal to notify_count
	 * - everyone except group_exit_task is stopped during signal delivery
	 *   of fatal signals, group_exit_task processes the signal.
	 */
 int Model1_notify_count;
 struct Model1_task_struct *Model1_group_exit_task;

 /* thread group stop support, overloads group_exit_code too */
 int Model1_group_stop_count;
 unsigned int Model1_flags; /* see SIGNAL_* flags below */

 /*
	 * PR_SET_CHILD_SUBREAPER marks a process, like a service
	 * manager, to re-parent orphan (double-forking) child processes
	 * to this process instead of 'init'. The service manager is
	 * able to receive SIGCHLD signals and is able to investigate
	 * the process until it calls wait(). All children of this
	 * process will inherit a flag if they should look for a
	 * child_subreaper process at exit.
	 */
 unsigned int Model1_is_child_subreaper:1;
 unsigned int Model1_has_child_subreaper:1;

 /* POSIX.1b Interval Timers */
 int Model1_posix_timer_id;
 struct Model1_list_head Model1_posix_timers;

 /* ITIMER_REAL timer for the process */
 struct Model1_hrtimer Model1_real_timer;
 struct Model1_pid *Model1_leader_pid;
 Model1_ktime_t Model1_it_real_incr;

 /*
	 * ITIMER_PROF and ITIMER_VIRTUAL timers for the process, we use
	 * CPUCLOCK_PROF and CPUCLOCK_VIRT for indexing array as these
	 * values are defined to 0 and 1 respectively
	 */
 struct Model1_cpu_itimer Model1_it[2];

 /*
	 * Thread group totals for process CPU timers.
	 * See thread_group_cputimer(), et al, for details.
	 */
 struct Model1_thread_group_cputimer Model1_cputimer;

 /* Earliest-expiration cache. */
 struct Model1_task_cputime Model1_cputime_expires;





 struct Model1_list_head Model1_cpu_timers[3];

 struct Model1_pid *Model1_tty_old_pgrp;

 /* boolean value for session group leader */
 int Model1_leader;

 struct Model1_tty_struct *Model1_tty; /* NULL if no tty */




 /*
	 * Cumulative resource counters for dead threads in the group,
	 * and for reaped dead child processes forked by this group.
	 * Live threads maintain their own counters and add to these
	 * in __exit_signal, except for the group leader.
	 */
 Model1_seqlock_t Model1_stats_lock;
 Model1_cputime_t Model1_utime, Model1_stime, Model1_cutime, Model1_cstime;
 Model1_cputime_t Model1_gtime;
 Model1_cputime_t Model1_cgtime;
 struct Model1_prev_cputime Model1_prev_cputime;
 unsigned long Model1_nvcsw, Model1_nivcsw, Model1_cnvcsw, Model1_cnivcsw;
 unsigned long Model1_min_flt, Model1_maj_flt, Model1_cmin_flt, Model1_cmaj_flt;
 unsigned long Model1_inblock, Model1_oublock, Model1_cinblock, Model1_coublock;
 unsigned long Model1_maxrss, Model1_cmaxrss;
 struct Model1_task_io_accounting Model1_ioac;

 /*
	 * Cumulative ns of schedule CPU time fo dead threads in the
	 * group, not including a zombie group leader, (This only differs
	 * from jiffies_to_ns(utime + stime) if sched_clock uses something
	 * other than jiffies.)
	 */
 unsigned long long Model1_sum_sched_runtime;

 /*
	 * We don't bother to synchronize most readers of this at all,
	 * because there is no reader checking a limit that actually needs
	 * to get both rlim_cur and rlim_max atomically, and either one
	 * alone is a single word that can safely be read normally.
	 * getrlimit/setrlimit use task_lock(current->group_leader) to
	 * protect this instead of the siglock, because they really
	 * have no need to disable irqs.
	 */
 struct Model1_rlimit Model1_rlim[16];


 struct Model1_pacct_struct Model1_pacct; /* per-process accounting information */


 struct Model1_taskstats *Model1_stats;


 unsigned Model1_audit_tty;
 struct Model1_tty_audit_buf *Model1_tty_audit_buf;


 /*
	 * Thread is the potential origin of an oom condition; kill first on
	 * oom
	 */
 bool Model1_oom_flag_origin;
 short Model1_oom_score_adj; /* OOM kill score adjustment */
 short Model1_oom_score_adj_min; /* OOM kill score adjustment min value.
					 * Only settable by CAP_SYS_RESOURCE. */

 struct Model1_mutex Model1_cred_guard_mutex; /* guard against foreign influences on
					 * credential calculations
					 * (notably. ptrace) */
};

/*
 * Bits in flags field of signal_struct.
 */




/*
 * Pending notifications to parent.
 */






/* If true, all threads except ->group_exit_task have pending SIGKILL */
static inline __attribute__((no_instrument_function)) int Model1_signal_group_exit(const struct Model1_signal_struct *Model1_sig)
{
 return (Model1_sig->Model1_flags & 0x00000004) ||
  (Model1_sig->Model1_group_exit_task != ((void *)0));
}

/*
 * Some day this will be a full-fledged user tracking system..
 */
struct Model1_user_struct {
 Model1_atomic_t Model1___count; /* reference count */
 Model1_atomic_t Model1_processes; /* How many processes does this user have? */
 Model1_atomic_t Model1_sigpending; /* How many pending signals does this user have? */

 Model1_atomic_t Model1_inotify_watches; /* How many inotify watches does this user have? */
 Model1_atomic_t Model1_inotify_devs; /* How many inotify devs does this user have opened? */





 Model1_atomic_long_t Model1_epoll_watches; /* The number of file descriptors currently watched */


 /* protected by mq_lock	*/
 unsigned long Model1_mq_bytes; /* How many bytes can be allocated to mqueue? */

 unsigned long Model1_locked_shm; /* How many pages of mlocked shm ? */
 unsigned long Model1_unix_inflight; /* How many files in flight in unix sockets */
 Model1_atomic_long_t Model1_pipe_bufs; /* how many pages are allocated in pipe buffers */


 struct Model1_key *Model1_uid_keyring; /* UID specific keyring */
 struct Model1_key *Model1_session_keyring; /* UID's default session keyring */


 /* Hash table maintenance information */
 struct Model1_hlist_node Model1_uidhash_node;
 Model1_kuid_t Model1_uid;


 Model1_atomic_long_t Model1_locked_vm;

};

extern int Model1_uids_sysfs_init(void);

extern struct Model1_user_struct *Model1_find_user(Model1_kuid_t);

extern struct Model1_user_struct Model1_root_user;



struct Model1_backing_dev_info;
struct Model1_reclaim_state;


struct Model1_sched_info {
 /* cumulative counters */
 unsigned long Model1_pcount; /* # of times run on this cpu */
 unsigned long long Model1_run_delay; /* time spent waiting on a runqueue */

 /* timestamps */
 unsigned long long Model1_last_arrival,/* when we last ran on a cpu */
      Model1_last_queued; /* when we were last queued to run */
};



struct Model1_task_delay_info {
 Model1_spinlock_t Model1_lock;
 unsigned int Model1_flags; /* Private per-task flags */

 /* For each stat XXX, add following, aligned appropriately
	 *
	 * struct timespec XXX_start, XXX_end;
	 * u64 XXX_delay;
	 * u32 XXX_count;
	 *
	 * Atomicity of updates to XXX_delay, XXX_count protected by
	 * single lock above (split into XXX_lock if contention is an issue).
	 */

 /*
	 * XXX_count is incremented on every XXX operation, the delay
	 * associated with the operation is added to XXX_delay.
	 * XXX_delay contains the accumulated delay time in nanoseconds.
	 */
 Model1_u64 Model1_blkio_start; /* Shared by blkio, swapin */
 Model1_u64 Model1_blkio_delay; /* wait for sync block io completion */
 Model1_u64 Model1_swapin_delay; /* wait for swapin block io completion */
 Model1_u32 Model1_blkio_count; /* total count of the number of sync block */
    /* io operations performed */
 Model1_u32 Model1_swapin_count; /* total count of the number of swapin block */
    /* io operations performed */

 Model1_u64 Model1_freepages_start;
 Model1_u64 Model1_freepages_delay; /* wait for memory reclaim */
 Model1_u32 Model1_freepages_count; /* total count of memory reclaim */
};


static inline __attribute__((no_instrument_function)) int Model1_sched_info_on(void)
{

 return 1;






}


void Model1_force_schedstat_enabled(void);


enum Model1_cpu_idle_type {
 Model1_CPU_IDLE,
 Model1_CPU_NOT_IDLE,
 Model1_CPU_NEWLY_IDLE,
 Model1_CPU_MAX_IDLE_TYPES
};

/*
 * Integer metrics need fixed point arithmetic, e.g., sched/fair
 * has a few: load, load_avg, util_avg, freq, and capacity.
 *
 * We define a basic fixed point arithmetic range, and then formalize
 * all these metrics based on that basic range.
 */



/*
 * Increase resolution of cpu_capacity calculations
 */



/*
 * Wake-queues are lists of tasks with a pending wakeup, whose
 * callers have already marked the task as woken internally,
 * and can thus carry on. A common use case is being able to
 * do the wakeups once the corresponding user lock as been
 * released.
 *
 * We hold reference to each task in the list across the wakeup,
 * thus guaranteeing that the memory is still valid by the time
 * the actual wakeups are performed in wake_up_q().
 *
 * One per task suffices, because there's never a need for a task to be
 * in two wake queues simultaneously; it is forbidden to abandon a task
 * in a wake queue (a call to wake_up_q() _must_ follow), so if a task is
 * already in a wake queue, the wakeup will happen soon and the second
 * waker can just skip it.
 *
 * The WAKE_Q macro declares and initializes the list head.
 * wake_up_q() does NOT reinitialize the list; it's expected to be
 * called near the end of a function, where the fact that the queue is
 * not used again will be easy to see by inspection.
 *
 * Note that this can cause spurious wakeups. schedule() callers
 * must ensure the call is done inside a loop, confirming that the
 * wakeup condition has in fact occurred.
 */
struct Model1_wake_q_node {
 struct Model1_wake_q_node *Model1_next;
};

struct Model1_wake_q_head {
 struct Model1_wake_q_node *Model1_first;
 struct Model1_wake_q_node **Model1_lastp;
};






extern void Model1_wake_q_add(struct Model1_wake_q_head *Model1_head,
         struct Model1_task_struct *Model1_task);
extern void Model1_wake_up_q(struct Model1_wake_q_head *Model1_head);

/*
 * sched-domains (multiprocessor balancing) declarations:
 */
static inline __attribute__((no_instrument_function)) int Model1_cpu_smt_flags(void)
{
 return 0x0080 | 0x0200;
}



static inline __attribute__((no_instrument_function)) int Model1_cpu_core_flags(void)
{
 return 0x0200;
}



static inline __attribute__((no_instrument_function)) int Model1_cpu_numa_flags(void)
{
 return 0x4000;
}


struct Model1_sched_domain_attr {
 int Model1_relax_domain_level;
};





extern int Model1_sched_domain_level_max;

struct Model1_sched_group;

struct Model1_sched_domain {
 /* These fields must be setup */
 struct Model1_sched_domain *Model1_parent; /* top domain must be null terminated */
 struct Model1_sched_domain *Model1_child; /* bottom domain must be null terminated */
 struct Model1_sched_group *Model1_groups; /* the balancing groups of the domain */
 unsigned long Model1_min_interval; /* Minimum balance interval ms */
 unsigned long Model1_max_interval; /* Maximum balance interval ms */
 unsigned int Model1_busy_factor; /* less balancing by factor if busy */
 unsigned int Model1_imbalance_pct; /* No balance until over watermark */
 unsigned int Model1_cache_nice_tries; /* Leave cache hot tasks for # tries */
 unsigned int Model1_busy_idx;
 unsigned int Model1_idle_idx;
 unsigned int Model1_newidle_idx;
 unsigned int Model1_wake_idx;
 unsigned int Model1_forkexec_idx;
 unsigned int Model1_smt_gain;

 int Model1_nohz_idle; /* NOHZ IDLE status */
 int Model1_flags; /* See SD_* */
 int Model1_level;

 /* Runtime fields. */
 unsigned long Model1_last_balance; /* init to jiffies. units in jiffies */
 unsigned int Model1_balance_interval; /* initialise to 1. units in ms. */
 unsigned int Model1_nr_balance_failed; /* initialise to 0 */

 /* idle_balance() stats */
 Model1_u64 Model1_max_newidle_lb_cost;
 unsigned long Model1_next_decay_max_lb_cost;


 /* load_balance() stats */
 unsigned int Model1_lb_count[Model1_CPU_MAX_IDLE_TYPES];
 unsigned int Model1_lb_failed[Model1_CPU_MAX_IDLE_TYPES];
 unsigned int Model1_lb_balanced[Model1_CPU_MAX_IDLE_TYPES];
 unsigned int Model1_lb_imbalance[Model1_CPU_MAX_IDLE_TYPES];
 unsigned int Model1_lb_gained[Model1_CPU_MAX_IDLE_TYPES];
 unsigned int Model1_lb_hot_gained[Model1_CPU_MAX_IDLE_TYPES];
 unsigned int Model1_lb_nobusyg[Model1_CPU_MAX_IDLE_TYPES];
 unsigned int Model1_lb_nobusyq[Model1_CPU_MAX_IDLE_TYPES];

 /* Active load balancing */
 unsigned int Model1_alb_count;
 unsigned int Model1_alb_failed;
 unsigned int Model1_alb_pushed;

 /* SD_BALANCE_EXEC stats */
 unsigned int Model1_sbe_count;
 unsigned int Model1_sbe_balanced;
 unsigned int Model1_sbe_pushed;

 /* SD_BALANCE_FORK stats */
 unsigned int Model1_sbf_count;
 unsigned int Model1_sbf_balanced;
 unsigned int Model1_sbf_pushed;

 /* try_to_wake_up() stats */
 unsigned int Model1_ttwu_wake_remote;
 unsigned int Model1_ttwu_move_affine;
 unsigned int Model1_ttwu_move_balance;




 union {
  void *Model1_private; /* used during construction */
  struct Model1_callback_head Model1_rcu; /* used during destruction */
 };

 unsigned int Model1_span_weight;
 /*
	 * Span of all CPUs in this domain.
	 *
	 * NOTE: this field is variable length. (Allocated dynamically
	 * by attaching extra space to the end of the structure,
	 * depending on how many CPUs the kernel has booted up with)
	 */
 unsigned long Model1_span[0];
};

static inline __attribute__((no_instrument_function)) struct Model1_cpumask *Model1_sched_domain_span(struct Model1_sched_domain *Model1_sd)
{
 return ((struct Model1_cpumask *)(1 ? (Model1_sd->Model1_span) : (void *)sizeof(Model1___check_is_bitmap(Model1_sd->Model1_span))));
}

extern void Model1_partition_sched_domains(int Model1_ndoms_new, Model1_cpumask_var_t Model1_doms_new[],
        struct Model1_sched_domain_attr *Model1_dattr_new);

/* Allocate an array of sched domains, for partition_sched_domains(). */
Model1_cpumask_var_t *Model1_alloc_sched_domains(unsigned int Model1_ndoms);
void Model1_free_sched_domains(Model1_cpumask_var_t Model1_doms[], unsigned int Model1_ndoms);

bool Model1_cpus_share_cache(int Model1_this_cpu, int Model1_that_cpu);

typedef const struct Model1_cpumask *(*Model1_sched_domain_mask_f)(int Model1_cpu);
typedef int (*Model1_sched_domain_flags_f)(void);



struct Model1_sd_data {
 struct Model1_sched_domain ** Model1_sd;
 struct Model1_sched_group ** Model1_sg;
 struct Model1_sched_group_capacity ** Model1_sgc;
};

struct Model1_sched_domain_topology_level {
 Model1_sched_domain_mask_f Model1_mask;
 Model1_sched_domain_flags_f Model1_sd_flags;
 int Model1_flags;
 int Model1_numa_level;
 struct Model1_sd_data Model1_data;



};

extern void Model1_set_sched_topology(struct Model1_sched_domain_topology_level *Model1_tl);
extern void Model1_wake_up_if_idle(int Model1_cpu);
struct Model1_io_context; /* See blkdev.h */





static inline __attribute__((no_instrument_function)) void Model1_prefetch_stack(struct Model1_task_struct *Model1_t) { }


struct Model1_audit_context; /* See audit.c */
struct Model1_mempolicy;
struct Model1_pipe_inode_info;
struct Model1_uts_namespace;

struct Model1_load_weight {
 unsigned long Model1_weight;
 Model1_u32 Model1_inv_weight;
};

/*
 * The load_avg/util_avg accumulates an infinite geometric series
 * (see __update_load_avg() in kernel/sched/fair.c).
 *
 * [load_avg definition]
 *
 *   load_avg = runnable% * scale_load_down(load)
 *
 * where runnable% is the time ratio that a sched_entity is runnable.
 * For cfs_rq, it is the aggregated load_avg of all runnable and
 * blocked sched_entities.
 *
 * load_avg may also take frequency scaling into account:
 *
 *   load_avg = runnable% * scale_load_down(load) * freq%
 *
 * where freq% is the CPU frequency normalized to the highest frequency.
 *
 * [util_avg definition]
 *
 *   util_avg = running% * SCHED_CAPACITY_SCALE
 *
 * where running% is the time ratio that a sched_entity is running on
 * a CPU. For cfs_rq, it is the aggregated util_avg of all runnable
 * and blocked sched_entities.
 *
 * util_avg may also factor frequency scaling and CPU capacity scaling:
 *
 *   util_avg = running% * SCHED_CAPACITY_SCALE * freq% * capacity%
 *
 * where freq% is the same as above, and capacity% is the CPU capacity
 * normalized to the greatest capacity (due to uarch differences, etc).
 *
 * N.B., the above ratios (runnable%, running%, freq%, and capacity%)
 * themselves are in the range of [0, 1]. To do fixed point arithmetics,
 * we therefore scale them to as large a range as necessary. This is for
 * example reflected by util_avg's SCHED_CAPACITY_SCALE.
 *
 * [Overflow issue]
 *
 * The 64-bit load_sum can have 4353082796 (=2^64/47742/88761) entities
 * with the highest load (=88761), always runnable on a single cfs_rq,
 * and should not overflow as the number already hits PID_MAX_LIMIT.
 *
 * For all other cases (including 32-bit kernels), struct load_weight's
 * weight will overflow first before we do, because:
 *
 *    Max(load_avg) <= Max(load.weight)
 *
 * Then it is the load_weight's responsibility to consider overflow
 * issues.
 */
struct Model1_sched_avg {
 Model1_u64 Model1_last_update_time, Model1_load_sum;
 Model1_u32 Model1_util_sum, Model1_period_contrib;
 unsigned long Model1_load_avg, Model1_util_avg;
};


struct Model1_sched_statistics {
 Model1_u64 Model1_wait_start;
 Model1_u64 Model1_wait_max;
 Model1_u64 Model1_wait_count;
 Model1_u64 Model1_wait_sum;
 Model1_u64 Model1_iowait_count;
 Model1_u64 Model1_iowait_sum;

 Model1_u64 Model1_sleep_start;
 Model1_u64 Model1_sleep_max;
 Model1_s64 Model1_sum_sleep_runtime;

 Model1_u64 Model1_block_start;
 Model1_u64 Model1_block_max;
 Model1_u64 Model1_exec_max;
 Model1_u64 Model1_slice_max;

 Model1_u64 Model1_nr_migrations_cold;
 Model1_u64 Model1_nr_failed_migrations_affine;
 Model1_u64 Model1_nr_failed_migrations_running;
 Model1_u64 Model1_nr_failed_migrations_hot;
 Model1_u64 Model1_nr_forced_migrations;

 Model1_u64 Model1_nr_wakeups;
 Model1_u64 Model1_nr_wakeups_sync;
 Model1_u64 Model1_nr_wakeups_migrate;
 Model1_u64 Model1_nr_wakeups_local;
 Model1_u64 Model1_nr_wakeups_remote;
 Model1_u64 Model1_nr_wakeups_affine;
 Model1_u64 Model1_nr_wakeups_affine_attempts;
 Model1_u64 Model1_nr_wakeups_passive;
 Model1_u64 Model1_nr_wakeups_idle;
};


struct Model1_sched_entity {
 struct Model1_load_weight Model1_load; /* for load-balancing */
 struct Model1_rb_node Model1_run_node;
 struct Model1_list_head Model1_group_node;
 unsigned int Model1_on_rq;

 Model1_u64 Model1_exec_start;
 Model1_u64 Model1_sum_exec_runtime;
 Model1_u64 Model1_vruntime;
 Model1_u64 Model1_prev_sum_exec_runtime;

 Model1_u64 Model1_nr_migrations;


 struct Model1_sched_statistics Model1_statistics;



 int Model1_depth;
 struct Model1_sched_entity *Model1_parent;
 /* rq on which this entity is (to be) queued: */
 struct Model1_cfs_rq *Model1_cfs_rq;
 /* rq "owned" by this entity/group: */
 struct Model1_cfs_rq *Model1_my_q;



 /*
	 * Per entity load average tracking.
	 *
	 * Put into separate cache line so it does not
	 * collide with read-mostly values above.
	 */
 struct Model1_sched_avg Model1_avg __attribute__((__aligned__((1 << (6)))));

};

struct Model1_sched_rt_entity {
 struct Model1_list_head Model1_run_list;
 unsigned long Model1_timeout;
 unsigned long Model1_watchdog_stamp;
 unsigned int Model1_time_slice;
 unsigned short Model1_on_rq;
 unsigned short Model1_on_list;

 struct Model1_sched_rt_entity *Model1_back;







};

struct Model1_sched_dl_entity {
 struct Model1_rb_node Model1_rb_node;

 /*
	 * Original scheduling parameters. Copied here from sched_attr
	 * during sched_setattr(), they will remain the same until
	 * the next sched_setattr().
	 */
 Model1_u64 Model1_dl_runtime; /* maximum runtime for each instance	*/
 Model1_u64 Model1_dl_deadline; /* relative deadline of each instance	*/
 Model1_u64 Model1_dl_period; /* separation of two instances (period) */
 Model1_u64 Model1_dl_bw; /* dl_runtime / dl_deadline		*/

 /*
	 * Actual scheduling parameters. Initialized with the values above,
	 * they are continously updated during task execution. Note that
	 * the remaining runtime could be < 0 in case we are in overrun.
	 */
 Model1_s64 Model1_runtime; /* remaining runtime for this instance	*/
 Model1_u64 Model1_deadline; /* absolute deadline for this instance	*/
 unsigned int Model1_flags; /* specifying the scheduler behaviour	*/

 /*
	 * Some bool flags:
	 *
	 * @dl_throttled tells if we exhausted the runtime. If so, the
	 * task has to wait for a replenishment to be performed at the
	 * next firing of dl_timer.
	 *
	 * @dl_boosted tells if we are boosted due to DI. If so we are
	 * outside bandwidth enforcement mechanism (but only until we
	 * exit the critical section);
	 *
	 * @dl_yielded tells if task gave up the cpu before consuming
	 * all its available runtime during the last job.
	 */
 int Model1_dl_throttled, Model1_dl_boosted, Model1_dl_yielded;

 /*
	 * Bandwidth enforcement timer. Each -deadline task has its
	 * own bandwidth to be enforced, thus we need one timer per task.
	 */
 struct Model1_hrtimer Model1_dl_timer;
};

union Model1_rcu_special {
 struct {
  Model1_u8 Model1_blocked;
  Model1_u8 Model1_need_qs;
  Model1_u8 Model1_exp_need_qs;
  Model1_u8 Model1_pad; /* Otherwise the compiler can store garbage here. */
 } Model1_b; /* Bits. */
 Model1_u32 Model1_s; /* Set of bits. */
};
struct Model1_rcu_node;

enum Model1_perf_event_task_context {
 Model1_perf_invalid_context = -1,
 Model1_perf_hw_context = 0,
 Model1_perf_sw_context,
 Model1_perf_nr_task_contexts,
};

/* Track pages that require TLB flushes */
struct Model1_tlbflush_unmap_batch {
 /*
	 * Each bit set is a CPU that potentially has a TLB entry for one of
	 * the PFNs being flushed. See set_tlb_ubc_flush_pending().
	 */
 struct Model1_cpumask Model1_cpumask;

 /* True if any bit in cpumask is set */
 bool Model1_flush_required;

 /*
	 * If true then the PTE was dirty when unmapped. The entry must be
	 * flushed before IO is initiated or a stale TLB entry potentially
	 * allows an update without redirtying the page.
	 */
 bool Model1_writable;
};

struct Model1_task_struct {
 volatile long Model1_state; /* -1 unrunnable, 0 runnable, >0 stopped */
 void *Model1_stack;
 Model1_atomic_t Model1_usage;
 unsigned int Model1_flags; /* per process flags, defined below */
 unsigned int Model1_ptrace;


 struct Model1_llist_node Model1_wake_entry;
 int Model1_on_cpu;
 unsigned int Model1_wakee_flips;
 unsigned long Model1_wakee_flip_decay_ts;
 struct Model1_task_struct *Model1_last_wakee;

 int Model1_wake_cpu;

 int Model1_on_rq;

 int Model1_prio, Model1_static_prio, Model1_normal_prio;
 unsigned int Model1_rt_priority;
 const struct Model1_sched_class *Model1_sched_class;
 struct Model1_sched_entity Model1_se;
 struct Model1_sched_rt_entity Model1_rt;

 struct Model1_task_group *Model1_sched_task_group;

 struct Model1_sched_dl_entity Model1_dl;







 unsigned int Model1_btrace_seq;


 unsigned int Model1_policy;
 int Model1_nr_cpus_allowed;
 Model1_cpumask_t Model1_cpus_allowed;
 struct Model1_sched_info Model1_sched_info;


 struct Model1_list_head Model1_tasks;

 struct Model1_plist_node Model1_pushable_tasks;
 struct Model1_rb_node Model1_pushable_dl_tasks;


 struct Model1_mm_struct *Model1_mm, *Model1_active_mm;
 /* per-thread vma caching */
 Model1_u32 Model1_vmacache_seqnum;
 struct Model1_vm_area_struct *Model1_vmacache[(1U << 2)];

 struct Model1_task_rss_stat Model1_rss_stat;

/* task state */
 int Model1_exit_state;
 int Model1_exit_code, Model1_exit_signal;
 int Model1_pdeath_signal; /*  The signal sent when the parent dies  */
 unsigned long Model1_jobctl; /* JOBCTL_*, siglock protected */

 /* Used for emulating ABI behavior of previous Linux versions */
 unsigned int Model1_personality;

 /* scheduler bits, serialized by scheduler locks */
 unsigned Model1_sched_reset_on_fork:1;
 unsigned Model1_sched_contributes_to_load:1;
 unsigned Model1_sched_migrated:1;
 unsigned Model1_sched_remote_wakeup:1;
 unsigned :0; /* force alignment to the next boundary */

 /* unserialized, strictly 'current' */
 unsigned Model1_in_execve:1; /* bit to tell LSMs we're in execve */
 unsigned Model1_in_iowait:1;

 unsigned Model1_restore_sigmask:1;
 unsigned long Model1_atomic_flags; /* Flags needing atomic access. */

 struct Model1_restart_block Model1_restart_block;

 Model1_pid_t Model1_pid;
 Model1_pid_t Model1_tgid;





 /*
	 * pointers to (original) parent process, youngest child, younger sibling,
	 * older sibling, respectively.  (p->father can be replaced with
	 * p->real_parent->pid)
	 */
 struct Model1_task_struct *Model1_real_parent; /* real parent process */
 struct Model1_task_struct *Model1_parent; /* recipient of SIGCHLD, wait4() reports */
 /*
	 * children/sibling forms the list of my natural children
	 */
 struct Model1_list_head Model1_children; /* list of my children */
 struct Model1_list_head Model1_sibling; /* linkage in my parent's children list */
 struct Model1_task_struct *Model1_group_leader; /* threadgroup leader */

 /*
	 * ptraced is the list of tasks this task is using ptrace on.
	 * This includes both natural children and PTRACE_ATTACH targets.
	 * p->ptrace_entry is p's link on the p->parent->ptraced list.
	 */
 struct Model1_list_head Model1_ptraced;
 struct Model1_list_head Model1_ptrace_entry;

 /* PID/PID hash table linkage. */
 struct Model1_pid_link Model1_pids[Model1_PIDTYPE_MAX];
 struct Model1_list_head Model1_thread_group;
 struct Model1_list_head Model1_thread_node;

 struct Model1_completion *Model1_vfork_done; /* for vfork() */
 int *Model1_set_child_tid; /* CLONE_CHILD_SETTID */
 int *Model1_clear_child_tid; /* CLONE_CHILD_CLEARTID */

 Model1_cputime_t Model1_utime, Model1_stime, Model1_utimescaled, Model1_stimescaled;
 Model1_cputime_t Model1_gtime;
 struct Model1_prev_cputime Model1_prev_cputime;
 unsigned long Model1_nvcsw, Model1_nivcsw; /* context switch counts */
 Model1_u64 Model1_start_time; /* monotonic time in nsec */
 Model1_u64 Model1_real_start_time; /* boot based time in nsec */
/* mm fault and swap info: this can arguably be seen as either mm-specific or thread-specific */
 unsigned long Model1_min_flt, Model1_maj_flt;

 struct Model1_task_cputime Model1_cputime_expires;
 struct Model1_list_head Model1_cpu_timers[3];

/* process credentials */
 const struct Model1_cred *Model1_real_cred; /* objective and real subjective task
					 * credentials (COW) */
 const struct Model1_cred *Model1_cred; /* effective (overridable) subjective task
					 * credentials (COW) */
 char Model1_comm[16]; /* executable name excluding path
				     - access with [gs]et_task_comm (which lock
				       it with task_lock())
				     - initialized normally by setup_new_exec */
/* file system info */
 struct Model1_nameidata *Model1_nameidata;

/* ipc stuff */
 struct Model1_sysv_sem Model1_sysvsem;
 struct Model1_sysv_shm Model1_sysvshm;





/* filesystem information */
 struct Model1_fs_struct *Model1_fs;
/* open file information */
 struct Model1_files_struct *Model1_files;
/* namespaces */
 struct Model1_nsproxy *Model1_nsproxy;
/* signal handlers */
 struct Model1_signal_struct *Model1_signal;
 struct Model1_sighand_struct *Model1_sighand;

 Model1_sigset_t Model1_blocked, Model1_real_blocked;
 Model1_sigset_t Model1_saved_sigmask; /* restored if set_restore_sigmask() was used */
 struct Model1_sigpending Model1_pending;

 unsigned long Model1_sas_ss_sp;
 Model1_size_t Model1_sas_ss_size;
 unsigned Model1_sas_ss_flags;

 struct Model1_callback_head *Model1_task_works;

 struct Model1_audit_context *Model1_audit_context;

 Model1_kuid_t Model1_loginuid;
 unsigned int Model1_sessionid;

 struct Model1_seccomp Model1_seccomp;

/* Thread group tracking */
    Model1_u32 Model1_parent_exec_id;
    Model1_u32 Model1_self_exec_id;
/* Protection of (de-)allocation: mm, files, fs, tty, keyrings, mems_allowed,
 * mempolicy */
 Model1_spinlock_t Model1_alloc_lock;

 /* Protection of the PI data structures: */
 Model1_raw_spinlock_t Model1_pi_lock;

 struct Model1_wake_q_node Model1_wake_q;


 /* PI waiters blocked on a rt_mutex held by this task */
 struct Model1_rb_root Model1_pi_waiters;
 struct Model1_rb_node *Model1_pi_waiters_leftmost;
 /* Deadlock detection and priority inheritance handling */
 struct Model1_rt_mutex_waiter *Model1_pi_blocked_on;
/* journalling filesystem info */
 void *Model1_journal_info;

/* stacked block device info */
 struct Model1_bio_list *Model1_bio_list;


/* stack plugging */
 struct Model1_blk_plug *Model1_plug;


/* VM state */
 struct Model1_reclaim_state *Model1_reclaim_state;

 struct Model1_backing_dev_info *Model1_backing_dev_info;

 struct Model1_io_context *Model1_io_context;

 unsigned long Model1_ptrace_message;
 Model1_siginfo_t *Model1_last_siginfo; /* For ptrace use.  */
 struct Model1_task_io_accounting Model1_ioac;

 Model1_u64 Model1_acct_rss_mem1; /* accumulated rss usage */
 Model1_u64 Model1_acct_vm_mem1; /* accumulated virtual memory usage */
 Model1_cputime_t Model1_acct_timexpd; /* stime + utime since last update */


 Model1_nodemask_t Model1_mems_allowed; /* Protected by alloc_lock */
 Model1_seqcount_t Model1_mems_allowed_seq; /* Seqence no to catch updates */
 int Model1_cpuset_mem_spread_rotor;
 int Model1_cpuset_slab_spread_rotor;


 /* Control Group info protected by css_set_lock */
 struct Model1_css_set *Model1_cgroups;
 /* cg_list protected by css_set_lock and tsk->alloc_lock */
 struct Model1_list_head Model1_cg_list;


 struct Model1_robust_list_head *Model1_robust_list;

 struct Model1_compat_robust_list_head *Model1_compat_robust_list;

 struct Model1_list_head Model1_pi_state_list;
 struct Model1_futex_pi_state *Model1_pi_state_cache;


 struct Model1_perf_event_context *Model1_perf_event_ctxp[Model1_perf_nr_task_contexts];
 struct Model1_mutex Model1_perf_event_mutex;
 struct Model1_list_head Model1_perf_event_list;





 struct Model1_mempolicy *Model1_mempolicy; /* Protected by alloc_lock */
 short Model1_il_next;
 short Model1_pref_node_fork;
 struct Model1_tlbflush_unmap_batch Model1_tlb_ubc;


 struct Model1_callback_head Model1_rcu;

 /*
	 * cache last used pipe for splice
	 */
 struct Model1_pipe_inode_info *Model1_splice_pipe;

 struct Model1_page_frag Model1_task_frag;


 struct Model1_task_delay_info *Model1_delays;




 /*
	 * when (nr_dirtied >= nr_dirtied_pause), it's time to call
	 * balance_dirty_pages() for some dirty throttling pause
	 */
 int Model1_nr_dirtied;
 int Model1_nr_dirtied_pause;
 unsigned long Model1_dirty_paused_when; /* start of a write-and-pause period */





 /*
	 * time slack values; these are used to round up poll() and
	 * select() etc timeout values. These are in nanoseconds.
	 */
 Model1_u64 Model1_timer_slack_ns;
 Model1_u64 Model1_default_timer_slack_ns;
 /* state flags for use by tracers */
 unsigned long Model1_trace;
 /* bitmask and counter of trace recursion */
 unsigned long Model1_trace_recursion;
 int Model1_pagefault_disabled;

 struct Model1_task_struct *Model1_oom_reaper_list;

/* CPU-specific state of this task */
 struct Model1_thread_struct thread;
/*
 * WARNING: on x86, 'thread_struct' contains a variable-sized
 * structure.  It *MUST* be at the end of 'task_struct'.
 *
 * Do not put anything below here!
 */
};


extern int Model1_arch_task_struct_size __attribute__((__section__(".data..read_mostly")));




/* Future-safe accessor for struct task_struct's cpus_allowed. */


static inline __attribute__((no_instrument_function)) int Model1_tsk_nr_cpus_allowed(struct Model1_task_struct *Model1_p)
{
 return Model1_p->Model1_nr_cpus_allowed;
}







static inline __attribute__((no_instrument_function)) bool Model1_in_vfork(struct Model1_task_struct *Model1_tsk)
{
 bool Model1_ret;

 /*
	 * need RCU to access ->real_parent if CLONE_VM was used along with
	 * CLONE_PARENT.
	 *
	 * We check real_parent->mm == tsk->mm because CLONE_VFORK does not
	 * imply CLONE_VM
	 *
	 * CLONE_VFORK can be used with CLONE_PARENT/CLONE_THREAD and thus
	 * ->real_parent is not necessarily the task doing vfork(), so in
	 * theory we can't rely on task_lock() if we want to dereference it.
	 *
	 * And in this case we can't trust the real_parent->mm == tsk->mm
	 * check, it can be false negative. But we do not care, if init or
	 * another oom-unkillable task does this it should blame itself.
	 */
 Model1_rcu_read_lock();
 Model1_ret = Model1_tsk->Model1_vfork_done && Model1_tsk->Model1_real_parent->Model1_mm == Model1_tsk->Model1_mm;
 Model1_rcu_read_unlock();

 return Model1_ret;
}
static inline __attribute__((no_instrument_function)) void Model1_task_numa_fault(int Model1_last_node, int Model1_node, int Model1_pages,
       int Model1_flags)
{
}
static inline __attribute__((no_instrument_function)) Model1_pid_t Model1_task_numa_group_id(struct Model1_task_struct *Model1_p)
{
 return 0;
}
static inline __attribute__((no_instrument_function)) void Model1_set_numabalancing_state(bool Model1_enabled)
{
}
static inline __attribute__((no_instrument_function)) void Model1_task_numa_free(struct Model1_task_struct *Model1_p)
{
}
static inline __attribute__((no_instrument_function)) bool Model1_should_numa_migrate_memory(struct Model1_task_struct *Model1_p,
    struct Model1_page *Model1_page, int Model1_src_nid, int Model1_dst_cpu)
{
 return true;
}


static inline __attribute__((no_instrument_function)) struct Model1_pid *Model1_task_pid(struct Model1_task_struct *Model1_task)
{
 return Model1_task->Model1_pids[Model1_PIDTYPE_PID].Model1_pid;
}

static inline __attribute__((no_instrument_function)) struct Model1_pid *Model1_task_tgid(struct Model1_task_struct *Model1_task)
{
 return Model1_task->Model1_group_leader->Model1_pids[Model1_PIDTYPE_PID].Model1_pid;
}

/*
 * Without tasklist or rcu lock it is not safe to dereference
 * the result of task_pgrp/task_session even if task == current,
 * we can race with another thread doing sys_setsid/sys_setpgid.
 */
static inline __attribute__((no_instrument_function)) struct Model1_pid *Model1_task_pgrp(struct Model1_task_struct *Model1_task)
{
 return Model1_task->Model1_group_leader->Model1_pids[Model1_PIDTYPE_PGID].Model1_pid;
}

static inline __attribute__((no_instrument_function)) struct Model1_pid *Model1_task_session(struct Model1_task_struct *Model1_task)
{
 return Model1_task->Model1_group_leader->Model1_pids[Model1_PIDTYPE_SID].Model1_pid;
}

struct Model1_pid_namespace;

/*
 * the helpers to get the task's different pids as they are seen
 * from various namespaces
 *
 * task_xid_nr()     : global id, i.e. the id seen from the init namespace;
 * task_xid_vnr()    : virtual id, i.e. the id seen from the pid namespace of
 *                     current.
 * task_xid_nr_ns()  : id seen from the ns specified;
 *
 * set_task_vxid()   : assigns a virtual id to a task;
 *
 * see also pid_nr() etc in include/linux/pid.h
 */
Model1_pid_t Model1___task_pid_nr_ns(struct Model1_task_struct *Model1_task, enum Model1_pid_type Model1_type,
   struct Model1_pid_namespace *Model1_ns);

static inline __attribute__((no_instrument_function)) Model1_pid_t Model1_task_pid_nr(struct Model1_task_struct *Model1_tsk)
{
 return Model1_tsk->Model1_pid;
}

static inline __attribute__((no_instrument_function)) Model1_pid_t Model1_task_pid_nr_ns(struct Model1_task_struct *Model1_tsk,
     struct Model1_pid_namespace *Model1_ns)
{
 return Model1___task_pid_nr_ns(Model1_tsk, Model1_PIDTYPE_PID, Model1_ns);
}

static inline __attribute__((no_instrument_function)) Model1_pid_t Model1_task_pid_vnr(struct Model1_task_struct *Model1_tsk)
{
 return Model1___task_pid_nr_ns(Model1_tsk, Model1_PIDTYPE_PID, ((void *)0));
}


static inline __attribute__((no_instrument_function)) Model1_pid_t Model1_task_tgid_nr(struct Model1_task_struct *Model1_tsk)
{
 return Model1_tsk->Model1_tgid;
}

Model1_pid_t Model1_task_tgid_nr_ns(struct Model1_task_struct *Model1_tsk, struct Model1_pid_namespace *Model1_ns);

static inline __attribute__((no_instrument_function)) Model1_pid_t Model1_task_tgid_vnr(struct Model1_task_struct *Model1_tsk)
{
 return Model1_pid_vnr(Model1_task_tgid(Model1_tsk));
}


static inline __attribute__((no_instrument_function)) int Model1_pid_alive(const struct Model1_task_struct *Model1_p);
static inline __attribute__((no_instrument_function)) Model1_pid_t Model1_task_ppid_nr_ns(const struct Model1_task_struct *Model1_tsk, struct Model1_pid_namespace *Model1_ns)
{
 Model1_pid_t Model1_pid = 0;

 Model1_rcu_read_lock();
 if (Model1_pid_alive(Model1_tsk))
  Model1_pid = Model1_task_tgid_nr_ns(({ typeof(*(Model1_tsk->Model1_real_parent)) *Model1_________p1 = (typeof(*(Model1_tsk->Model1_real_parent)) *)({ typeof((Model1_tsk->Model1_real_parent)) Model1__________p1 = ({ union { typeof((Model1_tsk->Model1_real_parent)) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&((Model1_tsk->Model1_real_parent)), Model1___u.Model1___c, sizeof((Model1_tsk->Model1_real_parent))); else Model1___read_once_size_nocheck(&((Model1_tsk->Model1_real_parent)), Model1___u.Model1___c, sizeof((Model1_tsk->Model1_real_parent))); Model1___u.Model1___val; }); typeof(*((Model1_tsk->Model1_real_parent))) *Model1____typecheck_p __attribute__((unused)); do { } while (0); (Model1__________p1); }); do { } while (0); ; ((typeof(*(Model1_tsk->Model1_real_parent)) *)(Model1_________p1)); }), Model1_ns);
 Model1_rcu_read_unlock();

 return Model1_pid;
}

static inline __attribute__((no_instrument_function)) Model1_pid_t Model1_task_ppid_nr(const struct Model1_task_struct *Model1_tsk)
{
 return Model1_task_ppid_nr_ns(Model1_tsk, &Model1_init_pid_ns);
}

static inline __attribute__((no_instrument_function)) Model1_pid_t Model1_task_pgrp_nr_ns(struct Model1_task_struct *Model1_tsk,
     struct Model1_pid_namespace *Model1_ns)
{
 return Model1___task_pid_nr_ns(Model1_tsk, Model1_PIDTYPE_PGID, Model1_ns);
}

static inline __attribute__((no_instrument_function)) Model1_pid_t Model1_task_pgrp_vnr(struct Model1_task_struct *Model1_tsk)
{
 return Model1___task_pid_nr_ns(Model1_tsk, Model1_PIDTYPE_PGID, ((void *)0));
}


static inline __attribute__((no_instrument_function)) Model1_pid_t Model1_task_session_nr_ns(struct Model1_task_struct *Model1_tsk,
     struct Model1_pid_namespace *Model1_ns)
{
 return Model1___task_pid_nr_ns(Model1_tsk, Model1_PIDTYPE_SID, Model1_ns);
}

static inline __attribute__((no_instrument_function)) Model1_pid_t Model1_task_session_vnr(struct Model1_task_struct *Model1_tsk)
{
 return Model1___task_pid_nr_ns(Model1_tsk, Model1_PIDTYPE_SID, ((void *)0));
}

/* obsolete, do not use */
static inline __attribute__((no_instrument_function)) Model1_pid_t Model1_task_pgrp_nr(struct Model1_task_struct *Model1_tsk)
{
 return Model1_task_pgrp_nr_ns(Model1_tsk, &Model1_init_pid_ns);
}

/**
 * pid_alive - check that a task structure is not stale
 * @p: Task structure to be checked.
 *
 * Test if a process is not yet dead (at most zombie state)
 * If pid_alive fails, then pointers within the task structure
 * can be stale and must not be dereferenced.
 *
 * Return: 1 if the process is alive. 0 otherwise.
 */
static inline __attribute__((no_instrument_function)) int Model1_pid_alive(const struct Model1_task_struct *Model1_p)
{
 return Model1_p->Model1_pids[Model1_PIDTYPE_PID].Model1_pid != ((void *)0);
}

/**
 * is_global_init - check if a task structure is init. Since init
 * is free to have sub-threads we need to check tgid.
 * @tsk: Task structure to be checked.
 *
 * Check if a task structure is the first user space task the kernel created.
 *
 * Return: 1 if the task structure is init. 0 otherwise.
 */
static inline __attribute__((no_instrument_function)) int Model1_is_global_init(struct Model1_task_struct *Model1_tsk)
{
 return Model1_task_tgid_nr(Model1_tsk) == 1;
}

extern struct Model1_pid *Model1_cad_pid;

extern void Model1_free_task(struct Model1_task_struct *Model1_tsk);


extern void Model1___put_task_struct(struct Model1_task_struct *Model1_t);

static inline __attribute__((no_instrument_function)) void Model1_put_task_struct(struct Model1_task_struct *Model1_t)
{
 if (Model1_atomic_dec_and_test(&Model1_t->Model1_usage))
  Model1___put_task_struct(Model1_t);
}

struct Model1_task_struct *Model1_task_rcu_dereference(struct Model1_task_struct **Model1_ptask);
struct Model1_task_struct *Model1_try_get_task_struct(struct Model1_task_struct **Model1_ptask);
static inline __attribute__((no_instrument_function)) void Model1_task_cputime(struct Model1_task_struct *Model1_t,
    Model1_cputime_t *Model1_utime, Model1_cputime_t *Model1_stime)
{
 if (Model1_utime)
  *Model1_utime = Model1_t->Model1_utime;
 if (Model1_stime)
  *Model1_stime = Model1_t->Model1_stime;
}

static inline __attribute__((no_instrument_function)) void Model1_task_cputime_scaled(struct Model1_task_struct *Model1_t,
           Model1_cputime_t *Model1_utimescaled,
           Model1_cputime_t *Model1_stimescaled)
{
 if (Model1_utimescaled)
  *Model1_utimescaled = Model1_t->Model1_utimescaled;
 if (Model1_stimescaled)
  *Model1_stimescaled = Model1_t->Model1_stimescaled;
}

static inline __attribute__((no_instrument_function)) Model1_cputime_t Model1_task_gtime(struct Model1_task_struct *Model1_t)
{
 return Model1_t->Model1_gtime;
}

extern void Model1_task_cputime_adjusted(struct Model1_task_struct *Model1_p, Model1_cputime_t *Model1_ut, Model1_cputime_t *Model1_st);
extern void Model1_thread_group_cputime_adjusted(struct Model1_task_struct *Model1_p, Model1_cputime_t *Model1_ut, Model1_cputime_t *Model1_st);

/*
 * Per process flags
 */
/*
 * Only the _current_ task can read/write to tsk->flags, but other
 * tasks can access tsk->flags in readonly mode for example
 * with tsk_used_math (like during threaded core dumping).
 * There is however an exception to this rule during ptrace
 * or during fork: the ptracer task is allowed to write to the
 * child->flags of its traced child (same goes for fork, the parent
 * can write to the child->flags), because we're guaranteed the
 * child is not running and in turn not changing child->flags
 * at the same time the parent does it.
 */
/* NOTE: this will return 0 or PF_USED_MATH, it will never return 1 */



/* __GFP_IO isn't allowed if PF_MEMALLOC_NOIO is set in current->flags
 * __GFP_FS is also cleared as it implies __GFP_IO.
 */
static inline __attribute__((no_instrument_function)) Model1_gfp_t Model1_memalloc_noio_flags(Model1_gfp_t Model1_flags)
{
 if (__builtin_expect(!!(Model1_get_current()->Model1_flags & 0x00080000), 0))
  Model1_flags &= ~((( Model1_gfp_t)0x40u) | (( Model1_gfp_t)0x80u));
 return Model1_flags;
}

static inline __attribute__((no_instrument_function)) unsigned int Model1_memalloc_noio_save(void)
{
 unsigned int Model1_flags = Model1_get_current()->Model1_flags & 0x00080000;
 Model1_get_current()->Model1_flags |= 0x00080000;
 return Model1_flags;
}

static inline __attribute__((no_instrument_function)) void Model1_memalloc_noio_restore(unsigned int Model1_flags)
{
 Model1_get_current()->Model1_flags = (Model1_get_current()->Model1_flags & ~0x00080000) | Model1_flags;
}

/* Per-process atomic flags. */
static inline __attribute__((no_instrument_function)) bool Model1_task_no_new_privs(struct Model1_task_struct *Model1_p) { return (__builtin_constant_p((0)) ? Model1_constant_test_bit((0), (&Model1_p->Model1_atomic_flags)) : Model1_variable_test_bit((0), (&Model1_p->Model1_atomic_flags))); }
static inline __attribute__((no_instrument_function)) void Model1_task_set_no_new_privs(struct Model1_task_struct *Model1_p) { Model1_set_bit(0, &Model1_p->Model1_atomic_flags); }

static inline __attribute__((no_instrument_function)) bool Model1_task_spread_page(struct Model1_task_struct *Model1_p) { return (__builtin_constant_p((1)) ? Model1_constant_test_bit((1), (&Model1_p->Model1_atomic_flags)) : Model1_variable_test_bit((1), (&Model1_p->Model1_atomic_flags))); }
static inline __attribute__((no_instrument_function)) void Model1_task_set_spread_page(struct Model1_task_struct *Model1_p) { Model1_set_bit(1, &Model1_p->Model1_atomic_flags); }
static inline __attribute__((no_instrument_function)) void Model1_task_clear_spread_page(struct Model1_task_struct *Model1_p) { Model1_clear_bit(1, &Model1_p->Model1_atomic_flags); }

static inline __attribute__((no_instrument_function)) bool Model1_task_spread_slab(struct Model1_task_struct *Model1_p) { return (__builtin_constant_p((2)) ? Model1_constant_test_bit((2), (&Model1_p->Model1_atomic_flags)) : Model1_variable_test_bit((2), (&Model1_p->Model1_atomic_flags))); }
static inline __attribute__((no_instrument_function)) void Model1_task_set_spread_slab(struct Model1_task_struct *Model1_p) { Model1_set_bit(2, &Model1_p->Model1_atomic_flags); }
static inline __attribute__((no_instrument_function)) void Model1_task_clear_spread_slab(struct Model1_task_struct *Model1_p) { Model1_clear_bit(2, &Model1_p->Model1_atomic_flags); }

static inline __attribute__((no_instrument_function)) bool Model1_task_lmk_waiting(struct Model1_task_struct *Model1_p) { return (__builtin_constant_p((3)) ? Model1_constant_test_bit((3), (&Model1_p->Model1_atomic_flags)) : Model1_variable_test_bit((3), (&Model1_p->Model1_atomic_flags))); }
static inline __attribute__((no_instrument_function)) void Model1_task_set_lmk_waiting(struct Model1_task_struct *Model1_p) { Model1_set_bit(3, &Model1_p->Model1_atomic_flags); }

/*
 * task->jobctl flags
 */
extern bool Model1_task_set_jobctl_pending(struct Model1_task_struct *Model1_task,
        unsigned long Model1_mask);
extern void Model1_task_clear_jobctl_trapping(struct Model1_task_struct *Model1_task);
extern void Model1_task_clear_jobctl_pending(struct Model1_task_struct *Model1_task,
          unsigned long Model1_mask);

static inline __attribute__((no_instrument_function)) void Model1_rcu_copy_process(struct Model1_task_struct *Model1_p)
{
}

static inline __attribute__((no_instrument_function)) void Model1_tsk_restore_flags(struct Model1_task_struct *Model1_task,
    unsigned long Model1_orig_flags, unsigned long Model1_flags)
{
 Model1_task->Model1_flags &= ~Model1_flags;
 Model1_task->Model1_flags |= Model1_orig_flags & Model1_flags;
}

extern int Model1_cpuset_cpumask_can_shrink(const struct Model1_cpumask *Model1_cur,
         const struct Model1_cpumask *Model1_trial);
extern int Model1_task_can_attach(struct Model1_task_struct *Model1_p,
      const struct Model1_cpumask *Model1_cs_cpus_allowed);

extern void Model1_do_set_cpus_allowed(struct Model1_task_struct *Model1_p,
          const struct Model1_cpumask *Model1_new_mask);

extern int Model1_set_cpus_allowed_ptr(struct Model1_task_struct *Model1_p,
    const struct Model1_cpumask *Model1_new_mask);
void Model1_calc_load_enter_idle(void);
void Model1_calc_load_exit_idle(void);





/*
 * Do not use outside of architecture code which knows its limitations.
 *
 * sched_clock() has no promise of monotonicity or bounded drift between
 * CPUs, use (which you should not) requires disabling IRQs.
 *
 * Please use one of the three interfaces below.
 */
extern unsigned long long __attribute__((no_instrument_function)) Model1_sched_clock(void);
/*
 * See the comment in kernel/sched/clock.c
 */
extern Model1_u64 Model1_running_clock(void);
extern Model1_u64 Model1_sched_clock_cpu(int Model1_cpu);


extern void Model1_sched_clock_init(void);
/*
 * Architectures can set this to 1 if they have specified
 * CONFIG_HAVE_UNSTABLE_SCHED_CLOCK in their arch Kconfig,
 * but then during bootup it turns out that sched_clock()
 * is reliable after all:
 */
extern int Model1_sched_clock_stable(void);
extern void Model1_set_sched_clock_stable(void);
extern void Model1_clear_sched_clock_stable(void);

extern void Model1_sched_clock_tick(void);
extern void Model1_sched_clock_idle_sleep_event(void);
extern void Model1_sched_clock_idle_wakeup_event(Model1_u64 Model1_delta_ns);

/*
 * As outlined in clock.c, provides a fast, high resolution, nanosecond
 * time source that is monotonic per cpu argument and has bounded drift
 * between cpus.
 *
 * ######################### BIG FAT WARNING ##########################
 * # when comparing cpu_clock(i) to cpu_clock(j) for i != j, time can #
 * # go backwards !!                                                  #
 * ####################################################################
 */
static inline __attribute__((no_instrument_function)) Model1_u64 Model1_cpu_clock(int Model1_cpu)
{
 return Model1_sched_clock_cpu(Model1_cpu);
}

static inline __attribute__((no_instrument_function)) Model1_u64 Model1_local_clock(void)
{
 return Model1_sched_clock_cpu((({ typeof(Model1_cpu_number) Model1_pscr_ret__; do { const void *Model1___vpp_verify = (typeof((&(Model1_cpu_number)) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); switch(sizeof(Model1_cpu_number)) { case 1: Model1_pscr_ret__ = ({ typeof(Model1_cpu_number) Model1_pfo_ret__; switch (sizeof(Model1_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 2: Model1_pscr_ret__ = ({ typeof(Model1_cpu_number) Model1_pfo_ret__; switch (sizeof(Model1_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 4: Model1_pscr_ret__ = ({ typeof(Model1_cpu_number) Model1_pfo_ret__; switch (sizeof(Model1_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 8: Model1_pscr_ret__ = ({ typeof(Model1_cpu_number) Model1_pfo_ret__; switch (sizeof(Model1_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; default: Model1___bad_size_call_parameter(); break; } Model1_pscr_ret__; })));
}
static inline __attribute__((no_instrument_function)) void Model1_enable_sched_clock_irqtime(void) {}
static inline __attribute__((no_instrument_function)) void Model1_disable_sched_clock_irqtime(void) {}


extern unsigned long long
Model1_task_sched_runtime(struct Model1_task_struct *Model1_task);

/* sched_exec is called by processes performing an exec */

extern void Model1_sched_exec(void);




extern void Model1_sched_clock_idle_sleep_event(void);
extern void Model1_sched_clock_idle_wakeup_event(Model1_u64 Model1_delta_ns);


extern void Model1_idle_task_exit(void);





extern void Model1_wake_up_nohz_cpu(int Model1_cpu);
static inline __attribute__((no_instrument_function)) void Model1_sched_autogroup_create_attach(struct Model1_task_struct *Model1_p) { }
static inline __attribute__((no_instrument_function)) void Model1_sched_autogroup_detach(struct Model1_task_struct *Model1_p) { }
static inline __attribute__((no_instrument_function)) void Model1_sched_autogroup_fork(struct Model1_signal_struct *Model1_sig) { }
static inline __attribute__((no_instrument_function)) void Model1_sched_autogroup_exit(struct Model1_signal_struct *Model1_sig) { }


extern int Model1_yield_to(struct Model1_task_struct *Model1_p, bool Model1_preempt);
extern void Model1_set_user_nice(struct Model1_task_struct *Model1_p, long Model1_nice);
extern int Model1_task_prio(const struct Model1_task_struct *Model1_p);
/**
 * task_nice - return the nice value of a given task.
 * @p: the task in question.
 *
 * Return: The nice value [ -20 ... 0 ... 19 ].
 */
static inline __attribute__((no_instrument_function)) int Model1_task_nice(const struct Model1_task_struct *Model1_p)
{
 return (((Model1_p)->Model1_static_prio) - (100 + (19 - -20 + 1) / 2));
}
extern int Model1_can_nice(const struct Model1_task_struct *Model1_p, const int Model1_nice);
extern int Model1_task_curr(const struct Model1_task_struct *Model1_p);
extern int Model1_idle_cpu(int Model1_cpu);
extern int Model1_sched_setscheduler(struct Model1_task_struct *, int,
         const struct Model1_sched_param *);
extern int Model1_sched_setscheduler_nocheck(struct Model1_task_struct *, int,
          const struct Model1_sched_param *);
extern int Model1_sched_setattr(struct Model1_task_struct *,
    const struct Model1_sched_attr *);
extern struct Model1_task_struct *Model1_idle_task(int Model1_cpu);
/**
 * is_idle_task - is the specified task an idle task?
 * @p: the task in question.
 *
 * Return: 1 if @p is an idle task. 0 otherwise.
 */
static inline __attribute__((no_instrument_function)) bool Model1_is_idle_task(const struct Model1_task_struct *Model1_p)
{
 return Model1_p->Model1_pid == 0;
}
extern struct Model1_task_struct *Model1_curr_task(int Model1_cpu);
extern void Model1_set_curr_task(int Model1_cpu, struct Model1_task_struct *Model1_p);

void Model1_yield(void);

union Model1_thread_union {
 struct Model1_thread_info Model1_thread_info;
 unsigned long Model1_stack[(((1UL) << 12) << (2 + 0))/sizeof(long)];
};


static inline __attribute__((no_instrument_function)) int Model1_kstack_end(void *Model1_addr)
{
 /* Reliable end of stack detection:
	 * Some APM bios versions misalign the stack
	 */
 return !(((unsigned long)Model1_addr+sizeof(void*)-1) & ((((1UL) << 12) << (2 + 0))-sizeof(void*)));
}


extern union Model1_thread_union Model1_init_thread_union;
extern struct Model1_task_struct Model1_init_task;

extern struct Model1_mm_struct Model1_init_mm;

extern struct Model1_pid_namespace Model1_init_pid_ns;

/*
 * find a task by one of its numerical ids
 *
 * find_task_by_pid_ns():
 *      finds a task by its pid in the specified namespace
 * find_task_by_vpid():
 *      finds a task by its virtual pid
 *
 * see also find_vpid() etc in include/linux/pid.h
 */

extern struct Model1_task_struct *Model1_find_task_by_vpid(Model1_pid_t Model1_nr);
extern struct Model1_task_struct *Model1_find_task_by_pid_ns(Model1_pid_t Model1_nr,
  struct Model1_pid_namespace *Model1_ns);

/* per-UID process charging. */
extern struct Model1_user_struct * Model1_alloc_uid(Model1_kuid_t);
static inline __attribute__((no_instrument_function)) struct Model1_user_struct *Model1_get_uid(struct Model1_user_struct *Model1_u)
{
 Model1_atomic_inc(&Model1_u->Model1___count);
 return Model1_u;
}
extern void Model1_free_uid(struct Model1_user_struct *);



extern void Model1_xtime_update(unsigned long Model1_ticks);

extern int Model1_wake_up_state(struct Model1_task_struct *Model1_tsk, unsigned int Model1_state);
extern int Model1_wake_up_process(struct Model1_task_struct *Model1_tsk);
extern void Model1_wake_up_new_task(struct Model1_task_struct *Model1_tsk);

 extern void Model1_kick_process(struct Model1_task_struct *Model1_tsk);



extern int Model1_sched_fork(unsigned long Model1_clone_flags, struct Model1_task_struct *Model1_p);
extern void Model1_sched_dead(struct Model1_task_struct *Model1_p);

extern void Model1_proc_caches_init(void);
extern void Model1_flush_signals(struct Model1_task_struct *);
extern void Model1_ignore_signals(struct Model1_task_struct *);
extern void Model1_flush_signal_handlers(struct Model1_task_struct *, int Model1_force_default);
extern int Model1_dequeue_signal(struct Model1_task_struct *Model1_tsk, Model1_sigset_t *Model1_mask, Model1_siginfo_t *Model1_info);

static inline __attribute__((no_instrument_function)) int Model1_kernel_dequeue_signal(Model1_siginfo_t *Model1_info)
{
 struct Model1_task_struct *Model1_tsk = Model1_get_current();
 Model1_siginfo_t Model1___info;
 int Model1_ret;

 Model1_spin_lock_irq(&Model1_tsk->Model1_sighand->Model1_siglock);
 Model1_ret = Model1_dequeue_signal(Model1_tsk, &Model1_tsk->Model1_blocked, Model1_info ?: &Model1___info);
 Model1_spin_unlock_irq(&Model1_tsk->Model1_sighand->Model1_siglock);

 return Model1_ret;
}

static inline __attribute__((no_instrument_function)) void Model1_kernel_signal_stop(void)
{
 Model1_spin_lock_irq(&Model1_get_current()->Model1_sighand->Model1_siglock);
 if (Model1_get_current()->Model1_jobctl & (1UL << 16))
  do { Model1_get_current()->Model1_state = ((128 | 4)); } while (0);
 Model1_spin_unlock_irq(&Model1_get_current()->Model1_sighand->Model1_siglock);

 Model1_schedule();
}

extern void Model1_release_task(struct Model1_task_struct * Model1_p);
extern int Model1_send_sig_info(int, struct Model1_siginfo *, struct Model1_task_struct *);
extern int Model1_force_sigsegv(int, struct Model1_task_struct *);
extern int Model1_force_sig_info(int, struct Model1_siginfo *, struct Model1_task_struct *);
extern int Model1___kill_pgrp_info(int Model1_sig, struct Model1_siginfo *Model1_info, struct Model1_pid *Model1_pgrp);
extern int Model1_kill_pid_info(int Model1_sig, struct Model1_siginfo *Model1_info, struct Model1_pid *Model1_pid);
extern int Model1_kill_pid_info_as_cred(int, struct Model1_siginfo *, struct Model1_pid *,
    const struct Model1_cred *, Model1_u32);
extern int Model1_kill_pgrp(struct Model1_pid *Model1_pid, int Model1_sig, int Model1_priv);
extern int Model1_kill_pid(struct Model1_pid *Model1_pid, int Model1_sig, int Model1_priv);
extern int Model1_kill_proc_info(int, struct Model1_siginfo *, Model1_pid_t);
extern __attribute__((warn_unused_result)) bool Model1_do_notify_parent(struct Model1_task_struct *, int);
extern void Model1___wake_up_parent(struct Model1_task_struct *Model1_p, struct Model1_task_struct *Model1_parent);
extern void Model1_force_sig(int, struct Model1_task_struct *);
extern int Model1_send_sig(int, struct Model1_task_struct *, int);
extern int Model1_zap_other_threads(struct Model1_task_struct *Model1_p);
extern struct Model1_sigqueue *Model1_sigqueue_alloc(void);
extern void Model1_sigqueue_free(struct Model1_sigqueue *);
extern int Model1_send_sigqueue(struct Model1_sigqueue *, struct Model1_task_struct *, int Model1_group);
extern int Model1_do_sigaction(int, struct Model1_k_sigaction *, struct Model1_k_sigaction *);
/* Higher-quality implementation, used if TIF_RESTORE_SIGMASK doesn't exist. */
static inline __attribute__((no_instrument_function)) void Model1_set_restore_sigmask(void)
{
 Model1_get_current()->Model1_restore_sigmask = true;
 ({ int Model1___ret_warn_on = !!(!Model1_test_ti_thread_flag(Model1_current_thread_info(), 2)); if (__builtin_expect(!!(Model1___ret_warn_on), 0)) Model1_warn_slowpath_null("./include/linux/sched.h", 2727); __builtin_expect(!!(Model1___ret_warn_on), 0); });
}
static inline __attribute__((no_instrument_function)) void Model1_clear_restore_sigmask(void)
{
 Model1_get_current()->Model1_restore_sigmask = false;
}
static inline __attribute__((no_instrument_function)) bool Model1_test_restore_sigmask(void)
{
 return Model1_get_current()->Model1_restore_sigmask;
}
static inline __attribute__((no_instrument_function)) bool Model1_test_and_clear_restore_sigmask(void)
{
 if (!Model1_get_current()->Model1_restore_sigmask)
  return false;
 Model1_get_current()->Model1_restore_sigmask = false;
 return true;
}


static inline __attribute__((no_instrument_function)) void Model1_restore_saved_sigmask(void)
{
 if (Model1_test_and_clear_restore_sigmask())
  Model1___set_current_blocked(&Model1_get_current()->Model1_saved_sigmask);
}

static inline __attribute__((no_instrument_function)) Model1_sigset_t *Model1_sigmask_to_save(void)
{
 Model1_sigset_t *Model1_res = &Model1_get_current()->Model1_blocked;
 if (__builtin_expect(!!(Model1_test_restore_sigmask()), 0))
  Model1_res = &Model1_get_current()->Model1_saved_sigmask;
 return Model1_res;
}

static inline __attribute__((no_instrument_function)) int Model1_kill_cad_pid(int Model1_sig, int Model1_priv)
{
 return Model1_kill_pid(Model1_cad_pid, Model1_sig, Model1_priv);
}

/* These can be the second arg to send_sig_info/send_group_sig_info.  */




/*
 * True if we are on the alternate signal stack.
 */
static inline __attribute__((no_instrument_function)) int Model1_on_sig_stack(unsigned long Model1_sp)
{
 /*
	 * If the signal stack is SS_AUTODISARM then, by construction, we
	 * can't be on the signal stack unless user code deliberately set
	 * SS_AUTODISARM when we were already on it.
	 *
	 * This improves reliability: if user state gets corrupted such that
	 * the stack pointer points very close to the end of the signal stack,
	 * then this check will enable the signal to be handled anyway.
	 */
 if (Model1_get_current()->Model1_sas_ss_flags & (1U << 31))
  return 0;





 return Model1_sp > Model1_get_current()->Model1_sas_ss_sp &&
  Model1_sp - Model1_get_current()->Model1_sas_ss_sp <= Model1_get_current()->Model1_sas_ss_size;

}

static inline __attribute__((no_instrument_function)) int Model1_sas_ss_flags(unsigned long Model1_sp)
{
 if (!Model1_get_current()->Model1_sas_ss_size)
  return 2;

 return Model1_on_sig_stack(Model1_sp) ? 1 : 0;
}

static inline __attribute__((no_instrument_function)) void Model1_sas_ss_reset(struct Model1_task_struct *Model1_p)
{
 Model1_p->Model1_sas_ss_sp = 0;
 Model1_p->Model1_sas_ss_size = 0;
 Model1_p->Model1_sas_ss_flags = 2;
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_sigsp(unsigned long Model1_sp, struct Model1_ksignal *Model1_ksig)
{
 if (__builtin_expect(!!((Model1_ksig->Model1_ka.Model1_sa.Model1_sa_flags & 0x08000000u)), 0) && ! Model1_sas_ss_flags(Model1_sp))



  return Model1_get_current()->Model1_sas_ss_sp + Model1_get_current()->Model1_sas_ss_size;

 return Model1_sp;
}

/*
 * Routines for handling mm_structs
 */
extern struct Model1_mm_struct * Model1_mm_alloc(void);

/* mmdrop drops the mm and the page tables */
extern void Model1___mmdrop(struct Model1_mm_struct *);
static inline __attribute__((no_instrument_function)) void Model1_mmdrop(struct Model1_mm_struct *Model1_mm)
{
 if (__builtin_expect(!!(Model1_atomic_dec_and_test(&Model1_mm->Model1_mm_count)), 0))
  Model1___mmdrop(Model1_mm);
}

static inline __attribute__((no_instrument_function)) bool Model1_mmget_not_zero(struct Model1_mm_struct *Model1_mm)
{
 return Model1_atomic_add_unless((&Model1_mm->Model1_mm_users), 1, 0);
}

/* mmput gets rid of the mappings and all user-space */
extern void Model1_mmput(struct Model1_mm_struct *);

/* same as above but performs the slow path from the async context. Can
 * be called from the atomic context as well
 */
extern void Model1_mmput_async(struct Model1_mm_struct *);


/* Grab a reference to a task's mm, if it is not already going away */
extern struct Model1_mm_struct *Model1_get_task_mm(struct Model1_task_struct *Model1_task);
/*
 * Grab a reference to a task's mm, if it is not already going away
 * and ptrace_may_access with the mode parameter passed to it
 * succeeds.
 */
extern struct Model1_mm_struct *Model1_mm_access(struct Model1_task_struct *Model1_task, unsigned int Model1_mode);
/* Remove the current tasks stale references to the old mm_struct */
extern void Model1_mm_release(struct Model1_task_struct *, struct Model1_mm_struct *);


extern int Model1_copy_thread_tls(unsigned long, unsigned long, unsigned long,
   struct Model1_task_struct *, unsigned long);
extern void Model1_flush_thread(void);


extern void Model1_exit_thread(struct Model1_task_struct *Model1_tsk);






extern void Model1_exit_files(struct Model1_task_struct *);
extern void Model1___cleanup_sighand(struct Model1_sighand_struct *);

extern void Model1_exit_itimers(struct Model1_signal_struct *);
extern void Model1_flush_itimer_signals(void);

extern void Model1_do_group_exit(int);

extern int Model1_do_execve(struct Model1_filename *,
       const char * const *,
       const char * const *);
extern int Model1_do_execveat(int, struct Model1_filename *,
         const char * const *,
         const char * const *,
         int);
extern long Model1__do_fork(unsigned long, unsigned long, unsigned long, int *, int *, unsigned long);
extern long Model1_do_fork(unsigned long, unsigned long, unsigned long, int *, int *);
struct Model1_task_struct *Model1_fork_idle(int);
extern Model1_pid_t Model1_kernel_thread(int (*Model1_fn)(void *), void *Model1_arg, unsigned long Model1_flags);

extern void Model1___set_task_comm(struct Model1_task_struct *Model1_tsk, const char *Model1_from, bool Model1_exec);
static inline __attribute__((no_instrument_function)) void Model1_set_task_comm(struct Model1_task_struct *Model1_tsk, const char *Model1_from)
{
 Model1___set_task_comm(Model1_tsk, Model1_from, false);
}
extern char *Model1_get_task_comm(char *Model1_to, struct Model1_task_struct *Model1_tsk);


void Model1_scheduler_ipi(void);
extern unsigned long Model1_wait_task_inactive(struct Model1_task_struct *, long Model1_match_state);
extern bool Model1_current_is_single_threaded(void);

/*
 * Careful: do_each_thread/while_each_thread is a double loop so
 *          'break' will not work as expected - use goto instead.
 */
/* Careful: this is a double loop, 'break' won't work as expected. */



static inline __attribute__((no_instrument_function)) int Model1_get_nr_threads(struct Model1_task_struct *Model1_tsk)
{
 return Model1_tsk->Model1_signal->Model1_nr_threads;
}

static inline __attribute__((no_instrument_function)) bool Model1_thread_group_leader(struct Model1_task_struct *Model1_p)
{
 return Model1_p->Model1_exit_signal >= 0;
}

/* Do to the insanities of de_thread it is possible for a process
 * to have the pid of the thread group leader without actually being
 * the thread group leader.  For iteration through the pids in proc
 * all we care about is that we have a task with the appropriate
 * pid, we don't actually care if we have the right task.
 */
static inline __attribute__((no_instrument_function)) bool Model1_has_group_leader_pid(struct Model1_task_struct *Model1_p)
{
 return Model1_task_pid(Model1_p) == Model1_p->Model1_signal->Model1_leader_pid;
}

static inline __attribute__((no_instrument_function))
bool Model1_same_thread_group(struct Model1_task_struct *Model1_p1, struct Model1_task_struct *Model1_p2)
{
 return Model1_p1->Model1_signal == Model1_p2->Model1_signal;
}

static inline __attribute__((no_instrument_function)) struct Model1_task_struct *Model1_next_thread(const struct Model1_task_struct *Model1_p)
{
 return ({ const typeof( ((struct Model1_task_struct *)0)->Model1_thread_group ) *Model1___mptr = (({ typeof(Model1_p->Model1_thread_group.Model1_next) Model1__________p1 = ({ union { typeof(Model1_p->Model1_thread_group.Model1_next) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&(Model1_p->Model1_thread_group.Model1_next), Model1___u.Model1___c, sizeof(Model1_p->Model1_thread_group.Model1_next)); else Model1___read_once_size_nocheck(&(Model1_p->Model1_thread_group.Model1_next), Model1___u.Model1___c, sizeof(Model1_p->Model1_thread_group.Model1_next)); Model1___u.Model1___val; }); typeof(*(Model1_p->Model1_thread_group.Model1_next)) *Model1____typecheck_p __attribute__((unused)); do { } while (0); (Model1__________p1); })); (struct Model1_task_struct *)( (char *)Model1___mptr - __builtin_offsetof(struct Model1_task_struct, Model1_thread_group) );});

}

static inline __attribute__((no_instrument_function)) int Model1_thread_group_empty(struct Model1_task_struct *Model1_p)
{
 return Model1_list_empty(&Model1_p->Model1_thread_group);
}




/*
 * Protects ->fs, ->files, ->mm, ->group_info, ->comm, keyring
 * subscriptions and synchronises with wait4().  Also used in procfs.  Also
 * pins the final release of task.io_context.  Also protects ->cpuset and
 * ->cgroup.subsys[]. And ->vfork_done.
 *
 * Nests both inside and outside of read_lock(&tasklist_lock).
 * It must not be nested with write_lock_irq(&tasklist_lock),
 * neither inside nor outside.
 */
static inline __attribute__((no_instrument_function)) void Model1_task_lock(struct Model1_task_struct *Model1_p)
{
 Model1_spin_lock(&Model1_p->Model1_alloc_lock);
}

static inline __attribute__((no_instrument_function)) void Model1_task_unlock(struct Model1_task_struct *Model1_p)
{
 Model1_spin_unlock(&Model1_p->Model1_alloc_lock);
}

extern struct Model1_sighand_struct *Model1___lock_task_sighand(struct Model1_task_struct *Model1_tsk,
       unsigned long *Model1_flags);

static inline __attribute__((no_instrument_function)) struct Model1_sighand_struct *Model1_lock_task_sighand(struct Model1_task_struct *Model1_tsk,
             unsigned long *Model1_flags)
{
 struct Model1_sighand_struct *Model1_ret;

 Model1_ret = Model1___lock_task_sighand(Model1_tsk, Model1_flags);
 (void)(Model1_ret);
 return Model1_ret;
}

static inline __attribute__((no_instrument_function)) void Model1_unlock_task_sighand(struct Model1_task_struct *Model1_tsk,
      unsigned long *Model1_flags)
{
 Model1_spin_unlock_irqrestore(&Model1_tsk->Model1_sighand->Model1_siglock, *Model1_flags);
}

/**
 * threadgroup_change_begin - mark the beginning of changes to a threadgroup
 * @tsk: task causing the changes
 *
 * All operations which modify a threadgroup - a new thread joining the
 * group, death of a member thread (the assertion of PF_EXITING) and
 * exec(2) dethreading the process and replacing the leader - are wrapped
 * by threadgroup_change_{begin|end}().  This is to provide a place which
 * subsystems needing threadgroup stability can hook into for
 * synchronization.
 */
static inline __attribute__((no_instrument_function)) void Model1_threadgroup_change_begin(struct Model1_task_struct *Model1_tsk)
{
 do { Model1__cond_resched(); } while (0);
 Model1_cgroup_threadgroup_change_begin(Model1_tsk);
}

/**
 * threadgroup_change_end - mark the end of changes to a threadgroup
 * @tsk: task causing the changes
 *
 * See threadgroup_change_begin().
 */
static inline __attribute__((no_instrument_function)) void Model1_threadgroup_change_end(struct Model1_task_struct *Model1_tsk)
{
 Model1_cgroup_threadgroup_change_end(Model1_tsk);
}






static inline __attribute__((no_instrument_function)) void Model1_setup_thread_stack(struct Model1_task_struct *Model1_p, struct Model1_task_struct *Model1_org)
{
 *((struct Model1_thread_info *)(Model1_p)->Model1_stack) = *((struct Model1_thread_info *)(Model1_org)->Model1_stack);
 ((struct Model1_thread_info *)(Model1_p)->Model1_stack)->Model1_task = Model1_p;
}

/*
 * Return the address of the last usable long on the stack.
 *
 * When the stack grows down, this is just above the thread
 * info struct. Going any lower will corrupt the threadinfo.
 *
 * When the stack grows up, this is the highest address.
 * Beyond that position, we corrupt data on the next page.
 */
static inline __attribute__((no_instrument_function)) unsigned long *Model1_end_of_stack(struct Model1_task_struct *Model1_p)
{



 return (unsigned long *)(((struct Model1_thread_info *)(Model1_p)->Model1_stack) + 1);

}





static inline __attribute__((no_instrument_function)) int Model1_object_is_on_stack(void *Model1_obj)
{
 void *Model1_stack = ((Model1_get_current())->Model1_stack);

 return (Model1_obj >= Model1_stack) && (Model1_obj < (Model1_stack + (((1UL) << 12) << (2 + 0))));
}

extern void Model1_thread_stack_cache_init(void);


static inline __attribute__((no_instrument_function)) unsigned long Model1_stack_not_used(struct Model1_task_struct *Model1_p)
{
 unsigned long *Model1_n = Model1_end_of_stack(Model1_p);

 do { /* Skip over canary */



  Model1_n++;

 } while (!*Model1_n);




 return (unsigned long)Model1_n - (unsigned long)Model1_end_of_stack(Model1_p);

}

extern void Model1_set_task_stack_end_magic(struct Model1_task_struct *Model1_tsk);

/* set thread flags in other task's structures
 * - see asm/thread_info.h for TIF_xxxx flags available
 */
static inline __attribute__((no_instrument_function)) void Model1_set_tsk_thread_flag(struct Model1_task_struct *Model1_tsk, int Model1_flag)
{
 Model1_set_ti_thread_flag(((struct Model1_thread_info *)(Model1_tsk)->Model1_stack), Model1_flag);
}

static inline __attribute__((no_instrument_function)) void Model1_clear_tsk_thread_flag(struct Model1_task_struct *Model1_tsk, int Model1_flag)
{
 Model1_clear_ti_thread_flag(((struct Model1_thread_info *)(Model1_tsk)->Model1_stack), Model1_flag);
}

static inline __attribute__((no_instrument_function)) int Model1_test_and_set_tsk_thread_flag(struct Model1_task_struct *Model1_tsk, int Model1_flag)
{
 return Model1_test_and_set_ti_thread_flag(((struct Model1_thread_info *)(Model1_tsk)->Model1_stack), Model1_flag);
}

static inline __attribute__((no_instrument_function)) int Model1_test_and_clear_tsk_thread_flag(struct Model1_task_struct *Model1_tsk, int Model1_flag)
{
 return Model1_test_and_clear_ti_thread_flag(((struct Model1_thread_info *)(Model1_tsk)->Model1_stack), Model1_flag);
}

static inline __attribute__((no_instrument_function)) int Model1_test_tsk_thread_flag(struct Model1_task_struct *Model1_tsk, int Model1_flag)
{
 return Model1_test_ti_thread_flag(((struct Model1_thread_info *)(Model1_tsk)->Model1_stack), Model1_flag);
}

static inline __attribute__((no_instrument_function)) void Model1_set_tsk_need_resched(struct Model1_task_struct *Model1_tsk)
{
 Model1_set_tsk_thread_flag(Model1_tsk,3);
}

static inline __attribute__((no_instrument_function)) void Model1_clear_tsk_need_resched(struct Model1_task_struct *Model1_tsk)
{
 Model1_clear_tsk_thread_flag(Model1_tsk,3);
}

static inline __attribute__((no_instrument_function)) int Model1_test_tsk_need_resched(struct Model1_task_struct *Model1_tsk)
{
 return __builtin_expect(!!(Model1_test_tsk_thread_flag(Model1_tsk,3)), 0);
}

static inline __attribute__((no_instrument_function)) int Model1_restart_syscall(void)
{
 Model1_set_tsk_thread_flag(Model1_get_current(), 2);
 return -513;
}

static inline __attribute__((no_instrument_function)) int Model1_signal_pending(struct Model1_task_struct *Model1_p)
{
 return __builtin_expect(!!(Model1_test_tsk_thread_flag(Model1_p,2)), 0);
}

static inline __attribute__((no_instrument_function)) int Model1___fatal_signal_pending(struct Model1_task_struct *Model1_p)
{
 return __builtin_expect(!!(Model1_sigismember(&Model1_p->Model1_pending.Model1_signal, 9)), 0);
}

static inline __attribute__((no_instrument_function)) int Model1_fatal_signal_pending(struct Model1_task_struct *Model1_p)
{
 return Model1_signal_pending(Model1_p) && Model1___fatal_signal_pending(Model1_p);
}

static inline __attribute__((no_instrument_function)) int Model1_signal_pending_state(long Model1_state, struct Model1_task_struct *Model1_p)
{
 if (!(Model1_state & (1 | 128)))
  return 0;
 if (!Model1_signal_pending(Model1_p))
  return 0;

 return (Model1_state & 1) || Model1___fatal_signal_pending(Model1_p);
}

/*
 * cond_resched() and cond_resched_lock(): latency reduction via
 * explicit rescheduling in places that are safe. The return
 * value indicates whether a reschedule was done in fact.
 * cond_resched_lock() will drop the spinlock before scheduling,
 * cond_resched_softirq() will enable bhs before scheduling.
 */
extern int Model1__cond_resched(void);






extern int Model1___cond_resched_lock(Model1_spinlock_t *Model1_lock);






extern int Model1___cond_resched_softirq(void);






static inline __attribute__((no_instrument_function)) void Model1_cond_resched_rcu(void)
{

 Model1_rcu_read_unlock();
 ({ Model1____might_sleep("./include/linux/sched.h", 3234, 0); Model1__cond_resched(); });
 Model1_rcu_read_lock();

}

/*
 * Does a critical section need to be broken due to another
 * task waiting?: (technically does not depend on CONFIG_PREEMPT,
 * but a general need for low latency)
 */
static inline __attribute__((no_instrument_function)) int Model1_spin_needbreak(Model1_spinlock_t *Model1_lock)
{



 return 0;

}

/*
 * Idle thread specific functions to determine the need_resched
 * polling state.
 */

static inline __attribute__((no_instrument_function)) int Model1_tsk_is_polling(struct Model1_task_struct *Model1_p)
{
 return Model1_test_tsk_thread_flag(Model1_p, 21);
}

static inline __attribute__((no_instrument_function)) void Model1___current_set_polling(void)
{
 Model1_set_ti_thread_flag(Model1_current_thread_info(), 21);
}

static inline __attribute__((no_instrument_function)) bool __attribute__((warn_unused_result)) Model1_current_set_polling_and_test(void)
{
 Model1___current_set_polling();

 /*
	 * Polling state must be visible before we test NEED_RESCHED,
	 * paired by resched_curr()
	 */
 __asm__ __volatile__("": : :"memory");

 return __builtin_expect(!!(Model1_test_ti_thread_flag(Model1_current_thread_info(), 3)), 0);
}

static inline __attribute__((no_instrument_function)) void Model1___current_clr_polling(void)
{
 Model1_clear_ti_thread_flag(Model1_current_thread_info(), 21);
}

static inline __attribute__((no_instrument_function)) bool __attribute__((warn_unused_result)) Model1_current_clr_polling_and_test(void)
{
 Model1___current_clr_polling();

 /*
	 * Polling state must be visible before we test NEED_RESCHED,
	 * paired by resched_curr()
	 */
 __asm__ __volatile__("": : :"memory");

 return __builtin_expect(!!(Model1_test_ti_thread_flag(Model1_current_thread_info(), 3)), 0);
}
static inline __attribute__((no_instrument_function)) void Model1_current_clr_polling(void)
{
 Model1___current_clr_polling();

 /*
	 * Ensure we check TIF_NEED_RESCHED after we clear the polling bit.
	 * Once the bit is cleared, we'll get IPIs with every new
	 * TIF_NEED_RESCHED and the IPI handler, scheduler_ipi(), will also
	 * fold.
	 */
 asm volatile("mfence":::"memory"); /* paired with resched_curr() */

 do { if (Model1_test_ti_thread_flag(Model1_current_thread_info(), 3)) Model1_set_preempt_need_resched(); } while (0);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) bool Model1_need_resched(void)
{
 return __builtin_expect(!!(Model1_test_ti_thread_flag(Model1_current_thread_info(), 3)), 0);
}

/*
 * Thread group CPU time accounting.
 */
void Model1_thread_group_cputime(struct Model1_task_struct *Model1_tsk, struct Model1_task_cputime *Model1_times);
void Model1_thread_group_cputimer(struct Model1_task_struct *Model1_tsk, struct Model1_task_cputime *Model1_times);

/*
 * Reevaluate whether the task has signals pending delivery.
 * Wake the task if so.
 * This is required every time the blocked sigset_t changes.
 * callers must hold sighand->siglock.
 */
extern void Model1_recalc_sigpending_and_wake(struct Model1_task_struct *Model1_t);
extern void Model1_recalc_sigpending(void);

extern void Model1_signal_wake_up_state(struct Model1_task_struct *Model1_t, unsigned int Model1_state);

static inline __attribute__((no_instrument_function)) void Model1_signal_wake_up(struct Model1_task_struct *Model1_t, bool Model1_resume)
{
 Model1_signal_wake_up_state(Model1_t, Model1_resume ? 128 : 0);
}
static inline __attribute__((no_instrument_function)) void Model1_ptrace_signal_wake_up(struct Model1_task_struct *Model1_t, bool Model1_resume)
{
 Model1_signal_wake_up_state(Model1_t, Model1_resume ? 8 : 0);
}

/*
 * Wrappers for p->thread_info->cpu access. No-op on UP.
 */


static inline __attribute__((no_instrument_function)) unsigned int Model1_task_cpu(const struct Model1_task_struct *Model1_p)
{
 return ((struct Model1_thread_info *)(Model1_p)->Model1_stack)->Model1_cpu;
}

static inline __attribute__((no_instrument_function)) int Model1_task_node(const struct Model1_task_struct *Model1_p)
{
 return Model1_cpu_to_node(Model1_task_cpu(Model1_p));
}

extern void Model1_set_task_cpu(struct Model1_task_struct *Model1_p, unsigned int Model1_cpu);
extern long Model1_sched_setaffinity(Model1_pid_t Model1_pid, const struct Model1_cpumask *Model1_new_mask);
extern long Model1_sched_getaffinity(Model1_pid_t Model1_pid, struct Model1_cpumask *Model1_mask);


extern struct Model1_task_group Model1_root_task_group;


extern int Model1_task_can_switch_user(struct Model1_user_struct *Model1_up,
     struct Model1_task_struct *Model1_tsk);


static inline __attribute__((no_instrument_function)) void Model1_add_rchar(struct Model1_task_struct *Model1_tsk, Model1_ssize_t Model1_amt)
{
 Model1_tsk->Model1_ioac.Model1_rchar += Model1_amt;
}

static inline __attribute__((no_instrument_function)) void Model1_add_wchar(struct Model1_task_struct *Model1_tsk, Model1_ssize_t Model1_amt)
{
 Model1_tsk->Model1_ioac.Model1_wchar += Model1_amt;
}

static inline __attribute__((no_instrument_function)) void Model1_inc_syscr(struct Model1_task_struct *Model1_tsk)
{
 Model1_tsk->Model1_ioac.Model1_syscr++;
}

static inline __attribute__((no_instrument_function)) void Model1_inc_syscw(struct Model1_task_struct *Model1_tsk)
{
 Model1_tsk->Model1_ioac.Model1_syscw++;
}
static inline __attribute__((no_instrument_function)) void Model1_mm_update_next_owner(struct Model1_mm_struct *Model1_mm)
{
}


static inline __attribute__((no_instrument_function)) unsigned long Model1_task_rlimit(const struct Model1_task_struct *Model1_tsk,
  unsigned int Model1_limit)
{
 return ({ union { typeof(Model1_tsk->Model1_signal->Model1_rlim[Model1_limit].Model1_rlim_cur) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&(Model1_tsk->Model1_signal->Model1_rlim[Model1_limit].Model1_rlim_cur), Model1___u.Model1___c, sizeof(Model1_tsk->Model1_signal->Model1_rlim[Model1_limit].Model1_rlim_cur)); else Model1___read_once_size_nocheck(&(Model1_tsk->Model1_signal->Model1_rlim[Model1_limit].Model1_rlim_cur), Model1___u.Model1___c, sizeof(Model1_tsk->Model1_signal->Model1_rlim[Model1_limit].Model1_rlim_cur)); Model1___u.Model1___val; });
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_task_rlimit_max(const struct Model1_task_struct *Model1_tsk,
  unsigned int Model1_limit)
{
 return ({ union { typeof(Model1_tsk->Model1_signal->Model1_rlim[Model1_limit].Model1_rlim_max) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&(Model1_tsk->Model1_signal->Model1_rlim[Model1_limit].Model1_rlim_max), Model1___u.Model1___c, sizeof(Model1_tsk->Model1_signal->Model1_rlim[Model1_limit].Model1_rlim_max)); else Model1___read_once_size_nocheck(&(Model1_tsk->Model1_signal->Model1_rlim[Model1_limit].Model1_rlim_max), Model1___u.Model1___c, sizeof(Model1_tsk->Model1_signal->Model1_rlim[Model1_limit].Model1_rlim_max)); Model1___u.Model1___val; });
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_rlimit(unsigned int Model1_limit)
{
 return Model1_task_rlimit(Model1_get_current(), Model1_limit);
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_rlimit_max(unsigned int Model1_limit)
{
 return Model1_task_rlimit_max(Model1_get_current(), Model1_limit);
}


struct Model1_update_util_data {
 void (*func)(struct Model1_update_util_data *Model1_data,
       Model1_u64 Model1_time, unsigned long Model1_util, unsigned long Model1_max);
};

void Model1_cpufreq_add_update_util_hook(int Model1_cpu, struct Model1_update_util_data *Model1_data,
   void (*func)(struct Model1_update_util_data *Model1_data, Model1_u64 Model1_time,
         unsigned long Model1_util, unsigned long Model1_max));
void Model1_cpufreq_remove_update_util_hook(int Model1_cpu);


struct Model1_kmem_cache;
struct Model1_page;
struct Model1_vm_struct;
static inline __attribute__((no_instrument_function)) void Model1_kasan_unpoison_shadow(const void *Model1_address, Model1_size_t Model1_size) {}

static inline __attribute__((no_instrument_function)) void Model1_kasan_unpoison_task_stack(struct Model1_task_struct *Model1_task) {}

static inline __attribute__((no_instrument_function)) void Model1_kasan_enable_current(void) {}
static inline __attribute__((no_instrument_function)) void Model1_kasan_disable_current(void) {}

static inline __attribute__((no_instrument_function)) void Model1_kasan_alloc_pages(struct Model1_page *Model1_page, unsigned int Model1_order) {}
static inline __attribute__((no_instrument_function)) void Model1_kasan_free_pages(struct Model1_page *Model1_page, unsigned int Model1_order) {}

static inline __attribute__((no_instrument_function)) void Model1_kasan_cache_create(struct Model1_kmem_cache *Model1_cache,
          Model1_size_t *Model1_size,
          unsigned long *Model1_flags) {}
static inline __attribute__((no_instrument_function)) void Model1_kasan_cache_shrink(struct Model1_kmem_cache *Model1_cache) {}
static inline __attribute__((no_instrument_function)) void Model1_kasan_cache_destroy(struct Model1_kmem_cache *Model1_cache) {}

static inline __attribute__((no_instrument_function)) void Model1_kasan_poison_slab(struct Model1_page *Model1_page) {}
static inline __attribute__((no_instrument_function)) void Model1_kasan_unpoison_object_data(struct Model1_kmem_cache *Model1_cache,
     void *Model1_object) {}
static inline __attribute__((no_instrument_function)) void Model1_kasan_poison_object_data(struct Model1_kmem_cache *Model1_cache,
     void *Model1_object) {}
static inline __attribute__((no_instrument_function)) void Model1_kasan_init_slab_obj(struct Model1_kmem_cache *Model1_cache,
    const void *Model1_object) {}

static inline __attribute__((no_instrument_function)) void Model1_kasan_kmalloc_large(void *Model1_ptr, Model1_size_t Model1_size, Model1_gfp_t Model1_flags) {}
static inline __attribute__((no_instrument_function)) void Model1_kasan_kfree_large(const void *Model1_ptr) {}
static inline __attribute__((no_instrument_function)) void Model1_kasan_poison_kfree(void *Model1_ptr) {}
static inline __attribute__((no_instrument_function)) void Model1_kasan_kmalloc(struct Model1_kmem_cache *Model1_s, const void *Model1_object,
    Model1_size_t Model1_size, Model1_gfp_t Model1_flags) {}
static inline __attribute__((no_instrument_function)) void Model1_kasan_krealloc(const void *Model1_object, Model1_size_t Model1_new_size,
     Model1_gfp_t Model1_flags) {}

static inline __attribute__((no_instrument_function)) void Model1_kasan_slab_alloc(struct Model1_kmem_cache *Model1_s, void *Model1_object,
       Model1_gfp_t Model1_flags) {}
static inline __attribute__((no_instrument_function)) bool Model1_kasan_slab_free(struct Model1_kmem_cache *Model1_s, void *Model1_object)
{
 return false;
}

static inline __attribute__((no_instrument_function)) int Model1_kasan_module_alloc(void *Model1_addr, Model1_size_t Model1_size) { return 0; }
static inline __attribute__((no_instrument_function)) void Model1_kasan_free_shadow(const struct Model1_vm_struct *Model1_vm) {}

static inline __attribute__((no_instrument_function)) void Model1_kasan_unpoison_slab(const void *Model1_ptr) { }
static inline __attribute__((no_instrument_function)) Model1_size_t Model1_kasan_metadata_size(struct Model1_kmem_cache *Model1_cache) { return 0; }

struct Model1_mem_cgroup;
/*
 * struct kmem_cache related prototypes
 */
void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_kmem_cache_init(void);
bool Model1_slab_is_available(void);

struct Model1_kmem_cache *Model1_kmem_cache_create(const char *, Model1_size_t, Model1_size_t,
   unsigned long,
   void (*)(void *));
void Model1_kmem_cache_destroy(struct Model1_kmem_cache *);
int Model1_kmem_cache_shrink(struct Model1_kmem_cache *);

void Model1_memcg_create_kmem_cache(struct Model1_mem_cgroup *, struct Model1_kmem_cache *);
void Model1_memcg_deactivate_kmem_caches(struct Model1_mem_cgroup *);
void Model1_memcg_destroy_kmem_caches(struct Model1_mem_cgroup *);

/*
 * Please use this macro to create slab caches. Simply specify the
 * name of the structure and maybe some flags that are listed above.
 *
 * The alignment of the struct determines object alignment. If you
 * f.e. add ____cacheline_aligned_in_smp to the struct declaration
 * then the objects will be properly aligned in SMP configurations.
 */




/*
 * Common kmalloc functions provided by all allocators
 */
void * __attribute__((warn_unused_result)) Model1___krealloc(const void *, Model1_size_t, Model1_gfp_t);
void * __attribute__((warn_unused_result)) Model1_krealloc(const void *, Model1_size_t, Model1_gfp_t);
void Model1_kfree(const void *);
void Model1_kzfree(const void *);
Model1_size_t Model1_ksize(const void *);


const char *Model1___check_heap_object(const void *Model1_ptr, unsigned long Model1_n,
    struct Model1_page *Model1_page);
/*
 * Some archs want to perform DMA into kmalloc caches and need a guaranteed
 * alignment larger than the alignment of a 64-bit integer.
 * Setting ARCH_KMALLOC_MINALIGN in arch headers allows that.
 */
/*
 * Setting ARCH_SLAB_MINALIGN in arch headers allows a different alignment.
 * Intended for arches that get misalignment faults even for 64 bit integer
 * aligned buffers.
 */




/*
 * kmalloc and friends return ARCH_KMALLOC_MINALIGN aligned
 * pointers. kmem_cache_alloc and friends return ARCH_SLAB_MINALIGN
 * aligned pointers.
 */




/*
 * Kmalloc array related definitions
 */
/*
 * SLUB directly allocates requests fitting in to an order-1 page
 * (PAGE_SIZE*2).  Larger requests are passed to the page allocator.
 */
/* Maximum allocatable size */

/* Maximum size for which we actually use a slab cache */

/* Maximum order allocatable via the slab allocagtor */


/*
 * Kmalloc subsystem.
 */




/*
 * This restriction comes from byte sized index implementation.
 * Page size is normally 2^12 bytes and, in this case, if we want to use
 * byte sized index which can represent 2^8 entries, the size of the object
 * should be equal or greater to 2^12 / 2^8 = 2^4 = 16.
 * If minimum size of kmalloc is less than 16, we use it as minimum object
 * size and give up to use byte sized index.
 */




extern struct Model1_kmem_cache *Model1_kmalloc_caches[(12 + 1) + 1];

extern struct Model1_kmem_cache *Model1_kmalloc_dma_caches[(12 + 1) + 1];


/*
 * Figure out which kmalloc slab an allocation of a certain size
 * belongs to.
 * 0 = zero alloc
 * 1 =  65 .. 96 bytes
 * 2 = 129 .. 192 bytes
 * n = 2^(n-1)+1 .. 2^n
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_kmalloc_index(Model1_size_t Model1_size)
{
 if (!Model1_size)
  return 0;

 if (Model1_size <= (1 << 3))
  return 3;

 if ((1 << 3) <= 32 && Model1_size > 64 && Model1_size <= 96)
  return 1;
 if ((1 << 3) <= 64 && Model1_size > 128 && Model1_size <= 192)
  return 2;
 if (Model1_size <= 8) return 3;
 if (Model1_size <= 16) return 4;
 if (Model1_size <= 32) return 5;
 if (Model1_size <= 64) return 6;
 if (Model1_size <= 128) return 7;
 if (Model1_size <= 256) return 8;
 if (Model1_size <= 512) return 9;
 if (Model1_size <= 1024) return 10;
 if (Model1_size <= 2 * 1024) return 11;
 if (Model1_size <= 4 * 1024) return 12;
 if (Model1_size <= 8 * 1024) return 13;
 if (Model1_size <= 16 * 1024) return 14;
 if (Model1_size <= 32 * 1024) return 15;
 if (Model1_size <= 64 * 1024) return 16;
 if (Model1_size <= 128 * 1024) return 17;
 if (Model1_size <= 256 * 1024) return 18;
 if (Model1_size <= 512 * 1024) return 19;
 if (Model1_size <= 1024 * 1024) return 20;
 if (Model1_size <= 2 * 1024 * 1024) return 21;
 if (Model1_size <= 4 * 1024 * 1024) return 22;
 if (Model1_size <= 8 * 1024 * 1024) return 23;
 if (Model1_size <= 16 * 1024 * 1024) return 24;
 if (Model1_size <= 32 * 1024 * 1024) return 25;
 if (Model1_size <= 64 * 1024 * 1024) return 26;
 do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/slab.h"), "i" (323), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0);

 /* Will never be reached. Needed because the compiler may complain */
 return -1;
}


void *Model1___kmalloc(Model1_size_t Model1_size, Model1_gfp_t Model1_flags) __attribute__((__malloc__));
void *Model1_kmem_cache_alloc(struct Model1_kmem_cache *, Model1_gfp_t Model1_flags) __attribute__((__malloc__));
void Model1_kmem_cache_free(struct Model1_kmem_cache *, void *);

/*
 * Bulk allocation and freeing operations. These are accelerated in an
 * allocator specific way to avoid taking locks repeatedly or building
 * metadata structures unnecessarily.
 *
 * Note that interrupts must be enabled when calling these functions.
 */
void Model1_kmem_cache_free_bulk(struct Model1_kmem_cache *, Model1_size_t, void **);
int Model1_kmem_cache_alloc_bulk(struct Model1_kmem_cache *, Model1_gfp_t, Model1_size_t, void **);

/*
 * Caller must not use kfree_bulk() on memory not originally allocated
 * by kmalloc(), because the SLOB allocator cannot handle this.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_kfree_bulk(Model1_size_t Model1_size, void **Model1_p)
{
 Model1_kmem_cache_free_bulk(((void *)0), Model1_size, Model1_p);
}


void *Model1___kmalloc_node(Model1_size_t Model1_size, Model1_gfp_t Model1_flags, int Model1_node) __attribute__((__malloc__));
void *Model1_kmem_cache_alloc_node(struct Model1_kmem_cache *, Model1_gfp_t Model1_flags, int Model1_node) __attribute__((__malloc__));
extern void *Model1_kmem_cache_alloc_trace(struct Model1_kmem_cache *, Model1_gfp_t, Model1_size_t) __attribute__((__malloc__));


extern void *Model1_kmem_cache_alloc_node_trace(struct Model1_kmem_cache *Model1_s,
        Model1_gfp_t Model1_gfpflags,
        int Model1_node, Model1_size_t Model1_size) __attribute__((__malloc__));
extern void *Model1_kmalloc_order(Model1_size_t Model1_size, Model1_gfp_t Model1_flags, unsigned int Model1_order) __attribute__((__malloc__));


extern void *Model1_kmalloc_order_trace(Model1_size_t Model1_size, Model1_gfp_t Model1_flags, unsigned int Model1_order) __attribute__((__malloc__));
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void *Model1_kmalloc_large(Model1_size_t Model1_size, Model1_gfp_t Model1_flags)
{
 unsigned int Model1_order = ( __builtin_constant_p(Model1_size) ? ( ((Model1_size) == 0UL) ? 64 - 12 : (((Model1_size) < (1UL << 12)) ? 0 : ( __builtin_constant_p((Model1_size) - 1) ? ( ((Model1_size) - 1) < 1 ? Model1_____ilog2_NaN() : ((Model1_size) - 1) & (1ULL << 63) ? 63 : ((Model1_size) - 1) & (1ULL << 62) ? 62 : ((Model1_size) - 1) & (1ULL << 61) ? 61 : ((Model1_size) - 1) & (1ULL << 60) ? 60 : ((Model1_size) - 1) & (1ULL << 59) ? 59 : ((Model1_size) - 1) & (1ULL << 58) ? 58 : ((Model1_size) - 1) & (1ULL << 57) ? 57 : ((Model1_size) - 1) & (1ULL << 56) ? 56 : ((Model1_size) - 1) & (1ULL << 55) ? 55 : ((Model1_size) - 1) & (1ULL << 54) ? 54 : ((Model1_size) - 1) & (1ULL << 53) ? 53 : ((Model1_size) - 1) & (1ULL << 52) ? 52 : ((Model1_size) - 1) & (1ULL << 51) ? 51 : ((Model1_size) - 1) & (1ULL << 50) ? 50 : ((Model1_size) - 1) & (1ULL << 49) ? 49 : ((Model1_size) - 1) & (1ULL << 48) ? 48 : ((Model1_size) - 1) & (1ULL << 47) ? 47 : ((Model1_size) - 1) & (1ULL << 46) ? 46 : ((Model1_size) - 1) & (1ULL << 45) ? 45 : ((Model1_size) - 1) & (1ULL << 44) ? 44 : ((Model1_size) - 1) & (1ULL << 43) ? 43 : ((Model1_size) - 1) & (1ULL << 42) ? 42 : ((Model1_size) - 1) & (1ULL << 41) ? 41 : ((Model1_size) - 1) & (1ULL << 40) ? 40 : ((Model1_size) - 1) & (1ULL << 39) ? 39 : ((Model1_size) - 1) & (1ULL << 38) ? 38 : ((Model1_size) - 1) & (1ULL << 37) ? 37 : ((Model1_size) - 1) & (1ULL << 36) ? 36 : ((Model1_size) - 1) & (1ULL << 35) ? 35 : ((Model1_size) - 1) & (1ULL << 34) ? 34 : ((Model1_size) - 1) & (1ULL << 33) ? 33 : ((Model1_size) - 1) & (1ULL << 32) ? 32 : ((Model1_size) - 1) & (1ULL << 31) ? 31 : ((Model1_size) - 1) & (1ULL << 30) ? 30 : ((Model1_size) - 1) & (1ULL << 29) ? 29 : ((Model1_size) - 1) & (1ULL << 28) ? 28 : ((Model1_size) - 1) & (1ULL << 27) ? 27 : ((Model1_size) - 1) & (1ULL << 26) ? 26 : ((Model1_size) - 1) & (1ULL << 25) ? 25 : ((Model1_size) - 1) & (1ULL << 24) ? 24 : ((Model1_size) - 1) & (1ULL << 23) ? 23 : ((Model1_size) - 1) & (1ULL << 22) ? 22 : ((Model1_size) - 1) & (1ULL << 21) ? 21 : ((Model1_size) - 1) & (1ULL << 20) ? 20 : ((Model1_size) - 1) & (1ULL << 19) ? 19 : ((Model1_size) - 1) & (1ULL << 18) ? 18 : ((Model1_size) - 1) & (1ULL << 17) ? 17 : ((Model1_size) - 1) & (1ULL << 16) ? 16 : ((Model1_size) - 1) & (1ULL << 15) ? 15 : ((Model1_size) - 1) & (1ULL << 14) ? 14 : ((Model1_size) - 1) & (1ULL << 13) ? 13 : ((Model1_size) - 1) & (1ULL << 12) ? 12 : ((Model1_size) - 1) & (1ULL << 11) ? 11 : ((Model1_size) - 1) & (1ULL << 10) ? 10 : ((Model1_size) - 1) & (1ULL << 9) ? 9 : ((Model1_size) - 1) & (1ULL << 8) ? 8 : ((Model1_size) - 1) & (1ULL << 7) ? 7 : ((Model1_size) - 1) & (1ULL << 6) ? 6 : ((Model1_size) - 1) & (1ULL << 5) ? 5 : ((Model1_size) - 1) & (1ULL << 4) ? 4 : ((Model1_size) - 1) & (1ULL << 3) ? 3 : ((Model1_size) - 1) & (1ULL << 2) ? 2 : ((Model1_size) - 1) & (1ULL << 1) ? 1 : ((Model1_size) - 1) & (1ULL << 0) ? 0 : Model1_____ilog2_NaN() ) : (sizeof((Model1_size) - 1) <= 4) ? Model1___ilog2_u32((Model1_size) - 1) : Model1___ilog2_u64((Model1_size) - 1) ) - 12 + 1) ) : Model1___get_order(Model1_size) );
 return Model1_kmalloc_order_trace(Model1_size, Model1_flags, Model1_order);
}

/**
 * kmalloc - allocate memory
 * @size: how many bytes of memory are required.
 * @flags: the type of memory to allocate.
 *
 * kmalloc is the normal method of allocating memory
 * for objects smaller than page size in the kernel.
 *
 * The @flags argument may be one of:
 *
 * %GFP_USER - Allocate memory on behalf of user.  May sleep.
 *
 * %GFP_KERNEL - Allocate normal kernel ram.  May sleep.
 *
 * %GFP_ATOMIC - Allocation will not sleep.  May use emergency pools.
 *   For example, use this inside interrupt handlers.
 *
 * %GFP_HIGHUSER - Allocate pages from high memory.
 *
 * %GFP_NOIO - Do not do any I/O at all while trying to get memory.
 *
 * %GFP_NOFS - Do not make any fs calls while trying to get memory.
 *
 * %GFP_NOWAIT - Allocation will not sleep.
 *
 * %__GFP_THISNODE - Allocate node-local memory only.
 *
 * %GFP_DMA - Allocation suitable for DMA.
 *   Should only be used for kmalloc() caches. Otherwise, use a
 *   slab created with SLAB_DMA.
 *
 * Also it is possible to set different flags by OR'ing
 * in one or more of the following additional @flags:
 *
 * %__GFP_COLD - Request cache-cold pages instead of
 *   trying to return cache-warm pages.
 *
 * %__GFP_HIGH - This allocation has high priority and may use emergency pools.
 *
 * %__GFP_NOFAIL - Indicate that this allocation is in no way allowed to fail
 *   (think twice before using).
 *
 * %__GFP_NORETRY - If memory is not immediately available,
 *   then give up at once.
 *
 * %__GFP_NOWARN - If allocation fails, don't issue any warnings.
 *
 * %__GFP_REPEAT - If allocation fails initially, try once more before failing.
 *
 * There are other flags available as well, but these are not intended
 * for general use, and so are not documented here. For a full list of
 * potential flags, always refer to linux/gfp.h.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void *Model1_kmalloc(Model1_size_t Model1_size, Model1_gfp_t Model1_flags)
{
 if (__builtin_constant_p(Model1_size)) {
  if (Model1_size > (1UL << (12 + 1)))
   return Model1_kmalloc_large(Model1_size, Model1_flags);

  if (!(Model1_flags & (( Model1_gfp_t)0x01u))) {
   int Model1_index = Model1_kmalloc_index(Model1_size);

   if (!Model1_index)
    return ((void *)16);

   return Model1_kmem_cache_alloc_trace(Model1_kmalloc_caches[Model1_index],
     Model1_flags, Model1_size);
  }

 }
 return Model1___kmalloc(Model1_size, Model1_flags);
}

/*
 * Determine size used for the nth kmalloc cache.
 * return size or 0 if a kmalloc cache for that
 * size does not exist
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int Model1_kmalloc_size(int Model1_n)
{

 if (Model1_n > 2)
  return 1 << Model1_n;

 if (Model1_n == 1 && (1 << 3) <= 32)
  return 96;

 if (Model1_n == 2 && (1 << 3) <= 64)
  return 192;

 return 0;
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void *Model1_kmalloc_node(Model1_size_t Model1_size, Model1_gfp_t Model1_flags, int Model1_node)
{

 if (__builtin_constant_p(Model1_size) &&
  Model1_size <= (1UL << (12 + 1)) && !(Model1_flags & (( Model1_gfp_t)0x01u))) {
  int Model1_i = Model1_kmalloc_index(Model1_size);

  if (!Model1_i)
   return ((void *)16);

  return Model1_kmem_cache_alloc_node_trace(Model1_kmalloc_caches[Model1_i],
      Model1_flags, Model1_node, Model1_size);
 }

 return Model1___kmalloc_node(Model1_size, Model1_flags, Model1_node);
}

struct Model1_memcg_cache_array {
 struct Model1_callback_head Model1_rcu;
 struct Model1_kmem_cache *Model1_entries[0];
};

/*
 * This is the main placeholder for memcg-related information in kmem caches.
 * Both the root cache and the child caches will have it. For the root cache,
 * this will hold a dynamically allocated array large enough to hold
 * information about the currently limited memcgs in the system. To allow the
 * array to be accessed without taking any locks, on relocation we free the old
 * version only after a grace period.
 *
 * Child caches will hold extra metadata needed for its operation. Fields are:
 *
 * @memcg: pointer to the memcg this cache belongs to
 * @root_cache: pointer to the global, root cache, this cache was derived from
 *
 * Both root and child caches of the same kind are linked into a list chained
 * through @list.
 */
struct Model1_memcg_cache_params {
 bool Model1_is_root_cache;
 struct Model1_list_head Model1_list;
 union {
  struct Model1_memcg_cache_array *Model1_memcg_caches;
  struct {
   struct Model1_mem_cgroup *Model1_memcg;
   struct Model1_kmem_cache *Model1_root_cache;
  };
 };
};

int Model1_memcg_update_all_caches(int Model1_num_memcgs);

/**
 * kmalloc_array - allocate memory for an array.
 * @n: number of elements.
 * @size: element size.
 * @flags: the type of memory to allocate (see kmalloc).
 */
static inline __attribute__((no_instrument_function)) void *Model1_kmalloc_array(Model1_size_t Model1_n, Model1_size_t Model1_size, Model1_gfp_t Model1_flags)
{
 if (Model1_size != 0 && Model1_n > (~(Model1_size_t)0) / Model1_size)
  return ((void *)0);
 if (__builtin_constant_p(Model1_n) && __builtin_constant_p(Model1_size))
  return Model1_kmalloc(Model1_n * Model1_size, Model1_flags);
 return Model1___kmalloc(Model1_n * Model1_size, Model1_flags);
}

/**
 * kcalloc - allocate memory for an array. The memory is set to zero.
 * @n: number of elements.
 * @size: element size.
 * @flags: the type of memory to allocate (see kmalloc).
 */
static inline __attribute__((no_instrument_function)) void *Model1_kcalloc(Model1_size_t Model1_n, Model1_size_t Model1_size, Model1_gfp_t Model1_flags)
{
 return Model1_kmalloc_array(Model1_n, Model1_size, Model1_flags | (( Model1_gfp_t)0x8000u));
}

/*
 * kmalloc_track_caller is a special version of kmalloc that records the
 * calling function of the routine calling it for slab leak tracking instead
 * of just the calling function (confusing, eh?).
 * It's useful when the call to kmalloc comes from a widely-used standard
 * allocator where we care about the real place the memory allocation
 * request comes from.
 */
extern void *Model1___kmalloc_track_caller(Model1_size_t, Model1_gfp_t, unsigned long);




extern void *Model1___kmalloc_node_track_caller(Model1_size_t, Model1_gfp_t, int, unsigned long);
/*
 * Shortcuts
 */
static inline __attribute__((no_instrument_function)) void *Model1_kmem_cache_zalloc(struct Model1_kmem_cache *Model1_k, Model1_gfp_t Model1_flags)
{
 return Model1_kmem_cache_alloc(Model1_k, Model1_flags | (( Model1_gfp_t)0x8000u));
}

/**
 * kzalloc - allocate memory. The memory is set to zero.
 * @size: how many bytes of memory are required.
 * @flags: the type of memory to allocate (see kmalloc).
 */
static inline __attribute__((no_instrument_function)) void *Model1_kzalloc(Model1_size_t Model1_size, Model1_gfp_t Model1_flags)
{
 return Model1_kmalloc(Model1_size, Model1_flags | (( Model1_gfp_t)0x8000u));
}

/**
 * kzalloc_node - allocate zeroed memory from a particular memory node.
 * @size: how many bytes of memory are required.
 * @flags: the type of memory to allocate (see kmalloc).
 * @node: memory node from which to allocate
 */
static inline __attribute__((no_instrument_function)) void *Model1_kzalloc_node(Model1_size_t Model1_size, Model1_gfp_t Model1_flags, int Model1_node)
{
 return Model1_kmalloc_node(Model1_size, Model1_flags | (( Model1_gfp_t)0x8000u), Model1_node);
}

unsigned int Model1_kmem_cache_size(struct Model1_kmem_cache *Model1_s);
void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_kmem_cache_init_late(void);


/*
 * Dynamic loading of modules into the kernel.
 *
 * Rewritten by Richard Henderson <rth@tamu.edu> Dec 1996
 * Rewritten again by Rusty Russell, 2002
 */









struct Model1_stat {
 Model1___kernel_ulong_t Model1_st_dev;
 Model1___kernel_ulong_t Model1_st_ino;
 Model1___kernel_ulong_t Model1_st_nlink;

 unsigned int Model1_st_mode;
 unsigned int Model1_st_uid;
 unsigned int Model1_st_gid;
 unsigned int Model1___pad0;
 Model1___kernel_ulong_t Model1_st_rdev;
 Model1___kernel_long_t Model1_st_size;
 Model1___kernel_long_t Model1_st_blksize;
 Model1___kernel_long_t Model1_st_blocks; /* Number 512-byte blocks allocated. */

 Model1___kernel_ulong_t Model1_st_atime;
 Model1___kernel_ulong_t Model1_st_atime_nsec;
 Model1___kernel_ulong_t Model1_st_mtime;
 Model1___kernel_ulong_t Model1_st_mtime_nsec;
 Model1___kernel_ulong_t Model1_st_ctime;
 Model1___kernel_ulong_t Model1_st_ctime_nsec;
 Model1___kernel_long_t Model1___unused[3];
};

/* We don't need to memset the whole thing just to initialize the padding */
/* for 32bit emulation and 32 bit kernels */
struct Model1___old_kernel_stat {
 unsigned short Model1_st_dev;
 unsigned short Model1_st_ino;
 unsigned short Model1_st_mode;
 unsigned short Model1_st_nlink;
 unsigned short Model1_st_uid;
 unsigned short Model1_st_gid;
 unsigned short Model1_st_rdev;






 unsigned int Model1_st_size;
 unsigned int Model1_st_atime;
 unsigned int Model1_st_mtime;
 unsigned int Model1_st_ctime;

};
struct Model1_kstat {
 Model1_u64 Model1_ino;
 Model1_dev_t Model1_dev;
 Model1_umode_t Model1_mode;
 unsigned int Model1_nlink;
 Model1_kuid_t Model1_uid;
 Model1_kgid_t Model1_gid;
 Model1_dev_t Model1_rdev;
 Model1_loff_t Model1_size;
 struct Model1_timespec Model1_atime;
 struct Model1_timespec Model1_mtime;
 struct Model1_timespec Model1_ctime;
 unsigned long Model1_blksize;
 unsigned long long Model1_blocks;
};





/*
 *	include/linux/kmod.h
 *
 *      This program is free software; you can redistribute it and/or modify
 *      it under the terms of the GNU General Public License as published by
 *      the Free Software Foundation; either version 2 of the License, or
 *      (at your option) any later version.
 *
 *      This program is distributed in the hope that it will be useful,
 *      but WITHOUT ANY WARRANTY; without even the implied warranty of
 *      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *      GNU General Public License for more details.
 *
 *      You should have received a copy of the GNU General Public License
 *      along with this program; if not, write to the Free Software
 *      Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
 */
extern char Model1_modprobe_path[]; /* for sysctl */
/* modprobe exit status on success, -ve on error.  Return value
 * usually useless though. */
extern __attribute__((format(printf, 2, 3)))
int Model1___request_module(bool Model1_wait, const char *Model1_name, ...);
struct Model1_cred;
struct Model1_file;






struct Model1_subprocess_info {
 struct Model1_work_struct Model1_work;
 struct Model1_completion *Model1_complete;
 char *Model1_path;
 char **Model1_argv;
 char **Model1_envp;
 int Model1_wait;
 int Model1_retval;
 int (*Model1_init)(struct Model1_subprocess_info *Model1_info, struct Model1_cred *Model1_new);
 void (*Model1_cleanup)(struct Model1_subprocess_info *Model1_info);
 void *Model1_data;
};

extern int
Model1_call_usermodehelper(char *Model1_path, char **Model1_argv, char **Model1_envp, int Model1_wait);

extern struct Model1_subprocess_info *
Model1_call_usermodehelper_setup(char *Model1_path, char **Model1_argv, char **Model1_envp, Model1_gfp_t Model1_gfp_mask,
     int (*Model1_init)(struct Model1_subprocess_info *Model1_info, struct Model1_cred *Model1_new),
     void (*Model1_cleanup)(struct Model1_subprocess_info *), void *Model1_data);

extern int
Model1_call_usermodehelper_exec(struct Model1_subprocess_info *Model1_info, int Model1_wait);

extern struct Model1_ctl_table Model1_usermodehelper_table[];

enum Model1_umh_disable_depth {
 Model1_UMH_ENABLED = 0,
 Model1_UMH_FREEZING,
 Model1_UMH_DISABLED,
};

extern int Model1___usermodehelper_disable(enum Model1_umh_disable_depth Model1_depth);
extern void Model1___usermodehelper_set_disable_depth(enum Model1_umh_disable_depth Model1_depth);

static inline __attribute__((no_instrument_function)) int Model1_usermodehelper_disable(void)
{
 return Model1___usermodehelper_disable(Model1_UMH_DISABLED);
}

static inline __attribute__((no_instrument_function)) void Model1_usermodehelper_enable(void)
{
 Model1___usermodehelper_set_disable_depth(Model1_UMH_ENABLED);
}

extern int Model1_usermodehelper_read_trylock(void);
extern long Model1_usermodehelper_read_lock_wait(long Model1_timeout);
extern void Model1_usermodehelper_read_unlock(void);







/*
 * ELF register definitions..
 */















/* Core file format: The core file is written in such a way that gdb
   can understand it and provide useful information to the user.
   There are quite a number of obstacles to being able to view the
   contents of the floating point registers, and until these are
   solved you will not be able to view the contents of them.
   Actually, you can read in the core file and look at the contents of
   the user struct to find out what the floating point registers
   contain.

   The actual file contents are as follows:
   UPAGE: 1 page consisting of a user struct that tells gdb what is present
   in the file.  Directly after this is a copy of the task_struct, which
   is currently not used by gdb, but it may come in useful at some point.
   All of the registers are stored as part of the upage.  The upage should
   always be only one page.
   DATA: The data area is stored.  We use current->end_text to
   current->brk to pick up all of the user variables, plus any memory
   that may have been malloced.  No attempt is made to determine if a page
   is demand-zero or if a page is totally unused, we just cover the entire
   range.  All of the addresses are rounded in such a way that an integral
   number of pages is written.
   STACK: We need the stack information in order to get a meaningful
   backtrace.  We need to write the data from (esp) to
   current->start_stack, so we round each of these off in order to be able
   to write an integer number of pages.
   The minimum core file size is 3 pages, or 12288 bytes.  */

/*
 * Pentium III FXSR, SSE support
 *	Gareth Hughes <gareth@valinux.com>, May 2000
 *
 * Provide support for the GDB 5.0+ PTRACE_{GET|SET}FPXREGS requests for
 * interacting with the FXSR-format floating point environment.  Floating
 * point data can be accessed in the regular format in the usual manner,
 * and both the standard and SIMD floating point data can be accessed via
 * the new ptrace requests.  In either case, changes to the FPU environment
 * will be reflected in the task's state as expected.
 *
 * x86-64 support by Andi Kleen.
 */

/* This matches the 64bit FXSAVE format as defined by AMD. It is the same
   as the 32bit format defined by Intel, except that the selector:offset pairs
   for data and eip are replaced with flat 64bit pointers. */
struct Model1_user_i387_struct {
 unsigned short Model1_cwd;
 unsigned short Model1_swd;
 unsigned short Model1_twd; /* Note this is not the same as
				   the 32bit/x87/FSAVE twd */
 unsigned short Model1_fop;
 __u64 Model1_rip;
 __u64 Model1_rdp;
 __u32 Model1_mxcsr;
 __u32 Model1_mxcsr_mask;
 __u32 Model1_st_space[32]; /* 8*16 bytes for each FP-reg = 128 bytes */
 __u32 Model1_xmm_space[64]; /* 16*16 bytes for each XMM-reg = 256 bytes */
 __u32 Model1_padding[24];
};

/*
 * Segment register layout in coredumps.
 */
struct Model1_user_regs_struct {
 unsigned long Model1_r15;
 unsigned long Model1_r14;
 unsigned long Model1_r13;
 unsigned long Model1_r12;
 unsigned long Model1_bp;
 unsigned long Model1_bx;
 unsigned long Model1_r11;
 unsigned long Model1_r10;
 unsigned long Model1_r9;
 unsigned long Model1_r8;
 unsigned long Model1_ax;
 unsigned long Model1_cx;
 unsigned long Model1_dx;
 unsigned long Model1_si;
 unsigned long Model1_di;
 unsigned long Model1_orig_ax;
 unsigned long Model1_ip;
 unsigned long Model1_cs;
 unsigned long Model1_flags;
 unsigned long Model1_sp;
 unsigned long Model1_ss;
 unsigned long Model1_fs_base;
 unsigned long Model1_gs_base;
 unsigned long Model1_ds;
 unsigned long Model1_es;
 unsigned long Model1_fs;
 unsigned long Model1_gs;
};

/* When the kernel dumps core, it starts by dumping the user struct -
   this will be used by gdb to figure out where the data and stack segments
   are within the file, and what virtual addresses to use. */

struct Model1_user {
/* We start with the registers, to mimic the way that "memory" is returned
   from the ptrace(3,...) function.  */
  struct Model1_user_regs_struct Model1_regs; /* Where the registers are actually stored */
/* ptrace does not yet supply these.  Someday.... */
  int Model1_u_fpvalid; /* True if math co-processor being used. */
    /* for this mess. Not yet used. */
  int Model1_pad0;
  struct Model1_user_i387_struct Model1_i387; /* Math Co-processor registers. */
/* The rest of this junk is to help gdb figure out what goes where */
  unsigned long int Model1_u_tsize; /* Text segment size (pages). */
  unsigned long int Model1_u_dsize; /* Data segment size (pages). */
  unsigned long int Model1_u_ssize; /* Stack segment size (pages). */
  unsigned long Model1_start_code; /* Starting virtual address of text. */
  unsigned long Model1_start_stack; /* Starting virtual address of stack area.
				   This is actually the bottom of the stack,
				   the top of the stack is always found in the
				   esp register.  */
  long int Model1_signal; /* Signal that caused the core dump. */
  int Model1_reserved; /* No longer used */
  int Model1_pad1;
  unsigned long Model1_u_ar0; /* Used by gdb to help find the values for */
    /* the registers. */
  struct Model1_user_i387_struct *Model1_u_fpstate; /* Math Co-processor pointer. */
  unsigned long Model1_magic; /* To uniquely identify a core file */
  char Model1_u_comm[32]; /* User command that was responsible */
  unsigned long Model1_u_debugreg[8];
  unsigned long Model1_error_code; /* CPU error code or 0 */
  unsigned long Model1_fault_address; /* CR3 or 0 */
};




struct Model1_user_ymmh_regs {
 /* 16 * 16 bytes for each YMMH-reg */
 __u32 Model1_ymmh_space[64];
};

struct Model1_user_xstate_header {
 __u64 Model1_xfeatures;
 __u64 Model1_reserved1[2];
 __u64 Model1_reserved2[5];
};

/*
 * The structure layout of user_xstateregs, used for exporting the
 * extended register state through ptrace and core-dump (NT_X86_XSTATE note)
 * interfaces will be same as the memory layout of xsave used by the processor
 * (except for the bytes 464..511, which can be used by the software) and hence
 * the size of this structure varies depending on the features supported by the
 * processor and OS. The size of the structure that users need to use can be
 * obtained by doing:
 *     cpuid_count(0xd, 0, &eax, &ptrace_xstateregs_struct_size, &ecx, &edx);
 * i.e., cpuid.(eax=0xd,ecx=0).ebx will be the size that user (debuggers, etc.)
 * need to use.
 *
 * For now, only the first 8 bytes of the software usable bytes[464..471] will
 * be used and will be set to OS enabled xstate mask (which is same as the
 * 64bit mask returned by the xgetbv's xCR0).  Users (analyzing core dump
 * remotely, etc.) can use this mask as well as the mask saved in the
 * xstate_hdr bytes and interpret what states the processor/OS supports
 * and what states are in modified/initialized conditions for the
 * particular process/thread.
 *
 * Also when the user modifies certain state FP/SSE/etc through the
 * ptrace interface, they must ensure that the header.xfeatures
 * bytes[512..519] of the memory layout are updated correspondingly.
 * i.e., for example when FP state is modified to a non-init state,
 * header.xfeatures's bit 0 must be set to '1', when SSE is modified to
 * non-init state, header.xfeatures's bit 1 must to be set to '1', etc.
 */



struct Model1_user_xstateregs {
 struct {
  __u64 Model1_fpx_space[58];
  __u64 Model1_xstate_fx_sw[6];
 } Model1_i387;
 struct Model1_user_xstate_header Model1_header;
 struct Model1_user_ymmh_regs Model1_ymmh;
 /* further processor state extensions go here */
};


typedef unsigned long Model1_elf_greg_t;


typedef Model1_elf_greg_t Model1_elf_gregset_t[(sizeof(struct Model1_user_regs_struct) / sizeof(Model1_elf_greg_t))];

typedef struct Model1_user_i387_struct Model1_elf_fpregset_t;
/* x86-64 relocation types */
/*
 * These are used to set parameters in the core dumps.
 */







struct Model1_vdso_image {
 void *Model1_data;
 unsigned long Model1_size; /* Always a multiple of PAGE_SIZE */

 unsigned long Model1_alt, Model1_alt_len;

 long Model1_sym_vvar_start; /* Negative offset to the vvar area */

 long Model1_sym_vvar_page;
 long Model1_sym_hpet_page;
 long Model1_sym_pvclock_page;
 long Model1_sym_VDSO32_NOTE_MASK;
 long Model1_sym___kernel_sigreturn;
 long Model1_sym___kernel_rt_sigreturn;
 long Model1_sym___kernel_vsyscall;
 long Model1_sym_int80_landing_pad;
};


extern const struct Model1_vdso_image Model1_vdso_image_64;







extern const struct Model1_vdso_image Model1_vdso_image_32;


extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_init_vdso_image(const struct Model1_vdso_image *Model1_image);


extern unsigned int Model1_vdso64_enabled;


extern unsigned int Model1_vdso32_enabled;


/*
 * This is used to ensure we don't load something for the wrong architecture.
 */
/*
 * This is used to ensure we don't load something for the wrong architecture.
 */
static inline __attribute__((no_instrument_function)) void Model1_elf_common_init(struct Model1_thread_struct *Model1_t,
       struct Model1_pt_regs *Model1_regs, const Model1_u16 Model1_ds)
{
 /* ax gets execve's return value. */
 /*regs->ax = */ Model1_regs->Model1_bx = Model1_regs->Model1_cx = Model1_regs->Model1_dx = 0;
 Model1_regs->Model1_si = Model1_regs->Model1_di = Model1_regs->Model1_bp = 0;
 Model1_regs->Model1_r8 = Model1_regs->Model1_r9 = Model1_regs->Model1_r10 = Model1_regs->Model1_r11 = 0;
 Model1_regs->Model1_r12 = Model1_regs->Model1_r13 = Model1_regs->Model1_r14 = Model1_regs->Model1_r15 = 0;
 Model1_t->Model1_fsbase = Model1_t->Model1_gsbase = 0;
 Model1_t->Model1_fsindex = Model1_t->Model1_gsindex = 0;
 Model1_t->Model1_ds = Model1_t->Model1_es = Model1_ds;
}







void Model1_compat_start_thread(struct Model1_pt_regs *Model1_regs, Model1_u32 Model1_new_ip, Model1_u32 Model1_new_sp);


void Model1_set_personality_ia32(bool);





/*
 * regs is struct pt_regs, pr_reg is elf_gregset_t (which is
 * now struct_user_regs, they are different). Assumes current is the process
 * getting dumped.
 */
/* I'm not sure if we can use '-' here */

extern void Model1_set_personality_64bit(void);
extern unsigned int Model1_sysctl_vsyscall32;
extern int Model1_force_personality32;






/* This is the location that an ET_DYN program is loaded if exec'ed.  Typical
   use of this is to invoke "./ld.so someprog" to test out a new version of
   the loader.  We need to make sure that it is out of the way of the program
   that it will "exec", and that there is sufficient room for the brk.  */



/* This yields a mask that user programs can use to figure out what
   instruction set this CPU supports.  This could be done in user space,
   but it's not easy, and we've already done it here.  */



/* This yields a string that ld.so will use to load implementation
   specific libraries for optimization.  This is more specific in
   intent than poking at uname or /proc/cpuinfo.

   For the moment, we have only optimizations for the Intel generations,
   but that could change... */



/*
 * An executable for which elf_read_implies_exec() returns TRUE will
 * have the READ_IMPLIES_EXEC personality flag set automatically.
 */



struct Model1_task_struct;
/* 1GB for 64bit, 8MB for 32bit */
/* As a historical oddity, the x32 and x86_64 vDSOs are controlled together. */
struct Model1_linux_binprm;


extern int Model1_arch_setup_additional_pages(struct Model1_linux_binprm *Model1_bprm,
           int Model1_uses_interp);
extern int Model1_compat_arch_setup_additional_pages(struct Model1_linux_binprm *Model1_bprm,
           int Model1_uses_interp);


/*
 * True on X86_32 or when emulating IA32 on X86_64
 */
static inline __attribute__((no_instrument_function)) int Model1_mmap_is_ia32(void)
{
 return 0 ||
        (1 &&
  Model1_test_ti_thread_flag(Model1_current_thread_info(), 29));
}

/* Do not change the values. See get_align_mask() */
enum Model1_align_flags {
 Model1_ALIGN_VA_32 = (1UL << (0)),
 Model1_ALIGN_VA_64 = (1UL << (1)),
};

struct Model1_va_alignment {
 int Model1_flags;
 unsigned long Model1_mask;
 unsigned long Model1_bits;
} __attribute__((__aligned__((1 << (6)))));

extern struct Model1_va_alignment Model1_va_align;
extern unsigned long Model1_align_vdso_addr(unsigned long);







/* These constants define the various ELF target machines */
    /* Next two are historical and binaries and
				   modules of these types will be rejected by
				   Linux.  */
/*
 * This is an interim value that we will use until the committee comes
 * up with a final number.
 */


/* Bogus old m32r magic number, used by old tools. */

/* This is the old interim value for S/390 architecture */

/* Also Panasonic/MEI MN10300, AM33 */

/* 32-bit ELF base types. */
typedef __u32 Model1_Elf32_Addr;
typedef Model1___u16 Model1_Elf32_Half;
typedef __u32 Model1_Elf32_Off;
typedef Model1___s32 Model1_Elf32_Sword;
typedef __u32 Model1_Elf32_Word;

/* 64-bit ELF base types. */
typedef __u64 Model1_Elf64_Addr;
typedef Model1___u16 Model1_Elf64_Half;
typedef Model1___s16 Model1_Elf64_SHalf;
typedef __u64 Model1_Elf64_Off;
typedef Model1___s32 Model1_Elf64_Sword;
typedef __u32 Model1_Elf64_Word;
typedef __u64 Model1_Elf64_Xword;
typedef Model1___s64 Model1_Elf64_Sxword;

/* These constants are for the segment types stored in the image headers */
/*
 * Extended Numbering
 *
 * If the real number of program header table entries is larger than
 * or equal to PN_XNUM(0xffff), it is set to sh_info field of the
 * section header at index 0, and PN_XNUM is set to e_phnum
 * field. Otherwise, the section header at index 0 is zero
 * initialized, if it exists.
 *
 * Specifications are available in:
 *
 * - Oracle: Linker and Libraries.
 *   Part No: 817198419, August 2011.
 *   http://docs.oracle.com/cd/E18752_01/pdf/817-1984.pdf
 *
 * - System V ABI AMD64 Architecture Processor Supplement
 *   Draft Version 0.99.4,
 *   January 13, 2010.
 *   http://www.cs.washington.edu/education/courses/cse351/12wi/supp-docs/abi.pdf
 */


/* These constants define the different elf file types */
/* This is the info that is needed to parse the dynamic section of the file */
/* This info is needed when parsing the symbol table */
typedef struct Model1_dynamic{
  Model1_Elf32_Sword Model1_d_tag;
  union{
    Model1_Elf32_Sword Model1_d_val;
    Model1_Elf32_Addr Model1_d_ptr;
  } Model1_d_un;
} Model1_Elf32_Dyn;

typedef struct {
  Model1_Elf64_Sxword Model1_d_tag; /* entry tag value */
  union {
    Model1_Elf64_Xword Model1_d_val;
    Model1_Elf64_Addr Model1_d_ptr;
  } Model1_d_un;
} Model1_Elf64_Dyn;

/* The following are used with relocations */






typedef struct Model1_elf32_rel {
  Model1_Elf32_Addr Model1_r_offset;
  Model1_Elf32_Word Model1_r_info;
} Model1_Elf32_Rel;

typedef struct Model1_elf64_rel {
  Model1_Elf64_Addr Model1_r_offset; /* Location at which to apply the action */
  Model1_Elf64_Xword Model1_r_info; /* index and type of relocation */
} Model1_Elf64_Rel;

typedef struct Model1_elf32_rela{
  Model1_Elf32_Addr Model1_r_offset;
  Model1_Elf32_Word Model1_r_info;
  Model1_Elf32_Sword Model1_r_addend;
} Model1_Elf32_Rela;

typedef struct Model1_elf64_rela {
  Model1_Elf64_Addr Model1_r_offset; /* Location at which to apply the action */
  Model1_Elf64_Xword Model1_r_info; /* index and type of relocation */
  Model1_Elf64_Sxword Model1_r_addend; /* Constant addend used to compute value */
} Model1_Elf64_Rela;

typedef struct Model1_elf32_sym{
  Model1_Elf32_Word Model1_st_name;
  Model1_Elf32_Addr Model1_st_value;
  Model1_Elf32_Word Model1_st_size;
  unsigned char Model1_st_info;
  unsigned char Model1_st_other;
  Model1_Elf32_Half Model1_st_shndx;
} Model1_Elf32_Sym;

typedef struct Model1_elf64_sym {
  Model1_Elf64_Word Model1_st_name; /* Symbol name, index in string tbl */
  unsigned char Model1_st_info; /* Type and binding attributes */
  unsigned char Model1_st_other; /* No defined meaning, 0 */
  Model1_Elf64_Half Model1_st_shndx; /* Associated section index */
  Model1_Elf64_Addr Model1_st_value; /* Value of the symbol */
  Model1_Elf64_Xword Model1_st_size; /* Associated symbol size */
} Model1_Elf64_Sym;




typedef struct Model1_elf32_hdr{
  unsigned char Model1_e_ident[16];
  Model1_Elf32_Half Model1_e_type;
  Model1_Elf32_Half Model1_e_machine;
  Model1_Elf32_Word Model1_e_version;
  Model1_Elf32_Addr Model1_e_entry; /* Entry point */
  Model1_Elf32_Off Model1_e_phoff;
  Model1_Elf32_Off Model1_e_shoff;
  Model1_Elf32_Word Model1_e_flags;
  Model1_Elf32_Half Model1_e_ehsize;
  Model1_Elf32_Half Model1_e_phentsize;
  Model1_Elf32_Half Model1_e_phnum;
  Model1_Elf32_Half Model1_e_shentsize;
  Model1_Elf32_Half Model1_e_shnum;
  Model1_Elf32_Half Model1_e_shstrndx;
} Model1_Elf32_Ehdr;

typedef struct Model1_elf64_hdr {
  unsigned char Model1_e_ident[16]; /* ELF "magic number" */
  Model1_Elf64_Half Model1_e_type;
  Model1_Elf64_Half Model1_e_machine;
  Model1_Elf64_Word Model1_e_version;
  Model1_Elf64_Addr Model1_e_entry; /* Entry point virtual address */
  Model1_Elf64_Off Model1_e_phoff; /* Program header table file offset */
  Model1_Elf64_Off Model1_e_shoff; /* Section header table file offset */
  Model1_Elf64_Word Model1_e_flags;
  Model1_Elf64_Half Model1_e_ehsize;
  Model1_Elf64_Half Model1_e_phentsize;
  Model1_Elf64_Half Model1_e_phnum;
  Model1_Elf64_Half Model1_e_shentsize;
  Model1_Elf64_Half Model1_e_shnum;
  Model1_Elf64_Half Model1_e_shstrndx;
} Model1_Elf64_Ehdr;

/* These constants define the permissions on sections in the program
   header, p_flags. */




typedef struct Model1_elf32_phdr{
  Model1_Elf32_Word Model1_p_type;
  Model1_Elf32_Off Model1_p_offset;
  Model1_Elf32_Addr Model1_p_vaddr;
  Model1_Elf32_Addr Model1_p_paddr;
  Model1_Elf32_Word Model1_p_filesz;
  Model1_Elf32_Word Model1_p_memsz;
  Model1_Elf32_Word Model1_p_flags;
  Model1_Elf32_Word Model1_p_align;
} Model1_Elf32_Phdr;

typedef struct Model1_elf64_phdr {
  Model1_Elf64_Word Model1_p_type;
  Model1_Elf64_Word Model1_p_flags;
  Model1_Elf64_Off Model1_p_offset; /* Segment file offset */
  Model1_Elf64_Addr Model1_p_vaddr; /* Segment virtual address */
  Model1_Elf64_Addr Model1_p_paddr; /* Segment physical address */
  Model1_Elf64_Xword Model1_p_filesz; /* Segment size in file */
  Model1_Elf64_Xword Model1_p_memsz; /* Segment size in memory */
  Model1_Elf64_Xword Model1_p_align; /* Segment alignment, file & memory */
} Model1_Elf64_Phdr;

/* sh_type */
/* sh_flags */







/* special section indexes */
typedef struct Model1_elf32_shdr {
  Model1_Elf32_Word Model1_sh_name;
  Model1_Elf32_Word Model1_sh_type;
  Model1_Elf32_Word Model1_sh_flags;
  Model1_Elf32_Addr Model1_sh_addr;
  Model1_Elf32_Off Model1_sh_offset;
  Model1_Elf32_Word Model1_sh_size;
  Model1_Elf32_Word Model1_sh_link;
  Model1_Elf32_Word Model1_sh_info;
  Model1_Elf32_Word Model1_sh_addralign;
  Model1_Elf32_Word Model1_sh_entsize;
} Model1_Elf32_Shdr;

typedef struct Model1_elf64_shdr {
  Model1_Elf64_Word Model1_sh_name; /* Section name, index in string tbl */
  Model1_Elf64_Word Model1_sh_type; /* Type of section */
  Model1_Elf64_Xword Model1_sh_flags; /* Miscellaneous section attributes */
  Model1_Elf64_Addr Model1_sh_addr; /* Section virtual addr at execution */
  Model1_Elf64_Off Model1_sh_offset; /* Section file offset */
  Model1_Elf64_Xword Model1_sh_size; /* Size of section in bytes */
  Model1_Elf64_Word Model1_sh_link; /* Index of another section */
  Model1_Elf64_Word Model1_sh_info; /* Additional section information */
  Model1_Elf64_Xword Model1_sh_addralign; /* Section alignment */
  Model1_Elf64_Xword Model1_sh_entsize; /* Entry size if section holds table */
} Model1_Elf64_Shdr;
/*
 * Notes used in ET_CORE. Architectures export some of the arch register sets
 * using the corresponding note types via the PTRACE_GETREGSET and
 * PTRACE_SETREGSET requests.
 */





/*
 * Note to userspace developers: size of NT_SIGINFO note may increase
 * in the future to accomodate more fields, don't assume it is fixed!
 */
/* Note header in a PT_NOTE section */
typedef struct Model1_elf32_note {
  Model1_Elf32_Word Model1_n_namesz; /* Name size */
  Model1_Elf32_Word Model1_n_descsz; /* Content size */
  Model1_Elf32_Word Model1_n_type; /* Content type */
} Model1_Elf32_Nhdr;

/* Note header in a PT_NOTE section */
typedef struct Model1_elf64_note {
  Model1_Elf64_Word Model1_n_namesz; /* Name size */
  Model1_Elf64_Word Model1_n_descsz; /* Content size */
  Model1_Elf64_Word Model1_n_type; /* Content type */
} Model1_Elf64_Nhdr;
extern Model1_Elf64_Dyn Model1__DYNAMIC [];
/* Optional callbacks to write extra ELF notes. */
struct Model1_file;
struct Model1_coredump_params;


static inline __attribute__((no_instrument_function)) int Model1_elf_coredump_extra_notes_size(void) { return 0; }
static inline __attribute__((no_instrument_function)) int Model1_elf_coredump_extra_notes_write(struct Model1_coredump_params *Model1_cprm) { return 0; }

/*
 * kobject.h - generic kernel object infrastructure.
 *
 * Copyright (c) 2002-2003 Patrick Mochel
 * Copyright (c) 2002-2003 Open Source Development Labs
 * Copyright (c) 2006-2008 Greg Kroah-Hartman <greg@kroah.com>
 * Copyright (c) 2006-2008 Novell Inc.
 *
 * This file is released under the GPLv2.
 *
 * Please read Documentation/kobject.txt before using the kobject
 * interface, ESPECIALLY the parts about reference counts and object
 * destructors.
 */







/*
 * sysfs.h - definitions for the device driver filesystem
 *
 * Copyright (c) 2001,2002 Patrick Mochel
 * Copyright (c) 2004 Silicon Graphics, Inc.
 * Copyright (c) 2007 SUSE Linux Products GmbH
 * Copyright (c) 2007 Tejun Heo <teheo@suse.de>
 *
 * Please see Documentation/filesystems/sysfs.txt for more information.
 */





/*
 * kernfs.h - pseudo filesystem decoupled from vfs locking
 *
 * This file is released under the GPLv2.
 */
struct Model1_file;
struct Model1_dentry;
struct Model1_iattr;
struct Model1_seq_file;
struct Model1_vm_area_struct;
struct Model1_super_block;
struct Model1_file_system_type;

struct Model1_kernfs_open_node;
struct Model1_kernfs_iattrs;

enum Model1_kernfs_node_type {
 Model1_KERNFS_DIR = 0x0001,
 Model1_KERNFS_FILE = 0x0002,
 Model1_KERNFS_LINK = 0x0004,
};




enum Model1_kernfs_node_flag {
 Model1_KERNFS_ACTIVATED = 0x0010,
 Model1_KERNFS_NS = 0x0020,
 Model1_KERNFS_HAS_SEQ_SHOW = 0x0040,
 Model1_KERNFS_HAS_MMAP = 0x0080,
 Model1_KERNFS_LOCKDEP = 0x0100,
 Model1_KERNFS_SUICIDAL = 0x0400,
 Model1_KERNFS_SUICIDED = 0x0800,
 Model1_KERNFS_EMPTY_DIR = 0x1000,
};

/* @flags for kernfs_create_root() */
enum Model1_kernfs_root_flag {
 /*
	 * kernfs_nodes are created in the deactivated state and invisible.
	 * They require explicit kernfs_activate() to become visible.  This
	 * can be used to make related nodes become visible atomically
	 * after all nodes are created successfully.
	 */
 Model1_KERNFS_ROOT_CREATE_DEACTIVATED = 0x0001,

 /*
	 * For regular flies, if the opener has CAP_DAC_OVERRIDE, open(2)
	 * succeeds regardless of the RW permissions.  sysfs had an extra
	 * layer of enforcement where open(2) fails with -EACCES regardless
	 * of CAP_DAC_OVERRIDE if the permission doesn't have the
	 * respective read or write access at all (none of S_IRUGO or
	 * S_IWUGO) or the respective operation isn't implemented.  The
	 * following flag enables that behavior.
	 */
 Model1_KERNFS_ROOT_EXTRA_OPEN_PERM_CHECK = 0x0002,
};

/* type-specific structures for kernfs_node union members */
struct Model1_kernfs_elem_dir {
 unsigned long Model1_subdirs;
 /* children rbtree starts here and goes through kn->rb */
 struct Model1_rb_root Model1_children;

 /*
	 * The kernfs hierarchy this directory belongs to.  This fits
	 * better directly in kernfs_node but is here to save space.
	 */
 struct Model1_kernfs_root *Model1_root;
};

struct Model1_kernfs_elem_symlink {
 struct Model1_kernfs_node *Model1_target_kn;
};

struct Model1_kernfs_elem_attr {
 const struct Model1_kernfs_ops *Model1_ops;
 struct Model1_kernfs_open_node *Model1_open;
 Model1_loff_t Model1_size;
 struct Model1_kernfs_node *Model1_notify_next; /* for kernfs_notify() */
};

/*
 * kernfs_node - the building block of kernfs hierarchy.  Each and every
 * kernfs node is represented by single kernfs_node.  Most fields are
 * private to kernfs and shouldn't be accessed directly by kernfs users.
 *
 * As long as s_count reference is held, the kernfs_node itself is
 * accessible.  Dereferencing elem or any other outer entity requires
 * active reference.
 */
struct Model1_kernfs_node {
 Model1_atomic_t Model1_count;
 Model1_atomic_t Model1_active;



 /*
	 * Use kernfs_get_parent() and kernfs_name/path() instead of
	 * accessing the following two fields directly.  If the node is
	 * never moved to a different parent, it is safe to access the
	 * parent directly.
	 */
 struct Model1_kernfs_node *Model1_parent;
 const char *Model1_name;

 struct Model1_rb_node Model1_rb;

 const void *Model1_ns; /* namespace tag */
 unsigned int Model1_hash; /* ns + name hash */
 union {
  struct Model1_kernfs_elem_dir Model1_dir;
  struct Model1_kernfs_elem_symlink Model1_symlink;
  struct Model1_kernfs_elem_attr Model1_attr;
 };

 void *Model1_priv;

 unsigned short Model1_flags;
 Model1_umode_t Model1_mode;
 unsigned int Model1_ino;
 struct Model1_kernfs_iattrs *Model1_iattr;
};

/*
 * kernfs_syscall_ops may be specified on kernfs_create_root() to support
 * syscalls.  These optional callbacks are invoked on the matching syscalls
 * and can perform any kernfs operations which don't necessarily have to be
 * the exact operation requested.  An active reference is held for each
 * kernfs_node parameter.
 */
struct Model1_kernfs_syscall_ops {
 int (*Model1_remount_fs)(struct Model1_kernfs_root *Model1_root, int *Model1_flags, char *Model1_data);
 int (*Model1_show_options)(struct Model1_seq_file *Model1_sf, struct Model1_kernfs_root *Model1_root);

 int (*Model1_mkdir)(struct Model1_kernfs_node *Model1_parent, const char *Model1_name,
       Model1_umode_t Model1_mode);
 int (*Model1_rmdir)(struct Model1_kernfs_node *Model1_kn);
 int (*Model1_rename)(struct Model1_kernfs_node *Model1_kn, struct Model1_kernfs_node *Model1_new_parent,
        const char *Model1_new_name);
 int (*Model1_show_path)(struct Model1_seq_file *Model1_sf, struct Model1_kernfs_node *Model1_kn,
    struct Model1_kernfs_root *Model1_root);
};

struct Model1_kernfs_root {
 /* published fields */
 struct Model1_kernfs_node *Model1_kn;
 unsigned int Model1_flags; /* KERNFS_ROOT_* flags */

 /* private fields, do not use outside kernfs proper */
 struct Model1_ida Model1_ino_ida;
 struct Model1_kernfs_syscall_ops *Model1_syscall_ops;

 /* list of kernfs_super_info of this root, protected by kernfs_mutex */
 struct Model1_list_head Model1_supers;

 Model1_wait_queue_head_t Model1_deactivate_waitq;
};

struct Model1_kernfs_open_file {
 /* published fields */
 struct Model1_kernfs_node *Model1_kn;
 struct Model1_file *Model1_file;
 void *Model1_priv;

 /* private fields, do not use outside kernfs proper */
 struct Model1_mutex Model1_mutex;
 struct Model1_mutex Model1_prealloc_mutex;
 int Model1_event;
 struct Model1_list_head Model1_list;
 char *Model1_prealloc_buf;

 Model1_size_t Model1_atomic_write_len;
 bool Model1_mmapped;
 const struct Model1_vm_operations_struct *Model1_vm_ops;
};

struct Model1_kernfs_ops {
 /*
	 * Read is handled by either seq_file or raw_read().
	 *
	 * If seq_show() is present, seq_file path is active.  Other seq
	 * operations are optional and if not implemented, the behavior is
	 * equivalent to single_open().  @sf->private points to the
	 * associated kernfs_open_file.
	 *
	 * read() is bounced through kernel buffer and a read larger than
	 * PAGE_SIZE results in partial operation of PAGE_SIZE.
	 */
 int (*Model1_seq_show)(struct Model1_seq_file *Model1_sf, void *Model1_v);

 void *(*Model1_seq_start)(struct Model1_seq_file *Model1_sf, Model1_loff_t *Model1_ppos);
 void *(*Model1_seq_next)(struct Model1_seq_file *Model1_sf, void *Model1_v, Model1_loff_t *Model1_ppos);
 void (*Model1_seq_stop)(struct Model1_seq_file *Model1_sf, void *Model1_v);

 Model1_ssize_t (*Model1_read)(struct Model1_kernfs_open_file *Model1_of, char *Model1_buf, Model1_size_t Model1_bytes,
   Model1_loff_t Model1_off);

 /*
	 * write() is bounced through kernel buffer.  If atomic_write_len
	 * is not set, a write larger than PAGE_SIZE results in partial
	 * operations of PAGE_SIZE chunks.  If atomic_write_len is set,
	 * writes upto the specified size are executed atomically but
	 * larger ones are rejected with -E2BIG.
	 */
 Model1_size_t Model1_atomic_write_len;
 /*
	 * "prealloc" causes a buffer to be allocated at open for
	 * all read/write requests.  As ->seq_show uses seq_read()
	 * which does its own allocation, it is incompatible with
	 * ->prealloc.  Provide ->read and ->write with ->prealloc.
	 */
 bool Model1_prealloc;
 Model1_ssize_t (*Model1_write)(struct Model1_kernfs_open_file *Model1_of, char *Model1_buf, Model1_size_t Model1_bytes,
    Model1_loff_t Model1_off);

 int (*Model1_mmap)(struct Model1_kernfs_open_file *Model1_of, struct Model1_vm_area_struct *Model1_vma);




};



static inline __attribute__((no_instrument_function)) enum Model1_kernfs_node_type Model1_kernfs_type(struct Model1_kernfs_node *Model1_kn)
{
 return Model1_kn->Model1_flags & 0x000f;
}

/**
 * kernfs_enable_ns - enable namespace under a directory
 * @kn: directory of interest, should be empty
 *
 * This is to be called right after @kn is created to enable namespace
 * under it.  All children of @kn must have non-NULL namespace tags and
 * only the ones which match the super_block's tag will be visible.
 */
static inline __attribute__((no_instrument_function)) void Model1_kernfs_enable_ns(struct Model1_kernfs_node *Model1_kn)
{
 ({ static bool __attribute__ ((__section__(".data.unlikely"))) Model1___warned; int Model1___ret_warn_once = !!(Model1_kernfs_type(Model1_kn) != Model1_KERNFS_DIR); if (__builtin_expect(!!(Model1___ret_warn_once && !Model1___warned), 0)) { Model1___warned = true; ({ int Model1___ret_warn_on = !!(1); if (__builtin_expect(!!(Model1___ret_warn_on), 0)) Model1_warn_slowpath_null("./include/linux/kernfs.h", 255); __builtin_expect(!!(Model1___ret_warn_on), 0); }); } __builtin_expect(!!(Model1___ret_warn_once), 0); });
 ({ static bool __attribute__ ((__section__(".data.unlikely"))) Model1___warned; int Model1___ret_warn_once = !!(!(({ union { typeof((&Model1_kn->Model1_dir.Model1_children)->Model1_rb_node) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&((&Model1_kn->Model1_dir.Model1_children)->Model1_rb_node), Model1___u.Model1___c, sizeof((&Model1_kn->Model1_dir.Model1_children)->Model1_rb_node)); else Model1___read_once_size_nocheck(&((&Model1_kn->Model1_dir.Model1_children)->Model1_rb_node), Model1___u.Model1___c, sizeof((&Model1_kn->Model1_dir.Model1_children)->Model1_rb_node)); Model1___u.Model1___val; }) == ((void *)0))); if (__builtin_expect(!!(Model1___ret_warn_once && !Model1___warned), 0)) { Model1___warned = true; ({ int Model1___ret_warn_on = !!(1); if (__builtin_expect(!!(Model1___ret_warn_on), 0)) Model1_warn_slowpath_null("./include/linux/kernfs.h", 256); __builtin_expect(!!(Model1___ret_warn_on), 0); }); } __builtin_expect(!!(Model1___ret_warn_once), 0); });
 Model1_kn->Model1_flags |= Model1_KERNFS_NS;
}

/**
 * kernfs_ns_enabled - test whether namespace is enabled
 * @kn: the node to test
 *
 * Test whether namespace filtering is enabled for the children of @ns.
 */
static inline __attribute__((no_instrument_function)) bool Model1_kernfs_ns_enabled(struct Model1_kernfs_node *Model1_kn)
{
 return Model1_kn->Model1_flags & Model1_KERNFS_NS;
}

int Model1_kernfs_name(struct Model1_kernfs_node *Model1_kn, char *Model1_buf, Model1_size_t Model1_buflen);
Model1_size_t Model1_kernfs_path_len(struct Model1_kernfs_node *Model1_kn);
int Model1_kernfs_path_from_node(struct Model1_kernfs_node *Model1_root_kn, struct Model1_kernfs_node *Model1_kn,
     char *Model1_buf, Model1_size_t Model1_buflen);
char *Model1_kernfs_path(struct Model1_kernfs_node *Model1_kn, char *Model1_buf, Model1_size_t Model1_buflen);
void Model1_pr_cont_kernfs_name(struct Model1_kernfs_node *Model1_kn);
void Model1_pr_cont_kernfs_path(struct Model1_kernfs_node *Model1_kn);
struct Model1_kernfs_node *Model1_kernfs_get_parent(struct Model1_kernfs_node *Model1_kn);
struct Model1_kernfs_node *Model1_kernfs_find_and_get_ns(struct Model1_kernfs_node *Model1_parent,
        const char *Model1_name, const void *Model1_ns);
struct Model1_kernfs_node *Model1_kernfs_walk_and_get_ns(struct Model1_kernfs_node *Model1_parent,
        const char *Model1_path, const void *Model1_ns);
void Model1_kernfs_get(struct Model1_kernfs_node *Model1_kn);
void Model1_kernfs_put(struct Model1_kernfs_node *Model1_kn);

struct Model1_kernfs_node *Model1_kernfs_node_from_dentry(struct Model1_dentry *Model1_dentry);
struct Model1_kernfs_root *Model1_kernfs_root_from_sb(struct Model1_super_block *Model1_sb);
struct Model1_inode *Model1_kernfs_get_inode(struct Model1_super_block *Model1_sb, struct Model1_kernfs_node *Model1_kn);

struct Model1_dentry *Model1_kernfs_node_dentry(struct Model1_kernfs_node *Model1_kn,
      struct Model1_super_block *Model1_sb);
struct Model1_kernfs_root *Model1_kernfs_create_root(struct Model1_kernfs_syscall_ops *Model1_scops,
           unsigned int Model1_flags, void *Model1_priv);
void Model1_kernfs_destroy_root(struct Model1_kernfs_root *Model1_root);

struct Model1_kernfs_node *Model1_kernfs_create_dir_ns(struct Model1_kernfs_node *Model1_parent,
      const char *Model1_name, Model1_umode_t Model1_mode,
      void *Model1_priv, const void *Model1_ns);
struct Model1_kernfs_node *Model1_kernfs_create_empty_dir(struct Model1_kernfs_node *Model1_parent,
         const char *Model1_name);
struct Model1_kernfs_node *Model1___kernfs_create_file(struct Model1_kernfs_node *Model1_parent,
      const char *Model1_name,
      Model1_umode_t Model1_mode, Model1_loff_t Model1_size,
      const struct Model1_kernfs_ops *Model1_ops,
      void *Model1_priv, const void *Model1_ns,
      struct Model1_lock_class_key *Model1_key);
struct Model1_kernfs_node *Model1_kernfs_create_link(struct Model1_kernfs_node *Model1_parent,
           const char *Model1_name,
           struct Model1_kernfs_node *Model1_target);
void Model1_kernfs_activate(struct Model1_kernfs_node *Model1_kn);
void Model1_kernfs_remove(struct Model1_kernfs_node *Model1_kn);
void Model1_kernfs_break_active_protection(struct Model1_kernfs_node *Model1_kn);
void Model1_kernfs_unbreak_active_protection(struct Model1_kernfs_node *Model1_kn);
bool Model1_kernfs_remove_self(struct Model1_kernfs_node *Model1_kn);
int Model1_kernfs_remove_by_name_ns(struct Model1_kernfs_node *Model1_parent, const char *Model1_name,
        const void *Model1_ns);
int Model1_kernfs_rename_ns(struct Model1_kernfs_node *Model1_kn, struct Model1_kernfs_node *Model1_new_parent,
       const char *Model1_new_name, const void *Model1_new_ns);
int Model1_kernfs_setattr(struct Model1_kernfs_node *Model1_kn, const struct Model1_iattr *Model1_iattr);
void Model1_kernfs_notify(struct Model1_kernfs_node *Model1_kn);

const void *Model1_kernfs_super_ns(struct Model1_super_block *Model1_sb);
struct Model1_dentry *Model1_kernfs_mount_ns(struct Model1_file_system_type *Model1_fs_type, int Model1_flags,
          struct Model1_kernfs_root *Model1_root, unsigned long Model1_magic,
          bool *Model1_new_sb_created, const void *Model1_ns);
void Model1_kernfs_kill_sb(struct Model1_super_block *Model1_sb);
struct Model1_super_block *Model1_kernfs_pin_sb(struct Model1_kernfs_root *Model1_root, const void *Model1_ns);

void Model1_kernfs_init(void);
static inline __attribute__((no_instrument_function)) struct Model1_kernfs_node *
Model1_kernfs_find_and_get(struct Model1_kernfs_node *Model1_kn, const char *Model1_name)
{
 return Model1_kernfs_find_and_get_ns(Model1_kn, Model1_name, ((void *)0));
}

static inline __attribute__((no_instrument_function)) struct Model1_kernfs_node *
Model1_kernfs_walk_and_get(struct Model1_kernfs_node *Model1_kn, const char *Model1_path)
{
 return Model1_kernfs_walk_and_get_ns(Model1_kn, Model1_path, ((void *)0));
}

static inline __attribute__((no_instrument_function)) struct Model1_kernfs_node *
Model1_kernfs_create_dir(struct Model1_kernfs_node *Model1_parent, const char *Model1_name, Model1_umode_t Model1_mode,
    void *Model1_priv)
{
 return Model1_kernfs_create_dir_ns(Model1_parent, Model1_name, Model1_mode, Model1_priv, ((void *)0));
}

static inline __attribute__((no_instrument_function)) struct Model1_kernfs_node *
Model1_kernfs_create_file_ns(struct Model1_kernfs_node *Model1_parent, const char *Model1_name,
        Model1_umode_t Model1_mode, Model1_loff_t Model1_size, const struct Model1_kernfs_ops *Model1_ops,
        void *Model1_priv, const void *Model1_ns)
{
 struct Model1_lock_class_key *Model1_key = ((void *)0);




 return Model1___kernfs_create_file(Model1_parent, Model1_name, Model1_mode, Model1_size, Model1_ops, Model1_priv, Model1_ns,
        Model1_key);
}

static inline __attribute__((no_instrument_function)) struct Model1_kernfs_node *
Model1_kernfs_create_file(struct Model1_kernfs_node *Model1_parent, const char *Model1_name, Model1_umode_t Model1_mode,
     Model1_loff_t Model1_size, const struct Model1_kernfs_ops *Model1_ops, void *Model1_priv)
{
 return Model1_kernfs_create_file_ns(Model1_parent, Model1_name, Model1_mode, Model1_size, Model1_ops, Model1_priv, ((void *)0));
}

static inline __attribute__((no_instrument_function)) int Model1_kernfs_remove_by_name(struct Model1_kernfs_node *Model1_parent,
     const char *Model1_name)
{
 return Model1_kernfs_remove_by_name_ns(Model1_parent, Model1_name, ((void *)0));
}

static inline __attribute__((no_instrument_function)) int Model1_kernfs_rename(struct Model1_kernfs_node *Model1_kn,
    struct Model1_kernfs_node *Model1_new_parent,
    const char *Model1_new_name)
{
 return Model1_kernfs_rename_ns(Model1_kn, Model1_new_parent, Model1_new_name, ((void *)0));
}

static inline __attribute__((no_instrument_function)) struct Model1_dentry *
Model1_kernfs_mount(struct Model1_file_system_type *Model1_fs_type, int Model1_flags,
  struct Model1_kernfs_root *Model1_root, unsigned long Model1_magic,
  bool *Model1_new_sb_created)
{
 return Model1_kernfs_mount_ns(Model1_fs_type, Model1_flags, Model1_root,
    Model1_magic, Model1_new_sb_created, ((void *)0));
}




/* Kernel object name space definitions
 *
 * Copyright (c) 2002-2003 Patrick Mochel
 * Copyright (c) 2002-2003 Open Source Development Labs
 * Copyright (c) 2006-2008 Greg Kroah-Hartman <greg@kroah.com>
 * Copyright (c) 2006-2008 Novell Inc.
 *
 * Split from kobject.h by David Howells (dhowells@redhat.com)
 *
 * This file is released under the GPLv2.
 *
 * Please read Documentation/kobject.txt before using the kobject
 * interface, ESPECIALLY the parts about reference counts and object
 * destructors.
 */




struct Model1_sock;
struct Model1_kobject;

/*
 * Namespace types which are used to tag kobjects and sysfs entries.
 * Network namespace will likely be the first.
 */
enum Model1_kobj_ns_type {
 Model1_KOBJ_NS_TYPE_NONE = 0,
 Model1_KOBJ_NS_TYPE_NET,
 Model1_KOBJ_NS_TYPES
};

/*
 * Callbacks so sysfs can determine namespaces
 *   @grab_current_ns: return a new reference to calling task's namespace
 *   @netlink_ns: return namespace to which a sock belongs (right?)
 *   @initial_ns: return the initial namespace (i.e. init_net_ns)
 *   @drop_ns: drops a reference to namespace
 */
struct Model1_kobj_ns_type_operations {
 enum Model1_kobj_ns_type Model1_type;
 bool (*Model1_current_may_mount)(void);
 void *(*Model1_grab_current_ns)(void);
 const void *(*Model1_netlink_ns)(struct Model1_sock *Model1_sk);
 const void *(*Model1_initial_ns)(void);
 void (*Model1_drop_ns)(void *);
};

int Model1_kobj_ns_type_register(const struct Model1_kobj_ns_type_operations *Model1_ops);
int Model1_kobj_ns_type_registered(enum Model1_kobj_ns_type Model1_type);
const struct Model1_kobj_ns_type_operations *Model1_kobj_child_ns_ops(struct Model1_kobject *Model1_parent);
const struct Model1_kobj_ns_type_operations *Model1_kobj_ns_ops(struct Model1_kobject *Model1_kobj);

bool Model1_kobj_ns_current_may_mount(enum Model1_kobj_ns_type Model1_type);
void *Model1_kobj_ns_grab_current(enum Model1_kobj_ns_type Model1_type);
const void *Model1_kobj_ns_netlink(enum Model1_kobj_ns_type Model1_type, struct Model1_sock *Model1_sk);
const void *Model1_kobj_ns_initial(enum Model1_kobj_ns_type Model1_type);
void Model1_kobj_ns_drop(enum Model1_kobj_ns_type Model1_type, void *Model1_ns);



struct Model1_kobject;
struct Model1_module;
struct Model1_bin_attribute;
enum Model1_kobj_ns_type;

struct Model1_attribute {
 const char *Model1_name;
 Model1_umode_t Model1_mode;





};

/**
 *	sysfs_attr_init - initialize a dynamically allocated sysfs attribute
 *	@attr: struct attribute to initialize
 *
 *	Initialize a dynamically allocated struct attribute so we can
 *	make lockdep happy.  This is a new requirement for attributes
 *	and initially this is only needed when lockdep is enabled.
 *	Lockdep gives a nice error when your attribute is added to
 *	sysfs if you don't have this.
 */
/**
 * struct attribute_group - data structure used to declare an attribute group.
 * @name:	Optional: Attribute group name
 *		If specified, the attribute group will be created in
 *		a new subdirectory with this name.
 * @is_visible:	Optional: Function to return permissions associated with an
 *		attribute of the group. Will be called repeatedly for each
 *		non-binary attribute in the group. Only read/write
 *		permissions as well as SYSFS_PREALLOC are accepted. Must
 *		return 0 if an attribute is not visible. The returned value
 *		will replace static permissions defined in struct attribute.
 * @is_bin_visible:
 *		Optional: Function to return permissions associated with a
 *		binary attribute of the group. Will be called repeatedly
 *		for each binary attribute in the group. Only read/write
 *		permissions as well as SYSFS_PREALLOC are accepted. Must
 *		return 0 if a binary attribute is not visible. The returned
 *		value will replace static permissions defined in
 *		struct bin_attribute.
 * @attrs:	Pointer to NULL terminated list of attributes.
 * @bin_attrs:	Pointer to NULL terminated list of binary attributes.
 *		Either attrs or bin_attrs or both must be provided.
 */
struct Model1_attribute_group {
 const char *Model1_name;
 Model1_umode_t (*Model1_is_visible)(struct Model1_kobject *,
           struct Model1_attribute *, int);
 Model1_umode_t (*Model1_is_bin_visible)(struct Model1_kobject *,
        struct Model1_bin_attribute *, int);
 struct Model1_attribute **Model1_attrs;
 struct Model1_bin_attribute **Model1_bin_attrs;
};

/**
 * Use these macros to make defining attributes easier. See include/linux/device.h
 * for examples..
 */
struct Model1_file;
struct Model1_vm_area_struct;

struct Model1_bin_attribute {
 struct Model1_attribute Model1_attr;
 Model1_size_t Model1_size;
 void *Model1_private;
 Model1_ssize_t (*Model1_read)(struct Model1_file *, struct Model1_kobject *, struct Model1_bin_attribute *,
   char *, Model1_loff_t, Model1_size_t);
 Model1_ssize_t (*Model1_write)(struct Model1_file *, struct Model1_kobject *, struct Model1_bin_attribute *,
    char *, Model1_loff_t, Model1_size_t);
 int (*Model1_mmap)(struct Model1_file *, struct Model1_kobject *, struct Model1_bin_attribute *Model1_attr,
      struct Model1_vm_area_struct *Model1_vma);
};

/**
 *	sysfs_bin_attr_init - initialize a dynamically allocated bin_attribute
 *	@attr: struct bin_attribute to initialize
 *
 *	Initialize a dynamically allocated struct bin_attribute so we
 *	can make lockdep happy.  This is a new requirement for
 *	attributes and initially this is only needed when lockdep is
 *	enabled.  Lockdep gives a nice error when your attribute is
 *	added to sysfs if you don't have this.
 */


/* macros to create static binary attributes easier */
struct Model1_sysfs_ops {
 Model1_ssize_t (*Model1_show)(struct Model1_kobject *, struct Model1_attribute *, char *);
 Model1_ssize_t (*Model1_store)(struct Model1_kobject *, struct Model1_attribute *, const char *, Model1_size_t);
};



int __attribute__((warn_unused_result)) Model1_sysfs_create_dir_ns(struct Model1_kobject *Model1_kobj, const void *Model1_ns);
void Model1_sysfs_remove_dir(struct Model1_kobject *Model1_kobj);
int __attribute__((warn_unused_result)) Model1_sysfs_rename_dir_ns(struct Model1_kobject *Model1_kobj, const char *Model1_new_name,
         const void *Model1_new_ns);
int __attribute__((warn_unused_result)) Model1_sysfs_move_dir_ns(struct Model1_kobject *Model1_kobj,
       struct Model1_kobject *Model1_new_parent_kobj,
       const void *Model1_new_ns);
int __attribute__((warn_unused_result)) Model1_sysfs_create_mount_point(struct Model1_kobject *Model1_parent_kobj,
       const char *Model1_name);
void Model1_sysfs_remove_mount_point(struct Model1_kobject *Model1_parent_kobj,
         const char *Model1_name);

int __attribute__((warn_unused_result)) Model1_sysfs_create_file_ns(struct Model1_kobject *Model1_kobj,
          const struct Model1_attribute *Model1_attr,
          const void *Model1_ns);
int __attribute__((warn_unused_result)) Model1_sysfs_create_files(struct Model1_kobject *Model1_kobj,
       const struct Model1_attribute **Model1_attr);
int __attribute__((warn_unused_result)) Model1_sysfs_chmod_file(struct Model1_kobject *Model1_kobj,
      const struct Model1_attribute *Model1_attr, Model1_umode_t Model1_mode);
void Model1_sysfs_remove_file_ns(struct Model1_kobject *Model1_kobj, const struct Model1_attribute *Model1_attr,
     const void *Model1_ns);
bool Model1_sysfs_remove_file_self(struct Model1_kobject *Model1_kobj, const struct Model1_attribute *Model1_attr);
void Model1_sysfs_remove_files(struct Model1_kobject *Model1_kobj, const struct Model1_attribute **Model1_attr);

int __attribute__((warn_unused_result)) Model1_sysfs_create_bin_file(struct Model1_kobject *Model1_kobj,
           const struct Model1_bin_attribute *Model1_attr);
void Model1_sysfs_remove_bin_file(struct Model1_kobject *Model1_kobj,
      const struct Model1_bin_attribute *Model1_attr);

int __attribute__((warn_unused_result)) Model1_sysfs_create_link(struct Model1_kobject *Model1_kobj, struct Model1_kobject *Model1_target,
       const char *Model1_name);
int __attribute__((warn_unused_result)) Model1_sysfs_create_link_nowarn(struct Model1_kobject *Model1_kobj,
       struct Model1_kobject *Model1_target,
       const char *Model1_name);
void Model1_sysfs_remove_link(struct Model1_kobject *Model1_kobj, const char *Model1_name);

int Model1_sysfs_rename_link_ns(struct Model1_kobject *Model1_kobj, struct Model1_kobject *Model1_target,
    const char *Model1_old_name, const char *Model1_new_name,
    const void *Model1_new_ns);

void Model1_sysfs_delete_link(struct Model1_kobject *Model1_dir, struct Model1_kobject *Model1_targ,
   const char *Model1_name);

int __attribute__((warn_unused_result)) Model1_sysfs_create_group(struct Model1_kobject *Model1_kobj,
        const struct Model1_attribute_group *Model1_grp);
int __attribute__((warn_unused_result)) Model1_sysfs_create_groups(struct Model1_kobject *Model1_kobj,
         const struct Model1_attribute_group **Model1_groups);
int Model1_sysfs_update_group(struct Model1_kobject *Model1_kobj,
         const struct Model1_attribute_group *Model1_grp);
void Model1_sysfs_remove_group(struct Model1_kobject *Model1_kobj,
   const struct Model1_attribute_group *Model1_grp);
void Model1_sysfs_remove_groups(struct Model1_kobject *Model1_kobj,
    const struct Model1_attribute_group **Model1_groups);
int Model1_sysfs_add_file_to_group(struct Model1_kobject *Model1_kobj,
   const struct Model1_attribute *Model1_attr, const char *Model1_group);
void Model1_sysfs_remove_file_from_group(struct Model1_kobject *Model1_kobj,
   const struct Model1_attribute *Model1_attr, const char *Model1_group);
int Model1_sysfs_merge_group(struct Model1_kobject *Model1_kobj,
         const struct Model1_attribute_group *Model1_grp);
void Model1_sysfs_unmerge_group(struct Model1_kobject *Model1_kobj,
         const struct Model1_attribute_group *Model1_grp);
int Model1_sysfs_add_link_to_group(struct Model1_kobject *Model1_kobj, const char *Model1_group_name,
       struct Model1_kobject *Model1_target, const char *Model1_link_name);
void Model1_sysfs_remove_link_from_group(struct Model1_kobject *Model1_kobj, const char *Model1_group_name,
      const char *Model1_link_name);
int Model1___compat_only_sysfs_link_entry_to_kobj(struct Model1_kobject *Model1_kobj,
          struct Model1_kobject *Model1_target_kobj,
          const char *Model1_target_name);

void Model1_sysfs_notify(struct Model1_kobject *Model1_kobj, const char *Model1_dir, const char *Model1_attr);

int __attribute__((warn_unused_result)) Model1_sysfs_init(void);

static inline __attribute__((no_instrument_function)) void Model1_sysfs_enable_ns(struct Model1_kernfs_node *Model1_kn)
{
 return Model1_kernfs_enable_ns(Model1_kn);
}
static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result)) Model1_sysfs_create_file(struct Model1_kobject *Model1_kobj,
       const struct Model1_attribute *Model1_attr)
{
 return Model1_sysfs_create_file_ns(Model1_kobj, Model1_attr, ((void *)0));
}

static inline __attribute__((no_instrument_function)) void Model1_sysfs_remove_file(struct Model1_kobject *Model1_kobj,
         const struct Model1_attribute *Model1_attr)
{
 Model1_sysfs_remove_file_ns(Model1_kobj, Model1_attr, ((void *)0));
}

static inline __attribute__((no_instrument_function)) int Model1_sysfs_rename_link(struct Model1_kobject *Model1_kobj, struct Model1_kobject *Model1_target,
        const char *Model1_old_name, const char *Model1_new_name)
{
 return Model1_sysfs_rename_link_ns(Model1_kobj, Model1_target, Model1_old_name, Model1_new_name, ((void *)0));
}

static inline __attribute__((no_instrument_function)) void Model1_sysfs_notify_dirent(struct Model1_kernfs_node *Model1_kn)
{
 Model1_kernfs_notify(Model1_kn);
}

static inline __attribute__((no_instrument_function)) struct Model1_kernfs_node *Model1_sysfs_get_dirent(struct Model1_kernfs_node *Model1_parent,
         const unsigned char *Model1_name)
{
 return Model1_kernfs_find_and_get(Model1_parent, Model1_name);
}

static inline __attribute__((no_instrument_function)) struct Model1_kernfs_node *Model1_sysfs_get(struct Model1_kernfs_node *Model1_kn)
{
 Model1_kernfs_get(Model1_kn);
 return Model1_kn;
}

static inline __attribute__((no_instrument_function)) void Model1_sysfs_put(struct Model1_kernfs_node *Model1_kn)
{
 Model1_kernfs_put(Model1_kn);
}


/*
 * kref.h - library routines for handling generic reference counted objects
 *
 * Copyright (C) 2004 Greg Kroah-Hartman <greg@kroah.com>
 * Copyright (C) 2004 IBM Corp.
 *
 * based on kobject.h which was:
 * Copyright (C) 2002-2003 Patrick Mochel <mochel@osdl.org>
 * Copyright (C) 2002-2003 Open Source Development Labs
 *
 * This file is released under the GPLv2.
 *
 */
struct Model1_kref {
 Model1_atomic_t Model1_refcount;
};

/**
 * kref_init - initialize object.
 * @kref: object in question.
 */
static inline __attribute__((no_instrument_function)) void Model1_kref_init(struct Model1_kref *Model1_kref)
{
 Model1_atomic_set(&Model1_kref->Model1_refcount, 1);
}

/**
 * kref_get - increment refcount for object.
 * @kref: object.
 */
static inline __attribute__((no_instrument_function)) void Model1_kref_get(struct Model1_kref *Model1_kref)
{
 /* If refcount was 0 before incrementing then we have a race
	 * condition when this kref is freeing by some other thread right now.
	 * In this case one should use kref_get_unless_zero()
	 */
 ({ static bool __attribute__ ((__section__(".data.unlikely"))) Model1___warned; int Model1___ret_warn_once = !!((Model1_atomic_add_return(1, &Model1_kref->Model1_refcount)) < 2); if (__builtin_expect(!!(Model1___ret_warn_once && !Model1___warned), 0)) { Model1___warned = true; ({ int Model1___ret_warn_on = !!(1); if (__builtin_expect(!!(Model1___ret_warn_on), 0)) Model1_warn_slowpath_null("./include/linux/kref.h", 46); __builtin_expect(!!(Model1___ret_warn_on), 0); }); } __builtin_expect(!!(Model1___ret_warn_once), 0); });
}

/**
 * kref_sub - subtract a number of refcounts for object.
 * @kref: object.
 * @count: Number of recounts to subtract.
 * @release: pointer to the function that will clean up the object when the
 *	     last reference to the object is released.
 *	     This pointer is required, and it is not acceptable to pass kfree
 *	     in as this function.  If the caller does pass kfree to this
 *	     function, you will be publicly mocked mercilessly by the kref
 *	     maintainer, and anyone else who happens to notice it.  You have
 *	     been warned.
 *
 * Subtract @count from the refcount, and if 0, call release().
 * Return 1 if the object was removed, otherwise return 0.  Beware, if this
 * function returns 0, you still can not count on the kref from remaining in
 * memory.  Only use the return value if you want to see if the kref is now
 * gone, not present.
 */
static inline __attribute__((no_instrument_function)) int Model1_kref_sub(struct Model1_kref *Model1_kref, unsigned int Model1_count,
      void (*Model1_release)(struct Model1_kref *Model1_kref))
{
 ({ int Model1___ret_warn_on = !!(Model1_release == ((void *)0)); if (__builtin_expect(!!(Model1___ret_warn_on), 0)) Model1_warn_slowpath_null("./include/linux/kref.h", 70); __builtin_expect(!!(Model1___ret_warn_on), 0); });

 if (Model1_atomic_sub_and_test((int) Model1_count, &Model1_kref->Model1_refcount)) {
  Model1_release(Model1_kref);
  return 1;
 }
 return 0;
}

/**
 * kref_put - decrement refcount for object.
 * @kref: object.
 * @release: pointer to the function that will clean up the object when the
 *	     last reference to the object is released.
 *	     This pointer is required, and it is not acceptable to pass kfree
 *	     in as this function.  If the caller does pass kfree to this
 *	     function, you will be publicly mocked mercilessly by the kref
 *	     maintainer, and anyone else who happens to notice it.  You have
 *	     been warned.
 *
 * Decrement the refcount, and if 0, call release().
 * Return 1 if the object was removed, otherwise return 0.  Beware, if this
 * function returns 0, you still can not count on the kref from remaining in
 * memory.  Only use the return value if you want to see if the kref is now
 * gone, not present.
 */
static inline __attribute__((no_instrument_function)) int Model1_kref_put(struct Model1_kref *Model1_kref, void (*Model1_release)(struct Model1_kref *Model1_kref))
{
 return Model1_kref_sub(Model1_kref, 1, Model1_release);
}

static inline __attribute__((no_instrument_function)) int Model1_kref_put_mutex(struct Model1_kref *Model1_kref,
     void (*Model1_release)(struct Model1_kref *Model1_kref),
     struct Model1_mutex *Model1_lock)
{
 ({ int Model1___ret_warn_on = !!(Model1_release == ((void *)0)); if (__builtin_expect(!!(Model1___ret_warn_on), 0)) Model1_warn_slowpath_null("./include/linux/kref.h", 105); __builtin_expect(!!(Model1___ret_warn_on), 0); });
 if (__builtin_expect(!!(!Model1_atomic_add_unless(&Model1_kref->Model1_refcount, -1, 1)), 0)) {
  Model1_mutex_lock(Model1_lock);
  if (__builtin_expect(!!(!Model1_atomic_dec_and_test(&Model1_kref->Model1_refcount)), 0)) {
   Model1_mutex_unlock(Model1_lock);
   return 0;
  }
  Model1_release(Model1_kref);
  return 1;
 }
 return 0;
}

/**
 * kref_get_unless_zero - Increment refcount for object unless it is zero.
 * @kref: object.
 *
 * Return non-zero if the increment succeeded. Otherwise return 0.
 *
 * This function is intended to simplify locking around refcounting for
 * objects that can be looked up from a lookup structure, and which are
 * removed from that lookup structure in the object destructor.
 * Operations on such objects require at least a read lock around
 * lookup + kref_get, and a write lock around kref_put + remove from lookup
 * structure. Furthermore, RCU implementations become extremely tricky.
 * With a lookup followed by a kref_get_unless_zero *with return value check*
 * locking in the kref_put path can be deferred to the actual removal from
 * the lookup structure and RCU lookups become trivial.
 */
static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result)) Model1_kref_get_unless_zero(struct Model1_kref *Model1_kref)
{
 return Model1_atomic_add_unless(&Model1_kref->Model1_refcount, 1, 0);
}
/* path to the userspace helper executed on an event */
extern char Model1_uevent_helper[];


/* counter to tag the uevent, read only except for the kobject core */
extern Model1_u64 Model1_uevent_seqnum;

/*
 * The actions here must match the index to the string array
 * in lib/kobject_uevent.c
 *
 * Do not add new actions here without checking with the driver-core
 * maintainers. Action strings are not meant to express subsystem
 * or device specific properties. In most cases you want to send a
 * kobject_uevent_env(kobj, KOBJ_CHANGE, env) with additional event
 * specific variables added to the event environment.
 */
enum Model1_kobject_action {
 Model1_KOBJ_ADD,
 Model1_KOBJ_REMOVE,
 Model1_KOBJ_CHANGE,
 Model1_KOBJ_MOVE,
 Model1_KOBJ_ONLINE,
 Model1_KOBJ_OFFLINE,
 Model1_KOBJ_MAX
};

struct Model1_kobject {
 const char *Model1_name;
 struct Model1_list_head Model1_entry;
 struct Model1_kobject *Model1_parent;
 struct Model1_kset *Model1_kset;
 struct Model1_kobj_type *Model1_ktype;
 struct Model1_kernfs_node *Model1_sd; /* sysfs directory entry */
 struct Model1_kref Model1_kref;



 unsigned int Model1_state_initialized:1;
 unsigned int Model1_state_in_sysfs:1;
 unsigned int Model1_state_add_uevent_sent:1;
 unsigned int Model1_state_remove_uevent_sent:1;
 unsigned int Model1_uevent_suppress:1;
};

extern __attribute__((format(printf, 2, 3)))
int Model1_kobject_set_name(struct Model1_kobject *Model1_kobj, const char *Model1_name, ...);
extern __attribute__((format(printf, 2, 0)))
int Model1_kobject_set_name_vargs(struct Model1_kobject *Model1_kobj, const char *Model1_fmt,
      Model1_va_list Model1_vargs);

static inline __attribute__((no_instrument_function)) const char *Model1_kobject_name(const struct Model1_kobject *Model1_kobj)
{
 return Model1_kobj->Model1_name;
}

extern void Model1_kobject_init(struct Model1_kobject *Model1_kobj, struct Model1_kobj_type *Model1_ktype);
extern __attribute__((format(printf, 3, 4))) __attribute__((warn_unused_result))
int Model1_kobject_add(struct Model1_kobject *Model1_kobj, struct Model1_kobject *Model1_parent,
  const char *Model1_fmt, ...);
extern __attribute__((format(printf, 4, 5))) __attribute__((warn_unused_result))
int Model1_kobject_init_and_add(struct Model1_kobject *Model1_kobj,
    struct Model1_kobj_type *Model1_ktype, struct Model1_kobject *Model1_parent,
    const char *Model1_fmt, ...);

extern void Model1_kobject_del(struct Model1_kobject *Model1_kobj);

extern struct Model1_kobject * __attribute__((warn_unused_result)) Model1_kobject_create(void);
extern struct Model1_kobject * __attribute__((warn_unused_result)) Model1_kobject_create_and_add(const char *Model1_name,
      struct Model1_kobject *Model1_parent);

extern int __attribute__((warn_unused_result)) Model1_kobject_rename(struct Model1_kobject *, const char *Model1_new_name);
extern int __attribute__((warn_unused_result)) Model1_kobject_move(struct Model1_kobject *, struct Model1_kobject *);

extern struct Model1_kobject *Model1_kobject_get(struct Model1_kobject *Model1_kobj);
extern void Model1_kobject_put(struct Model1_kobject *Model1_kobj);

extern const void *Model1_kobject_namespace(struct Model1_kobject *Model1_kobj);
extern char *Model1_kobject_get_path(struct Model1_kobject *Model1_kobj, Model1_gfp_t Model1_flag);

struct Model1_kobj_type {
 void (*Model1_release)(struct Model1_kobject *Model1_kobj);
 const struct Model1_sysfs_ops *Model1_sysfs_ops;
 struct Model1_attribute **Model1_default_attrs;
 const struct Model1_kobj_ns_type_operations *(*Model1_child_ns_type)(struct Model1_kobject *Model1_kobj);
 const void *(*Model1_namespace)(struct Model1_kobject *Model1_kobj);
};

struct Model1_kobj_uevent_env {
 char *Model1_argv[3];
 char *Model1_envp[32];
 int Model1_envp_idx;
 char Model1_buf[2048];
 int Model1_buflen;
};

struct Model1_kset_uevent_ops {
 int (* const Model1_filter)(struct Model1_kset *Model1_kset, struct Model1_kobject *Model1_kobj);
 const char *(* const Model1_name)(struct Model1_kset *Model1_kset, struct Model1_kobject *Model1_kobj);
 int (* const Model1_uevent)(struct Model1_kset *Model1_kset, struct Model1_kobject *Model1_kobj,
        struct Model1_kobj_uevent_env *Model1_env);
};

struct Model1_kobj_attribute {
 struct Model1_attribute Model1_attr;
 Model1_ssize_t (*Model1_show)(struct Model1_kobject *Model1_kobj, struct Model1_kobj_attribute *Model1_attr,
   char *Model1_buf);
 Model1_ssize_t (*Model1_store)(struct Model1_kobject *Model1_kobj, struct Model1_kobj_attribute *Model1_attr,
    const char *Model1_buf, Model1_size_t Model1_count);
};

extern const struct Model1_sysfs_ops Model1_kobj_sysfs_ops;

struct Model1_sock;

/**
 * struct kset - a set of kobjects of a specific type, belonging to a specific subsystem.
 *
 * A kset defines a group of kobjects.  They can be individually
 * different "types" but overall these kobjects all want to be grouped
 * together and operated on in the same manner.  ksets are used to
 * define the attribute callbacks and other common events that happen to
 * a kobject.
 *
 * @list: the list of all kobjects for this kset
 * @list_lock: a lock for iterating over the kobjects
 * @kobj: the embedded kobject for this kset (recursion, isn't it fun...)
 * @uevent_ops: the set of uevent operations for this kset.  These are
 * called whenever a kobject has something happen to it so that the kset
 * can add new environment variables, or filter out the uevents if so
 * desired.
 */
struct Model1_kset {
 struct Model1_list_head Model1_list;
 Model1_spinlock_t Model1_list_lock;
 struct Model1_kobject Model1_kobj;
 const struct Model1_kset_uevent_ops *Model1_uevent_ops;
};

extern void Model1_kset_init(struct Model1_kset *Model1_kset);
extern int __attribute__((warn_unused_result)) Model1_kset_register(struct Model1_kset *Model1_kset);
extern void Model1_kset_unregister(struct Model1_kset *Model1_kset);
extern struct Model1_kset * __attribute__((warn_unused_result)) Model1_kset_create_and_add(const char *Model1_name,
      const struct Model1_kset_uevent_ops *Model1_u,
      struct Model1_kobject *Model1_parent_kobj);

static inline __attribute__((no_instrument_function)) struct Model1_kset *Model1_to_kset(struct Model1_kobject *Model1_kobj)
{
 return Model1_kobj ? ({ const typeof( ((struct Model1_kset *)0)->Model1_kobj ) *Model1___mptr = (Model1_kobj); (struct Model1_kset *)( (char *)Model1___mptr - __builtin_offsetof(struct Model1_kset, Model1_kobj) );}) : ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model1_kset *Model1_kset_get(struct Model1_kset *Model1_k)
{
 return Model1_k ? Model1_to_kset(Model1_kobject_get(&Model1_k->Model1_kobj)) : ((void *)0);
}

static inline __attribute__((no_instrument_function)) void Model1_kset_put(struct Model1_kset *Model1_k)
{
 Model1_kobject_put(&Model1_k->Model1_kobj);
}

static inline __attribute__((no_instrument_function)) struct Model1_kobj_type *Model1_get_ktype(struct Model1_kobject *Model1_kobj)
{
 return Model1_kobj->Model1_ktype;
}

extern struct Model1_kobject *Model1_kset_find_obj(struct Model1_kset *, const char *);

/* The global /sys/kernel/ kobject for people to chain off of */
extern struct Model1_kobject *Model1_kernel_kobj;
/* The global /sys/kernel/mm/ kobject for people to chain off of */
extern struct Model1_kobject *Model1_mm_kobj;
/* The global /sys/hypervisor/ kobject for people to chain off of */
extern struct Model1_kobject *Model1_hypervisor_kobj;
/* The global /sys/power/ kobject for people to chain off of */
extern struct Model1_kobject *Model1_power_kobj;
/* The global /sys/firmware/ kobject for people to chain off of */
extern struct Model1_kobject *Model1_firmware_kobj;

int Model1_kobject_uevent(struct Model1_kobject *Model1_kobj, enum Model1_kobject_action Model1_action);
int Model1_kobject_uevent_env(struct Model1_kobject *Model1_kobj, enum Model1_kobject_action Model1_action,
   char *Model1_envp[]);

__attribute__((format(printf, 2, 3)))
int Model1_add_uevent_var(struct Model1_kobj_uevent_env *Model1_env, const char *format, ...);

int Model1_kobject_action_type(const char *Model1_buf, Model1_size_t Model1_count,
   enum Model1_kobject_action *Model1_type);


/* (C) Copyright 2001, 2002 Rusty Russell IBM Corporation */




/* You can override this manually, but generally this should match the
   module name. */






/* Chosen so that structs with an unsigned long line up. */
/* This struct is here for syntactic coherency, it is not used */






/* One for each parameter, describing how to use it.  Some files do
   multiple of these per line, so can't just use MODULE_INFO. */



struct Model1_kernel_param;

/*
 * Flags available for kernel_param_ops
 *
 * NOARG - the parameter allows for no argument (foo instead of foo=1)
 */
enum {
 Model1_KERNEL_PARAM_OPS_FL_NOARG = (1 << 0)
};

struct Model1_kernel_param_ops {
 /* How the ops should behave */
 unsigned int Model1_flags;
 /* Returns 0, or -errno.  arg is in kp->arg. */
 int (*Model1_set)(const char *Model1_val, const struct Model1_kernel_param *Model1_kp);
 /* Returns length written or -errno.  Buffer is 4k (ie. be short!) */
 int (*Model1_get)(char *Model1_buffer, const struct Model1_kernel_param *Model1_kp);
 /* Optional function to free kp->arg when module unloaded. */
 void (*Model1_free)(void *Model1_arg);
};

/*
 * Flags available for kernel_param
 *
 * UNSAFE - the parameter is dangerous and setting it will taint the kernel
 */
enum {
 Model1_KERNEL_PARAM_FL_UNSAFE = (1 << 0)
};

struct Model1_kernel_param {
 const char *Model1_name;
 struct Model1_module *Model1_mod;
 const struct Model1_kernel_param_ops *Model1_ops;
 const Model1_u16 Model1_perm;
 Model1_s8 Model1_level;
 Model1_u8 Model1_flags;
 union {
  void *Model1_arg;
  const struct Model1_kparam_string *Model1_str;
  const struct Model1_kparam_array *Model1_arr;
 };
};

extern const struct Model1_kernel_param Model1___start___param[], Model1___stop___param[];

/* Special one for strings we want to copy into */
struct Model1_kparam_string {
 unsigned int Model1_maxlen;
 char *Model1_string;
};

/* Special one for arrays */
struct Model1_kparam_array
{
 unsigned int Model1_max;
 unsigned int Model1_elemsize;
 unsigned int *Model1_num;
 const struct Model1_kernel_param_ops *Model1_ops;
 void *Model1_elem;
};

/**
 * module_param - typesafe helper for a module/cmdline parameter
 * @value: the variable to alter, and exposed parameter name.
 * @type: the type of the parameter
 * @perm: visibility in sysfs.
 *
 * @value becomes the module parameter, or (prefixed by KBUILD_MODNAME and a
 * ".") the kernel commandline parameter.  Note that - is changed to _, so
 * the user can use "foo-bar=1" even for variable "foo_bar".
 *
 * @perm is 0 if the the variable is not to appear in sysfs, or 0444
 * for world-readable, 0644 for root-writable, etc.  Note that if it
 * is writable, you may need to use kernel_param_lock() around
 * accesses (esp. charp, which can be kfreed when it changes).
 *
 * The @type is simply pasted to refer to a param_ops_##type and a
 * param_check_##type: for convenience many standard types are provided but
 * you can create your own by defining those variables.
 *
 * Standard types are:
 *	byte, short, ushort, int, uint, long, ulong
 *	charp: a character pointer
 *	bool: a bool, values 0/1, y/n, Y/N.
 *	invbool: the above, only sense-reversed (N = true).
 */



/**
 * module_param_unsafe - same as module_param but taints kernel
 */



/**
 * module_param_named - typesafe helper for a renamed module/cmdline parameter
 * @name: a valid C identifier which is the parameter name.
 * @value: the actual lvalue to alter.
 * @type: the type of the parameter
 * @perm: visibility in sysfs.
 *
 * Usually it's a good idea to have variable names and user-exposed names the
 * same, but that's harder if the variable must be non-static or is inside a
 * structure.  This allows exposure under a different name.
 */





/**
 * module_param_named_unsafe - same as module_param_named but taints kernel
 */





/**
 * module_param_cb - general callback for a module/cmdline parameter
 * @name: a valid C identifier which is the parameter name.
 * @ops: the set & get operations for this parameter.
 * @perm: visibility in sysfs.
 *
 * The ops can have NULL set or get functions.
 */







/**
 * <level>_param_cb - general callback for a module/cmdline parameter
 *                    to be evaluated before certain initcall level
 * @name: a valid C identifier which is the parameter name.
 * @ops: the set & get operations for this parameter.
 * @perm: visibility in sysfs.
 *
 * The ops can have NULL set or get functions.
 */
/* On alpha, ia64 and ppc64 relocations to global data cannot go into
   read-only sections (which is part of respective UNIX ABI on these
   platforms). So 'const' makes no sense and even causes compile failures
   with some compilers. */






/* This is the fundamental function for registering boot/module
   parameters. */
/* Obsolete - use module_param_cb() */







/* We don't get oldget: it's often a new-style param_get_uint, etc. */
static inline __attribute__((no_instrument_function)) int
Model1___check_old_set_param(int (*Model1_oldset)(const char *, struct Model1_kernel_param *))
{
 return 0;
}


extern void Model1_kernel_param_lock(struct Model1_module *Model1_mod);
extern void Model1_kernel_param_unlock(struct Model1_module *Model1_mod);
/**
 * core_param - define a historical core kernel parameter.
 * @name: the name of the cmdline and sysfs parameter (often the same as var)
 * @var: the variable
 * @type: the type of the parameter
 * @perm: visibility in sysfs
 *
 * core_param is just like module_param(), but cannot be modular and
 * doesn't add a prefix (such as "printk.").  This is for compatibility
 * with __setup(), and it makes sense as truly core parameters aren't
 * tied to the particular file they're in.
 */




/**
 * core_param_unsafe - same as core_param but taints kernel
 */







/**
 * module_param_string - a char array parameter
 * @name: the name of the parameter
 * @string: the string variable
 * @len: the maximum length of the string, incl. terminator
 * @perm: visibility in sysfs.
 *
 * This actually copies the string when it's set (unlike type charp).
 * @len is usually just sizeof(string).
 */
/**
 * parameq - checks if two parameter names match
 * @name1: parameter name 1
 * @name2: parameter name 2
 *
 * Returns true if the two parameter names are equal.
 * Dashes (-) are considered equal to underscores (_).
 */
extern bool Model1_parameq(const char *Model1_name1, const char *Model1_name2);

/**
 * parameqn - checks if two parameter names match
 * @name1: parameter name 1
 * @name2: parameter name 2
 * @n: the length to compare
 *
 * Similar to parameq(), except it compares @n characters.
 */
extern bool Model1_parameqn(const char *Model1_name1, const char *Model1_name2, Model1_size_t Model1_n);

/* Called on module insert or kernel boot */
extern char *Model1_parse_args(const char *Model1_name,
        char *Model1_args,
        const struct Model1_kernel_param *Model1_params,
        unsigned Model1_num,
        Model1_s16 Model1_level_min,
        Model1_s16 Model1_level_max,
        void *Model1_arg,
        int (*Model1_unknown)(char *Model1_param, char *Model1_val,
         const char *Model1_doing, void *Model1_arg));

/* Called by module remove. */

extern void Model1_destroy_params(const struct Model1_kernel_param *Model1_params, unsigned Model1_num);







/* All the helper functions */
/* The macros to do compile-time type checking stolen from Jakub
   Jelinek, who IIRC came up with this idea for the 2.4 module init code. */



extern const struct Model1_kernel_param_ops Model1_param_ops_byte;
extern int Model1_param_set_byte(const char *Model1_val, const struct Model1_kernel_param *Model1_kp);
extern int Model1_param_get_byte(char *Model1_buffer, const struct Model1_kernel_param *Model1_kp);


extern const struct Model1_kernel_param_ops Model1_param_ops_short;
extern int Model1_param_set_short(const char *Model1_val, const struct Model1_kernel_param *Model1_kp);
extern int Model1_param_get_short(char *Model1_buffer, const struct Model1_kernel_param *Model1_kp);


extern const struct Model1_kernel_param_ops Model1_param_ops_ushort;
extern int Model1_param_set_ushort(const char *Model1_val, const struct Model1_kernel_param *Model1_kp);
extern int Model1_param_get_ushort(char *Model1_buffer, const struct Model1_kernel_param *Model1_kp);


extern const struct Model1_kernel_param_ops Model1_param_ops_int;
extern int Model1_param_set_int(const char *Model1_val, const struct Model1_kernel_param *Model1_kp);
extern int Model1_param_get_int(char *Model1_buffer, const struct Model1_kernel_param *Model1_kp);


extern const struct Model1_kernel_param_ops Model1_param_ops_uint;
extern int Model1_param_set_uint(const char *Model1_val, const struct Model1_kernel_param *Model1_kp);
extern int Model1_param_get_uint(char *Model1_buffer, const struct Model1_kernel_param *Model1_kp);


extern const struct Model1_kernel_param_ops Model1_param_ops_long;
extern int Model1_param_set_long(const char *Model1_val, const struct Model1_kernel_param *Model1_kp);
extern int Model1_param_get_long(char *Model1_buffer, const struct Model1_kernel_param *Model1_kp);


extern const struct Model1_kernel_param_ops Model1_param_ops_ulong;
extern int Model1_param_set_ulong(const char *Model1_val, const struct Model1_kernel_param *Model1_kp);
extern int Model1_param_get_ulong(char *Model1_buffer, const struct Model1_kernel_param *Model1_kp);


extern const struct Model1_kernel_param_ops Model1_param_ops_ullong;
extern int Model1_param_set_ullong(const char *Model1_val, const struct Model1_kernel_param *Model1_kp);
extern int Model1_param_get_ullong(char *Model1_buffer, const struct Model1_kernel_param *Model1_kp);


extern const struct Model1_kernel_param_ops Model1_param_ops_charp;
extern int Model1_param_set_charp(const char *Model1_val, const struct Model1_kernel_param *Model1_kp);
extern int Model1_param_get_charp(char *Model1_buffer, const struct Model1_kernel_param *Model1_kp);
extern void Model1_param_free_charp(void *Model1_arg);


/* We used to allow int as well as bool.  We're taking that away! */
extern const struct Model1_kernel_param_ops Model1_param_ops_bool;
extern int Model1_param_set_bool(const char *Model1_val, const struct Model1_kernel_param *Model1_kp);
extern int Model1_param_get_bool(char *Model1_buffer, const struct Model1_kernel_param *Model1_kp);


extern const struct Model1_kernel_param_ops Model1_param_ops_bool_enable_only;
extern int Model1_param_set_bool_enable_only(const char *Model1_val,
          const struct Model1_kernel_param *Model1_kp);
/* getter is the same as for the regular bool */


extern const struct Model1_kernel_param_ops Model1_param_ops_invbool;
extern int Model1_param_set_invbool(const char *Model1_val, const struct Model1_kernel_param *Model1_kp);
extern int Model1_param_get_invbool(char *Model1_buffer, const struct Model1_kernel_param *Model1_kp);


/* An int, which can only be set like a bool (though it shows as an int). */
extern const struct Model1_kernel_param_ops Model1_param_ops_bint;
extern int Model1_param_set_bint(const char *Model1_val, const struct Model1_kernel_param *Model1_kp);



/**
 * module_param_array - a parameter which is an array of some type
 * @name: the name of the array variable
 * @type: the type, as per module_param()
 * @nump: optional pointer filled in with the number written
 * @perm: visibility in sysfs
 *
 * Input and output are as comma-separated values.  Commas inside values
 * don't work properly (eg. an array of charp).
 *
 * ARRAY_SIZE(@name) is used to determine the number of elements in the
 * array, so the definition must be visible.
 */



/**
 * module_param_array_named - renamed parameter which is an array of some type
 * @name: a valid C identifier which is the parameter name
 * @array: the name of the array variable
 * @type: the type, as per module_param()
 * @nump: optional pointer filled in with the number written
 * @perm: visibility in sysfs
 *
 * This exposes a different name than the actual variable name.  See
 * module_param_named() for why this might be necessary.
 */
extern const struct Model1_kernel_param_ops Model1_param_array_ops;

extern const struct Model1_kernel_param_ops Model1_param_ops_string;
extern int Model1_param_set_copystring(const char *Model1_val, const struct Model1_kernel_param *);
extern int Model1_param_get_string(char *Model1_buffer, const struct Model1_kernel_param *Model1_kp);

/* for exporting parameters in /sys/module/.../parameters */

struct Model1_module;


extern int Model1_module_param_sysfs_setup(struct Model1_module *Model1_mod,
        const struct Model1_kernel_param *Model1_kparam,
        unsigned int Model1_num_params);

extern void Model1_module_param_sysfs_remove(struct Model1_module *Model1_mod);







struct Model1_module;
struct Model1_exception_table_entry;

const struct Model1_exception_table_entry *
Model1_search_extable(const struct Model1_exception_table_entry *Model1_first,
        const struct Model1_exception_table_entry *Model1_last,
        unsigned long Model1_value);
void Model1_sort_extable(struct Model1_exception_table_entry *Model1_start,
    struct Model1_exception_table_entry *Model1_finish);
void Model1_sort_main_extable(void);
void Model1_trim_init_extable(struct Model1_module *Model1_m);

/* Given an address, look for it in the exception tables */
const struct Model1_exception_table_entry *Model1_search_exception_tables(unsigned long Model1_add);


/* For extable.c to search modules' exception tables. */
const struct Model1_exception_table_entry *Model1_search_module_extables(unsigned long Model1_addr);
/*
 * Latched RB-trees
 *
 * Copyright (C) 2015 Intel Corp., Peter Zijlstra <peterz@infradead.org>
 *
 * Since RB-trees have non-atomic modifications they're not immediately suited
 * for RCU/lockless queries. Even though we made RB-tree lookups non-fatal for
 * lockless lookups; we cannot guarantee they return a correct result.
 *
 * The simplest solution is a seqlock + RB-tree, this will allow lockless
 * lookups; but has the constraint (inherent to the seqlock) that read sides
 * cannot nest in write sides.
 *
 * If we need to allow unconditional lookups (say as required for NMI context
 * usage) we need a more complex setup; this data structure provides this by
 * employing the latch technique -- see @raw_write_seqcount_latch -- to
 * implement a latched RB-tree which does allow for unconditional lookups by
 * virtue of always having (at least) one stable copy of the tree.
 *
 * However, while we have the guarantee that there is at all times one stable
 * copy, this does not guarantee an iteration will not observe modifications.
 * What might have been a stable copy at the start of the iteration, need not
 * remain so for the duration of the iteration.
 *
 * Therefore, this does require a lockless RB-tree iteration to be non-fatal;
 * see the comment in lib/rbtree.c. Note however that we only require the first
 * condition -- not seeing partial stores -- because the latch thing isolates
 * us from loops. If we were to interrupt a modification the lookup would be
 * pointed at the stable tree and complete while the modification was halted.
 */







struct Model1_latch_tree_node {
 struct Model1_rb_node Model1_node[2];
};

struct Model1_latch_tree_root {
 Model1_seqcount_t Model1_seq;
 struct Model1_rb_root Model1_tree[2];
};

/**
 * latch_tree_ops - operators to define the tree order
 * @less: used for insertion; provides the (partial) order between two elements.
 * @comp: used for lookups; provides the order between the search key and an element.
 *
 * The operators are related like:
 *
 *	comp(a->key,b) < 0  := less(a,b)
 *	comp(a->key,b) > 0  := less(b,a)
 *	comp(a->key,b) == 0 := !less(a,b) && !less(b,a)
 *
 * If these operators define a partial order on the elements we make no
 * guarantee on which of the elements matching the key is found. See
 * latch_tree_find().
 */
struct Model1_latch_tree_ops {
 bool (*Model1_less)(struct Model1_latch_tree_node *Model1_a, struct Model1_latch_tree_node *Model1_b);
 int (*Model1_comp)(void *Model1_key, struct Model1_latch_tree_node *Model1_b);
};

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) struct Model1_latch_tree_node *
Model1___lt_from_rb(struct Model1_rb_node *Model1_node, int Model1_idx)
{
 return ({ const typeof( ((struct Model1_latch_tree_node *)0)->Model1_node[Model1_idx] ) *Model1___mptr = (Model1_node); (struct Model1_latch_tree_node *)( (char *)Model1___mptr - __builtin_offsetof(struct Model1_latch_tree_node, Model1_node[Model1_idx]) );});
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void
Model1___lt_insert(struct Model1_latch_tree_node *Model1_ltn, struct Model1_latch_tree_root *Model1_ltr, int Model1_idx,
     bool (*Model1_less)(struct Model1_latch_tree_node *Model1_a, struct Model1_latch_tree_node *Model1_b))
{
 struct Model1_rb_root *Model1_root = &Model1_ltr->Model1_tree[Model1_idx];
 struct Model1_rb_node **Model1_link = &Model1_root->Model1_rb_node;
 struct Model1_rb_node *Model1_node = &Model1_ltn->Model1_node[Model1_idx];
 struct Model1_rb_node *Model1_parent = ((void *)0);
 struct Model1_latch_tree_node *Model1_ltp;

 while (*Model1_link) {
  Model1_parent = *Model1_link;
  Model1_ltp = Model1___lt_from_rb(Model1_parent, Model1_idx);

  if (Model1_less(Model1_ltn, Model1_ltp))
   Model1_link = &Model1_parent->Model1_rb_left;
  else
   Model1_link = &Model1_parent->Model1_rb_right;
 }

 Model1_rb_link_node_rcu(Model1_node, Model1_parent, Model1_link);
 Model1_rb_insert_color(Model1_node, Model1_root);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void
Model1___lt_erase(struct Model1_latch_tree_node *Model1_ltn, struct Model1_latch_tree_root *Model1_ltr, int Model1_idx)
{
 Model1_rb_erase(&Model1_ltn->Model1_node[Model1_idx], &Model1_ltr->Model1_tree[Model1_idx]);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) struct Model1_latch_tree_node *
Model1___lt_find(void *Model1_key, struct Model1_latch_tree_root *Model1_ltr, int Model1_idx,
   int (*Model1_comp)(void *Model1_key, struct Model1_latch_tree_node *Model1_node))
{
 struct Model1_rb_node *Model1_node = ({ typeof(Model1_ltr->Model1_tree[Model1_idx].Model1_rb_node) Model1_________p1 = ({ typeof(Model1_ltr->Model1_tree[Model1_idx].Model1_rb_node) Model1__________p1 = ({ union { typeof(Model1_ltr->Model1_tree[Model1_idx].Model1_rb_node) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&(Model1_ltr->Model1_tree[Model1_idx].Model1_rb_node), Model1___u.Model1___c, sizeof(Model1_ltr->Model1_tree[Model1_idx].Model1_rb_node)); else Model1___read_once_size_nocheck(&(Model1_ltr->Model1_tree[Model1_idx].Model1_rb_node), Model1___u.Model1___c, sizeof(Model1_ltr->Model1_tree[Model1_idx].Model1_rb_node)); Model1___u.Model1___val; }); typeof(*(Model1_ltr->Model1_tree[Model1_idx].Model1_rb_node)) *Model1____typecheck_p __attribute__((unused)); do { } while (0); (Model1__________p1); }); ((typeof(*Model1_ltr->Model1_tree[Model1_idx].Model1_rb_node) *)(Model1_________p1)); });
 struct Model1_latch_tree_node *Model1_ltn;
 int Model1_c;

 while (Model1_node) {
  Model1_ltn = Model1___lt_from_rb(Model1_node, Model1_idx);
  Model1_c = Model1_comp(Model1_key, Model1_ltn);

  if (Model1_c < 0)
   Model1_node = ({ typeof(Model1_node->Model1_rb_left) Model1_________p1 = ({ typeof(Model1_node->Model1_rb_left) Model1__________p1 = ({ union { typeof(Model1_node->Model1_rb_left) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&(Model1_node->Model1_rb_left), Model1___u.Model1___c, sizeof(Model1_node->Model1_rb_left)); else Model1___read_once_size_nocheck(&(Model1_node->Model1_rb_left), Model1___u.Model1___c, sizeof(Model1_node->Model1_rb_left)); Model1___u.Model1___val; }); typeof(*(Model1_node->Model1_rb_left)) *Model1____typecheck_p __attribute__((unused)); do { } while (0); (Model1__________p1); }); ((typeof(*Model1_node->Model1_rb_left) *)(Model1_________p1)); });
  else if (Model1_c > 0)
   Model1_node = ({ typeof(Model1_node->Model1_rb_right) Model1_________p1 = ({ typeof(Model1_node->Model1_rb_right) Model1__________p1 = ({ union { typeof(Model1_node->Model1_rb_right) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&(Model1_node->Model1_rb_right), Model1___u.Model1___c, sizeof(Model1_node->Model1_rb_right)); else Model1___read_once_size_nocheck(&(Model1_node->Model1_rb_right), Model1___u.Model1___c, sizeof(Model1_node->Model1_rb_right)); Model1___u.Model1___val; }); typeof(*(Model1_node->Model1_rb_right)) *Model1____typecheck_p __attribute__((unused)); do { } while (0); (Model1__________p1); }); ((typeof(*Model1_node->Model1_rb_right) *)(Model1_________p1)); });
  else
   return Model1_ltn;
 }

 return ((void *)0);
}

/**
 * latch_tree_insert() - insert @node into the trees @root
 * @node: nodes to insert
 * @root: trees to insert @node into
 * @ops: operators defining the node order
 *
 * It inserts @node into @root in an ordered fashion such that we can always
 * observe one complete tree. See the comment for raw_write_seqcount_latch().
 *
 * The inserts use rcu_assign_pointer() to publish the element such that the
 * tree structure is stored before we can observe the new @node.
 *
 * All modifications (latch_tree_insert, latch_tree_remove) are assumed to be
 * serialized.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void
Model1_latch_tree_insert(struct Model1_latch_tree_node *Model1_node,
    struct Model1_latch_tree_root *Model1_root,
    const struct Model1_latch_tree_ops *Model1_ops)
{
 Model1_raw_write_seqcount_latch(&Model1_root->Model1_seq);
 Model1___lt_insert(Model1_node, Model1_root, 0, Model1_ops->Model1_less);
 Model1_raw_write_seqcount_latch(&Model1_root->Model1_seq);
 Model1___lt_insert(Model1_node, Model1_root, 1, Model1_ops->Model1_less);
}

/**
 * latch_tree_erase() - removes @node from the trees @root
 * @node: nodes to remote
 * @root: trees to remove @node from
 * @ops: operators defining the node order
 *
 * Removes @node from the trees @root in an ordered fashion such that we can
 * always observe one complete tree. See the comment for
 * raw_write_seqcount_latch().
 *
 * It is assumed that @node will observe one RCU quiescent state before being
 * reused of freed.
 *
 * All modifications (latch_tree_insert, latch_tree_remove) are assumed to be
 * serialized.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void
Model1_latch_tree_erase(struct Model1_latch_tree_node *Model1_node,
   struct Model1_latch_tree_root *Model1_root,
   const struct Model1_latch_tree_ops *Model1_ops)
{
 Model1_raw_write_seqcount_latch(&Model1_root->Model1_seq);
 Model1___lt_erase(Model1_node, Model1_root, 0);
 Model1_raw_write_seqcount_latch(&Model1_root->Model1_seq);
 Model1___lt_erase(Model1_node, Model1_root, 1);
}

/**
 * latch_tree_find() - find the node matching @key in the trees @root
 * @key: search key
 * @root: trees to search for @key
 * @ops: operators defining the node order
 *
 * Does a lockless lookup in the trees @root for the node matching @key.
 *
 * It is assumed that this is called while holding the appropriate RCU read
 * side lock.
 *
 * If the operators define a partial order on the elements (there are multiple
 * elements which have the same key value) it is undefined which of these
 * elements will be found. Nor is it possible to iterate the tree to find
 * further elements with the same key value.
 *
 * Returns: a pointer to the node matching @key or NULL.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) struct Model1_latch_tree_node *
Model1_latch_tree_find(void *Model1_key, struct Model1_latch_tree_root *Model1_root,
  const struct Model1_latch_tree_ops *Model1_ops)
{
 struct Model1_latch_tree_node *Model1_node;
 unsigned int Model1_seq;

 do {
  Model1_seq = Model1_raw_read_seqcount_latch(&Model1_root->Model1_seq);
  Model1_node = Model1___lt_find(Model1_key, Model1_root, Model1_seq & 1, Model1_ops->Model1_comp);
 } while (Model1_read_seqcount_retry(&Model1_root->Model1_seq, Model1_seq));

 return Model1_node;
}








/*
 * Many architectures just need a simple module
 * loader without arch specific data.
 */

struct Model1_mod_arch_specific
{
};


/* X86_64 does not define MODULE_PROC_FAMILY */

/* In stripped ARM and x86-64 modules, ~ is surprisingly rare. */


/* Not Yet Implemented */




struct Model1_modversion_info {
 unsigned long Model1_crc;
 char Model1_name[(64 - sizeof(unsigned long))];
};

struct Model1_module;
struct Model1_exception_table_entry;

struct Model1_module_kobject {
 struct Model1_kobject Model1_kobj;
 struct Model1_module *Model1_mod;
 struct Model1_kobject *Model1_drivers_dir;
 struct Model1_module_param_attrs *Model1_mp;
 struct Model1_completion *Model1_kobj_completion;
};

struct Model1_module_attribute {
 struct Model1_attribute Model1_attr;
 Model1_ssize_t (*Model1_show)(struct Model1_module_attribute *, struct Model1_module_kobject *,
   char *);
 Model1_ssize_t (*Model1_store)(struct Model1_module_attribute *, struct Model1_module_kobject *,
    const char *, Model1_size_t Model1_count);
 void (*Model1_setup)(struct Model1_module *, const char *);
 int (*Model1_test)(struct Model1_module *);
 void (*Model1_free)(struct Model1_module *);
};

struct Model1_module_version_attribute {
 struct Model1_module_attribute Model1_mattr;
 const char *Model1_module_name;
 const char *Model1_version;
} __attribute__ ((__aligned__(sizeof(void *))));

extern Model1_ssize_t Model1___modver_version_show(struct Model1_module_attribute *,
         struct Model1_module_kobject *, char *);

extern struct Model1_module_attribute Model1_module_uevent;

/* These are either module local, or the kernel's dummy ones. */
extern int Model1_init_module(void);
extern void Model1_cleanup_module(void);


/**
 * module_init() - driver initialization entry point
 * @x: function to be run at kernel boot time or module insertion
 *
 * module_init() will either be called during do_initcalls() (if
 * builtin) or at module insertion time (if a module).  There can only
 * be one per module.
 */


/**
 * module_exit() - driver exit entry point
 * @x: function to be run when driver is removed
 *
 * module_exit() will wrap the driver clean-up code
 * with cleanup_module() when used with rmmod when
 * the driver is a module.  If the driver is statically
 * compiled into the kernel, module_exit() has no effect.
 * There can only be one per module.
 */
/* This means "can be init if no module support, otherwise module load
   may call it." */
/* Generic info of form tag = "info" */


/* For userspace: you can also call me... */


/* Soft module dependencies. See man modprobe.d for details.
 * Example: MODULE_SOFTDEP("pre: module-foo module-bar post: module-baz")
 */


/*
 * The following license idents are currently accepted as indicating free
 * software modules
 *
 *	"GPL"				[GNU Public License v2 or later]
 *	"GPL v2"			[GNU Public License v2]
 *	"GPL and additional rights"	[GNU Public License v2 rights and more]
 *	"Dual BSD/GPL"			[GNU Public License v2
 *					 or BSD license choice]
 *	"Dual MIT/GPL"			[GNU Public License v2
 *					 or MIT license choice]
 *	"Dual MPL/GPL"			[GNU Public License v2
 *					 or Mozilla license choice]
 *
 * The following other idents are available
 *
 *	"Proprietary"			[Non free products]
 *
 * There are dual licensed components, but when running with Linux it is the
 * GPL that is relevant so this is a non issue. Similarly LGPL linked with GPL
 * is a GPL combined work.
 *
 * This exists for several reasons
 * 1.	So modinfo can show license info for users wanting to vet their setup
 *	is free
 * 2.	So the community can ignore bug reports including proprietary modules
 * 3.	So vendors can do likewise based on their own policies
 */


/*
 * Author(s), use "Name <email>" or just "Name", for multiple
 * authors use multiple MODULE_AUTHOR() statements/lines.
 */


/* What your module does. */
/* Version of form [<epoch>:]<version>[-<extra-version>].
 * Or for CVS/RCS ID version, everything but the number is stripped.
 * <epoch>: A (small) unsigned integer which allows you to start versions
 * anew. If not mentioned, it's zero.  eg. "2:1.0" is after
 * "1:2.0".

 * <version>: The <version> may contain only alphanumerics and the
 * character `.'.  Ordered by numeric sort for numeric parts,
 * ascii sort for ascii parts (as per RPM or DEB algorithm).

 * <extraversion>: Like <version>, but inserted for local
 * customizations, eg "rh3" or "rusty1".

 * Using this automatically adds a checksum of the .c files and the
 * local headers in "srcversion".
 */
/* Optional firmware file (or files) needed by the module
 * format is simply firmware file name.  Multiple firmware
 * files require multiple MODULE_FIRMWARE() specifiers */


struct Model1_notifier_block;



extern int Model1_modules_disabled; /* for sysctl */
/* Get/put a kernel symbol (calls must be symmetric) */
void *Model1___symbol_get(const char *Model1_symbol);
void *Model1___symbol_get_gpl(const char *Model1_symbol);


/* modules using other modules: kdb wants to see this. */
struct Model1_module_use {
 struct Model1_list_head Model1_source_list;
 struct Model1_list_head Model1_target_list;
 struct Model1_module *Model1_source, *Model1_target;
};

enum Model1_module_state {
 Model1_MODULE_STATE_LIVE, /* Normal state. */
 Model1_MODULE_STATE_COMING, /* Full formed, running module_init. */
 Model1_MODULE_STATE_GOING, /* Going away. */
 Model1_MODULE_STATE_UNFORMED, /* Still setting it up. */
};

struct Model1_module;

struct Model1_mod_tree_node {
 struct Model1_module *Model1_mod;
 struct Model1_latch_tree_node Model1_node;
};

struct Model1_module_layout {
 /* The actual code + data. */
 void *Model1_base;
 /* Total size. */
 unsigned int Model1_size;
 /* The size of the executable code.  */
 unsigned int Model1_text_size;
 /* Size of RO section of the module (text+rodata) */
 unsigned int Model1_ro_size;
 /* Size of RO after init section */
 unsigned int Model1_ro_after_init_size;


 struct Model1_mod_tree_node Model1_mtn;

};


/* Only touch one cacheline for common rbtree-for-core-layout case. */





struct Model1_mod_kallsyms {
 Model1_Elf64_Sym *Model1_symtab;
 unsigned int Model1_num_symtab;
 char *Model1_strtab;
};
struct Model1_module {
 enum Model1_module_state Model1_state;

 /* Member of list of modules */
 struct Model1_list_head Model1_list;

 /* Unique handle for this module */
 char Model1_name[(64 - sizeof(unsigned long))];

 /* Sysfs stuff. */
 struct Model1_module_kobject Model1_mkobj;
 struct Model1_module_attribute *Model1_modinfo_attrs;
 const char *Model1_version;
 const char *Model1_srcversion;
 struct Model1_kobject *Model1_holders_dir;

 /* Exported symbols */
 const struct Model1_kernel_symbol *Model1_syms;
 const unsigned long *Model1_crcs;
 unsigned int Model1_num_syms;

 /* Kernel parameters. */

 struct Model1_mutex Model1_param_lock;

 struct Model1_kernel_param *Model1_kp;
 unsigned int Model1_num_kp;

 /* GPL-only exported symbols. */
 unsigned int Model1_num_gpl_syms;
 const struct Model1_kernel_symbol *Model1_gpl_syms;
 const unsigned long *Model1_gpl_crcs;
 bool Model1_async_probe_requested;

 /* symbols that will be GPL-only in the near future. */
 const struct Model1_kernel_symbol *Model1_gpl_future_syms;
 const unsigned long *Model1_gpl_future_crcs;
 unsigned int Model1_num_gpl_future_syms;

 /* Exception table */
 unsigned int Model1_num_exentries;
 struct Model1_exception_table_entry *Model1_extable;

 /* Startup function. */
 int (*Model1_init)(void);

 /* Core layout: rbtree is accessed frequently, so keep together. */
 struct Model1_module_layout Model1_core_layout __attribute__((__aligned__((1 << (6)))));
 struct Model1_module_layout Model1_init_layout;

 /* Arch-specific module values */
 struct Model1_mod_arch_specific Model1_arch;

 unsigned int Model1_taints; /* same bits as kernel:tainted */


 /* Support for BUG */
 unsigned Model1_num_bugs;
 struct Model1_list_head Model1_bug_list;
 struct Model1_bug_entry *Model1_bug_table;



 /* Protected by RCU and/or module_mutex: use rcu_dereference() */
 struct Model1_mod_kallsyms *Model1_kallsyms;
 struct Model1_mod_kallsyms Model1_core_kallsyms;

 /* Section attributes */
 struct Model1_module_sect_attrs *Model1_sect_attrs;

 /* Notes attributes */
 struct Model1_module_notes_attrs *Model1_notes_attrs;


 /* The command line arguments (may be mangled).  People like
	   keeping pointers to this stuff */
 char *Model1_args;


 /* Per-cpu data. */
 void *Model1_percpu;
 unsigned int Model1_percpu_size;



 unsigned int Model1_num_tracepoints;
 struct Model1_tracepoint * const *Model1_tracepoints_ptrs;






 unsigned int Model1_num_trace_bprintk_fmt;
 const char **Model1_trace_bprintk_fmt_start;


 struct Model1_trace_event_call **Model1_trace_events;
 unsigned int Model1_num_trace_events;
 struct Model1_trace_enum_map **Model1_trace_enums;
 unsigned int Model1_num_trace_enums;
 /* What modules depend on me? */
 struct Model1_list_head Model1_source_list;
 /* What modules do I depend on? */
 struct Model1_list_head Model1_target_list;

 /* Destruction function. */
 void (*Model1_exit)(void);

 Model1_atomic_t Model1_refcnt;







} __attribute__((__aligned__((1 << (6)))));




extern struct Model1_mutex Model1_module_mutex;

/* FIXME: It'd be nice to isolate modules during init, too, so they
   aren't used before they (may) fail.  But presently too much code
   (IDE & SCSI) require entry into the module during init.*/
static inline __attribute__((no_instrument_function)) int Model1_module_is_live(struct Model1_module *Model1_mod)
{
 return Model1_mod->Model1_state != Model1_MODULE_STATE_GOING;
}

struct Model1_module *Model1___module_text_address(unsigned long Model1_addr);
struct Model1_module *Model1___module_address(unsigned long Model1_addr);
bool Model1_is_module_address(unsigned long Model1_addr);
bool Model1_is_module_percpu_address(unsigned long Model1_addr);
bool Model1_is_module_text_address(unsigned long Model1_addr);

static inline __attribute__((no_instrument_function)) bool Model1_within_module_core(unsigned long Model1_addr,
          const struct Model1_module *Model1_mod)
{
 return (unsigned long)Model1_mod->Model1_core_layout.Model1_base <= Model1_addr &&
        Model1_addr < (unsigned long)Model1_mod->Model1_core_layout.Model1_base + Model1_mod->Model1_core_layout.Model1_size;
}

static inline __attribute__((no_instrument_function)) bool Model1_within_module_init(unsigned long Model1_addr,
          const struct Model1_module *Model1_mod)
{
 return (unsigned long)Model1_mod->Model1_init_layout.Model1_base <= Model1_addr &&
        Model1_addr < (unsigned long)Model1_mod->Model1_init_layout.Model1_base + Model1_mod->Model1_init_layout.Model1_size;
}

static inline __attribute__((no_instrument_function)) bool Model1_within_module(unsigned long Model1_addr, const struct Model1_module *Model1_mod)
{
 return Model1_within_module_init(Model1_addr, Model1_mod) || Model1_within_module_core(Model1_addr, Model1_mod);
}

/* Search for module by name: must hold module_mutex. */
struct Model1_module *Model1_find_module(const char *Model1_name);

struct Model1_symsearch {
 const struct Model1_kernel_symbol *Model1_start, *Model1_stop;
 const unsigned long *Model1_crcs;
 enum {
  Model1_NOT_GPL_ONLY,
  Model1_GPL_ONLY,
  Model1_WILL_BE_GPL_ONLY,
 } Model1_licence;
 bool unused;
};

/*
 * Search for an exported symbol by name.
 *
 * Must be called with module_mutex held or preemption disabled.
 */
const struct Model1_kernel_symbol *Model1_find_symbol(const char *Model1_name,
     struct Model1_module **Model1_owner,
     const unsigned long **Model1_crc,
     bool Model1_gplok,
     bool Model1_warn);

/*
 * Walk the exported symbol table
 *
 * Must be called with module_mutex held or preemption disabled.
 */
bool Model1_each_symbol_section(bool (*Model1_fn)(const struct Model1_symsearch *Model1_arr,
        struct Model1_module *Model1_owner,
        void *Model1_data), void *Model1_data);

/* Returns 0 and fills in value, defined and namebuf, or -ERANGE if
   symnum out of range. */
int Model1_module_get_kallsym(unsigned int Model1_symnum, unsigned long *Model1_value, char *Model1_type,
   char *Model1_name, char *Model1_module_name, int *Model1_exported);

/* Look for this name: can be of form module:name. */
unsigned long Model1_module_kallsyms_lookup_name(const char *Model1_name);

int Model1_module_kallsyms_on_each_symbol(int (*Model1_fn)(void *, const char *,
          struct Model1_module *, unsigned long),
       void *Model1_data);

extern void __attribute__((noreturn)) Model1___module_put_and_exit(struct Model1_module *Model1_mod,
   long Model1_code);



int Model1_module_refcount(struct Model1_module *Model1_mod);
void Model1___symbol_put(const char *Model1_symbol);

void Model1_symbol_put_addr(void *Model1_addr);

/* Sometimes we know we already have a refcount, and it's easier not
   to handle the error case (which only happens with rmmod --wait). */
extern void Model1___module_get(struct Model1_module *Model1_module);

/* This is the Right Way to get a module: if it fails, it's being removed,
 * so pretend it's not there. */
extern bool Model1_try_module_get(struct Model1_module *Model1_module);

extern void Model1_module_put(struct Model1_module *Model1_module);
int Model1_ref_module(struct Model1_module *Model1_a, struct Model1_module *Model1_b);

/* This is a #define so the string doesn't get put in every .o file */






/* For kallsyms to ask for address resolution.  namebuf should be at
 * least KSYM_NAME_LEN long: a pointer to namebuf is returned if
 * found, otherwise NULL. */
const char *Model1_module_address_lookup(unsigned long Model1_addr,
       unsigned long *Model1_symbolsize,
       unsigned long *Model1_offset,
       char **Model1_modname,
       char *Model1_namebuf);
int Model1_lookup_module_symbol_name(unsigned long Model1_addr, char *Model1_symname);
int Model1_lookup_module_symbol_attrs(unsigned long Model1_addr, unsigned long *Model1_size, unsigned long *Model1_offset, char *Model1_modname, char *Model1_name);

int Model1_register_module_notifier(struct Model1_notifier_block *Model1_nb);
int Model1_unregister_module_notifier(struct Model1_notifier_block *Model1_nb);

extern void Model1_print_modules(void);

static inline __attribute__((no_instrument_function)) bool Model1_module_requested_async_probing(struct Model1_module *Model1_module)
{
 return Model1_module && Model1_module->Model1_async_probe_requested;
}







static inline __attribute__((no_instrument_function)) bool Model1_is_livepatch_module(struct Model1_module *Model1_mod)
{
 return false;
}
extern struct Model1_kset *Model1_module_kset;
extern struct Model1_kobj_type Model1_module_ktype;
extern int Model1_module_sysfs_initialized;




/* BELOW HERE ALL THESE ARE OBSOLETE AND WILL VANISH */
static inline __attribute__((no_instrument_function)) void Model1_set_all_modules_text_rw(void) { }
static inline __attribute__((no_instrument_function)) void Model1_set_all_modules_text_ro(void) { }
static inline __attribute__((no_instrument_function)) void Model1_module_enable_ro(const struct Model1_module *Model1_mod, bool Model1_after_init) { }
static inline __attribute__((no_instrument_function)) void Model1_module_disable_ro(const struct Model1_module *Model1_mod) { }



void Model1_module_bug_finalize(const Model1_Elf64_Ehdr *, const Model1_Elf64_Shdr *,
    struct Model1_module *);
void Model1_module_bug_cleanup(struct Model1_module *);
static inline __attribute__((no_instrument_function)) bool Model1_module_sig_ok(struct Model1_module *Model1_module)
{
 return true;
}


/*
 *  Generic cache management functions. Everything is arch-specific,  
 *  but this header exists to make sure the defines/functions can be
 *  used in a generic way.
 *
 *  2000-11-13  Arjan van de Ven   <arjan@fenrus.demon.nl>
 *
 */
/*
	prefetch(x) attempts to pre-emptively get the memory pointed to
	by address "x" into the CPU L1 cache. 
	prefetch(x) should not cause any kind of exception, prefetch(0) is
	specifically ok.

	prefetch() should be defined by the architecture, if not, the 
	#define below provides a no-op define.	
	
	There are 3 prefetch() macros:
	
	prefetch(x)  	- prefetches the cacheline at "x" for read
	prefetchw(x)	- prefetches the cacheline at "x" for write
	spin_lock_prefetch(x) - prefetches the spinlock *x for taking
	
	there is also PREFETCH_STRIDE which is the architecure-preferred 
	"lookahead" size for prefetching streamed operations.
	
*/
static inline __attribute__((no_instrument_function)) void Model1_prefetch_range(void *Model1_addr, Model1_size_t Model1_len)
{







}
/*
 * net/dst.h	Protocol independent destination cache definitions.
 *
 * Authors:	Alexey Kuznetsov, <kuznet@ms2.inr.ac.ru>
 *
 */










/*
 * A simple "approximate counter" for use in ext2 and ext3 superblocks.
 *
 * WARNING: these things are HUGE.  4 kbytes per counter on 32-way P4.
 */
struct Model1_percpu_counter {
 Model1_raw_spinlock_t Model1_lock;
 Model1_s64 Model1_count;

 struct Model1_list_head Model1_list; /* All percpu_counters are on a list */

 Model1_s32 *Model1_counters;
};

extern int Model1_percpu_counter_batch;

int Model1___percpu_counter_init(struct Model1_percpu_counter *Model1_fbc, Model1_s64 Model1_amount, Model1_gfp_t Model1_gfp,
     struct Model1_lock_class_key *Model1_key);
void Model1_percpu_counter_destroy(struct Model1_percpu_counter *Model1_fbc);
void Model1_percpu_counter_set(struct Model1_percpu_counter *Model1_fbc, Model1_s64 Model1_amount);
void Model1___percpu_counter_add(struct Model1_percpu_counter *Model1_fbc, Model1_s64 Model1_amount, Model1_s32 Model1_batch);
Model1_s64 Model1___percpu_counter_sum(struct Model1_percpu_counter *Model1_fbc);
int Model1___percpu_counter_compare(struct Model1_percpu_counter *Model1_fbc, Model1_s64 Model1_rhs, Model1_s32 Model1_batch);

static inline __attribute__((no_instrument_function)) int Model1_percpu_counter_compare(struct Model1_percpu_counter *Model1_fbc, Model1_s64 Model1_rhs)
{
 return Model1___percpu_counter_compare(Model1_fbc, Model1_rhs, Model1_percpu_counter_batch);
}

static inline __attribute__((no_instrument_function)) void Model1_percpu_counter_add(struct Model1_percpu_counter *Model1_fbc, Model1_s64 Model1_amount)
{
#if CY_ABSTRACT1/* !CONFIG_SMP */
    //preempt_disable();
    Model1_fbc->Model1_count += Model1_amount;
    //preempt_enable();
#else
 Model1___percpu_counter_add(Model1_fbc, Model1_amount, Model1_percpu_counter_batch);
#endif
}

static inline __attribute__((no_instrument_function)) Model1_s64 Model1_percpu_counter_sum_positive(struct Model1_percpu_counter *Model1_fbc)
{
 Model1_s64 Model1_ret = Model1___percpu_counter_sum(Model1_fbc);
 return Model1_ret < 0 ? 0 : Model1_ret;
}

static inline __attribute__((no_instrument_function)) Model1_s64 Model1_percpu_counter_sum(struct Model1_percpu_counter *Model1_fbc)
{
 return Model1___percpu_counter_sum(Model1_fbc);
}

static inline __attribute__((no_instrument_function)) Model1_s64 Model1_percpu_counter_read(struct Model1_percpu_counter *Model1_fbc)
{
 return Model1_fbc->Model1_count;
}

/*
 * It is possible for the percpu_counter_read() to return a small negative
 * number for some counter which should never be negative.
 *
 */
static inline __attribute__((no_instrument_function)) Model1_s64 Model1_percpu_counter_read_positive(struct Model1_percpu_counter *Model1_fbc)
{
 Model1_s64 Model1_ret = Model1_fbc->Model1_count;

 __asm__ __volatile__("": : :"memory"); /* Prevent reloads of fbc->count */
 if (Model1_ret >= 0)
  return Model1_ret;
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_percpu_counter_initialized(struct Model1_percpu_counter *Model1_fbc)
{
 return (Model1_fbc->Model1_counters != ((void *)0));
}
static inline __attribute__((no_instrument_function)) void Model1_percpu_counter_inc(struct Model1_percpu_counter *Model1_fbc)
{
 Model1_percpu_counter_add(Model1_fbc, 1);
}

static inline __attribute__((no_instrument_function)) void Model1_percpu_counter_dec(struct Model1_percpu_counter *Model1_fbc)
{
 Model1_percpu_counter_add(Model1_fbc, -1);
}

static inline __attribute__((no_instrument_function)) void Model1_percpu_counter_sub(struct Model1_percpu_counter *Model1_fbc, Model1_s64 Model1_amount)
{
 Model1_percpu_counter_add(Model1_fbc, -Model1_amount);
}


struct Model1_dst_entry;
struct Model1_kmem_cachep;
struct Model1_net_device;
struct Model1_sk_buff;
struct Model1_sock;
struct Model1_net;

struct Model1_dst_ops {
 unsigned short Model1_family;
 unsigned int Model1_gc_thresh;

 int (*Model1_gc)(struct Model1_dst_ops *Model1_ops);
 struct Model1_dst_entry * (*Model1_check)(struct Model1_dst_entry *, __u32 Model1_cookie);
 unsigned int (*Model1_default_advmss)(const struct Model1_dst_entry *);
 unsigned int (*Model1_mtu)(const struct Model1_dst_entry *);
 Model1_u32 * (*Model1_cow_metrics)(struct Model1_dst_entry *, unsigned long);
 void (*Model1_destroy)(struct Model1_dst_entry *);
 void (*Model1_ifdown)(struct Model1_dst_entry *,
       struct Model1_net_device *Model1_dev, int Model1_how);
 struct Model1_dst_entry * (*Model1_negative_advice)(struct Model1_dst_entry *);
 void (*Model1_link_failure)(struct Model1_sk_buff *);
 void (*Model1_update_pmtu)(struct Model1_dst_entry *Model1_dst, struct Model1_sock *Model1_sk,
            struct Model1_sk_buff *Model1_skb, Model1_u32 Model1_mtu);
 void (*Model1_redirect)(struct Model1_dst_entry *Model1_dst, struct Model1_sock *Model1_sk,
         struct Model1_sk_buff *Model1_skb);
 int (*Model1_local_out)(struct Model1_net *Model1_net, struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);
 struct Model1_neighbour * (*Model1_neigh_lookup)(const struct Model1_dst_entry *Model1_dst,
      struct Model1_sk_buff *Model1_skb,
      const void *Model1_daddr);

 struct Model1_kmem_cache *Model1_kmem_cachep;

 struct Model1_percpu_counter Model1_pcpuc_entries __attribute__((__aligned__((1 << (6)))));
};

static inline __attribute__((no_instrument_function)) int Model1_dst_entries_get_fast(struct Model1_dst_ops *Model1_dst)
{
 return Model1_percpu_counter_read_positive(&Model1_dst->Model1_pcpuc_entries);
}

static inline __attribute__((no_instrument_function)) int Model1_dst_entries_get_slow(struct Model1_dst_ops *Model1_dst)
{
 int Model1_res;

 Model1_local_bh_disable();
 Model1_res = Model1_percpu_counter_sum_positive(&Model1_dst->Model1_pcpuc_entries);
 Model1_local_bh_enable();
 return Model1_res;
}

static inline __attribute__((no_instrument_function)) void Model1_dst_entries_add(struct Model1_dst_ops *Model1_dst, int Model1_val)
{
 Model1_local_bh_disable();
 Model1_percpu_counter_add(&Model1_dst->Model1_pcpuc_entries, Model1_val);
 Model1_local_bh_enable();
}

static inline __attribute__((no_instrument_function)) int Model1_dst_entries_init(struct Model1_dst_ops *Model1_dst)
{
 return ({ static struct Model1_lock_class_key Model1___key; Model1___percpu_counter_init(&Model1_dst->Model1_pcpuc_entries, 0, ((( Model1_gfp_t)(0x400000u|0x2000000u)) | (( Model1_gfp_t)0x40u) | (( Model1_gfp_t)0x80u)), &Model1___key); });
}

static inline __attribute__((no_instrument_function)) void Model1_dst_entries_destroy(struct Model1_dst_ops *Model1_dst)
{
 Model1_percpu_counter_destroy(&Model1_dst->Model1_pcpuc_entries);
}
/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for the Interfaces handler.
 *
 * Version:	@(#)dev.h	1.0.10	08/12/93
 *
 * Authors:	Ross Biro
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *		Corey Minyard <wf-rch!minyard@relay.EU.net>
 *		Donald J. Becker, <becker@cesdis.gsfc.nasa.gov>
 *		Alan Cox, <alan@lxorguk.ukuu.org.uk>
 *		Bjorn Ekwall. <bj0rn@blox.se>
 *              Pekka Riikonen <priikone@poseidon.pspt.fi>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 *
 *		Moved to /usr/include/linux for NET3
 */









/*
 * Copyright (C) 1993 Linus Torvalds
 *
 * Delay routines, using a pre-computed "loops_per_jiffy" value.
 */



extern unsigned long Model1_loops_per_jiffy;








/* Undefined functions to get compile-time errors */
extern void Model1___bad_udelay(void);
extern void Model1___bad_ndelay(void);

extern void Model1___udelay(unsigned long Model1_usecs);
extern void Model1___ndelay(unsigned long Model1_nsecs);
extern void Model1___const_udelay(unsigned long Model1_xloops);
extern void Model1___delay(unsigned long Model1_loops);

/*
 * The weird n/20000 thing suppresses a "comparison is always false due to
 * limited range of data type" warning with non-const 8-bit arguments.
 */

/* 0x10c7 is 2**32 / 1000000 (rounded up) */
/* 0x5 is 2**32 / 1000000000 (rounded up) */

void Model1_use_tsc_delay(void);
void Model1_use_mwaitx_delay(void);

/*
 * Using udelay() for intervals greater than a few milliseconds can
 * risk overflow for high loops_per_jiffy (high bogomips) machines. The
 * mdelay() provides a wrapper to prevent this.  For delays greater
 * than MAX_UDELAY_MS milliseconds, the wrapper is used.  Architecture
 * specific values can be defined in asm-???/delay.h as an override.
 * The 2nd mdelay() definition ensures GCC will optimize away the 
 * while loop for the common cases where n <= MAX_UDELAY_MS  --  Paul G.
 */
extern unsigned long Model1_lpj_fine;
void Model1_calibrate_delay(void);
void Model1_msleep(unsigned int Model1_msecs);
unsigned long Model1_msleep_interruptible(unsigned int Model1_msecs);
void Model1_usleep_range(unsigned long Model1_min, unsigned long Model1_max);

static inline __attribute__((no_instrument_function)) void Model1_ssleep(unsigned int Model1_seconds)
{
 Model1_msleep(Model1_seconds * 1000);
}







/*
 * Copyright(c) 2004 - 2006 Intel Corporation. All rights reserved.
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the Free
 * Software Foundation; either version 2 of the License, or (at your option)
 * any later version.
 *
 * This program is distributed in the hope that it will be useful, but WITHOUT
 * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
 * more details.
 *
 * The full GNU General Public License is included in this distribution in the
 * file called COPYING.
 */




/*
 * device.h - generic, centralized driver model
 *
 * Copyright (c) 2001-2003 Patrick Mochel <mochel@osdl.org>
 * Copyright (c) 2004-2009 Greg Kroah-Hartman <gregkh@suse.de>
 * Copyright (c) 2008-2009 Novell Inc.
 *
 * This file is released under the GPLv2
 *
 * See Documentation/driver-model/ for more information.
 */







/*
 *	klist.h - Some generic list helpers, extending struct list_head a bit.
 *
 *	Implementations are found in lib/klist.c
 *
 *
 *	Copyright (C) 2005 Patrick Mochel
 *
 *	This file is rleased under the GPL v2.
 */
struct Model1_klist_node;
struct Model1_klist {
 Model1_spinlock_t Model1_k_lock;
 struct Model1_list_head Model1_k_list;
 void (*Model1_get)(struct Model1_klist_node *);
 void (*Model1_put)(struct Model1_klist_node *);
} __attribute__ ((aligned (sizeof(void *))));
extern void Model1_klist_init(struct Model1_klist *Model1_k, void (*Model1_get)(struct Model1_klist_node *),
         void (*Model1_put)(struct Model1_klist_node *));

struct Model1_klist_node {
 void *Model1_n_klist; /* never access directly */
 struct Model1_list_head Model1_n_node;
 struct Model1_kref Model1_n_ref;
};

extern void Model1_klist_add_tail(struct Model1_klist_node *Model1_n, struct Model1_klist *Model1_k);
extern void Model1_klist_add_head(struct Model1_klist_node *Model1_n, struct Model1_klist *Model1_k);
extern void Model1_klist_add_behind(struct Model1_klist_node *Model1_n, struct Model1_klist_node *Model1_pos);
extern void Model1_klist_add_before(struct Model1_klist_node *Model1_n, struct Model1_klist_node *Model1_pos);

extern void Model1_klist_del(struct Model1_klist_node *Model1_n);
extern void Model1_klist_remove(struct Model1_klist_node *Model1_n);

extern int Model1_klist_node_attached(struct Model1_klist_node *Model1_n);


struct Model1_klist_iter {
 struct Model1_klist *Model1_i_klist;
 struct Model1_klist_node *Model1_i_cur;
};


extern void Model1_klist_iter_init(struct Model1_klist *Model1_k, struct Model1_klist_iter *Model1_i);
extern void Model1_klist_iter_init_node(struct Model1_klist *Model1_k, struct Model1_klist_iter *Model1_i,
     struct Model1_klist_node *Model1_n);
extern void Model1_klist_iter_exit(struct Model1_klist_iter *Model1_i);
extern struct Model1_klist_node *Model1_klist_prev(struct Model1_klist_iter *Model1_i);
extern struct Model1_klist_node *Model1_klist_next(struct Model1_klist_iter *Model1_i);





/*
 * Per-device information from the pin control system.
 * This is the stuff that get included into the device
 * core.
 *
 * Copyright (C) 2012 ST-Ericsson SA
 * Written on behalf of Linaro for ST-Ericsson
 * This interface is used in the core to keep track of pins.
 *
 * Author: Linus Walleij <linus.walleij@linaro.org>
 *
 * License terms: GNU General Public License (GPL) version 2
 */
/* Stubs if we're not using pinctrl */

static inline __attribute__((no_instrument_function)) int Model1_pinctrl_bind_pins(struct Model1_device *Model1_dev)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_pinctrl_init_done(struct Model1_device *Model1_dev)
{
 return 0;
}


/* issue num suppressed message on exit */


struct Model1_ratelimit_state {
 Model1_raw_spinlock_t Model1_lock; /* protect the state */

 int Model1_interval;
 int Model1_burst;
 int Model1_printed;
 int Model1_missed;
 unsigned long Model1_begin;
 unsigned long Model1_flags;
};
static inline __attribute__((no_instrument_function)) void Model1_ratelimit_state_init(struct Model1_ratelimit_state *Model1_rs,
     int Model1_interval, int Model1_burst)
{
 memset(Model1_rs, 0, sizeof(*Model1_rs));

 do { *(&Model1_rs->Model1_lock) = (Model1_raw_spinlock_t) { .Model1_raw_lock = { { (0) } }, }; } while (0);
 Model1_rs->Model1_interval = Model1_interval;
 Model1_rs->Model1_burst = Model1_burst;
}

static inline __attribute__((no_instrument_function)) void Model1_ratelimit_default_init(struct Model1_ratelimit_state *Model1_rs)
{
 return Model1_ratelimit_state_init(Model1_rs, (5 * 1000),
     10);
}

static inline __attribute__((no_instrument_function)) void Model1_ratelimit_state_exit(struct Model1_ratelimit_state *Model1_rs)
{
 if (!(Model1_rs->Model1_flags & (1UL << (0))))
  return;

 if (Model1_rs->Model1_missed) {
  Model1_printk("\001" "4" "TCP: " "%s: %d output lines suppressed due to ratelimiting\n", Model1_get_current()->Model1_comm, Model1_rs->Model1_missed);

  Model1_rs->Model1_missed = 0;
 }
}

static inline __attribute__((no_instrument_function)) void
Model1_ratelimit_set_flags(struct Model1_ratelimit_state *Model1_rs, unsigned long Model1_flags)
{
 Model1_rs->Model1_flags = Model1_flags;
}

extern struct Model1_ratelimit_state Model1_printk_ratelimit_state;

extern int Model1____ratelimit(struct Model1_ratelimit_state *Model1_rs, const char *func);





struct Model1_dev_archdata {

 struct Model1_dma_map_ops *Model1_dma_ops;


 void *Model1_iommu; /* hook for IOMMU specific extension */

};


struct Model1_dma_domain {
 struct Model1_list_head Model1_node;
 struct Model1_dma_map_ops *Model1_dma_ops;
 int Model1_domain_nr;
};
void Model1_add_dma_domain(struct Model1_dma_domain *Model1_domain);
void Model1_del_dma_domain(struct Model1_dma_domain *Model1_domain);


struct Model1_pdev_archdata {
};

struct Model1_device;
struct Model1_device_private;
struct Model1_device_driver;
struct Model1_driver_private;
struct Model1_module;
struct Model1_class;
struct Model1_subsys_private;
struct Model1_bus_type;
struct Model1_device_node;
struct Model1_fwnode_handle;
struct Model1_iommu_ops;
struct Model1_iommu_group;

struct Model1_bus_attribute {
 struct Model1_attribute Model1_attr;
 Model1_ssize_t (*Model1_show)(struct Model1_bus_type *Model1_bus, char *Model1_buf);
 Model1_ssize_t (*Model1_store)(struct Model1_bus_type *Model1_bus, const char *Model1_buf, Model1_size_t Model1_count);
};
extern int __attribute__((warn_unused_result)) Model1_bus_create_file(struct Model1_bus_type *,
     struct Model1_bus_attribute *);
extern void Model1_bus_remove_file(struct Model1_bus_type *, struct Model1_bus_attribute *);

/**
 * struct bus_type - The bus type of the device
 *
 * @name:	The name of the bus.
 * @dev_name:	Used for subsystems to enumerate devices like ("foo%u", dev->id).
 * @dev_root:	Default device to use as the parent.
 * @dev_attrs:	Default attributes of the devices on the bus.
 * @bus_groups:	Default attributes of the bus.
 * @dev_groups:	Default attributes of the devices on the bus.
 * @drv_groups: Default attributes of the device drivers on the bus.
 * @match:	Called, perhaps multiple times, whenever a new device or driver
 *		is added for this bus. It should return a positive value if the
 *		given device can be handled by the given driver and zero
 *		otherwise. It may also return error code if determining that
 *		the driver supports the device is not possible. In case of
 *		-EPROBE_DEFER it will queue the device for deferred probing.
 * @uevent:	Called when a device is added, removed, or a few other things
 *		that generate uevents to add the environment variables.
 * @probe:	Called when a new device or driver add to this bus, and callback
 *		the specific driver's probe to initial the matched device.
 * @remove:	Called when a device removed from this bus.
 * @shutdown:	Called at shut-down time to quiesce the device.
 *
 * @online:	Called to put the device back online (after offlining it).
 * @offline:	Called to put the device offline for hot-removal. May fail.
 *
 * @suspend:	Called when a device on this bus wants to go to sleep mode.
 * @resume:	Called to bring a device on this bus out of sleep mode.
 * @pm:		Power management operations of this bus, callback the specific
 *		device driver's pm-ops.
 * @iommu_ops:  IOMMU specific operations for this bus, used to attach IOMMU
 *              driver implementations to a bus and allow the driver to do
 *              bus-specific setup
 * @p:		The private data of the driver core, only the driver core can
 *		touch this.
 * @lock_key:	Lock class key for use by the lock validator
 *
 * A bus is a channel between the processor and one or more devices. For the
 * purposes of the device model, all devices are connected via a bus, even if
 * it is an internal, virtual, "platform" bus. Buses can plug into each other.
 * A USB controller is usually a PCI device, for example. The device model
 * represents the actual connections between buses and the devices they control.
 * A bus is represented by the bus_type structure. It contains the name, the
 * default attributes, the bus' methods, PM operations, and the driver core's
 * private data.
 */
struct Model1_bus_type {
 const char *Model1_name;
 const char *Model1_dev_name;
 struct Model1_device *Model1_dev_root;
 struct Model1_device_attribute *Model1_dev_attrs; /* use dev_groups instead */
 const struct Model1_attribute_group **Model1_bus_groups;
 const struct Model1_attribute_group **Model1_dev_groups;
 const struct Model1_attribute_group **Model1_drv_groups;

 int (*Model1_match)(struct Model1_device *Model1_dev, struct Model1_device_driver *Model1_drv);
 int (*Model1_uevent)(struct Model1_device *Model1_dev, struct Model1_kobj_uevent_env *Model1_env);
 int (*Model1_probe)(struct Model1_device *Model1_dev);
 int (*Model1_remove)(struct Model1_device *Model1_dev);
 void (*Model1_shutdown)(struct Model1_device *Model1_dev);

 int (*Model1_online)(struct Model1_device *Model1_dev);
 int (*Model1_offline)(struct Model1_device *Model1_dev);

 int (*Model1_suspend)(struct Model1_device *Model1_dev, Model1_pm_message_t Model1_state);
 int (*Model1_resume)(struct Model1_device *Model1_dev);

 const struct Model1_dev_pm_ops *Model1_pm;

 const struct Model1_iommu_ops *Model1_iommu_ops;

 struct Model1_subsys_private *Model1_p;
 struct Model1_lock_class_key Model1_lock_key;
};

extern int __attribute__((warn_unused_result)) Model1_bus_register(struct Model1_bus_type *Model1_bus);

extern void Model1_bus_unregister(struct Model1_bus_type *Model1_bus);

extern int __attribute__((warn_unused_result)) Model1_bus_rescan_devices(struct Model1_bus_type *Model1_bus);

/* iterator helpers for buses */
struct Model1_subsys_dev_iter {
 struct Model1_klist_iter Model1_ki;
 const struct Model1_device_type *Model1_type;
};
void Model1_subsys_dev_iter_init(struct Model1_subsys_dev_iter *Model1_iter,
    struct Model1_bus_type *Model1_subsys,
    struct Model1_device *Model1_start,
    const struct Model1_device_type *Model1_type);
struct Model1_device *Model1_subsys_dev_iter_next(struct Model1_subsys_dev_iter *Model1_iter);
void Model1_subsys_dev_iter_exit(struct Model1_subsys_dev_iter *Model1_iter);

int Model1_bus_for_each_dev(struct Model1_bus_type *Model1_bus, struct Model1_device *Model1_start, void *Model1_data,
       int (*Model1_fn)(struct Model1_device *Model1_dev, void *Model1_data));
struct Model1_device *Model1_bus_find_device(struct Model1_bus_type *Model1_bus, struct Model1_device *Model1_start,
          void *Model1_data,
          int (*Model1_match)(struct Model1_device *Model1_dev, void *Model1_data));
struct Model1_device *Model1_bus_find_device_by_name(struct Model1_bus_type *Model1_bus,
           struct Model1_device *Model1_start,
           const char *Model1_name);
struct Model1_device *Model1_subsys_find_device_by_id(struct Model1_bus_type *Model1_bus, unsigned int Model1_id,
     struct Model1_device *Model1_hint);
int Model1_bus_for_each_drv(struct Model1_bus_type *Model1_bus, struct Model1_device_driver *Model1_start,
       void *Model1_data, int (*Model1_fn)(struct Model1_device_driver *, void *));
void Model1_bus_sort_breadthfirst(struct Model1_bus_type *Model1_bus,
      int (*Model1_compare)(const struct Model1_device *Model1_a,
       const struct Model1_device *Model1_b));
/*
 * Bus notifiers: Get notified of addition/removal of devices
 * and binding/unbinding of drivers to devices.
 * In the long run, it should be a replacement for the platform
 * notify hooks.
 */
struct Model1_notifier_block;

extern int Model1_bus_register_notifier(struct Model1_bus_type *Model1_bus,
     struct Model1_notifier_block *Model1_nb);
extern int Model1_bus_unregister_notifier(struct Model1_bus_type *Model1_bus,
       struct Model1_notifier_block *Model1_nb);

/* All 4 notifers below get called with the target struct device *
 * as an argument. Note that those functions are likely to be called
 * with the device lock held in the core, so be careful.
 */
extern struct Model1_kset *Model1_bus_get_kset(struct Model1_bus_type *Model1_bus);
extern struct Model1_klist *Model1_bus_get_device_klist(struct Model1_bus_type *Model1_bus);

/**
 * enum probe_type - device driver probe type to try
 *	Device drivers may opt in for special handling of their
 *	respective probe routines. This tells the core what to
 *	expect and prefer.
 *
 * @PROBE_DEFAULT_STRATEGY: Used by drivers that work equally well
 *	whether probed synchronously or asynchronously.
 * @PROBE_PREFER_ASYNCHRONOUS: Drivers for "slow" devices which
 *	probing order is not essential for booting the system may
 *	opt into executing their probes asynchronously.
 * @PROBE_FORCE_SYNCHRONOUS: Use this to annotate drivers that need
 *	their probe routines to run synchronously with driver and
 *	device registration (with the exception of -EPROBE_DEFER
 *	handling - re-probing always ends up being done asynchronously).
 *
 * Note that the end goal is to switch the kernel to use asynchronous
 * probing by default, so annotating drivers with
 * %PROBE_PREFER_ASYNCHRONOUS is a temporary measure that allows us
 * to speed up boot process while we are validating the rest of the
 * drivers.
 */
enum Model1_probe_type {
 Model1_PROBE_DEFAULT_STRATEGY,
 Model1_PROBE_PREFER_ASYNCHRONOUS,
 Model1_PROBE_FORCE_SYNCHRONOUS,
};

/**
 * struct device_driver - The basic device driver structure
 * @name:	Name of the device driver.
 * @bus:	The bus which the device of this driver belongs to.
 * @owner:	The module owner.
 * @mod_name:	Used for built-in modules.
 * @suppress_bind_attrs: Disables bind/unbind via sysfs.
 * @probe_type:	Type of the probe (synchronous or asynchronous) to use.
 * @of_match_table: The open firmware table.
 * @acpi_match_table: The ACPI match table.
 * @probe:	Called to query the existence of a specific device,
 *		whether this driver can work with it, and bind the driver
 *		to a specific device.
 * @remove:	Called when the device is removed from the system to
 *		unbind a device from this driver.
 * @shutdown:	Called at shut-down time to quiesce the device.
 * @suspend:	Called to put the device to sleep mode. Usually to a
 *		low power state.
 * @resume:	Called to bring a device from sleep mode.
 * @groups:	Default attributes that get created by the driver core
 *		automatically.
 * @pm:		Power management operations of the device which matched
 *		this driver.
 * @p:		Driver core's private data, no one other than the driver
 *		core can touch this.
 *
 * The device driver-model tracks all of the drivers known to the system.
 * The main reason for this tracking is to enable the driver core to match
 * up drivers with new devices. Once drivers are known objects within the
 * system, however, a number of other things become possible. Device drivers
 * can export information and configuration variables that are independent
 * of any specific device.
 */
struct Model1_device_driver {
 const char *Model1_name;
 struct Model1_bus_type *Model1_bus;

 struct Model1_module *Model1_owner;
 const char *Model1_mod_name; /* used for built-in modules */

 bool Model1_suppress_bind_attrs; /* disables bind/unbind via sysfs */
 enum Model1_probe_type Model1_probe_type;

 const struct Model1_of_device_id *Model1_of_match_table;
 const struct Model1_acpi_device_id *Model1_acpi_match_table;

 int (*Model1_probe) (struct Model1_device *Model1_dev);
 int (*Model1_remove) (struct Model1_device *Model1_dev);
 void (*Model1_shutdown) (struct Model1_device *Model1_dev);
 int (*Model1_suspend) (struct Model1_device *Model1_dev, Model1_pm_message_t Model1_state);
 int (*Model1_resume) (struct Model1_device *Model1_dev);
 const struct Model1_attribute_group **Model1_groups;

 const struct Model1_dev_pm_ops *Model1_pm;

 struct Model1_driver_private *Model1_p;
};


extern int __attribute__((warn_unused_result)) Model1_driver_register(struct Model1_device_driver *Model1_drv);
extern void Model1_driver_unregister(struct Model1_device_driver *Model1_drv);

extern struct Model1_device_driver *Model1_driver_find(const char *Model1_name,
      struct Model1_bus_type *Model1_bus);
extern int Model1_driver_probe_done(void);
extern void Model1_wait_for_device_probe(void);


/* sysfs interface for exporting driver attributes */

struct Model1_driver_attribute {
 struct Model1_attribute Model1_attr;
 Model1_ssize_t (*Model1_show)(struct Model1_device_driver *Model1_driver, char *Model1_buf);
 Model1_ssize_t (*Model1_store)(struct Model1_device_driver *Model1_driver, const char *Model1_buf,
    Model1_size_t Model1_count);
};
extern int __attribute__((warn_unused_result)) Model1_driver_create_file(struct Model1_device_driver *Model1_driver,
     const struct Model1_driver_attribute *Model1_attr);
extern void Model1_driver_remove_file(struct Model1_device_driver *Model1_driver,
          const struct Model1_driver_attribute *Model1_attr);

extern int __attribute__((warn_unused_result)) Model1_driver_for_each_device(struct Model1_device_driver *Model1_drv,
            struct Model1_device *Model1_start,
            void *Model1_data,
            int (*Model1_fn)(struct Model1_device *Model1_dev,
        void *));
struct Model1_device *Model1_driver_find_device(struct Model1_device_driver *Model1_drv,
      struct Model1_device *Model1_start, void *Model1_data,
      int (*Model1_match)(struct Model1_device *Model1_dev, void *Model1_data));

/**
 * struct subsys_interface - interfaces to device functions
 * @name:       name of the device function
 * @subsys:     subsytem of the devices to attach to
 * @node:       the list of functions registered at the subsystem
 * @add_dev:    device hookup to device function handler
 * @remove_dev: device hookup to device function handler
 *
 * Simple interfaces attached to a subsystem. Multiple interfaces can
 * attach to a subsystem and its devices. Unlike drivers, they do not
 * exclusively claim or control devices. Interfaces usually represent
 * a specific functionality of a subsystem/class of devices.
 */
struct Model1_subsys_interface {
 const char *Model1_name;
 struct Model1_bus_type *Model1_subsys;
 struct Model1_list_head Model1_node;
 int (*Model1_add_dev)(struct Model1_device *Model1_dev, struct Model1_subsys_interface *Model1_sif);
 void (*Model1_remove_dev)(struct Model1_device *Model1_dev, struct Model1_subsys_interface *Model1_sif);
};

int Model1_subsys_interface_register(struct Model1_subsys_interface *Model1_sif);
void Model1_subsys_interface_unregister(struct Model1_subsys_interface *Model1_sif);

int Model1_subsys_system_register(struct Model1_bus_type *Model1_subsys,
      const struct Model1_attribute_group **Model1_groups);
int Model1_subsys_virtual_register(struct Model1_bus_type *Model1_subsys,
       const struct Model1_attribute_group **Model1_groups);

/**
 * struct class - device classes
 * @name:	Name of the class.
 * @owner:	The module owner.
 * @class_attrs: Default attributes of this class.
 * @dev_groups:	Default attributes of the devices that belong to the class.
 * @dev_kobj:	The kobject that represents this class and links it into the hierarchy.
 * @dev_uevent:	Called when a device is added, removed from this class, or a
 *		few other things that generate uevents to add the environment
 *		variables.
 * @devnode:	Callback to provide the devtmpfs.
 * @class_release: Called to release this class.
 * @dev_release: Called to release the device.
 * @suspend:	Used to put the device to sleep mode, usually to a low power
 *		state.
 * @resume:	Used to bring the device from the sleep mode.
 * @ns_type:	Callbacks so sysfs can detemine namespaces.
 * @namespace:	Namespace of the device belongs to this class.
 * @pm:		The default device power management operations of this class.
 * @p:		The private data of the driver core, no one other than the
 *		driver core can touch this.
 *
 * A class is a higher-level view of a device that abstracts out low-level
 * implementation details. Drivers may see a SCSI disk or an ATA disk, but,
 * at the class level, they are all simply disks. Classes allow user space
 * to work with devices based on what they do, rather than how they are
 * connected or how they work.
 */
struct Model1_class {
 const char *Model1_name;
 struct Model1_module *Model1_owner;

 struct Model1_class_attribute *Model1_class_attrs;
 const struct Model1_attribute_group **Model1_dev_groups;
 struct Model1_kobject *Model1_dev_kobj;

 int (*Model1_dev_uevent)(struct Model1_device *Model1_dev, struct Model1_kobj_uevent_env *Model1_env);
 char *(*Model1_devnode)(struct Model1_device *Model1_dev, Model1_umode_t *Model1_mode);

 void (*Model1_class_release)(struct Model1_class *Model1_class);
 void (*Model1_dev_release)(struct Model1_device *Model1_dev);

 int (*Model1_suspend)(struct Model1_device *Model1_dev, Model1_pm_message_t Model1_state);
 int (*Model1_resume)(struct Model1_device *Model1_dev);

 const struct Model1_kobj_ns_type_operations *Model1_ns_type;
 const void *(*Model1_namespace)(struct Model1_device *Model1_dev);

 const struct Model1_dev_pm_ops *Model1_pm;

 struct Model1_subsys_private *Model1_p;
};

struct Model1_class_dev_iter {
 struct Model1_klist_iter Model1_ki;
 const struct Model1_device_type *Model1_type;
};

extern struct Model1_kobject *Model1_sysfs_dev_block_kobj;
extern struct Model1_kobject *Model1_sysfs_dev_char_kobj;
extern int __attribute__((warn_unused_result)) Model1___class_register(struct Model1_class *Model1_class,
      struct Model1_lock_class_key *Model1_key);
extern void Model1_class_unregister(struct Model1_class *Model1_class);

/* This is a #define to keep the compiler from merging different
 * instances of the __key variable */






struct Model1_class_compat;
struct Model1_class_compat *Model1_class_compat_register(const char *Model1_name);
void Model1_class_compat_unregister(struct Model1_class_compat *Model1_cls);
int Model1_class_compat_create_link(struct Model1_class_compat *Model1_cls, struct Model1_device *Model1_dev,
        struct Model1_device *Model1_device_link);
void Model1_class_compat_remove_link(struct Model1_class_compat *Model1_cls, struct Model1_device *Model1_dev,
         struct Model1_device *Model1_device_link);

extern void Model1_class_dev_iter_init(struct Model1_class_dev_iter *Model1_iter,
    struct Model1_class *Model1_class,
    struct Model1_device *Model1_start,
    const struct Model1_device_type *Model1_type);
extern struct Model1_device *Model1_class_dev_iter_next(struct Model1_class_dev_iter *Model1_iter);
extern void Model1_class_dev_iter_exit(struct Model1_class_dev_iter *Model1_iter);

extern int Model1_class_for_each_device(struct Model1_class *Model1_class, struct Model1_device *Model1_start,
     void *Model1_data,
     int (*Model1_fn)(struct Model1_device *Model1_dev, void *Model1_data));
extern struct Model1_device *Model1_class_find_device(struct Model1_class *Model1_class,
     struct Model1_device *Model1_start, const void *Model1_data,
     int (*Model1_match)(struct Model1_device *, const void *));

struct Model1_class_attribute {
 struct Model1_attribute Model1_attr;
 Model1_ssize_t (*Model1_show)(struct Model1_class *Model1_class, struct Model1_class_attribute *Model1_attr,
   char *Model1_buf);
 Model1_ssize_t (*Model1_store)(struct Model1_class *Model1_class, struct Model1_class_attribute *Model1_attr,
   const char *Model1_buf, Model1_size_t Model1_count);
};
extern int __attribute__((warn_unused_result)) Model1_class_create_file_ns(struct Model1_class *Model1_class,
          const struct Model1_class_attribute *Model1_attr,
          const void *Model1_ns);
extern void Model1_class_remove_file_ns(struct Model1_class *Model1_class,
     const struct Model1_class_attribute *Model1_attr,
     const void *Model1_ns);

static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result)) Model1_class_create_file(struct Model1_class *Model1_class,
     const struct Model1_class_attribute *Model1_attr)
{
 return Model1_class_create_file_ns(Model1_class, Model1_attr, ((void *)0));
}

static inline __attribute__((no_instrument_function)) void Model1_class_remove_file(struct Model1_class *Model1_class,
         const struct Model1_class_attribute *Model1_attr)
{
 return Model1_class_remove_file_ns(Model1_class, Model1_attr, ((void *)0));
}

/* Simple class attribute that is just a static string */
struct Model1_class_attribute_string {
 struct Model1_class_attribute Model1_attr;
 char *Model1_str;
};

/* Currently read-only only */






extern Model1_ssize_t Model1_show_class_attr_string(struct Model1_class *Model1_class, struct Model1_class_attribute *Model1_attr,
                        char *Model1_buf);

struct Model1_class_interface {
 struct Model1_list_head Model1_node;
 struct Model1_class *Model1_class;

 int (*Model1_add_dev) (struct Model1_device *, struct Model1_class_interface *);
 void (*Model1_remove_dev) (struct Model1_device *, struct Model1_class_interface *);
};

extern int __attribute__((warn_unused_result)) Model1_class_interface_register(struct Model1_class_interface *);
extern void Model1_class_interface_unregister(struct Model1_class_interface *);

extern struct Model1_class * __attribute__((warn_unused_result)) Model1___class_create(struct Model1_module *Model1_owner,
        const char *Model1_name,
        struct Model1_lock_class_key *Model1_key);
extern void Model1_class_destroy(struct Model1_class *Model1_cls);

/* This is a #define to keep the compiler from merging different
 * instances of the __key variable */






/*
 * The type of device, "struct device" is embedded in. A class
 * or bus can contain devices of different types
 * like "partitions" and "disks", "mouse" and "event".
 * This identifies the device type and carries type-specific
 * information, equivalent to the kobj_type of a kobject.
 * If "name" is specified, the uevent will contain it in
 * the DEVTYPE variable.
 */
struct Model1_device_type {
 const char *Model1_name;
 const struct Model1_attribute_group **Model1_groups;
 int (*Model1_uevent)(struct Model1_device *Model1_dev, struct Model1_kobj_uevent_env *Model1_env);
 char *(*Model1_devnode)(struct Model1_device *Model1_dev, Model1_umode_t *Model1_mode,
    Model1_kuid_t *Model1_uid, Model1_kgid_t *Model1_gid);
 void (*Model1_release)(struct Model1_device *Model1_dev);

 const struct Model1_dev_pm_ops *Model1_pm;
};

/* interface for exporting device attributes */
struct Model1_device_attribute {
 struct Model1_attribute Model1_attr;
 Model1_ssize_t (*Model1_show)(struct Model1_device *Model1_dev, struct Model1_device_attribute *Model1_attr,
   char *Model1_buf);
 Model1_ssize_t (*Model1_store)(struct Model1_device *Model1_dev, struct Model1_device_attribute *Model1_attr,
    const char *Model1_buf, Model1_size_t Model1_count);
};

struct Model1_dev_ext_attribute {
 struct Model1_device_attribute Model1_attr;
 void *Model1_var;
};

Model1_ssize_t Model1_device_show_ulong(struct Model1_device *Model1_dev, struct Model1_device_attribute *Model1_attr,
     char *Model1_buf);
Model1_ssize_t Model1_device_store_ulong(struct Model1_device *Model1_dev, struct Model1_device_attribute *Model1_attr,
      const char *Model1_buf, Model1_size_t Model1_count);
Model1_ssize_t Model1_device_show_int(struct Model1_device *Model1_dev, struct Model1_device_attribute *Model1_attr,
   char *Model1_buf);
Model1_ssize_t Model1_device_store_int(struct Model1_device *Model1_dev, struct Model1_device_attribute *Model1_attr,
    const char *Model1_buf, Model1_size_t Model1_count);
Model1_ssize_t Model1_device_show_bool(struct Model1_device *Model1_dev, struct Model1_device_attribute *Model1_attr,
   char *Model1_buf);
Model1_ssize_t Model1_device_store_bool(struct Model1_device *Model1_dev, struct Model1_device_attribute *Model1_attr,
    const char *Model1_buf, Model1_size_t Model1_count);
extern int Model1_device_create_file(struct Model1_device *Model1_device,
         const struct Model1_device_attribute *Model1_entry);
extern void Model1_device_remove_file(struct Model1_device *Model1_dev,
          const struct Model1_device_attribute *Model1_attr);
extern bool Model1_device_remove_file_self(struct Model1_device *Model1_dev,
        const struct Model1_device_attribute *Model1_attr);
extern int __attribute__((warn_unused_result)) Model1_device_create_bin_file(struct Model1_device *Model1_dev,
     const struct Model1_bin_attribute *Model1_attr);
extern void Model1_device_remove_bin_file(struct Model1_device *Model1_dev,
       const struct Model1_bin_attribute *Model1_attr);

/* device resource management */
typedef void (*Model1_dr_release_t)(struct Model1_device *Model1_dev, void *Model1_res);
typedef int (*Model1_dr_match_t)(struct Model1_device *Model1_dev, void *Model1_res, void *Model1_match_data);


extern void *Model1___devres_alloc_node(Model1_dr_release_t Model1_release, Model1_size_t Model1_size, Model1_gfp_t Model1_gfp,
     int Model1_nid, const char *Model1_name) __attribute__((__malloc__));
extern void Model1_devres_for_each_res(struct Model1_device *Model1_dev, Model1_dr_release_t Model1_release,
    Model1_dr_match_t Model1_match, void *Model1_match_data,
    void (*Model1_fn)(struct Model1_device *, void *, void *),
    void *Model1_data);
extern void Model1_devres_free(void *Model1_res);
extern void Model1_devres_add(struct Model1_device *Model1_dev, void *Model1_res);
extern void *Model1_devres_find(struct Model1_device *Model1_dev, Model1_dr_release_t Model1_release,
    Model1_dr_match_t Model1_match, void *Model1_match_data);
extern void *Model1_devres_get(struct Model1_device *Model1_dev, void *Model1_new_res,
   Model1_dr_match_t Model1_match, void *Model1_match_data);
extern void *Model1_devres_remove(struct Model1_device *Model1_dev, Model1_dr_release_t Model1_release,
      Model1_dr_match_t Model1_match, void *Model1_match_data);
extern int Model1_devres_destroy(struct Model1_device *Model1_dev, Model1_dr_release_t Model1_release,
     Model1_dr_match_t Model1_match, void *Model1_match_data);
extern int Model1_devres_release(struct Model1_device *Model1_dev, Model1_dr_release_t Model1_release,
     Model1_dr_match_t Model1_match, void *Model1_match_data);

/* devres group */
extern void * __attribute__((warn_unused_result)) Model1_devres_open_group(struct Model1_device *Model1_dev, void *Model1_id,
          Model1_gfp_t Model1_gfp);
extern void Model1_devres_close_group(struct Model1_device *Model1_dev, void *Model1_id);
extern void Model1_devres_remove_group(struct Model1_device *Model1_dev, void *Model1_id);
extern int Model1_devres_release_group(struct Model1_device *Model1_dev, void *Model1_id);

/* managed devm_k.alloc/kfree for device drivers */
extern void *Model1_devm_kmalloc(struct Model1_device *Model1_dev, Model1_size_t Model1_size, Model1_gfp_t Model1_gfp) __attribute__((__malloc__));
extern __attribute__((format(printf, 3, 0)))
char *Model1_devm_kvasprintf(struct Model1_device *Model1_dev, Model1_gfp_t Model1_gfp, const char *Model1_fmt,
        Model1_va_list Model1_ap) __attribute__((__malloc__));
extern __attribute__((format(printf, 3, 4)))
char *Model1_devm_kasprintf(struct Model1_device *Model1_dev, Model1_gfp_t Model1_gfp, const char *Model1_fmt, ...) __attribute__((__malloc__));
static inline __attribute__((no_instrument_function)) void *Model1_devm_kzalloc(struct Model1_device *Model1_dev, Model1_size_t Model1_size, Model1_gfp_t Model1_gfp)
{
 return Model1_devm_kmalloc(Model1_dev, Model1_size, Model1_gfp | (( Model1_gfp_t)0x8000u));
}
static inline __attribute__((no_instrument_function)) void *Model1_devm_kmalloc_array(struct Model1_device *Model1_dev,
           Model1_size_t Model1_n, Model1_size_t Model1_size, Model1_gfp_t Model1_flags)
{
 if (Model1_size != 0 && Model1_n > (~(Model1_size_t)0) / Model1_size)
  return ((void *)0);
 return Model1_devm_kmalloc(Model1_dev, Model1_n * Model1_size, Model1_flags);
}
static inline __attribute__((no_instrument_function)) void *Model1_devm_kcalloc(struct Model1_device *Model1_dev,
     Model1_size_t Model1_n, Model1_size_t Model1_size, Model1_gfp_t Model1_flags)
{
 return Model1_devm_kmalloc_array(Model1_dev, Model1_n, Model1_size, Model1_flags | (( Model1_gfp_t)0x8000u));
}
extern void Model1_devm_kfree(struct Model1_device *Model1_dev, void *Model1_p);
extern char *Model1_devm_kstrdup(struct Model1_device *Model1_dev, const char *Model1_s, Model1_gfp_t Model1_gfp) __attribute__((__malloc__));
extern void *Model1_devm_kmemdup(struct Model1_device *Model1_dev, const void *Model1_src, Model1_size_t Model1_len,
     Model1_gfp_t Model1_gfp);

extern unsigned long Model1_devm_get_free_pages(struct Model1_device *Model1_dev,
      Model1_gfp_t Model1_gfp_mask, unsigned int Model1_order);
extern void Model1_devm_free_pages(struct Model1_device *Model1_dev, unsigned long Model1_addr);

void *Model1_devm_ioremap_resource(struct Model1_device *Model1_dev, struct Model1_resource *Model1_res);

/* allows to add/remove a custom action to devres stack */
int Model1_devm_add_action(struct Model1_device *Model1_dev, void (*Model1_action)(void *), void *Model1_data);
void Model1_devm_remove_action(struct Model1_device *Model1_dev, void (*Model1_action)(void *), void *Model1_data);

static inline __attribute__((no_instrument_function)) int Model1_devm_add_action_or_reset(struct Model1_device *Model1_dev,
        void (*Model1_action)(void *), void *Model1_data)
{
 int Model1_ret;

 Model1_ret = Model1_devm_add_action(Model1_dev, Model1_action, Model1_data);
 if (Model1_ret)
  Model1_action(Model1_data);

 return Model1_ret;
}

struct Model1_device_dma_parameters {
 /*
	 * a low level driver may set these to teach IOMMU code about
	 * sg limitations.
	 */
 unsigned int Model1_max_segment_size;
 unsigned long Model1_segment_boundary_mask;
};

/**
 * struct device - The basic device structure
 * @parent:	The device's "parent" device, the device to which it is attached.
 * 		In most cases, a parent device is some sort of bus or host
 * 		controller. If parent is NULL, the device, is a top-level device,
 * 		which is not usually what you want.
 * @p:		Holds the private data of the driver core portions of the device.
 * 		See the comment of the struct device_private for detail.
 * @kobj:	A top-level, abstract class from which other classes are derived.
 * @init_name:	Initial name of the device.
 * @type:	The type of device.
 * 		This identifies the device type and carries type-specific
 * 		information.
 * @mutex:	Mutex to synchronize calls to its driver.
 * @bus:	Type of bus device is on.
 * @driver:	Which driver has allocated this
 * @platform_data: Platform data specific to the device.
 * 		Example: For devices on custom boards, as typical of embedded
 * 		and SOC based hardware, Linux often uses platform_data to point
 * 		to board-specific structures describing devices and how they
 * 		are wired.  That can include what ports are available, chip
 * 		variants, which GPIO pins act in what additional roles, and so
 * 		on.  This shrinks the "Board Support Packages" (BSPs) and
 * 		minimizes board-specific #ifdefs in drivers.
 * @driver_data: Private pointer for driver specific info.
 * @power:	For device power management.
 * 		See Documentation/power/devices.txt for details.
 * @pm_domain:	Provide callbacks that are executed during system suspend,
 * 		hibernation, system resume and during runtime PM transitions
 * 		along with subsystem-level and driver-level callbacks.
 * @pins:	For device pin management.
 *		See Documentation/pinctrl.txt for details.
 * @msi_list:	Hosts MSI descriptors
 * @msi_domain: The generic MSI domain this device is using.
 * @numa_node:	NUMA node this device is close to.
 * @dma_mask:	Dma mask (if dma'ble device).
 * @coherent_dma_mask: Like dma_mask, but for alloc_coherent mapping as not all
 * 		hardware supports 64-bit addresses for consistent allocations
 * 		such descriptors.
 * @dma_pfn_offset: offset of DMA memory range relatively of RAM
 * @dma_parms:	A low level driver may set these to teach IOMMU code about
 * 		segment limitations.
 * @dma_pools:	Dma pools (if dma'ble device).
 * @dma_mem:	Internal for coherent mem override.
 * @cma_area:	Contiguous memory area for dma allocations
 * @archdata:	For arch-specific additions.
 * @of_node:	Associated device tree node.
 * @fwnode:	Associated device node supplied by platform firmware.
 * @devt:	For creating the sysfs "dev".
 * @id:		device instance
 * @devres_lock: Spinlock to protect the resource of the device.
 * @devres_head: The resources list of the device.
 * @knode_class: The node used to add the device to the class list.
 * @class:	The class of the device.
 * @groups:	Optional attribute groups.
 * @release:	Callback to free the device after all references have
 * 		gone away. This should be set by the allocator of the
 * 		device (i.e. the bus driver that discovered the device).
 * @iommu_group: IOMMU group the device belongs to.
 *
 * @offline_disabled: If set, the device is permanently online.
 * @offline:	Set after successful invocation of bus type's .offline().
 *
 * At the lowest level, every device in a Linux system is represented by an
 * instance of struct device. The device structure contains the information
 * that the device model core needs to model the system. Most subsystems,
 * however, track additional information about the devices they host. As a
 * result, it is rare for devices to be represented by bare device structures;
 * instead, that structure, like kobject structures, is usually embedded within
 * a higher-level representation of the device.
 */
struct Model1_device {
 struct Model1_device *Model1_parent;

 struct Model1_device_private *Model1_p;

 struct Model1_kobject Model1_kobj;
 const char *Model1_init_name; /* initial name of the device */
 const struct Model1_device_type *Model1_type;

 struct Model1_mutex Model1_mutex; /* mutex to synchronize calls to
					 * its driver.
					 */

 struct Model1_bus_type *Model1_bus; /* type of bus device is on */
 struct Model1_device_driver *Model1_driver; /* which driver has allocated this
					   device */
 void *Model1_platform_data; /* Platform specific data, device
					   core doesn't touch it */
 void *Model1_driver_data; /* Driver data, set and get with
					   dev_set/get_drvdata */
 struct Model1_dev_pm_info Model1_power;
 struct Model1_dev_pm_domain *Model1_pm_domain;


 struct Model1_irq_domain *Model1_msi_domain;





 struct Model1_list_head Model1_msi_list;



 int Model1_numa_node; /* NUMA node this device is close to */

 Model1_u64 *Model1_dma_mask; /* dma mask (if dma'able device) */
 Model1_u64 Model1_coherent_dma_mask;/* Like dma_mask, but for
					     alloc_coherent mappings as
					     not all hardware supports
					     64 bit addresses for consistent
					     allocations such descriptors. */
 unsigned long Model1_dma_pfn_offset;

 struct Model1_device_dma_parameters *Model1_dma_parms;

 struct Model1_list_head Model1_dma_pools; /* dma pools (if dma'ble) */

 struct Model1_dma_coherent_mem *Model1_dma_mem; /* internal for coherent mem
					     override */




 /* arch specific additions */
 struct Model1_dev_archdata Model1_archdata;

 struct Model1_device_node *Model1_of_node; /* associated device tree node */
 struct Model1_fwnode_handle *Model1_fwnode; /* firmware device node */

 Model1_dev_t Model1_devt; /* dev_t, creates the sysfs "dev" */
 Model1_u32 Model1_id; /* device instance */

 Model1_spinlock_t Model1_devres_lock;
 struct Model1_list_head Model1_devres_head;

 struct Model1_klist_node Model1_knode_class;
 struct Model1_class *Model1_class;
 const struct Model1_attribute_group **Model1_groups; /* optional groups */

 void (*Model1_release)(struct Model1_device *Model1_dev);
 struct Model1_iommu_group *Model1_iommu_group;

 bool Model1_offline_disabled:1;
 bool Model1_offline:1;
};

static inline __attribute__((no_instrument_function)) struct Model1_device *Model1_kobj_to_dev(struct Model1_kobject *Model1_kobj)
{
 return ({ const typeof( ((struct Model1_device *)0)->Model1_kobj ) *Model1___mptr = (Model1_kobj); (struct Model1_device *)( (char *)Model1___mptr - __builtin_offsetof(struct Model1_device, Model1_kobj) );});
}

/* Get the wakeup routines, which depend on struct device */

/*
 *  pm_wakeup.h - Power management wakeup interface
 *
 *  Copyright (C) 2008 Alan Stern
 *  Copyright (C) 2010 Rafael J. Wysocki, Novell Inc.
 *
 *  This program is free software; you can redistribute it and/or modify
 *  it under the terms of the GNU General Public License as published by
 *  the Free Software Foundation; either version 2 of the License, or
 *  (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU General Public License for more details.
 *
 *  You should have received a copy of the GNU General Public License
 *  along with this program; if not, write to the Free Software
 *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
 */
struct Model1_wake_irq;

/**
 * struct wakeup_source - Representation of wakeup sources
 *
 * @name: Name of the wakeup source
 * @entry: Wakeup source list entry
 * @lock: Wakeup source lock
 * @wakeirq: Optional device specific wakeirq
 * @timer: Wakeup timer list
 * @timer_expires: Wakeup timer expiration
 * @total_time: Total time this wakeup source has been active.
 * @max_time: Maximum time this wakeup source has been continuously active.
 * @last_time: Monotonic clock when the wakeup source's was touched last time.
 * @prevent_sleep_time: Total time this source has been preventing autosleep.
 * @event_count: Number of signaled wakeup events.
 * @active_count: Number of times the wakeup source was activated.
 * @relax_count: Number of times the wakeup source was deactivated.
 * @expire_count: Number of times the wakeup source's timeout has expired.
 * @wakeup_count: Number of times the wakeup source might abort suspend.
 * @active: Status of the wakeup source.
 * @has_timeout: The wakeup source has been activated with a timeout.
 */
struct Model1_wakeup_source {
 const char *Model1_name;
 struct Model1_list_head Model1_entry;
 Model1_spinlock_t Model1_lock;
 struct Model1_wake_irq *Model1_wakeirq;
 struct Model1_timer_list Model1_timer;
 unsigned long Model1_timer_expires;
 Model1_ktime_t Model1_total_time;
 Model1_ktime_t Model1_max_time;
 Model1_ktime_t Model1_last_time;
 Model1_ktime_t Model1_start_prevent_time;
 Model1_ktime_t Model1_prevent_sleep_time;
 unsigned long Model1_event_count;
 unsigned long Model1_active_count;
 unsigned long Model1_relax_count;
 unsigned long Model1_expire_count;
 unsigned long Model1_wakeup_count;
 bool Model1_active:1;
 bool Model1_autosleep_enabled:1;
};



/*
 * Changes to device_may_wakeup take effect on the next pm state change.
 */

static inline __attribute__((no_instrument_function)) bool Model1_device_can_wakeup(struct Model1_device *Model1_dev)
{
 return Model1_dev->Model1_power.Model1_can_wakeup;
}

static inline __attribute__((no_instrument_function)) bool Model1_device_may_wakeup(struct Model1_device *Model1_dev)
{
 return Model1_dev->Model1_power.Model1_can_wakeup && !!Model1_dev->Model1_power.Model1_wakeup;
}

/* drivers/base/power/wakeup.c */
extern void Model1_wakeup_source_prepare(struct Model1_wakeup_source *Model1_ws, const char *Model1_name);
extern struct Model1_wakeup_source *Model1_wakeup_source_create(const char *Model1_name);
extern void Model1_wakeup_source_drop(struct Model1_wakeup_source *Model1_ws);
extern void Model1_wakeup_source_destroy(struct Model1_wakeup_source *Model1_ws);
extern void Model1_wakeup_source_add(struct Model1_wakeup_source *Model1_ws);
extern void Model1_wakeup_source_remove(struct Model1_wakeup_source *Model1_ws);
extern struct Model1_wakeup_source *Model1_wakeup_source_register(const char *Model1_name);
extern void Model1_wakeup_source_unregister(struct Model1_wakeup_source *Model1_ws);
extern int Model1_device_wakeup_enable(struct Model1_device *Model1_dev);
extern int Model1_device_wakeup_disable(struct Model1_device *Model1_dev);
extern void Model1_device_set_wakeup_capable(struct Model1_device *Model1_dev, bool Model1_capable);
extern int Model1_device_init_wakeup(struct Model1_device *Model1_dev, bool Model1_val);
extern int Model1_device_set_wakeup_enable(struct Model1_device *Model1_dev, bool Model1_enable);
extern void Model1___pm_stay_awake(struct Model1_wakeup_source *Model1_ws);
extern void Model1_pm_stay_awake(struct Model1_device *Model1_dev);
extern void Model1___pm_relax(struct Model1_wakeup_source *Model1_ws);
extern void Model1_pm_relax(struct Model1_device *Model1_dev);
extern void Model1___pm_wakeup_event(struct Model1_wakeup_source *Model1_ws, unsigned int Model1_msec);
extern void Model1_pm_wakeup_event(struct Model1_device *Model1_dev, unsigned int Model1_msec);
static inline __attribute__((no_instrument_function)) void Model1_wakeup_source_init(struct Model1_wakeup_source *Model1_ws,
          const char *Model1_name)
{
 Model1_wakeup_source_prepare(Model1_ws, Model1_name);
 Model1_wakeup_source_add(Model1_ws);
}

static inline __attribute__((no_instrument_function)) void Model1_wakeup_source_trash(struct Model1_wakeup_source *Model1_ws)
{
 Model1_wakeup_source_remove(Model1_ws);
 Model1_wakeup_source_drop(Model1_ws);
}

static inline __attribute__((no_instrument_function)) const char *Model1_dev_name(const struct Model1_device *Model1_dev)
{
 /* Use the init name until the kobject becomes available */
 if (Model1_dev->Model1_init_name)
  return Model1_dev->Model1_init_name;

 return Model1_kobject_name(&Model1_dev->Model1_kobj);
}

extern __attribute__((format(printf, 2, 3)))
int Model1_dev_set_name(struct Model1_device *Model1_dev, const char *Model1_name, ...);


static inline __attribute__((no_instrument_function)) int Model1_dev_to_node(struct Model1_device *Model1_dev)
{
 return Model1_dev->Model1_numa_node;
}
static inline __attribute__((no_instrument_function)) void Model1_set_dev_node(struct Model1_device *Model1_dev, int Model1_node)
{
 Model1_dev->Model1_numa_node = Model1_node;
}
static inline __attribute__((no_instrument_function)) struct Model1_irq_domain *Model1_dev_get_msi_domain(const struct Model1_device *Model1_dev)
{

 return Model1_dev->Model1_msi_domain;



}

static inline __attribute__((no_instrument_function)) void Model1_dev_set_msi_domain(struct Model1_device *Model1_dev, struct Model1_irq_domain *Model1_d)
{

 Model1_dev->Model1_msi_domain = Model1_d;

}

static inline __attribute__((no_instrument_function)) void *Model1_dev_get_drvdata(const struct Model1_device *Model1_dev)
{
 return Model1_dev->Model1_driver_data;
}

static inline __attribute__((no_instrument_function)) void Model1_dev_set_drvdata(struct Model1_device *Model1_dev, void *Model1_data)
{
 Model1_dev->Model1_driver_data = Model1_data;
}

static inline __attribute__((no_instrument_function)) struct Model1_pm_subsys_data *Model1_dev_to_psd(struct Model1_device *Model1_dev)
{
 return Model1_dev ? Model1_dev->Model1_power.Model1_subsys_data : ((void *)0);
}

static inline __attribute__((no_instrument_function)) unsigned int Model1_dev_get_uevent_suppress(const struct Model1_device *Model1_dev)
{
 return Model1_dev->Model1_kobj.Model1_uevent_suppress;
}

static inline __attribute__((no_instrument_function)) void Model1_dev_set_uevent_suppress(struct Model1_device *Model1_dev, int Model1_val)
{
 Model1_dev->Model1_kobj.Model1_uevent_suppress = Model1_val;
}

static inline __attribute__((no_instrument_function)) int Model1_device_is_registered(struct Model1_device *Model1_dev)
{
 return Model1_dev->Model1_kobj.Model1_state_in_sysfs;
}

static inline __attribute__((no_instrument_function)) void Model1_device_enable_async_suspend(struct Model1_device *Model1_dev)
{
 if (!Model1_dev->Model1_power.Model1_is_prepared)
  Model1_dev->Model1_power.Model1_async_suspend = true;
}

static inline __attribute__((no_instrument_function)) void Model1_device_disable_async_suspend(struct Model1_device *Model1_dev)
{
 if (!Model1_dev->Model1_power.Model1_is_prepared)
  Model1_dev->Model1_power.Model1_async_suspend = false;
}

static inline __attribute__((no_instrument_function)) bool Model1_device_async_suspend_enabled(struct Model1_device *Model1_dev)
{
 return !!Model1_dev->Model1_power.Model1_async_suspend;
}

static inline __attribute__((no_instrument_function)) void Model1_dev_pm_syscore_device(struct Model1_device *Model1_dev, bool Model1_val)
{

 Model1_dev->Model1_power.Model1_syscore = Model1_val;

}

static inline __attribute__((no_instrument_function)) void Model1_device_lock(struct Model1_device *Model1_dev)
{
 Model1_mutex_lock(&Model1_dev->Model1_mutex);
}

static inline __attribute__((no_instrument_function)) int Model1_device_lock_interruptible(struct Model1_device *Model1_dev)
{
 return Model1_mutex_lock_interruptible(&Model1_dev->Model1_mutex);
}

static inline __attribute__((no_instrument_function)) int Model1_device_trylock(struct Model1_device *Model1_dev)
{
 return Model1_mutex_trylock(&Model1_dev->Model1_mutex);
}

static inline __attribute__((no_instrument_function)) void Model1_device_unlock(struct Model1_device *Model1_dev)
{
 Model1_mutex_unlock(&Model1_dev->Model1_mutex);
}

static inline __attribute__((no_instrument_function)) void Model1_device_lock_assert(struct Model1_device *Model1_dev)
{
 do { (void)(&Model1_dev->Model1_mutex); } while (0);
}

static inline __attribute__((no_instrument_function)) struct Model1_device_node *Model1_dev_of_node(struct Model1_device *Model1_dev)
{
 if (!0)
  return ((void *)0);
 return Model1_dev->Model1_of_node;
}

void Model1_driver_init(void);

/*
 * High level routines for use by the bus drivers
 */
extern int __attribute__((warn_unused_result)) Model1_device_register(struct Model1_device *Model1_dev);
extern void Model1_device_unregister(struct Model1_device *Model1_dev);
extern void Model1_device_initialize(struct Model1_device *Model1_dev);
extern int __attribute__((warn_unused_result)) Model1_device_add(struct Model1_device *Model1_dev);
extern void Model1_device_del(struct Model1_device *Model1_dev);
extern int Model1_device_for_each_child(struct Model1_device *Model1_dev, void *Model1_data,
       int (*Model1_fn)(struct Model1_device *Model1_dev, void *Model1_data));
extern int Model1_device_for_each_child_reverse(struct Model1_device *Model1_dev, void *Model1_data,
       int (*Model1_fn)(struct Model1_device *Model1_dev, void *Model1_data));
extern struct Model1_device *Model1_device_find_child(struct Model1_device *Model1_dev, void *Model1_data,
    int (*Model1_match)(struct Model1_device *Model1_dev, void *Model1_data));
extern int Model1_device_rename(struct Model1_device *Model1_dev, const char *Model1_new_name);
extern int Model1_device_move(struct Model1_device *Model1_dev, struct Model1_device *Model1_new_parent,
         enum Model1_dpm_order Model1_dpm_order);
extern const char *Model1_device_get_devnode(struct Model1_device *Model1_dev,
          Model1_umode_t *Model1_mode, Model1_kuid_t *Model1_uid, Model1_kgid_t *Model1_gid,
          const char **Model1_tmp);

static inline __attribute__((no_instrument_function)) bool Model1_device_supports_offline(struct Model1_device *Model1_dev)
{
 return Model1_dev->Model1_bus && Model1_dev->Model1_bus->Model1_offline && Model1_dev->Model1_bus->Model1_online;
}

extern void Model1_lock_device_hotplug(void);
extern void Model1_unlock_device_hotplug(void);
extern int Model1_lock_device_hotplug_sysfs(void);
extern int Model1_device_offline(struct Model1_device *Model1_dev);
extern int Model1_device_online(struct Model1_device *Model1_dev);
extern void Model1_set_primary_fwnode(struct Model1_device *Model1_dev, struct Model1_fwnode_handle *Model1_fwnode);
extern void Model1_set_secondary_fwnode(struct Model1_device *Model1_dev, struct Model1_fwnode_handle *Model1_fwnode);

/*
 * Root device objects for grouping under /sys/devices
 */
extern struct Model1_device *Model1___root_device_register(const char *Model1_name,
          struct Model1_module *Model1_owner);

/* This is a macro to avoid include problems with THIS_MODULE */



extern void Model1_root_device_unregister(struct Model1_device *Model1_root);

static inline __attribute__((no_instrument_function)) void *Model1_dev_get_platdata(const struct Model1_device *Model1_dev)
{
 return Model1_dev->Model1_platform_data;
}

/*
 * Manual binding of a device to driver. See drivers/base/bus.c
 * for information on use.
 */
extern int __attribute__((warn_unused_result)) Model1_device_bind_driver(struct Model1_device *Model1_dev);
extern void Model1_device_release_driver(struct Model1_device *Model1_dev);
extern int __attribute__((warn_unused_result)) Model1_device_attach(struct Model1_device *Model1_dev);
extern int __attribute__((warn_unused_result)) Model1_driver_attach(struct Model1_device_driver *Model1_drv);
extern void Model1_device_initial_probe(struct Model1_device *Model1_dev);
extern int __attribute__((warn_unused_result)) Model1_device_reprobe(struct Model1_device *Model1_dev);

extern bool Model1_device_is_bound(struct Model1_device *Model1_dev);

/*
 * Easy functions for dynamically creating devices on the fly
 */
extern __attribute__((format(printf, 5, 0)))
struct Model1_device *Model1_device_create_vargs(struct Model1_class *Model1_cls, struct Model1_device *Model1_parent,
       Model1_dev_t Model1_devt, void *Model1_drvdata,
       const char *Model1_fmt, Model1_va_list Model1_vargs);
extern __attribute__((format(printf, 5, 6)))
struct Model1_device *Model1_device_create(struct Model1_class *Model1_cls, struct Model1_device *Model1_parent,
        Model1_dev_t Model1_devt, void *Model1_drvdata,
        const char *Model1_fmt, ...);
extern __attribute__((format(printf, 6, 7)))
struct Model1_device *Model1_device_create_with_groups(struct Model1_class *Model1_cls,
        struct Model1_device *Model1_parent, Model1_dev_t Model1_devt, void *Model1_drvdata,
        const struct Model1_attribute_group **Model1_groups,
        const char *Model1_fmt, ...);
extern void Model1_device_destroy(struct Model1_class *Model1_cls, Model1_dev_t Model1_devt);

/*
 * Platform "fixup" functions - allow the platform to have their say
 * about devices and actions that the general device layer doesn't
 * know about.
 */
/* Notify platform of device discovery */
extern int (*Model1_platform_notify)(struct Model1_device *Model1_dev);

extern int (*Model1_platform_notify_remove)(struct Model1_device *Model1_dev);


/*
 * get_device - atomically increment the reference count for the device.
 *
 */
extern struct Model1_device *Model1_get_device(struct Model1_device *Model1_dev);
extern void Model1_put_device(struct Model1_device *Model1_dev);


extern int Model1_devtmpfs_create_node(struct Model1_device *Model1_dev);
extern int Model1_devtmpfs_delete_node(struct Model1_device *Model1_dev);
extern int Model1_devtmpfs_mount(const char *Model1_mntdir);






/* drivers/base/power/shutdown.c */
extern void Model1_device_shutdown(void);

/* debugging and troubleshooting/diagnostic helpers. */
extern const char *Model1_dev_driver_string(const struct Model1_device *Model1_dev);




extern __attribute__((format(printf, 3, 0)))
int Model1_dev_vprintk_emit(int Model1_level, const struct Model1_device *Model1_dev,
       const char *Model1_fmt, Model1_va_list Model1_args);
extern __attribute__((format(printf, 3, 4)))
int Model1_dev_printk_emit(int Model1_level, const struct Model1_device *Model1_dev, const char *Model1_fmt, ...);

extern __attribute__((format(printf, 3, 4)))
void Model1_dev_printk(const char *Model1_level, const struct Model1_device *Model1_dev,
  const char *Model1_fmt, ...);
extern __attribute__((format(printf, 2, 3)))
void Model1_dev_emerg(const struct Model1_device *Model1_dev, const char *Model1_fmt, ...);
extern __attribute__((format(printf, 2, 3)))
void Model1_dev_alert(const struct Model1_device *Model1_dev, const char *Model1_fmt, ...);
extern __attribute__((format(printf, 2, 3)))
void Model1_dev_crit(const struct Model1_device *Model1_dev, const char *Model1_fmt, ...);
extern __attribute__((format(printf, 2, 3)))
void Model1_dev_err(const struct Model1_device *Model1_dev, const char *Model1_fmt, ...);
extern __attribute__((format(printf, 2, 3)))
void Model1_dev_warn(const struct Model1_device *Model1_dev, const char *Model1_fmt, ...);
extern __attribute__((format(printf, 2, 3)))
void Model1_dev_notice(const struct Model1_device *Model1_dev, const char *Model1_fmt, ...);
extern __attribute__((format(printf, 2, 3)))
void Model1__dev_info(const struct Model1_device *Model1_dev, const char *Model1_fmt, ...);
/*
 * Stupid hackaround for existing uses of non-printk uses dev_info
 *
 * Note that the definition of dev_info below is actually _dev_info
 * and a macro is used to avoid redefining dev_info
 */
/*
 * dev_WARN*() acts like dev_printk(), but with the key difference of
 * using WARN/WARN_ONCE to include file/line information and a backtrace.
 */







/* Create alias, so I can be autoloaded. */
/**
 * module_driver() - Helper macro for drivers that don't do anything
 * special in module init/exit. This eliminates a lot of boilerplate.
 * Each module may only use this macro once, and calling it replaces
 * module_init() and module_exit().
 *
 * @__driver: driver name
 * @__register: register function for this driver type
 * @__unregister: unregister function for this driver type
 * @...: Additional arguments to be passed to __register and __unregister.
 *
 * Use this macro to construct bus specific macros for registering
 * drivers, and do not use it on its own.
 */
/**
 * builtin_driver() - Helper macro for drivers that don't do anything
 * special in init and have no exit. This eliminates some boilerplate.
 * Each driver may only use this macro once, and calling it replaces
 * device_initcall (or in some cases, the legacy __initcall).  This is
 * meant to be a direct parallel of module_driver() above but without
 * the __exit stuff that is not used for builtin cases.
 *
 * @__driver: driver name
 * @__register: register function for this driver type
 * @...: Additional arguments to be passed to __register
 *
 * Use this macro to construct bus specific macros for registering
 * drivers, and do not use it on its own.
 */

/*
 *	Berkeley style UIO structures	-	Alan Cox 1994.
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */





/*
 *	Berkeley style UIO structures	-	Alan Cox 1994.
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */







struct Model1_iovec
{
 void *Model1_iov_base; /* BSD uses caddr_t (1003.1g requires void *) */
 Model1___kernel_size_t Model1_iov_len; /* Must be size_t (1003.1g) */
};

/*
 *	UIO_MAXIOV shall be at least 16 1003.1g (5.4.1.1)
 */

struct Model1_page;

struct Model1_kvec {
 void *Model1_iov_base; /* and that should *never* hold a userland pointer */
 Model1_size_t Model1_iov_len;
};

enum {
 Model1_ITER_IOVEC = 0,
 Model1_ITER_KVEC = 2,
 Model1_ITER_BVEC = 4,
};

struct Model1_iov_iter {
 int Model1_type;
 Model1_size_t Model1_iov_offset;
 Model1_size_t Model1_count;
 union {
  const struct Model1_iovec *Model1_iov;
  const struct Model1_kvec *Model1_kvec;
  const struct Model1_bio_vec *Model1_bvec;
 };
 unsigned long Model1_nr_segs;
};

/*
 * Total number of bytes covered by an iovec.
 *
 * NOTE that it is not safe to use this function until all the iovec's
 * segment lengths have been validated.  Because the individual lengths can
 * overflow a size_t when added together.
 */
static inline __attribute__((no_instrument_function)) Model1_size_t Model1_iov_length(const struct Model1_iovec *Model1_iov, unsigned long Model1_nr_segs)
{
 unsigned long Model1_seg;
 Model1_size_t Model1_ret = 0;

 for (Model1_seg = 0; Model1_seg < Model1_nr_segs; Model1_seg++)
  Model1_ret += Model1_iov[Model1_seg].Model1_iov_len;
 return Model1_ret;
}

static inline __attribute__((no_instrument_function)) struct Model1_iovec Model1_iov_iter_iovec(const struct Model1_iov_iter *Model1_iter)
{
 return (struct Model1_iovec) {
  .Model1_iov_base = Model1_iter->Model1_iov->Model1_iov_base + Model1_iter->Model1_iov_offset,
  .Model1_iov_len = ({ typeof(Model1_iter->Model1_count) Model1__min1 = (Model1_iter->Model1_count); typeof(Model1_iter->Model1_iov->Model1_iov_len - Model1_iter->Model1_iov_offset) Model1__min2 = (Model1_iter->Model1_iov->Model1_iov_len - Model1_iter->Model1_iov_offset); (void) (&Model1__min1 == &Model1__min2); Model1__min1 < Model1__min2 ? Model1__min1 : Model1__min2; }),

 };
}
unsigned long Model1_iov_shorten(struct Model1_iovec *Model1_iov, unsigned long Model1_nr_segs, Model1_size_t Model1_to);

Model1_size_t Model1_iov_iter_copy_from_user_atomic(struct Model1_page *Model1_page,
  struct Model1_iov_iter *Model1_i, unsigned long Model1_offset, Model1_size_t Model1_bytes);
void Model1_iov_iter_advance(struct Model1_iov_iter *Model1_i, Model1_size_t Model1_bytes);
int Model1_iov_iter_fault_in_readable(struct Model1_iov_iter *Model1_i, Model1_size_t Model1_bytes);

Model1_size_t Model1_iov_iter_single_seg_count(const struct Model1_iov_iter *Model1_i);
Model1_size_t Model1_copy_page_to_iter(struct Model1_page *Model1_page, Model1_size_t Model1_offset, Model1_size_t Model1_bytes,
    struct Model1_iov_iter *Model1_i);
Model1_size_t Model1_copy_page_from_iter(struct Model1_page *Model1_page, Model1_size_t Model1_offset, Model1_size_t Model1_bytes,
    struct Model1_iov_iter *Model1_i);
Model1_size_t Model1_copy_to_iter(const void *Model1_addr, Model1_size_t Model1_bytes, struct Model1_iov_iter *Model1_i);
Model1_size_t Model1_copy_from_iter(void *Model1_addr, Model1_size_t Model1_bytes, struct Model1_iov_iter *Model1_i);
Model1_size_t Model1_copy_from_iter_nocache(void *Model1_addr, Model1_size_t Model1_bytes, struct Model1_iov_iter *Model1_i);
Model1_size_t Model1_iov_iter_zero(Model1_size_t Model1_bytes, struct Model1_iov_iter *);
unsigned long Model1_iov_iter_alignment(const struct Model1_iov_iter *Model1_i);
unsigned long Model1_iov_iter_gap_alignment(const struct Model1_iov_iter *Model1_i);
void Model1_iov_iter_init(struct Model1_iov_iter *Model1_i, int Model1_direction, const struct Model1_iovec *Model1_iov,
   unsigned long Model1_nr_segs, Model1_size_t Model1_count);
void Model1_iov_iter_kvec(struct Model1_iov_iter *Model1_i, int Model1_direction, const struct Model1_kvec *Model1_kvec,
   unsigned long Model1_nr_segs, Model1_size_t Model1_count);
void Model1_iov_iter_bvec(struct Model1_iov_iter *Model1_i, int Model1_direction, const struct Model1_bio_vec *Model1_bvec,
   unsigned long Model1_nr_segs, Model1_size_t Model1_count);
Model1_ssize_t Model1_iov_iter_get_pages(struct Model1_iov_iter *Model1_i, struct Model1_page **Model1_pages,
   Model1_size_t Model1_maxsize, unsigned Model1_maxpages, Model1_size_t *Model1_start);
Model1_ssize_t Model1_iov_iter_get_pages_alloc(struct Model1_iov_iter *Model1_i, struct Model1_page ***Model1_pages,
   Model1_size_t Model1_maxsize, Model1_size_t *Model1_start);
int Model1_iov_iter_npages(const struct Model1_iov_iter *Model1_i, int Model1_maxpages);

const void *Model1_dup_iter(struct Model1_iov_iter *Model1_new, struct Model1_iov_iter *old, Model1_gfp_t Model1_flags);

static inline __attribute__((no_instrument_function)) Model1_size_t Model1_iov_iter_count(struct Model1_iov_iter *Model1_i)
{
 return Model1_i->Model1_count;
}

static inline __attribute__((no_instrument_function)) bool Model1_iter_is_iovec(struct Model1_iov_iter *Model1_i)
{
 return !(Model1_i->Model1_type & (Model1_ITER_BVEC | Model1_ITER_KVEC));
}

/*
 * Get one of READ or WRITE out of iter->type without any other flags OR'd in
 * with it.
 *
 * The ?: is just for type safety.
 */


/*
 * Cap the iov_iter by given limit; note that the second argument is
 * *not* the new size - it's upper limit for such.  Passing it a value
 * greater than the amount of data in iov_iter is fine - it'll just do
 * nothing in that case.
 */
static inline __attribute__((no_instrument_function)) void Model1_iov_iter_truncate(struct Model1_iov_iter *Model1_i, Model1_u64 Model1_count)
{
 /*
	 * count doesn't have to fit in size_t - comparison extends both
	 * operands to u64 here and any value that would be truncated by
	 * conversion in assignement is by definition greater than all
	 * values of size_t, including old i->count.
	 */
 if (Model1_i->Model1_count > Model1_count)
  Model1_i->Model1_count = Model1_count;
}

/*
 * reexpand a previously truncated iterator; count must be no more than how much
 * we had shrunk it.
 */
static inline __attribute__((no_instrument_function)) void Model1_iov_iter_reexpand(struct Model1_iov_iter *Model1_i, Model1_size_t Model1_count)
{
 Model1_i->Model1_count = Model1_count;
}
Model1_size_t Model1_csum_and_copy_to_iter(const void *Model1_addr, Model1_size_t Model1_bytes, Model1___wsum *Model1_csum, struct Model1_iov_iter *Model1_i);
Model1_size_t Model1_csum_and_copy_from_iter(void *Model1_addr, Model1_size_t Model1_bytes, Model1___wsum *Model1_csum, struct Model1_iov_iter *Model1_i);

int Model1_import_iovec(int Model1_type, const struct Model1_iovec * Model1_uvector,
   unsigned Model1_nr_segs, unsigned Model1_fast_segs,
   struct Model1_iovec **Model1_iov, struct Model1_iov_iter *Model1_i);


struct Model1_compat_iovec;
int Model1_compat_import_iovec(int Model1_type, const struct Model1_compat_iovec * Model1_uvector,
   unsigned Model1_nr_segs, unsigned Model1_fast_segs,
   struct Model1_iovec **Model1_iov, struct Model1_iov_iter *Model1_i);


int Model1_import_single_range(int Model1_type, void *Model1_buf, Model1_size_t Model1_len,
   struct Model1_iovec *Model1_iov, struct Model1_iov_iter *Model1_i);

struct Model1_scatterlist {



 unsigned long Model1_page_link;
 unsigned int Model1_offset;
 unsigned int Model1_length;
 Model1_dma_addr_t Model1_dma_address;

 unsigned int Model1_dma_length;

};

/*
 * These macros should be used after a dma_map_sg call has been done
 * to get bus addresses of each of the SG entries and their lengths.
 * You should only work with the number of sg entries dma_map_sg
 * returns, or alternatively stop on the first sg_dma_len(sg) which
 * is 0.
 */
struct Model1_sg_table {
 struct Model1_scatterlist *Model1_sgl; /* the list */
 unsigned int Model1_nents; /* number of mapped entries */
 unsigned int Model1_orig_nents; /* original size of list */
};

/*
 * Notes on SG table design.
 *
 * We use the unsigned long page_link field in the scatterlist struct to place
 * the page pointer AND encode information about the sg table as well. The two
 * lower bits are reserved for this information.
 *
 * If bit 0 is set, then the page_link contains a pointer to the next sg
 * table list. Otherwise the next entry is at sg + 1.
 *
 * If bit 1 is set, then this sg entry is the last element in a list.
 *
 * See sg_next().
 *
 */



/*
 * We overload the LSB of the page pointer to indicate whether it's
 * a valid sg entry, or whether it points to the start of a new scatterlist.
 * Those low bits are there for everyone! (thanks mason :-)
 */





/**
 * sg_assign_page - Assign a given page to an SG entry
 * @sg:		    SG entry
 * @page:	    The page
 *
 * Description:
 *   Assign page to sg entry. Also see sg_set_page(), the most commonly used
 *   variant.
 *
 **/
static inline __attribute__((no_instrument_function)) void Model1_sg_assign_page(struct Model1_scatterlist *Model1_sg, struct Model1_page *Model1_page)
{
 unsigned long Model1_page_link = Model1_sg->Model1_page_link & 0x3;

 /*
	 * In order for the low bit stealing approach to work, pages
	 * must be aligned at a 32-bit boundary as a minimum.
	 */
 do { if (__builtin_expect(!!((unsigned long) Model1_page & 0x03), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/scatterlist.h"), "i" (90), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);




 Model1_sg->Model1_page_link = Model1_page_link | (unsigned long) Model1_page;
}

/**
 * sg_set_page - Set sg entry to point at given page
 * @sg:		 SG entry
 * @page:	 The page
 * @len:	 Length of data
 * @offset:	 Offset into page
 *
 * Description:
 *   Use this function to set an sg entry pointing at a page, never assign
 *   the page directly. We encode sg table information in the lower bits
 *   of the page pointer. See sg_page() for looking up the page belonging
 *   to an sg entry.
 *
 **/
static inline __attribute__((no_instrument_function)) void Model1_sg_set_page(struct Model1_scatterlist *Model1_sg, struct Model1_page *Model1_page,
          unsigned int Model1_len, unsigned int Model1_offset)
{
 Model1_sg_assign_page(Model1_sg, Model1_page);
 Model1_sg->Model1_offset = Model1_offset;
 Model1_sg->Model1_length = Model1_len;
}

static inline __attribute__((no_instrument_function)) struct Model1_page *Model1_sg_page(struct Model1_scatterlist *Model1_sg)
{




 return (struct Model1_page *)((Model1_sg)->Model1_page_link & ~0x3);
}

/**
 * sg_set_buf - Set sg entry to point at given data
 * @sg:		 SG entry
 * @buf:	 Data
 * @buflen:	 Data length
 *
 **/
static inline __attribute__((no_instrument_function)) void Model1_sg_set_buf(struct Model1_scatterlist *Model1_sg, const void *Model1_buf,
         unsigned int Model1_buflen)
{



 Model1_sg_set_page(Model1_sg, (((struct Model1_page *)(0xffffea0000000000UL)) + (Model1___phys_addr_nodebug((unsigned long)(Model1_buf)) >> 12)), Model1_buflen, ((unsigned long)(Model1_buf) & ~(~(((1UL) << 12)-1))));
}

/*
 * Loop over each sg element, following the pointer to a new list if necessary
 */



/**
 * sg_chain - Chain two sglists together
 * @prv:	First scatterlist
 * @prv_nents:	Number of entries in prv
 * @sgl:	Second scatterlist
 *
 * Description:
 *   Links @prv@ and @sgl@ together, to form a longer scatterlist.
 *
 **/
static inline __attribute__((no_instrument_function)) void Model1_sg_chain(struct Model1_scatterlist *Model1_prv, unsigned int Model1_prv_nents,
       struct Model1_scatterlist *Model1_sgl)
{
 /*
	 * offset and length are unused for chain entry.  Clear them.
	 */
 Model1_prv[Model1_prv_nents - 1].Model1_offset = 0;
 Model1_prv[Model1_prv_nents - 1].Model1_length = 0;

 /*
	 * Set lowest bit to indicate a link pointer, and make sure to clear
	 * the termination bit if it happens to be set.
	 */
 Model1_prv[Model1_prv_nents - 1].Model1_page_link = ((unsigned long) Model1_sgl | 0x01) & ~0x02;
}

/**
 * sg_mark_end - Mark the end of the scatterlist
 * @sg:		 SG entryScatterlist
 *
 * Description:
 *   Marks the passed in sg entry as the termination point for the sg
 *   table. A call to sg_next() on this entry will return NULL.
 *
 **/
static inline __attribute__((no_instrument_function)) void Model1_sg_mark_end(struct Model1_scatterlist *Model1_sg)
{



 /*
	 * Set termination bit, clear potential chain bit
	 */
 Model1_sg->Model1_page_link |= 0x02;
 Model1_sg->Model1_page_link &= ~0x01;
}

/**
 * sg_unmark_end - Undo setting the end of the scatterlist
 * @sg:		 SG entryScatterlist
 *
 * Description:
 *   Removes the termination marker from the given entry of the scatterlist.
 *
 **/
static inline __attribute__((no_instrument_function)) void Model1_sg_unmark_end(struct Model1_scatterlist *Model1_sg)
{



 Model1_sg->Model1_page_link &= ~0x02;
}

/**
 * sg_phys - Return physical address of an sg entry
 * @sg:	     SG entry
 *
 * Description:
 *   This calls page_to_phys() on the page in this sg entry, and adds the
 *   sg offset. The caller must know that it is legal to call page_to_phys()
 *   on the sg page.
 *
 **/
static inline __attribute__((no_instrument_function)) Model1_dma_addr_t Model1_sg_phys(struct Model1_scatterlist *Model1_sg)
{
 return ((Model1_dma_addr_t)(unsigned long)((Model1_sg_page(Model1_sg)) - ((struct Model1_page *)(0xffffea0000000000UL))) << 12) + Model1_sg->Model1_offset;
}

/**
 * sg_virt - Return virtual address of an sg entry
 * @sg:      SG entry
 *
 * Description:
 *   This calls page_address() on the page in this sg entry, and adds the
 *   sg offset. The caller must know that the sg page has a valid virtual
 *   mapping.
 *
 **/
static inline __attribute__((no_instrument_function)) void *Model1_sg_virt(struct Model1_scatterlist *Model1_sg)
{
 return Model1_lowmem_page_address(Model1_sg_page(Model1_sg)) + Model1_sg->Model1_offset;
}

int Model1_sg_nents(struct Model1_scatterlist *Model1_sg);
int Model1_sg_nents_for_len(struct Model1_scatterlist *Model1_sg, Model1_u64 Model1_len);
struct Model1_scatterlist *Model1_sg_next(struct Model1_scatterlist *);
struct Model1_scatterlist *Model1_sg_last(struct Model1_scatterlist *Model1_s, unsigned int);
void Model1_sg_init_table(struct Model1_scatterlist *, unsigned int);
void Model1_sg_init_one(struct Model1_scatterlist *, const void *, unsigned int);
int Model1_sg_split(struct Model1_scatterlist *Model1_in, const int Model1_in_mapped_nents,
      const Model1_off_t Model1_skip, const int Model1_nb_splits,
      const Model1_size_t *Model1_split_sizes,
      struct Model1_scatterlist **Model1_out, int *Model1_out_mapped_nents,
      Model1_gfp_t Model1_gfp_mask);

typedef struct Model1_scatterlist *(Model1_sg_alloc_fn)(unsigned int, Model1_gfp_t);
typedef void (Model1_sg_free_fn)(struct Model1_scatterlist *, unsigned int);

void Model1___sg_free_table(struct Model1_sg_table *, unsigned int, bool, Model1_sg_free_fn *);
void Model1_sg_free_table(struct Model1_sg_table *);
int Model1___sg_alloc_table(struct Model1_sg_table *, unsigned int, unsigned int,
       struct Model1_scatterlist *, Model1_gfp_t, Model1_sg_alloc_fn *);
int Model1_sg_alloc_table(struct Model1_sg_table *, unsigned int, Model1_gfp_t);
int Model1_sg_alloc_table_from_pages(struct Model1_sg_table *Model1_sgt,
 struct Model1_page **Model1_pages, unsigned int Model1_n_pages,
 unsigned long Model1_offset, unsigned long Model1_size,
 Model1_gfp_t Model1_gfp_mask);

Model1_size_t Model1_sg_copy_buffer(struct Model1_scatterlist *Model1_sgl, unsigned int Model1_nents, void *Model1_buf,
        Model1_size_t Model1_buflen, Model1_off_t Model1_skip, bool Model1_to_buffer);

Model1_size_t Model1_sg_copy_from_buffer(struct Model1_scatterlist *Model1_sgl, unsigned int Model1_nents,
      const void *Model1_buf, Model1_size_t Model1_buflen);
Model1_size_t Model1_sg_copy_to_buffer(struct Model1_scatterlist *Model1_sgl, unsigned int Model1_nents,
    void *Model1_buf, Model1_size_t Model1_buflen);

Model1_size_t Model1_sg_pcopy_from_buffer(struct Model1_scatterlist *Model1_sgl, unsigned int Model1_nents,
       const void *Model1_buf, Model1_size_t Model1_buflen, Model1_off_t Model1_skip);
Model1_size_t Model1_sg_pcopy_to_buffer(struct Model1_scatterlist *Model1_sgl, unsigned int Model1_nents,
     void *Model1_buf, Model1_size_t Model1_buflen, Model1_off_t Model1_skip);

/*
 * Maximum number of entries that will be allocated in one piece, if
 * a list larger than this is required then chaining will be utilized.
 */


/*
 * The maximum number of SG segments that we will put inside a
 * scatterlist (unless chaining is used). Should ideally fit inside a
 * single page, to avoid a higher order allocation.  We could define this
 * to SG_MAX_SINGLE_ALLOC to pack correctly at the highest order.  The
 * minimum value is 32
 */


/*
 * Like SG_CHUNK_SIZE, but for archs that have sg chaining. This limit
 * is totally arbitrary, a setting of 2048 will get you at least 8mb ios.
 */







void Model1_sg_free_table_chained(struct Model1_sg_table *Model1_table, bool Model1_first_chunk);
int Model1_sg_alloc_table_chained(struct Model1_sg_table *Model1_table, int Model1_nents,
      struct Model1_scatterlist *Model1_first_chunk);


/*
 * sg page iterator
 *
 * Iterates over sg entries page-by-page.  On each successful iteration,
 * you can call sg_page_iter_page(@piter) and sg_page_iter_dma_address(@piter)
 * to get the current page and its dma address. @piter->sg will point to the
 * sg holding this page and @piter->sg_pgoffset to the page's page offset
 * within the sg. The iteration will stop either when a maximum number of sg
 * entries was reached or a terminating sg (sg_last(sg) == true) was reached.
 */
struct Model1_sg_page_iter {
 struct Model1_scatterlist *Model1_sg; /* sg holding the page */
 unsigned int Model1_sg_pgoffset; /* page offset within the sg */

 /* these are internal states, keep away */
 unsigned int Model1___nents; /* remaining sg entries */
 int Model1___pg_advance; /* nr pages to advance at the
						 * next step */
};

bool Model1___sg_page_iter_next(struct Model1_sg_page_iter *Model1_piter);
void Model1___sg_page_iter_start(struct Model1_sg_page_iter *Model1_piter,
     struct Model1_scatterlist *Model1_sglist, unsigned int Model1_nents,
     unsigned long Model1_pgoffset);
/**
 * sg_page_iter_page - get the current page held by the page iterator
 * @piter:	page iterator holding the page
 */
static inline __attribute__((no_instrument_function)) struct Model1_page *Model1_sg_page_iter_page(struct Model1_sg_page_iter *Model1_piter)
{
 return (((struct Model1_page *)(0xffffea0000000000UL)) + ((unsigned long)(((Model1_sg_page(Model1_piter->Model1_sg))) - ((struct Model1_page *)(0xffffea0000000000UL))) + (Model1_piter->Model1_sg_pgoffset)));
}

/**
 * sg_page_iter_dma_address - get the dma address of the current page held by
 * the page iterator.
 * @piter:	page iterator holding the page
 */
static inline __attribute__((no_instrument_function)) Model1_dma_addr_t Model1_sg_page_iter_dma_address(struct Model1_sg_page_iter *Model1_piter)
{
 return ((Model1_piter->Model1_sg)->Model1_dma_address) + (Model1_piter->Model1_sg_pgoffset << 12);
}

/**
 * for_each_sg_page - iterate over the pages of the given sg list
 * @sglist:	sglist to iterate over
 * @piter:	page iterator to hold current page, sg, sg_pgoffset
 * @nents:	maximum number of sg entries to iterate over
 * @pgoffset:	starting page offset
 */




/*
 * Mapping sg iterator
 *
 * Iterates over sg entries mapping page-by-page.  On each successful
 * iteration, @miter->page points to the mapped page and
 * @miter->length bytes of data can be accessed at @miter->addr.  As
 * long as an interation is enclosed between start and stop, the user
 * is free to choose control structure and when to stop.
 *
 * @miter->consumed is set to @miter->length on each iteration.  It
 * can be adjusted if the user can't consume all the bytes in one go.
 * Also, a stopped iteration can be resumed by calling next on it.
 * This is useful when iteration needs to release all resources and
 * continue later (e.g. at the next interrupt).
 */





struct Model1_sg_mapping_iter {
 /* the following three fields can be accessed directly */
 struct Model1_page *Model1_page; /* currently mapped page */
 void *Model1_addr; /* pointer to the mapped area */
 Model1_size_t Model1_length; /* length of the mapped area */
 Model1_size_t Model1_consumed; /* number of consumed bytes */
 struct Model1_sg_page_iter Model1_piter; /* page iterator */

 /* these are internal states, keep away */
 unsigned int Model1___offset; /* offset within page */
 unsigned int Model1___remaining; /* remaining bytes on page */
 unsigned int Model1___flags;
};

void Model1_sg_miter_start(struct Model1_sg_mapping_iter *Model1_miter, struct Model1_scatterlist *Model1_sgl,
      unsigned int Model1_nents, unsigned int Model1_flags);
bool Model1_sg_miter_skip(struct Model1_sg_mapping_iter *Model1_miter, Model1_off_t Model1_offset);
bool Model1_sg_miter_next(struct Model1_sg_mapping_iter *Model1_miter);
void Model1_sg_miter_stop(struct Model1_sg_mapping_iter *Model1_miter);




/**
 * typedef dma_cookie_t - an opaque DMA cookie
 *
 * if dma_cookie_t is >0 it's a DMA request cookie, <0 it's an error code
 */
typedef Model1_s32 Model1_dma_cookie_t;


static inline __attribute__((no_instrument_function)) int Model1_dma_submit_error(Model1_dma_cookie_t Model1_cookie)
{
 return Model1_cookie < 0 ? Model1_cookie : 0;
}

/**
 * enum dma_status - DMA transaction status
 * @DMA_COMPLETE: transaction completed
 * @DMA_IN_PROGRESS: transaction not yet processed
 * @DMA_PAUSED: transaction is paused
 * @DMA_ERROR: transaction failed
 */
enum Model1_dma_status {
 Model1_DMA_COMPLETE,
 Model1_DMA_IN_PROGRESS,
 Model1_DMA_PAUSED,
 Model1_DMA_ERROR,
};

/**
 * enum dma_transaction_type - DMA transaction types/indexes
 *
 * Note: The DMA_ASYNC_TX capability is not to be set by drivers.  It is
 * automatically set as dma devices are registered.
 */
enum Model1_dma_transaction_type {
 Model1_DMA_MEMCPY,
 Model1_DMA_XOR,
 Model1_DMA_PQ,
 Model1_DMA_XOR_VAL,
 Model1_DMA_PQ_VAL,
 Model1_DMA_MEMSET,
 Model1_DMA_MEMSET_SG,
 Model1_DMA_INTERRUPT,
 Model1_DMA_SG,
 Model1_DMA_PRIVATE,
 Model1_DMA_ASYNC_TX,
 Model1_DMA_SLAVE,
 Model1_DMA_CYCLIC,
 Model1_DMA_INTERLEAVE,
/* last transaction type for creation of the capabilities mask */
 Model1_DMA_TX_TYPE_END,
};

/**
 * enum dma_transfer_direction - dma transfer mode and direction indicator
 * @DMA_MEM_TO_MEM: Async/Memcpy mode
 * @DMA_MEM_TO_DEV: Slave mode & From Memory to Device
 * @DMA_DEV_TO_MEM: Slave mode & From Device to Memory
 * @DMA_DEV_TO_DEV: Slave mode & From Device to Device
 */
enum Model1_dma_transfer_direction {
 Model1_DMA_MEM_TO_MEM,
 Model1_DMA_MEM_TO_DEV,
 Model1_DMA_DEV_TO_MEM,
 Model1_DMA_DEV_TO_DEV,
 Model1_DMA_TRANS_NONE,
};

/**
 * Interleaved Transfer Request
 * ----------------------------
 * A chunk is collection of contiguous bytes to be transfered.
 * The gap(in bytes) between two chunks is called inter-chunk-gap(ICG).
 * ICGs may or maynot change between chunks.
 * A FRAME is the smallest series of contiguous {chunk,icg} pairs,
 *  that when repeated an integral number of times, specifies the transfer.
 * A transfer template is specification of a Frame, the number of times
 *  it is to be repeated and other per-transfer attributes.
 *
 * Practically, a client driver would have ready a template for each
 *  type of transfer it is going to need during its lifetime and
 *  set only 'src_start' and 'dst_start' before submitting the requests.
 *
 *
 *  |      Frame-1        |       Frame-2       | ~ |       Frame-'numf'  |
 *  |====....==.===...=...|====....==.===...=...| ~ |====....==.===...=...|
 *
 *    ==  Chunk size
 *    ... ICG
 */

/**
 * struct data_chunk - Element of scatter-gather list that makes a frame.
 * @size: Number of bytes to read from source.
 *	  size_dst := fn(op, size_src), so doesn't mean much for destination.
 * @icg: Number of bytes to jump after last src/dst address of this
 *	 chunk and before first src/dst address for next chunk.
 *	 Ignored for dst(assumed 0), if dst_inc is true and dst_sgl is false.
 *	 Ignored for src(assumed 0), if src_inc is true and src_sgl is false.
 * @dst_icg: Number of bytes to jump after last dst address of this
 *	 chunk and before the first dst address for next chunk.
 *	 Ignored if dst_inc is true and dst_sgl is false.
 * @src_icg: Number of bytes to jump after last src address of this
 *	 chunk and before the first src address for next chunk.
 *	 Ignored if src_inc is true and src_sgl is false.
 */
struct Model1_data_chunk {
 Model1_size_t Model1_size;
 Model1_size_t Model1_icg;
 Model1_size_t Model1_dst_icg;
 Model1_size_t Model1_src_icg;
};

/**
 * struct dma_interleaved_template - Template to convey DMAC the transfer pattern
 *	 and attributes.
 * @src_start: Bus address of source for the first chunk.
 * @dst_start: Bus address of destination for the first chunk.
 * @dir: Specifies the type of Source and Destination.
 * @src_inc: If the source address increments after reading from it.
 * @dst_inc: If the destination address increments after writing to it.
 * @src_sgl: If the 'icg' of sgl[] applies to Source (scattered read).
 *		Otherwise, source is read contiguously (icg ignored).
 *		Ignored if src_inc is false.
 * @dst_sgl: If the 'icg' of sgl[] applies to Destination (scattered write).
 *		Otherwise, destination is filled contiguously (icg ignored).
 *		Ignored if dst_inc is false.
 * @numf: Number of frames in this template.
 * @frame_size: Number of chunks in a frame i.e, size of sgl[].
 * @sgl: Array of {chunk,icg} pairs that make up a frame.
 */
struct Model1_dma_interleaved_template {
 Model1_dma_addr_t Model1_src_start;
 Model1_dma_addr_t Model1_dst_start;
 enum Model1_dma_transfer_direction Model1_dir;
 bool Model1_src_inc;
 bool Model1_dst_inc;
 bool Model1_src_sgl;
 bool Model1_dst_sgl;
 Model1_size_t Model1_numf;
 Model1_size_t Model1_frame_size;
 struct Model1_data_chunk Model1_sgl[0];
};

/**
 * enum dma_ctrl_flags - DMA flags to augment operation preparation,
 *  control completion, and communicate status.
 * @DMA_PREP_INTERRUPT - trigger an interrupt (callback) upon completion of
 *  this transaction
 * @DMA_CTRL_ACK - if clear, the descriptor cannot be reused until the client
 *  acknowledges receipt, i.e. has has a chance to establish any dependency
 *  chains
 * @DMA_PREP_PQ_DISABLE_P - prevent generation of P while generating Q
 * @DMA_PREP_PQ_DISABLE_Q - prevent generation of Q while generating P
 * @DMA_PREP_CONTINUE - indicate to a driver that it is reusing buffers as
 *  sources that were the result of a previous operation, in the case of a PQ
 *  operation it continues the calculation with new sources
 * @DMA_PREP_FENCE - tell the driver that subsequent operations depend
 *  on the result of this operation
 * @DMA_CTRL_REUSE: client can reuse the descriptor and submit again till
 *  cleared or freed
 */
enum Model1_dma_ctrl_flags {
 Model1_DMA_PREP_INTERRUPT = (1 << 0),
 Model1_DMA_CTRL_ACK = (1 << 1),
 Model1_DMA_PREP_PQ_DISABLE_P = (1 << 2),
 Model1_DMA_PREP_PQ_DISABLE_Q = (1 << 3),
 Model1_DMA_PREP_CONTINUE = (1 << 4),
 Model1_DMA_PREP_FENCE = (1 << 5),
 Model1_DMA_CTRL_REUSE = (1 << 6),
};

/**
 * enum sum_check_bits - bit position of pq_check_flags
 */
enum Model1_sum_check_bits {
 Model1_SUM_CHECK_P = 0,
 Model1_SUM_CHECK_Q = 1,
};

/**
 * enum pq_check_flags - result of async_{xor,pq}_zero_sum operations
 * @SUM_CHECK_P_RESULT - 1 if xor zero sum error, 0 otherwise
 * @SUM_CHECK_Q_RESULT - 1 if reed-solomon zero sum error, 0 otherwise
 */
enum Model1_sum_check_flags {
 Model1_SUM_CHECK_P_RESULT = (1 << Model1_SUM_CHECK_P),
 Model1_SUM_CHECK_Q_RESULT = (1 << Model1_SUM_CHECK_Q),
};


/**
 * dma_cap_mask_t - capabilities bitmap modeled after cpumask_t.
 * See linux/cpumask.h
 */
typedef struct { unsigned long Model1_bits[(((Model1_DMA_TX_TYPE_END) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))]; } Model1_dma_cap_mask_t;

/**
 * struct dma_chan_percpu - the per-CPU part of struct dma_chan
 * @memcpy_count: transaction counter
 * @bytes_transferred: byte counter
 */

struct Model1_dma_chan_percpu {
 /* stats */
 unsigned long Model1_memcpy_count;
 unsigned long Model1_bytes_transferred;
};

/**
 * struct dma_router - DMA router structure
 * @dev: pointer to the DMA router device
 * @route_free: function to be called when the route can be disconnected
 */
struct Model1_dma_router {
 struct Model1_device *Model1_dev;
 void (*Model1_route_free)(struct Model1_device *Model1_dev, void *Model1_route_data);
};

/**
 * struct dma_chan - devices supply DMA channels, clients use them
 * @device: ptr to the dma device who supplies this channel, always !%NULL
 * @cookie: last cookie value returned to client
 * @completed_cookie: last completed cookie for this channel
 * @chan_id: channel ID for sysfs
 * @dev: class device for sysfs
 * @device_node: used to add this to the device chan list
 * @local: per-cpu pointer to a struct dma_chan_percpu
 * @client_count: how many clients are using this channel
 * @table_count: number of appearances in the mem-to-mem allocation table
 * @router: pointer to the DMA router structure
 * @route_data: channel specific data for the router
 * @private: private data for certain client-channel associations
 */
struct Model1_dma_chan {
 struct Model1_dma_device *Model1_device;
 Model1_dma_cookie_t Model1_cookie;
 Model1_dma_cookie_t Model1_completed_cookie;

 /* sysfs */
 int Model1_chan_id;
 struct Model1_dma_chan_dev *Model1_dev;

 struct Model1_list_head Model1_device_node;
 struct Model1_dma_chan_percpu *Model1_local;
 int Model1_client_count;
 int Model1_table_count;

 /* DMA router */
 struct Model1_dma_router *Model1_router;
 void *Model1_route_data;

 void *Model1_private;
};

/**
 * struct dma_chan_dev - relate sysfs device node to backing channel device
 * @chan: driver channel device
 * @device: sysfs device
 * @dev_id: parent dma_device dev_id
 * @idr_ref: reference count to gate release of dma_device dev_id
 */
struct Model1_dma_chan_dev {
 struct Model1_dma_chan *Model1_chan;
 struct Model1_device Model1_device;
 int Model1_dev_id;
 Model1_atomic_t *Model1_idr_ref;
};

/**
 * enum dma_slave_buswidth - defines bus width of the DMA slave
 * device, source or target buses
 */
enum Model1_dma_slave_buswidth {
 Model1_DMA_SLAVE_BUSWIDTH_UNDEFINED = 0,
 Model1_DMA_SLAVE_BUSWIDTH_1_BYTE = 1,
 Model1_DMA_SLAVE_BUSWIDTH_2_BYTES = 2,
 Model1_DMA_SLAVE_BUSWIDTH_3_BYTES = 3,
 Model1_DMA_SLAVE_BUSWIDTH_4_BYTES = 4,
 Model1_DMA_SLAVE_BUSWIDTH_8_BYTES = 8,
 Model1_DMA_SLAVE_BUSWIDTH_16_BYTES = 16,
 Model1_DMA_SLAVE_BUSWIDTH_32_BYTES = 32,
 Model1_DMA_SLAVE_BUSWIDTH_64_BYTES = 64,
};

/**
 * struct dma_slave_config - dma slave channel runtime config
 * @direction: whether the data shall go in or out on this slave
 * channel, right now. DMA_MEM_TO_DEV and DMA_DEV_TO_MEM are
 * legal values. DEPRECATED, drivers should use the direction argument
 * to the device_prep_slave_sg and device_prep_dma_cyclic functions or
 * the dir field in the dma_interleaved_template structure.
 * @src_addr: this is the physical address where DMA slave data
 * should be read (RX), if the source is memory this argument is
 * ignored.
 * @dst_addr: this is the physical address where DMA slave data
 * should be written (TX), if the source is memory this argument
 * is ignored.
 * @src_addr_width: this is the width in bytes of the source (RX)
 * register where DMA data shall be read. If the source
 * is memory this may be ignored depending on architecture.
 * Legal values: 1, 2, 4, 8.
 * @dst_addr_width: same as src_addr_width but for destination
 * target (TX) mutatis mutandis.
 * @src_maxburst: the maximum number of words (note: words, as in
 * units of the src_addr_width member, not bytes) that can be sent
 * in one burst to the device. Typically something like half the
 * FIFO depth on I/O peripherals so you don't overflow it. This
 * may or may not be applicable on memory sources.
 * @dst_maxburst: same as src_maxburst but for destination target
 * mutatis mutandis.
 * @device_fc: Flow Controller Settings. Only valid for slave channels. Fill
 * with 'true' if peripheral should be flow controller. Direction will be
 * selected at Runtime.
 * @slave_id: Slave requester id. Only valid for slave channels. The dma
 * slave peripheral will have unique id as dma requester which need to be
 * pass as slave config.
 *
 * This struct is passed in as configuration data to a DMA engine
 * in order to set up a certain channel for DMA transport at runtime.
 * The DMA device/engine has to provide support for an additional
 * callback in the dma_device structure, device_config and this struct
 * will then be passed in as an argument to the function.
 *
 * The rationale for adding configuration information to this struct is as
 * follows: if it is likely that more than one DMA slave controllers in
 * the world will support the configuration option, then make it generic.
 * If not: if it is fixed so that it be sent in static from the platform
 * data, then prefer to do that.
 */
struct Model1_dma_slave_config {
 enum Model1_dma_transfer_direction Model1_direction;
 Model1_phys_addr_t Model1_src_addr;
 Model1_phys_addr_t Model1_dst_addr;
 enum Model1_dma_slave_buswidth Model1_src_addr_width;
 enum Model1_dma_slave_buswidth Model1_dst_addr_width;
 Model1_u32 Model1_src_maxburst;
 Model1_u32 Model1_dst_maxburst;
 bool Model1_device_fc;
 unsigned int Model1_slave_id;
};

/**
 * enum dma_residue_granularity - Granularity of the reported transfer residue
 * @DMA_RESIDUE_GRANULARITY_DESCRIPTOR: Residue reporting is not support. The
 *  DMA channel is only able to tell whether a descriptor has been completed or
 *  not, which means residue reporting is not supported by this channel. The
 *  residue field of the dma_tx_state field will always be 0.
 * @DMA_RESIDUE_GRANULARITY_SEGMENT: Residue is updated after each successfully
 *  completed segment of the transfer (For cyclic transfers this is after each
 *  period). This is typically implemented by having the hardware generate an
 *  interrupt after each transferred segment and then the drivers updates the
 *  outstanding residue by the size of the segment. Another possibility is if
 *  the hardware supports scatter-gather and the segment descriptor has a field
 *  which gets set after the segment has been completed. The driver then counts
 *  the number of segments without the flag set to compute the residue.
 * @DMA_RESIDUE_GRANULARITY_BURST: Residue is updated after each transferred
 *  burst. This is typically only supported if the hardware has a progress
 *  register of some sort (E.g. a register with the current read/write address
 *  or a register with the amount of bursts/beats/bytes that have been
 *  transferred or still need to be transferred).
 */
enum Model1_dma_residue_granularity {
 Model1_DMA_RESIDUE_GRANULARITY_DESCRIPTOR = 0,
 Model1_DMA_RESIDUE_GRANULARITY_SEGMENT = 1,
 Model1_DMA_RESIDUE_GRANULARITY_BURST = 2,
};

/* struct dma_slave_caps - expose capabilities of a slave channel only
 *
 * @src_addr_widths: bit mask of src addr widths the channel supports
 * @dst_addr_widths: bit mask of dstn addr widths the channel supports
 * @directions: bit mask of slave direction the channel supported
 * 	since the enum dma_transfer_direction is not defined as bits for each
 * 	type of direction, the dma controller should fill (1 << <TYPE>) and same
 * 	should be checked by controller as well
 * @max_burst: max burst capability per-transfer
 * @cmd_pause: true, if pause and thereby resume is supported
 * @cmd_terminate: true, if terminate cmd is supported
 * @residue_granularity: granularity of the reported transfer residue
 * @descriptor_reuse: if a descriptor can be reused by client and
 * resubmitted multiple times
 */
struct Model1_dma_slave_caps {
 Model1_u32 Model1_src_addr_widths;
 Model1_u32 Model1_dst_addr_widths;
 Model1_u32 Model1_directions;
 Model1_u32 Model1_max_burst;
 bool Model1_cmd_pause;
 bool Model1_cmd_terminate;
 enum Model1_dma_residue_granularity Model1_residue_granularity;
 bool Model1_descriptor_reuse;
};

static inline __attribute__((no_instrument_function)) const char *Model1_dma_chan_name(struct Model1_dma_chan *Model1_chan)
{
 return Model1_dev_name(&Model1_chan->Model1_dev->Model1_device);
}

void Model1_dma_chan_cleanup(struct Model1_kref *Model1_kref);

/**
 * typedef dma_filter_fn - callback filter for dma_request_channel
 * @chan: channel to be reviewed
 * @filter_param: opaque parameter passed through dma_request_channel
 *
 * When this optional parameter is specified in a call to dma_request_channel a
 * suitable channel is passed to this routine for further dispositioning before
 * being returned.  Where 'suitable' indicates a non-busy channel that
 * satisfies the given capability mask.  It returns 'true' to indicate that the
 * channel is suitable.
 */
typedef bool (*Model1_dma_filter_fn)(struct Model1_dma_chan *Model1_chan, void *Model1_filter_param);

typedef void (*Model1_dma_async_tx_callback)(void *Model1_dma_async_param);

struct Model1_dmaengine_unmap_data {
 Model1_u8 Model1_map_cnt;
 Model1_u8 Model1_to_cnt;
 Model1_u8 Model1_from_cnt;
 Model1_u8 Model1_bidi_cnt;
 struct Model1_device *Model1_dev;
 struct Model1_kref Model1_kref;
 Model1_size_t Model1_len;
 Model1_dma_addr_t Model1_addr[0];
};

/**
 * struct dma_async_tx_descriptor - async transaction descriptor
 * ---dma generic offload fields---
 * @cookie: tracking cookie for this transaction, set to -EBUSY if
 *	this tx is sitting on a dependency list
 * @flags: flags to augment operation preparation, control completion, and
 * 	communicate status
 * @phys: physical address of the descriptor
 * @chan: target channel for this operation
 * @tx_submit: accept the descriptor, assign ordered cookie and mark the
 * descriptor pending. To be pushed on .issue_pending() call
 * @callback: routine to call after this operation is complete
 * @callback_param: general parameter to pass to the callback routine
 * ---async_tx api specific fields---
 * @next: at completion submit this descriptor
 * @parent: pointer to the next level up in the dependency chain
 * @lock: protect the parent and next pointers
 */
struct Model1_dma_async_tx_descriptor {
 Model1_dma_cookie_t Model1_cookie;
 enum Model1_dma_ctrl_flags Model1_flags; /* not a 'long' to pack with cookie */
 Model1_dma_addr_t Model1_phys;
 struct Model1_dma_chan *Model1_chan;
 Model1_dma_cookie_t (*Model1_tx_submit)(struct Model1_dma_async_tx_descriptor *Model1_tx);
 int (*Model1_desc_free)(struct Model1_dma_async_tx_descriptor *Model1_tx);
 Model1_dma_async_tx_callback Model1_callback;
 void *Model1_callback_param;
 struct Model1_dmaengine_unmap_data *Model1_unmap;





};


static inline __attribute__((no_instrument_function)) void Model1_dma_set_unmap(struct Model1_dma_async_tx_descriptor *Model1_tx,
     struct Model1_dmaengine_unmap_data *Model1_unmap)
{
 Model1_kref_get(&Model1_unmap->Model1_kref);
 Model1_tx->Model1_unmap = Model1_unmap;
}

struct Model1_dmaengine_unmap_data *
Model1_dmaengine_get_unmap_data(struct Model1_device *Model1_dev, int Model1_nr, Model1_gfp_t Model1_flags);
void Model1_dmaengine_unmap_put(struct Model1_dmaengine_unmap_data *Model1_unmap);
static inline __attribute__((no_instrument_function)) void Model1_dma_descriptor_unmap(struct Model1_dma_async_tx_descriptor *Model1_tx)
{
 if (Model1_tx->Model1_unmap) {
  Model1_dmaengine_unmap_put(Model1_tx->Model1_unmap);
  Model1_tx->Model1_unmap = ((void *)0);
 }
}


static inline __attribute__((no_instrument_function)) void Model1_txd_lock(struct Model1_dma_async_tx_descriptor *Model1_txd)
{
}
static inline __attribute__((no_instrument_function)) void Model1_txd_unlock(struct Model1_dma_async_tx_descriptor *Model1_txd)
{
}
static inline __attribute__((no_instrument_function)) void Model1_txd_chain(struct Model1_dma_async_tx_descriptor *Model1_txd, struct Model1_dma_async_tx_descriptor *Model1_next)
{
 do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dmaengine.h"), "i" (533), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0);
}
static inline __attribute__((no_instrument_function)) void Model1_txd_clear_parent(struct Model1_dma_async_tx_descriptor *Model1_txd)
{
}
static inline __attribute__((no_instrument_function)) void Model1_txd_clear_next(struct Model1_dma_async_tx_descriptor *Model1_txd)
{
}
static inline __attribute__((no_instrument_function)) struct Model1_dma_async_tx_descriptor *Model1_txd_next(struct Model1_dma_async_tx_descriptor *Model1_txd)
{
 return ((void *)0);
}
static inline __attribute__((no_instrument_function)) struct Model1_dma_async_tx_descriptor *Model1_txd_parent(struct Model1_dma_async_tx_descriptor *Model1_txd)
{
 return ((void *)0);
}
/**
 * struct dma_tx_state - filled in to report the status of
 * a transfer.
 * @last: last completed DMA cookie
 * @used: last issued DMA cookie (i.e. the one in progress)
 * @residue: the remaining number of bytes left to transmit
 *	on the selected transfer for states DMA_IN_PROGRESS and
 *	DMA_PAUSED if this is implemented in the driver, else 0
 */
struct Model1_dma_tx_state {
 Model1_dma_cookie_t Model1_last;
 Model1_dma_cookie_t Model1_used;
 Model1_u32 Model1_residue;
};

/**
 * enum dmaengine_alignment - defines alignment of the DMA async tx
 * buffers
 */
enum Model1_dmaengine_alignment {
 Model1_DMAENGINE_ALIGN_1_BYTE = 0,
 Model1_DMAENGINE_ALIGN_2_BYTES = 1,
 Model1_DMAENGINE_ALIGN_4_BYTES = 2,
 Model1_DMAENGINE_ALIGN_8_BYTES = 3,
 Model1_DMAENGINE_ALIGN_16_BYTES = 4,
 Model1_DMAENGINE_ALIGN_32_BYTES = 5,
 Model1_DMAENGINE_ALIGN_64_BYTES = 6,
};

/**
 * struct dma_slave_map - associates slave device and it's slave channel with
 * parameter to be used by a filter function
 * @devname: name of the device
 * @slave: slave channel name
 * @param: opaque parameter to pass to struct dma_filter.fn
 */
struct Model1_dma_slave_map {
 const char *Model1_devname;
 const char *Model1_slave;
 void *Model1_param;
};

/**
 * struct dma_filter - information for slave device/channel to filter_fn/param
 * mapping
 * @fn: filter function callback
 * @mapcnt: number of slave device/channel in the map
 * @map: array of channel to filter mapping data
 */
struct Model1_dma_filter {
 Model1_dma_filter_fn Model1_fn;
 int Model1_mapcnt;
 const struct Model1_dma_slave_map *Model1_map;
};

/**
 * struct dma_device - info on the entity supplying DMA services
 * @chancnt: how many DMA channels are supported
 * @privatecnt: how many DMA channels are requested by dma_request_channel
 * @channels: the list of struct dma_chan
 * @global_node: list_head for global dma_device_list
 * @filter: information for device/slave to filter function/param mapping
 * @cap_mask: one or more dma_capability flags
 * @max_xor: maximum number of xor sources, 0 if no capability
 * @max_pq: maximum number of PQ sources and PQ-continue capability
 * @copy_align: alignment shift for memcpy operations
 * @xor_align: alignment shift for xor operations
 * @pq_align: alignment shift for pq operations
 * @fill_align: alignment shift for memset operations
 * @dev_id: unique device ID
 * @dev: struct device reference for dma mapping api
 * @src_addr_widths: bit mask of src addr widths the device supports
 * @dst_addr_widths: bit mask of dst addr widths the device supports
 * @directions: bit mask of slave direction the device supports since
 * 	the enum dma_transfer_direction is not defined as bits for
 * 	each type of direction, the dma controller should fill (1 <<
 * 	<TYPE>) and same should be checked by controller as well
 * @max_burst: max burst capability per-transfer
 * @residue_granularity: granularity of the transfer residue reported
 *	by tx_status
 * @device_alloc_chan_resources: allocate resources and return the
 *	number of allocated descriptors
 * @device_free_chan_resources: release DMA channel's resources
 * @device_prep_dma_memcpy: prepares a memcpy operation
 * @device_prep_dma_xor: prepares a xor operation
 * @device_prep_dma_xor_val: prepares a xor validation operation
 * @device_prep_dma_pq: prepares a pq operation
 * @device_prep_dma_pq_val: prepares a pqzero_sum operation
 * @device_prep_dma_memset: prepares a memset operation
 * @device_prep_dma_memset_sg: prepares a memset operation over a scatter list
 * @device_prep_dma_interrupt: prepares an end of chain interrupt operation
 * @device_prep_slave_sg: prepares a slave dma operation
 * @device_prep_dma_cyclic: prepare a cyclic dma operation suitable for audio.
 *	The function takes a buffer of size buf_len. The callback function will
 *	be called after period_len bytes have been transferred.
 * @device_prep_interleaved_dma: Transfer expression in a generic way.
 * @device_prep_dma_imm_data: DMA's 8 byte immediate data to the dst address
 * @device_config: Pushes a new configuration to a channel, return 0 or an error
 *	code
 * @device_pause: Pauses any transfer happening on a channel. Returns
 *	0 or an error code
 * @device_resume: Resumes any transfer on a channel previously
 *	paused. Returns 0 or an error code
 * @device_terminate_all: Aborts all transfers on a channel. Returns 0
 *	or an error code
 * @device_synchronize: Synchronizes the termination of a transfers to the
 *  current context.
 * @device_tx_status: poll for transaction completion, the optional
 *	txstate parameter can be supplied with a pointer to get a
 *	struct with auxiliary transfer status information, otherwise the call
 *	will just return a simple status code
 * @device_issue_pending: push pending transactions to hardware
 * @descriptor_reuse: a submitted transfer can be resubmitted after completion
 */
struct Model1_dma_device {

 unsigned int Model1_chancnt;
 unsigned int Model1_privatecnt;
 struct Model1_list_head Model1_channels;
 struct Model1_list_head Model1_global_node;
 struct Model1_dma_filter Model1_filter;
 Model1_dma_cap_mask_t Model1_cap_mask;
 unsigned short Model1_max_xor;
 unsigned short Model1_max_pq;
 enum Model1_dmaengine_alignment Model1_copy_align;
 enum Model1_dmaengine_alignment Model1_xor_align;
 enum Model1_dmaengine_alignment Model1_pq_align;
 enum Model1_dmaengine_alignment Model1_fill_align;


 int Model1_dev_id;
 struct Model1_device *Model1_dev;

 Model1_u32 Model1_src_addr_widths;
 Model1_u32 Model1_dst_addr_widths;
 Model1_u32 Model1_directions;
 Model1_u32 Model1_max_burst;
 bool Model1_descriptor_reuse;
 enum Model1_dma_residue_granularity Model1_residue_granularity;

 int (*Model1_device_alloc_chan_resources)(struct Model1_dma_chan *Model1_chan);
 void (*Model1_device_free_chan_resources)(struct Model1_dma_chan *Model1_chan);

 struct Model1_dma_async_tx_descriptor *(*Model1_device_prep_dma_memcpy)(
  struct Model1_dma_chan *Model1_chan, Model1_dma_addr_t Model1_dst, Model1_dma_addr_t Model1_src,
  Model1_size_t Model1_len, unsigned long Model1_flags);
 struct Model1_dma_async_tx_descriptor *(*Model1_device_prep_dma_xor)(
  struct Model1_dma_chan *Model1_chan, Model1_dma_addr_t Model1_dst, Model1_dma_addr_t *Model1_src,
  unsigned int Model1_src_cnt, Model1_size_t Model1_len, unsigned long Model1_flags);
 struct Model1_dma_async_tx_descriptor *(*Model1_device_prep_dma_xor_val)(
  struct Model1_dma_chan *Model1_chan, Model1_dma_addr_t *Model1_src, unsigned int Model1_src_cnt,
  Model1_size_t Model1_len, enum Model1_sum_check_flags *Model1_result, unsigned long Model1_flags);
 struct Model1_dma_async_tx_descriptor *(*Model1_device_prep_dma_pq)(
  struct Model1_dma_chan *Model1_chan, Model1_dma_addr_t *Model1_dst, Model1_dma_addr_t *Model1_src,
  unsigned int Model1_src_cnt, const unsigned char *Model1_scf,
  Model1_size_t Model1_len, unsigned long Model1_flags);
 struct Model1_dma_async_tx_descriptor *(*Model1_device_prep_dma_pq_val)(
  struct Model1_dma_chan *Model1_chan, Model1_dma_addr_t *Model1_pq, Model1_dma_addr_t *Model1_src,
  unsigned int Model1_src_cnt, const unsigned char *Model1_scf, Model1_size_t Model1_len,
  enum Model1_sum_check_flags *Model1_pqres, unsigned long Model1_flags);
 struct Model1_dma_async_tx_descriptor *(*Model1_device_prep_dma_memset)(
  struct Model1_dma_chan *Model1_chan, Model1_dma_addr_t Model1_dest, int Model1_value, Model1_size_t Model1_len,
  unsigned long Model1_flags);
 struct Model1_dma_async_tx_descriptor *(*Model1_device_prep_dma_memset_sg)(
  struct Model1_dma_chan *Model1_chan, struct Model1_scatterlist *Model1_sg,
  unsigned int Model1_nents, int Model1_value, unsigned long Model1_flags);
 struct Model1_dma_async_tx_descriptor *(*Model1_device_prep_dma_interrupt)(
  struct Model1_dma_chan *Model1_chan, unsigned long Model1_flags);
 struct Model1_dma_async_tx_descriptor *(*Model1_device_prep_dma_sg)(
  struct Model1_dma_chan *Model1_chan,
  struct Model1_scatterlist *Model1_dst_sg, unsigned int Model1_dst_nents,
  struct Model1_scatterlist *Model1_src_sg, unsigned int Model1_src_nents,
  unsigned long Model1_flags);

 struct Model1_dma_async_tx_descriptor *(*Model1_device_prep_slave_sg)(
  struct Model1_dma_chan *Model1_chan, struct Model1_scatterlist *Model1_sgl,
  unsigned int Model1_sg_len, enum Model1_dma_transfer_direction Model1_direction,
  unsigned long Model1_flags, void *Model1_context);
 struct Model1_dma_async_tx_descriptor *(*Model1_device_prep_dma_cyclic)(
  struct Model1_dma_chan *Model1_chan, Model1_dma_addr_t Model1_buf_addr, Model1_size_t Model1_buf_len,
  Model1_size_t Model1_period_len, enum Model1_dma_transfer_direction Model1_direction,
  unsigned long Model1_flags);
 struct Model1_dma_async_tx_descriptor *(*Model1_device_prep_interleaved_dma)(
  struct Model1_dma_chan *Model1_chan, struct Model1_dma_interleaved_template *Model1_xt,
  unsigned long Model1_flags);
 struct Model1_dma_async_tx_descriptor *(*Model1_device_prep_dma_imm_data)(
  struct Model1_dma_chan *Model1_chan, Model1_dma_addr_t Model1_dst, Model1_u64 Model1_data,
  unsigned long Model1_flags);

 int (*Model1_device_config)(struct Model1_dma_chan *Model1_chan,
        struct Model1_dma_slave_config *Model1_config);
 int (*Model1_device_pause)(struct Model1_dma_chan *Model1_chan);
 int (*Model1_device_resume)(struct Model1_dma_chan *Model1_chan);
 int (*Model1_device_terminate_all)(struct Model1_dma_chan *Model1_chan);
 void (*Model1_device_synchronize)(struct Model1_dma_chan *Model1_chan);

 enum Model1_dma_status (*Model1_device_tx_status)(struct Model1_dma_chan *Model1_chan,
         Model1_dma_cookie_t Model1_cookie,
         struct Model1_dma_tx_state *Model1_txstate);
 void (*Model1_device_issue_pending)(struct Model1_dma_chan *Model1_chan);
};

static inline __attribute__((no_instrument_function)) int Model1_dmaengine_slave_config(struct Model1_dma_chan *Model1_chan,
       struct Model1_dma_slave_config *Model1_config)
{
 if (Model1_chan->Model1_device->Model1_device_config)
  return Model1_chan->Model1_device->Model1_device_config(Model1_chan, Model1_config);

 return -38;
}

static inline __attribute__((no_instrument_function)) bool Model1_is_slave_direction(enum Model1_dma_transfer_direction Model1_direction)
{
 return (Model1_direction == Model1_DMA_MEM_TO_DEV) || (Model1_direction == Model1_DMA_DEV_TO_MEM);
}

static inline __attribute__((no_instrument_function)) struct Model1_dma_async_tx_descriptor *Model1_dmaengine_prep_slave_single(
 struct Model1_dma_chan *Model1_chan, Model1_dma_addr_t Model1_buf, Model1_size_t Model1_len,
 enum Model1_dma_transfer_direction Model1_dir, unsigned long Model1_flags)
{
 struct Model1_scatterlist Model1_sg;
 Model1_sg_init_table(&Model1_sg, 1);
 ((&Model1_sg)->Model1_dma_address) = Model1_buf;
 ((&Model1_sg)->Model1_dma_length) = Model1_len;

 if (!Model1_chan || !Model1_chan->Model1_device || !Model1_chan->Model1_device->Model1_device_prep_slave_sg)
  return ((void *)0);

 return Model1_chan->Model1_device->Model1_device_prep_slave_sg(Model1_chan, &Model1_sg, 1,
        Model1_dir, Model1_flags, ((void *)0));
}

static inline __attribute__((no_instrument_function)) struct Model1_dma_async_tx_descriptor *Model1_dmaengine_prep_slave_sg(
 struct Model1_dma_chan *Model1_chan, struct Model1_scatterlist *Model1_sgl, unsigned int Model1_sg_len,
 enum Model1_dma_transfer_direction Model1_dir, unsigned long Model1_flags)
{
 if (!Model1_chan || !Model1_chan->Model1_device || !Model1_chan->Model1_device->Model1_device_prep_slave_sg)
  return ((void *)0);

 return Model1_chan->Model1_device->Model1_device_prep_slave_sg(Model1_chan, Model1_sgl, Model1_sg_len,
        Model1_dir, Model1_flags, ((void *)0));
}
static inline __attribute__((no_instrument_function)) struct Model1_dma_async_tx_descriptor *Model1_dmaengine_prep_dma_cyclic(
  struct Model1_dma_chan *Model1_chan, Model1_dma_addr_t Model1_buf_addr, Model1_size_t Model1_buf_len,
  Model1_size_t Model1_period_len, enum Model1_dma_transfer_direction Model1_dir,
  unsigned long Model1_flags)
{
 if (!Model1_chan || !Model1_chan->Model1_device || !Model1_chan->Model1_device->Model1_device_prep_dma_cyclic)
  return ((void *)0);

 return Model1_chan->Model1_device->Model1_device_prep_dma_cyclic(Model1_chan, Model1_buf_addr, Model1_buf_len,
      Model1_period_len, Model1_dir, Model1_flags);
}

static inline __attribute__((no_instrument_function)) struct Model1_dma_async_tx_descriptor *Model1_dmaengine_prep_interleaved_dma(
  struct Model1_dma_chan *Model1_chan, struct Model1_dma_interleaved_template *Model1_xt,
  unsigned long Model1_flags)
{
 if (!Model1_chan || !Model1_chan->Model1_device || !Model1_chan->Model1_device->Model1_device_prep_interleaved_dma)
  return ((void *)0);

 return Model1_chan->Model1_device->Model1_device_prep_interleaved_dma(Model1_chan, Model1_xt, Model1_flags);
}

static inline __attribute__((no_instrument_function)) struct Model1_dma_async_tx_descriptor *Model1_dmaengine_prep_dma_memset(
  struct Model1_dma_chan *Model1_chan, Model1_dma_addr_t Model1_dest, int Model1_value, Model1_size_t Model1_len,
  unsigned long Model1_flags)
{
 if (!Model1_chan || !Model1_chan->Model1_device || !Model1_chan->Model1_device->Model1_device_prep_dma_memset)
  return ((void *)0);

 return Model1_chan->Model1_device->Model1_device_prep_dma_memset(Model1_chan, Model1_dest, Model1_value,
          Model1_len, Model1_flags);
}

static inline __attribute__((no_instrument_function)) struct Model1_dma_async_tx_descriptor *Model1_dmaengine_prep_dma_sg(
  struct Model1_dma_chan *Model1_chan,
  struct Model1_scatterlist *Model1_dst_sg, unsigned int Model1_dst_nents,
  struct Model1_scatterlist *Model1_src_sg, unsigned int Model1_src_nents,
  unsigned long Model1_flags)
{
 if (!Model1_chan || !Model1_chan->Model1_device || !Model1_chan->Model1_device->Model1_device_prep_dma_sg)
  return ((void *)0);

 return Model1_chan->Model1_device->Model1_device_prep_dma_sg(Model1_chan, Model1_dst_sg, Model1_dst_nents,
   Model1_src_sg, Model1_src_nents, Model1_flags);
}

/**
 * dmaengine_terminate_all() - Terminate all active DMA transfers
 * @chan: The channel for which to terminate the transfers
 *
 * This function is DEPRECATED use either dmaengine_terminate_sync() or
 * dmaengine_terminate_async() instead.
 */
static inline __attribute__((no_instrument_function)) int Model1_dmaengine_terminate_all(struct Model1_dma_chan *Model1_chan)
{
 if (Model1_chan->Model1_device->Model1_device_terminate_all)
  return Model1_chan->Model1_device->Model1_device_terminate_all(Model1_chan);

 return -38;
}

/**
 * dmaengine_terminate_async() - Terminate all active DMA transfers
 * @chan: The channel for which to terminate the transfers
 *
 * Calling this function will terminate all active and pending descriptors
 * that have previously been submitted to the channel. It is not guaranteed
 * though that the transfer for the active descriptor has stopped when the
 * function returns. Furthermore it is possible the complete callback of a
 * submitted transfer is still running when this function returns.
 *
 * dmaengine_synchronize() needs to be called before it is safe to free
 * any memory that is accessed by previously submitted descriptors or before
 * freeing any resources accessed from within the completion callback of any
 * perviously submitted descriptors.
 *
 * This function can be called from atomic context as well as from within a
 * complete callback of a descriptor submitted on the same channel.
 *
 * If none of the two conditions above apply consider using
 * dmaengine_terminate_sync() instead.
 */
static inline __attribute__((no_instrument_function)) int Model1_dmaengine_terminate_async(struct Model1_dma_chan *Model1_chan)
{
 if (Model1_chan->Model1_device->Model1_device_terminate_all)
  return Model1_chan->Model1_device->Model1_device_terminate_all(Model1_chan);

 return -22;
}

/**
 * dmaengine_synchronize() - Synchronize DMA channel termination
 * @chan: The channel to synchronize
 *
 * Synchronizes to the DMA channel termination to the current context. When this
 * function returns it is guaranteed that all transfers for previously issued
 * descriptors have stopped and and it is safe to free the memory assoicated
 * with them. Furthermore it is guaranteed that all complete callback functions
 * for a previously submitted descriptor have finished running and it is safe to
 * free resources accessed from within the complete callbacks.
 *
 * The behavior of this function is undefined if dma_async_issue_pending() has
 * been called between dmaengine_terminate_async() and this function.
 *
 * This function must only be called from non-atomic context and must not be
 * called from within a complete callback of a descriptor submitted on the same
 * channel.
 */
static inline __attribute__((no_instrument_function)) void Model1_dmaengine_synchronize(struct Model1_dma_chan *Model1_chan)
{
 do { Model1__cond_resched(); } while (0);

 if (Model1_chan->Model1_device->Model1_device_synchronize)
  Model1_chan->Model1_device->Model1_device_synchronize(Model1_chan);
}

/**
 * dmaengine_terminate_sync() - Terminate all active DMA transfers
 * @chan: The channel for which to terminate the transfers
 *
 * Calling this function will terminate all active and pending transfers
 * that have previously been submitted to the channel. It is similar to
 * dmaengine_terminate_async() but guarantees that the DMA transfer has actually
 * stopped and that all complete callbacks have finished running when the
 * function returns.
 *
 * This function must only be called from non-atomic context and must not be
 * called from within a complete callback of a descriptor submitted on the same
 * channel.
 */
static inline __attribute__((no_instrument_function)) int Model1_dmaengine_terminate_sync(struct Model1_dma_chan *Model1_chan)
{
 int Model1_ret;

 Model1_ret = Model1_dmaengine_terminate_async(Model1_chan);
 if (Model1_ret)
  return Model1_ret;

 Model1_dmaengine_synchronize(Model1_chan);

 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_dmaengine_pause(struct Model1_dma_chan *Model1_chan)
{
 if (Model1_chan->Model1_device->Model1_device_pause)
  return Model1_chan->Model1_device->Model1_device_pause(Model1_chan);

 return -38;
}

static inline __attribute__((no_instrument_function)) int Model1_dmaengine_resume(struct Model1_dma_chan *Model1_chan)
{
 if (Model1_chan->Model1_device->Model1_device_resume)
  return Model1_chan->Model1_device->Model1_device_resume(Model1_chan);

 return -38;
}

static inline __attribute__((no_instrument_function)) enum Model1_dma_status Model1_dmaengine_tx_status(struct Model1_dma_chan *Model1_chan,
 Model1_dma_cookie_t Model1_cookie, struct Model1_dma_tx_state *Model1_state)
{
 return Model1_chan->Model1_device->Model1_device_tx_status(Model1_chan, Model1_cookie, Model1_state);
}

static inline __attribute__((no_instrument_function)) Model1_dma_cookie_t Model1_dmaengine_submit(struct Model1_dma_async_tx_descriptor *Model1_desc)
{
 return Model1_desc->Model1_tx_submit(Model1_desc);
}

static inline __attribute__((no_instrument_function)) bool Model1_dmaengine_check_align(enum Model1_dmaengine_alignment Model1_align,
      Model1_size_t Model1_off1, Model1_size_t Model1_off2, Model1_size_t Model1_len)
{
 Model1_size_t Model1_mask;

 if (!Model1_align)
  return true;
 Model1_mask = (1 << Model1_align) - 1;
 if (Model1_mask & (Model1_off1 | Model1_off2 | Model1_len))
  return false;
 return true;
}

static inline __attribute__((no_instrument_function)) bool Model1_is_dma_copy_aligned(struct Model1_dma_device *Model1_dev, Model1_size_t Model1_off1,
           Model1_size_t Model1_off2, Model1_size_t Model1_len)
{
 return Model1_dmaengine_check_align(Model1_dev->Model1_copy_align, Model1_off1, Model1_off2, Model1_len);
}

static inline __attribute__((no_instrument_function)) bool Model1_is_dma_xor_aligned(struct Model1_dma_device *Model1_dev, Model1_size_t Model1_off1,
          Model1_size_t Model1_off2, Model1_size_t Model1_len)
{
 return Model1_dmaengine_check_align(Model1_dev->Model1_xor_align, Model1_off1, Model1_off2, Model1_len);
}

static inline __attribute__((no_instrument_function)) bool Model1_is_dma_pq_aligned(struct Model1_dma_device *Model1_dev, Model1_size_t Model1_off1,
         Model1_size_t Model1_off2, Model1_size_t Model1_len)
{
 return Model1_dmaengine_check_align(Model1_dev->Model1_pq_align, Model1_off1, Model1_off2, Model1_len);
}

static inline __attribute__((no_instrument_function)) bool Model1_is_dma_fill_aligned(struct Model1_dma_device *Model1_dev, Model1_size_t Model1_off1,
           Model1_size_t Model1_off2, Model1_size_t Model1_len)
{
 return Model1_dmaengine_check_align(Model1_dev->Model1_fill_align, Model1_off1, Model1_off2, Model1_len);
}

static inline __attribute__((no_instrument_function)) void
Model1_dma_set_maxpq(struct Model1_dma_device *Model1_dma, int Model1_maxpq, int Model1_has_pq_continue)
{
 Model1_dma->Model1_max_pq = Model1_maxpq;
 if (Model1_has_pq_continue)
  Model1_dma->Model1_max_pq |= (1 << 15);
}

static inline __attribute__((no_instrument_function)) bool Model1_dmaf_continue(enum Model1_dma_ctrl_flags Model1_flags)
{
 return (Model1_flags & Model1_DMA_PREP_CONTINUE) == Model1_DMA_PREP_CONTINUE;
}

static inline __attribute__((no_instrument_function)) bool Model1_dmaf_p_disabled_continue(enum Model1_dma_ctrl_flags Model1_flags)
{
 enum Model1_dma_ctrl_flags Model1_mask = Model1_DMA_PREP_CONTINUE | Model1_DMA_PREP_PQ_DISABLE_P;

 return (Model1_flags & Model1_mask) == Model1_mask;
}

static inline __attribute__((no_instrument_function)) bool Model1_dma_dev_has_pq_continue(struct Model1_dma_device *Model1_dma)
{
 return (Model1_dma->Model1_max_pq & (1 << 15)) == (1 << 15);
}

static inline __attribute__((no_instrument_function)) unsigned short Model1_dma_dev_to_maxpq(struct Model1_dma_device *Model1_dma)
{
 return Model1_dma->Model1_max_pq & ~(1 << 15);
}

/* dma_maxpq - reduce maxpq in the face of continued operations
 * @dma - dma device with PQ capability
 * @flags - to check if DMA_PREP_CONTINUE and DMA_PREP_PQ_DISABLE_P are set
 *
 * When an engine does not support native continuation we need 3 extra
 * source slots to reuse P and Q with the following coefficients:
 * 1/ {00} * P : remove P from Q', but use it as a source for P'
 * 2/ {01} * Q : use Q to continue Q' calculation
 * 3/ {00} * Q : subtract Q from P' to cancel (2)
 *
 * In the case where P is disabled we only need 1 extra source:
 * 1/ {01} * Q : use Q to continue Q' calculation
 */
static inline __attribute__((no_instrument_function)) int Model1_dma_maxpq(struct Model1_dma_device *Model1_dma, enum Model1_dma_ctrl_flags Model1_flags)
{
 if (Model1_dma_dev_has_pq_continue(Model1_dma) || !Model1_dmaf_continue(Model1_flags))
  return Model1_dma_dev_to_maxpq(Model1_dma);
 else if (Model1_dmaf_p_disabled_continue(Model1_flags))
  return Model1_dma_dev_to_maxpq(Model1_dma) - 1;
 else if (Model1_dmaf_continue(Model1_flags))
  return Model1_dma_dev_to_maxpq(Model1_dma) - 3;
 do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dmaengine.h"), "i" (1098), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0);
}

static inline __attribute__((no_instrument_function)) Model1_size_t Model1_dmaengine_get_icg(bool Model1_inc, bool Model1_sgl, Model1_size_t Model1_icg,
          Model1_size_t Model1_dir_icg)
{
 if (Model1_inc) {
  if (Model1_dir_icg)
   return Model1_dir_icg;
  else if (Model1_sgl)
   return Model1_icg;
 }

 return 0;
}

static inline __attribute__((no_instrument_function)) Model1_size_t Model1_dmaengine_get_dst_icg(struct Model1_dma_interleaved_template *Model1_xt,
        struct Model1_data_chunk *Model1_chunk)
{
 return Model1_dmaengine_get_icg(Model1_xt->Model1_dst_inc, Model1_xt->Model1_dst_sgl,
     Model1_chunk->Model1_icg, Model1_chunk->Model1_dst_icg);
}

static inline __attribute__((no_instrument_function)) Model1_size_t Model1_dmaengine_get_src_icg(struct Model1_dma_interleaved_template *Model1_xt,
        struct Model1_data_chunk *Model1_chunk)
{
 return Model1_dmaengine_get_icg(Model1_xt->Model1_src_inc, Model1_xt->Model1_src_sgl,
     Model1_chunk->Model1_icg, Model1_chunk->Model1_src_icg);
}

/* --- public DMA engine API --- */


void Model1_dmaengine_get(void);
void Model1_dmaengine_put(void);
static inline __attribute__((no_instrument_function)) void Model1_async_dmaengine_get(void)
{
}
static inline __attribute__((no_instrument_function)) void Model1_async_dmaengine_put(void)
{
}
static inline __attribute__((no_instrument_function)) struct Model1_dma_chan *
Model1_async_dma_find_channel(enum Model1_dma_transaction_type Model1_type)
{
 return ((void *)0);
}

void Model1_dma_async_tx_descriptor_init(struct Model1_dma_async_tx_descriptor *Model1_tx,
      struct Model1_dma_chan *Model1_chan);

static inline __attribute__((no_instrument_function)) void Model1_async_tx_ack(struct Model1_dma_async_tx_descriptor *Model1_tx)
{
 Model1_tx->Model1_flags |= Model1_DMA_CTRL_ACK;
}

static inline __attribute__((no_instrument_function)) void Model1_async_tx_clear_ack(struct Model1_dma_async_tx_descriptor *Model1_tx)
{
 Model1_tx->Model1_flags &= ~Model1_DMA_CTRL_ACK;
}

static inline __attribute__((no_instrument_function)) bool Model1_async_tx_test_ack(struct Model1_dma_async_tx_descriptor *Model1_tx)
{
 return (Model1_tx->Model1_flags & Model1_DMA_CTRL_ACK) == Model1_DMA_CTRL_ACK;
}


static inline __attribute__((no_instrument_function)) void
Model1___dma_cap_set(enum Model1_dma_transaction_type Model1_tx_type, Model1_dma_cap_mask_t *Model1_dstp)
{
 Model1_set_bit(Model1_tx_type, Model1_dstp->Model1_bits);
}


static inline __attribute__((no_instrument_function)) void
Model1___dma_cap_clear(enum Model1_dma_transaction_type Model1_tx_type, Model1_dma_cap_mask_t *Model1_dstp)
{
 Model1_clear_bit(Model1_tx_type, Model1_dstp->Model1_bits);
}


static inline __attribute__((no_instrument_function)) void Model1___dma_cap_zero(Model1_dma_cap_mask_t *Model1_dstp)
{
 Model1_bitmap_zero(Model1_dstp->Model1_bits, Model1_DMA_TX_TYPE_END);
}


static inline __attribute__((no_instrument_function)) int
Model1___dma_has_cap(enum Model1_dma_transaction_type Model1_tx_type, Model1_dma_cap_mask_t *Model1_srcp)
{
 return (__builtin_constant_p((Model1_tx_type)) ? Model1_constant_test_bit((Model1_tx_type), (Model1_srcp->Model1_bits)) : Model1_variable_test_bit((Model1_tx_type), (Model1_srcp->Model1_bits)));
}




/**
 * dma_async_issue_pending - flush pending transactions to HW
 * @chan: target DMA channel
 *
 * This allows drivers to push copies to HW in batches,
 * reducing MMIO writes where possible.
 */
static inline __attribute__((no_instrument_function)) void Model1_dma_async_issue_pending(struct Model1_dma_chan *Model1_chan)
{
 Model1_chan->Model1_device->Model1_device_issue_pending(Model1_chan);
}

/**
 * dma_async_is_tx_complete - poll for transaction completion
 * @chan: DMA channel
 * @cookie: transaction identifier to check status of
 * @last: returns last completed cookie, can be NULL
 * @used: returns last issued cookie, can be NULL
 *
 * If @last and @used are passed in, upon return they reflect the driver
 * internal state and can be used with dma_async_is_complete() to check
 * the status of multiple cookies without re-checking hardware state.
 */
static inline __attribute__((no_instrument_function)) enum Model1_dma_status Model1_dma_async_is_tx_complete(struct Model1_dma_chan *Model1_chan,
 Model1_dma_cookie_t Model1_cookie, Model1_dma_cookie_t *Model1_last, Model1_dma_cookie_t *Model1_used)
{
 struct Model1_dma_tx_state Model1_state;
 enum Model1_dma_status Model1_status;

 Model1_status = Model1_chan->Model1_device->Model1_device_tx_status(Model1_chan, Model1_cookie, &Model1_state);
 if (Model1_last)
  *Model1_last = Model1_state.Model1_last;
 if (Model1_used)
  *Model1_used = Model1_state.Model1_used;
 return Model1_status;
}

/**
 * dma_async_is_complete - test a cookie against chan state
 * @cookie: transaction identifier to test status of
 * @last_complete: last know completed transaction
 * @last_used: last cookie value handed out
 *
 * dma_async_is_complete() is used in dma_async_is_tx_complete()
 * the test logic is separated for lightweight testing of multiple cookies
 */
static inline __attribute__((no_instrument_function)) enum Model1_dma_status Model1_dma_async_is_complete(Model1_dma_cookie_t Model1_cookie,
   Model1_dma_cookie_t Model1_last_complete, Model1_dma_cookie_t Model1_last_used)
{
 if (Model1_last_complete <= Model1_last_used) {
  if ((Model1_cookie <= Model1_last_complete) || (Model1_cookie > Model1_last_used))
   return Model1_DMA_COMPLETE;
 } else {
  if ((Model1_cookie <= Model1_last_complete) && (Model1_cookie > Model1_last_used))
   return Model1_DMA_COMPLETE;
 }
 return Model1_DMA_IN_PROGRESS;
}

static inline __attribute__((no_instrument_function)) void
Model1_dma_set_tx_state(struct Model1_dma_tx_state *Model1_st, Model1_dma_cookie_t Model1_last, Model1_dma_cookie_t Model1_used, Model1_u32 Model1_residue)
{
 if (Model1_st) {
  Model1_st->Model1_last = Model1_last;
  Model1_st->Model1_used = Model1_used;
  Model1_st->Model1_residue = Model1_residue;
 }
}


struct Model1_dma_chan *Model1_dma_find_channel(enum Model1_dma_transaction_type Model1_tx_type);
enum Model1_dma_status Model1_dma_sync_wait(struct Model1_dma_chan *Model1_chan, Model1_dma_cookie_t Model1_cookie);
enum Model1_dma_status Model1_dma_wait_for_async_tx(struct Model1_dma_async_tx_descriptor *Model1_tx);
void Model1_dma_issue_pending_all(void);
struct Model1_dma_chan *Model1___dma_request_channel(const Model1_dma_cap_mask_t *Model1_mask,
     Model1_dma_filter_fn Model1_fn, void *Model1_fn_param);
struct Model1_dma_chan *Model1_dma_request_slave_channel(struct Model1_device *Model1_dev, const char *Model1_name);

struct Model1_dma_chan *Model1_dma_request_chan(struct Model1_device *Model1_dev, const char *Model1_name);
struct Model1_dma_chan *Model1_dma_request_chan_by_mask(const Model1_dma_cap_mask_t *Model1_mask);

void Model1_dma_release_channel(struct Model1_dma_chan *Model1_chan);
int Model1_dma_get_slave_caps(struct Model1_dma_chan *Model1_chan, struct Model1_dma_slave_caps *Model1_caps);
static inline __attribute__((no_instrument_function)) int Model1_dmaengine_desc_set_reuse(struct Model1_dma_async_tx_descriptor *Model1_tx)
{
 struct Model1_dma_slave_caps Model1_caps;

 Model1_dma_get_slave_caps(Model1_tx->Model1_chan, &Model1_caps);

 if (Model1_caps.Model1_descriptor_reuse) {
  Model1_tx->Model1_flags |= Model1_DMA_CTRL_REUSE;
  return 0;
 } else {
  return -1;
 }
}

static inline __attribute__((no_instrument_function)) void Model1_dmaengine_desc_clear_reuse(struct Model1_dma_async_tx_descriptor *Model1_tx)
{
 Model1_tx->Model1_flags &= ~Model1_DMA_CTRL_REUSE;
}

static inline __attribute__((no_instrument_function)) bool Model1_dmaengine_desc_test_reuse(struct Model1_dma_async_tx_descriptor *Model1_tx)
{
 return (Model1_tx->Model1_flags & Model1_DMA_CTRL_REUSE) == Model1_DMA_CTRL_REUSE;
}

static inline __attribute__((no_instrument_function)) int Model1_dmaengine_desc_free(struct Model1_dma_async_tx_descriptor *Model1_desc)
{
 /* this is supported for reusable desc, so check that */
 if (Model1_dmaengine_desc_test_reuse(Model1_desc))
  return Model1_desc->Model1_desc_free(Model1_desc);
 else
  return -1;
}

/* --- DMA device --- */

int Model1_dma_async_device_register(struct Model1_dma_device *Model1_device);
void Model1_dma_async_device_unregister(struct Model1_dma_device *Model1_device);
void Model1_dma_run_dependencies(struct Model1_dma_async_tx_descriptor *Model1_tx);
struct Model1_dma_chan *Model1_dma_get_slave_channel(struct Model1_dma_chan *Model1_chan);
struct Model1_dma_chan *Model1_dma_get_any_slave_channel(struct Model1_dma_device *Model1_device);




static inline __attribute__((no_instrument_function)) struct Model1_dma_chan
*Model1___dma_request_slave_channel_compat(const Model1_dma_cap_mask_t *Model1_mask,
      Model1_dma_filter_fn Model1_fn, void *Model1_fn_param,
      struct Model1_device *Model1_dev, const char *Model1_name)
{
 struct Model1_dma_chan *Model1_chan;

 Model1_chan = Model1_dma_request_slave_channel(Model1_dev, Model1_name);
 if (Model1_chan)
  return Model1_chan;

 if (!Model1_fn || !Model1_fn_param)
  return ((void *)0);

 return Model1___dma_request_channel(Model1_mask, Model1_fn, Model1_fn_param);
}

/*
 * Dynamic queue limits (dql) - Definitions
 *
 * Copyright (c) 2011, Tom Herbert <therbert@google.com>
 *
 * This header file contains the definitions for dynamic queue limits (dql).
 * dql would be used in conjunction with a producer/consumer type queue
 * (possibly a HW queue).  Such a queue would have these general properties:
 *
 *   1) Objects are queued up to some limit specified as number of objects.
 *   2) Periodically a completion process executes which retires consumed
 *      objects.
 *   3) Starvation occurs when limit has been reached, all queued data has
 *      actually been consumed, but completion processing has not yet run
 *      so queuing new data is blocked.
 *   4) Minimizing the amount of queued data is desirable.
 *
 * The goal of dql is to calculate the limit as the minimum number of objects
 * needed to prevent starvation.
 *
 * The primary functions of dql are:
 *    dql_queued - called when objects are enqueued to record number of objects
 *    dql_avail - returns how many objects are available to be queued based
 *      on the object limit and how many objects are already enqueued
 *    dql_completed - called at completion time to indicate how many objects
 *      were retired from the queue
 *
 * The dql implementation does not implement any locking for the dql data
 * structures, the higher layer should provide this.  dql_queued should
 * be serialized to prevent concurrent execution of the function; this
 * is also true for  dql_completed.  However, dql_queued and dlq_completed  can
 * be executed concurrently (i.e. they can be protected by different locks).
 */






struct Model1_dql {
 /* Fields accessed in enqueue path (dql_queued) */
 unsigned int Model1_num_queued; /* Total ever queued */
 unsigned int Model1_adj_limit; /* limit + num_completed */
 unsigned int Model1_last_obj_cnt; /* Count at last queuing */

 /* Fields accessed only by completion path (dql_completed) */

 unsigned int Model1_limit __attribute__((__aligned__((1 << (6))))); /* Current limit */
 unsigned int Model1_num_completed; /* Total ever completed */

 unsigned int Model1_prev_ovlimit; /* Previous over limit */
 unsigned int Model1_prev_num_queued; /* Previous queue total */
 unsigned int Model1_prev_last_obj_cnt; /* Previous queuing cnt */

 unsigned int Model1_lowest_slack; /* Lowest slack found */
 unsigned long Model1_slack_start_time; /* Time slacks seen */

 /* Configuration */
 unsigned int Model1_max_limit; /* Max limit */
 unsigned int Model1_min_limit; /* Minimum limit */
 unsigned int Model1_slack_hold_time; /* Time to measure slack */
};

/* Set some static maximums */



/*
 * Record number of objects queued. Assumes that caller has already checked
 * availability in the queue with dql_avail.
 */
static inline __attribute__((no_instrument_function)) void Model1_dql_queued(struct Model1_dql *Model1_dql, unsigned int Model1_count)
{
 do { if (__builtin_expect(!!(Model1_count > ((~0U) / 16)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dynamic_queue_limits.h"), "i" (74), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);

 Model1_dql->Model1_last_obj_cnt = Model1_count;

 /* We want to force a write first, so that cpu do not attempt
	 * to get cache line containing last_obj_cnt, num_queued, adj_limit
	 * in Shared state, but directly does a Request For Ownership
	 * It is only a hint, we use barrier() only.
	 */
 __asm__ __volatile__("": : :"memory");

 Model1_dql->Model1_num_queued += Model1_count;
}

/* Returns how many objects can be queued, < 0 indicates over limit. */
static inline __attribute__((no_instrument_function)) int Model1_dql_avail(const struct Model1_dql *Model1_dql)
{
 return (*({ __attribute__((unused)) typeof(Model1_dql->Model1_adj_limit) Model1___var = ( typeof(Model1_dql->Model1_adj_limit)) 0; (volatile typeof(Model1_dql->Model1_adj_limit) *)&(Model1_dql->Model1_adj_limit); })) - (*({ __attribute__((unused)) typeof(Model1_dql->Model1_num_queued) Model1___var = ( typeof(Model1_dql->Model1_num_queued)) 0; (volatile typeof(Model1_dql->Model1_num_queued) *)&(Model1_dql->Model1_num_queued); }));
}

/* Record number of completed objects and recalculate the limit. */
void Model1_dql_completed(struct Model1_dql *Model1_dql, unsigned int Model1_count);

/* Reset dql state */
void Model1_dql_reset(struct Model1_dql *Model1_dql);

/* Initialize dql state */
int Model1_dql_init(struct Model1_dql *Model1_dql, unsigned Model1_hold_time);

/*
 * ethtool.h: Defines for Linux ethtool.
 *
 * Copyright (C) 1998 David S. Miller (davem@redhat.com)
 * Copyright 2001 Jeff Garzik <jgarzik@pobox.com>
 * Portions Copyright 2001 Sun Microsystems (thockin@sun.com)
 * Portions Copyright 2002 Intel (eli.kupermann@intel.com,
 *                                christopher.leech@intel.com,
 *                                scott.feldman@intel.com)
 * Portions Copyright (C) Sun Microsystems 2008
 */







/*
 * These are the type definitions for the architecture specific
 * syscall compatibility layer.
 */










/* Socket-level I/O control calls. */

/* For setsockopt(2) */
/* Security levels - as per NRL IPv6 - don't actually do anything */






/* Socket filtering */
/* Instruct lower device to use last 4-bytes of skb data as FCS */
/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions of the socket-level I/O control calls.
 *
 * Version:	@(#)sockios.h	1.0.2	03/09/93
 *
 * Authors:	Ross Biro
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */





/* Linux-specific socket ioctls */



/* Routing table calls. */




/* Socket configuration controls. */
/* SIOCGIFDIVERT was:	0x8944		Frame diversion support */
/* SIOCSIFDIVERT was:	0x8945		Set frame diversion options */
/* ARP cache control calls. */
      /*  0x8950 - 0x8952  * obsolete calls, don't re-use */




/* RARP cache control calls. */




/* Driver configuration calls */




/* DLCI configuration calls */







/* bonding calls */
/* bridge calls */





/* hardware time stamping: parameters in linux/net_tstamp.h */



/* Device private ioctl calls */

/*
 *	These 16 ioctls are available to devices via the do_ioctl() device
 *	vector. Each device should include this file and redefine these names
 *	as their own. Because these are device dependent it is a good idea
 *	_NOT_ to issue them to random objects and hope.
 *
 *	THESE IOCTLS ARE _DEPRECATED_ AND WILL DISAPPEAR IN 2.5.X -DaveM
 */



/*
 *	These 16 ioctl calls are protocol private
 */






/*
 * Desired design of maximum size and alignment (see RFC2553)
 */


    /* Implementation specific desired alignment */

typedef unsigned short Model1___kernel_sa_family_t;

struct Model1___kernel_sockaddr_storage {
 Model1___kernel_sa_family_t Model1_ss_family; /* address family */
 /* Following field(s) are implementation specific */
 char Model1___data[128 - sizeof(unsigned short)];
    /* space to achieve desired size, */
    /* _SS_MAXSIZE value minus size of ss_family */
} __attribute__ ((aligned((__alignof__ (struct Model1_sockaddr *))))); /* force desired alignment */

struct Model1_pid;
struct Model1_cred;





struct Model1_seq_file;
extern void Model1_socket_seq_show(struct Model1_seq_file *Model1_seq);


typedef Model1___kernel_sa_family_t Model1_sa_family_t;

/*
 *	1003.1g requires sa_family_t and that sa_data is char.
 */

struct Model1_sockaddr {
 Model1_sa_family_t Model1_sa_family; /* address family, AF_xxx	*/
 char Model1_sa_data[14]; /* 14 bytes of protocol address	*/
};

struct Model1_linger {
 int Model1_l_onoff; /* Linger active		*/
 int Model1_l_linger; /* How long to linger for	*/
};



/*
 *	As we do 4.4BSD message passing we use a 4.4BSD message passing
 *	system, not 4.3. Thus msg_accrights(len) are now missing. They
 *	belong in an obscure libc emulation or the bin.
 */

struct Model1_msghdr {
 void *Model1_msg_name; /* ptr to socket address structure */
 int Model1_msg_namelen; /* size of socket address structure */
 struct Model1_iov_iter Model1_msg_iter; /* data */
 void *Model1_msg_control; /* ancillary data */
 Model1___kernel_size_t Model1_msg_controllen; /* ancillary data buffer length */
 unsigned int Model1_msg_flags; /* flags on received message */
 struct Model1_kiocb *Model1_msg_iocb; /* ptr to iocb for async requests */
};

struct Model1_user_msghdr {
 void *Model1_msg_name; /* ptr to socket address structure */
 int Model1_msg_namelen; /* size of socket address structure */
 struct Model1_iovec *Model1_msg_iov; /* scatter/gather array */
 Model1___kernel_size_t Model1_msg_iovlen; /* # elements in msg_iov */
 void *Model1_msg_control; /* ancillary data */
 Model1___kernel_size_t Model1_msg_controllen; /* ancillary data buffer length */
 unsigned int Model1_msg_flags; /* flags on received message */
};

/* For recvmmsg/sendmmsg */
struct Model1_mmsghdr {
 struct Model1_user_msghdr Model1_msg_hdr;
 unsigned int Model1_msg_len;
};

/*
 *	POSIX 1003.1g - ancillary data object information
 *	Ancillary data consits of a sequence of pairs of
 *	(cmsghdr, cmsg_data[])
 */

struct Model1_cmsghdr {
 Model1___kernel_size_t Model1_cmsg_len; /* data byte count, including hdr */
        int Model1_cmsg_level; /* originating protocol */
        int Model1_cmsg_type; /* protocol-specific type */
};

/*
 *	Ancillary data object information MACROS
 *	Table 5-14 of POSIX 1003.1g
 */
/*
 *	Get the next cmsg header
 *
 *	PLEASE, do not touch this function. If you think, that it is
 *	incorrect, grep kernel sources and think about consequences
 *	before trying to improve it.
 *
 *	Now it always returns valid, not truncated ancillary object
 *	HEADER. But caller still MUST check, that cmsg->cmsg_len is
 *	inside range, given by msg->msg_controllen before using
 *	ancillary object DATA.				--ANK (980731)
 */

static inline __attribute__((no_instrument_function)) struct Model1_cmsghdr * Model1___cmsg_nxthdr(void *Model1___ctl, Model1___kernel_size_t Model1___size,
            struct Model1_cmsghdr *Model1___cmsg)
{
 struct Model1_cmsghdr * Model1___ptr;

 Model1___ptr = (struct Model1_cmsghdr*)(((unsigned char *) Model1___cmsg) + ( ((Model1___cmsg->Model1_cmsg_len)+sizeof(long)-1) & ~(sizeof(long)-1) ));
 if ((unsigned long)((char*)(Model1___ptr+1) - (char *) Model1___ctl) > Model1___size)
  return (struct Model1_cmsghdr *)0;

 return Model1___ptr;
}

static inline __attribute__((no_instrument_function)) struct Model1_cmsghdr * Model1_cmsg_nxthdr (struct Model1_msghdr *Model1___msg, struct Model1_cmsghdr *Model1___cmsg)
{
 return Model1___cmsg_nxthdr(Model1___msg->Model1_msg_control, Model1___msg->Model1_msg_controllen, Model1___cmsg);
}

static inline __attribute__((no_instrument_function)) Model1_size_t Model1_msg_data_left(struct Model1_msghdr *Model1_msg)
{
 return Model1_iov_iter_count(&Model1_msg->Model1_msg_iter);
}

/* "Socket"-level control message types: */





struct Model1_ucred {
 __u32 Model1_pid;
 __u32 Model1_uid;
 __u32 Model1_gid;
};

/* Supported address families. */
/* Protocol families, same as address families. */
/* Maximum queue length specifiable by listen.  */


/* Flags we can use with send/ and recv. 
   Added those for 1003.1g not all are supported yet
 */
/* Setsockoptions(2) level. Thanks to BSD these must match IPPROTO_xxx */

/* #define SOL_ICMP	1	No-no-no! Due to Linux :-) we cannot use SOL_ICMP=1 */
/* IPX options */


extern int Model1_move_addr_to_kernel(void *Model1_uaddr, int Model1_ulen, struct Model1___kernel_sockaddr_storage *Model1_kaddr);
extern int Model1_put_cmsg(struct Model1_msghdr*, int Model1_level, int Model1_type, int Model1_len, void *Model1_data);

struct Model1_timespec;

/* The __sys_...msg variants allow MSG_CMSG_COMPAT */
extern long Model1___sys_recvmsg(int Model1_fd, struct Model1_user_msghdr *Model1_msg, unsigned Model1_flags);
extern long Model1___sys_sendmsg(int Model1_fd, struct Model1_user_msghdr *Model1_msg, unsigned Model1_flags);
extern int Model1___sys_recvmmsg(int Model1_fd, struct Model1_mmsghdr *Model1_mmsg, unsigned int Model1_vlen,
     unsigned int Model1_flags, struct Model1_timespec *Model1_timeout);
extern int Model1___sys_sendmmsg(int Model1_fd, struct Model1_mmsghdr *Model1_mmsg,
     unsigned int Model1_vlen, unsigned int Model1_flags);
/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Global definitions for the INET interface module.
 *
 * Version:	@(#)if.h	1.0.2	04/18/93
 *
 * Authors:	Original taken from Berkeley UNIX 4.3, (c) UCB 1982-1988
 *		Ross Biro
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */




/*
 * Compatibility interface for userspace libc header coordination:
 *
 * Define compatibility macros that are used to control the inclusion or
 * exclusion of UAPI structures and definitions in coordination with another
 * userspace C library.
 *
 * This header is intended to solve the problem of UAPI definitions that
 * conflict with userspace definitions. If a UAPI header has such conflicting
 * definitions then the solution is as follows:
 *
 * * Synchronize the UAPI header and the libc headers so either one can be
 *   used and such that the ABI is preserved. If this is not possible then
 *   no simple compatibility interface exists (you need to write translating
 *   wrappers and rename things) and you can't use this interface.
 *
 * Then follow this process:
 *
 * (a) Include libc-compat.h in the UAPI header.
 *      e.g. #include <linux/libc-compat.h>
 *     This include must be as early as possible.
 *
 * (b) In libc-compat.h add enough code to detect that the comflicting
 *     userspace libc header has been included first.
 *
 * (c) If the userspace libc header has been included first define a set of
 *     guard macros of the form __UAPI_DEF_FOO and set their values to 1, else
 *     set their values to 0.
 *
 * (d) Back in the UAPI header with the conflicting definitions, guard the
 *     definitions with:
 *     #if __UAPI_DEF_FOO
 *       ...
 *     #endif
 *
 * This fixes the situation where the linux headers are included *after* the
 * libc headers. To fix the problem with the inclusion in the other order the
 * userspace libc headers must be fixed like this:
 *
 * * For all definitions that conflict with kernel definitions wrap those
 *   defines in the following:
 *   #if !__UAPI_DEF_FOO
 *     ...
 *   #endif
 *
 * This prevents the redefinition of a construct already defined by the kernel.
 */



/* We have included glibc headers... */
/* Definitions for if.h */




/* Everything up to IFF_DYNAMIC, matches net/if.h until glibc 2.23 */

/* For the future if glibc adds IFF_LOWER_UP, IFF_DORMANT and IFF_ECHO */


/* Definitions for in.h */







/* Definitions for in6.h */
/* Definitions for ipx.h */






/* Definitions for xattr.h */








typedef struct {
 unsigned int Model1_clock_rate; /* bits per second */
 unsigned int Model1_clock_type; /* internal, external, TX-internal etc. */
 unsigned short Model1_loopback;
} Model1_sync_serial_settings; /* V.35, V.24, X.21 */

typedef struct {
 unsigned int Model1_clock_rate; /* bits per second */
 unsigned int Model1_clock_type; /* internal, external, TX-internal etc. */
 unsigned short Model1_loopback;
 unsigned int Model1_slot_map;
} Model1_te1_settings; /* T1, E1 */

typedef struct {
 unsigned short Model1_encoding;
 unsigned short Model1_parity;
} Model1_raw_hdlc_proto;

typedef struct {
 unsigned int Model1_t391;
 unsigned int Model1_t392;
 unsigned int Model1_n391;
 unsigned int Model1_n392;
 unsigned int Model1_n393;
 unsigned short Model1_lmi;
 unsigned short Model1_dce; /* 1 for DCE (network side) operation */
} Model1_fr_proto;

typedef struct {
 unsigned int Model1_dlci;
} Model1_fr_proto_pvc; /* for creating/deleting FR PVCs */

typedef struct {
 unsigned int Model1_dlci;
 char Model1_master[16]; /* Name of master FRAD device */
}Model1_fr_proto_pvc_info; /* for returning PVC information only */

typedef struct {
    unsigned int Model1_interval;
    unsigned int Model1_timeout;
} Model1_cisco_proto;

/* PPP doesn't need any info now - supply length = 0 to ioctl */

/* For glibc compatibility. An empty enum does not compile. */


/**
 * enum net_device_flags - &struct net_device flags
 *
 * These are the &struct net_device flags, they can be set by drivers, the
 * kernel and some can be triggered by userspace. Userspace can query and
 * set these flags using userspace utilities but there is also a sysfs
 * entry available for all dev flags which can be queried and set. These flags
 * are shared for all types of net_devices. The sysfs entries are available
 * via /sys/class/net/<dev>/flags. Flags which can be toggled through sysfs
 * are annotated below, note that only a few flags can be toggled and some
 * other flags are always preserved from the original net_device flags
 * even if you try to set them via sysfs. Flags which are always preserved
 * are kept under the flag grouping @IFF_VOLATILE. Flags which are volatile
 * are annotated below as such.
 *
 * You should have a pretty good reason to be extending these flags.
 *
 * @IFF_UP: interface is up. Can be toggled through sysfs.
 * @IFF_BROADCAST: broadcast address valid. Volatile.
 * @IFF_DEBUG: turn on debugging. Can be toggled through sysfs.
 * @IFF_LOOPBACK: is a loopback net. Volatile.
 * @IFF_POINTOPOINT: interface is has p-p link. Volatile.
 * @IFF_NOTRAILERS: avoid use of trailers. Can be toggled through sysfs.
 *	Volatile.
 * @IFF_RUNNING: interface RFC2863 OPER_UP. Volatile.
 * @IFF_NOARP: no ARP protocol. Can be toggled through sysfs. Volatile.
 * @IFF_PROMISC: receive all packets. Can be toggled through sysfs.
 * @IFF_ALLMULTI: receive all multicast packets. Can be toggled through
 *	sysfs.
 * @IFF_MASTER: master of a load balancer. Volatile.
 * @IFF_SLAVE: slave of a load balancer. Volatile.
 * @IFF_MULTICAST: Supports multicast. Can be toggled through sysfs.
 * @IFF_PORTSEL: can set media type. Can be toggled through sysfs.
 * @IFF_AUTOMEDIA: auto media select active. Can be toggled through sysfs.
 * @IFF_DYNAMIC: dialup device with changing addresses. Can be toggled
 *	through sysfs.
 * @IFF_LOWER_UP: driver signals L1 up. Volatile.
 * @IFF_DORMANT: driver signals dormant. Volatile.
 * @IFF_ECHO: echo sent packets. Volatile.
 */
enum Model1_net_device_flags {
/* for compatibility with glibc net/if.h */

 Model1_IFF_UP = 1<<0, /* sysfs */
 Model1_IFF_BROADCAST = 1<<1, /* volatile */
 Model1_IFF_DEBUG = 1<<2, /* sysfs */
 Model1_IFF_LOOPBACK = 1<<3, /* volatile */
 Model1_IFF_POINTOPOINT = 1<<4, /* volatile */
 Model1_IFF_NOTRAILERS = 1<<5, /* sysfs */
 Model1_IFF_RUNNING = 1<<6, /* volatile */
 Model1_IFF_NOARP = 1<<7, /* sysfs */
 Model1_IFF_PROMISC = 1<<8, /* sysfs */
 Model1_IFF_ALLMULTI = 1<<9, /* sysfs */
 Model1_IFF_MASTER = 1<<10, /* volatile */
 Model1_IFF_SLAVE = 1<<11, /* volatile */
 Model1_IFF_MULTICAST = 1<<12, /* sysfs */
 Model1_IFF_PORTSEL = 1<<13, /* sysfs */
 Model1_IFF_AUTOMEDIA = 1<<14, /* sysfs */
 Model1_IFF_DYNAMIC = 1<<15, /* sysfs */


 Model1_IFF_LOWER_UP = 1<<16, /* volatile */
 Model1_IFF_DORMANT = 1<<17, /* volatile */
 Model1_IFF_ECHO = 1<<18, /* volatile */

};


/* for compatibility with glibc net/if.h */
/* For definitions see hdlc.h */
/* For definitions see hdlc.h */
/* RFC 2863 operational status */
enum {
 Model1_IF_OPER_UNKNOWN,
 Model1_IF_OPER_NOTPRESENT,
 Model1_IF_OPER_DOWN,
 Model1_IF_OPER_LOWERLAYERDOWN,
 Model1_IF_OPER_TESTING,
 Model1_IF_OPER_DORMANT,
 Model1_IF_OPER_UP,
};

/* link modes */
enum {
 Model1_IF_LINK_MODE_DEFAULT,
 Model1_IF_LINK_MODE_DORMANT, /* limit upward transition to dormant */
};

/*
 *	Device mapping structure. I'd just gone off and designed a 
 *	beautiful scheme using only loadable modules with arguments
 *	for driver options and along come the PCMCIA people 8)
 *
 *	Ah well. The get() side of this is good for WDSETUP, and it'll
 *	be handy for debugging things. The set side is fine for now and
 *	being very small might be worth keeping for clean configuration.
 */

/* for compatibility with glibc net/if.h */

struct Model1_ifmap {
 unsigned long Model1_mem_start;
 unsigned long Model1_mem_end;
 unsigned short Model1_base_addr;
 unsigned char Model1_irq;
 unsigned char Model1_dma;
 unsigned char Model1_port;
 /* 3 bytes spare */
};


struct Model1_if_settings {
 unsigned int Model1_type; /* Type of physical device or protocol */
 unsigned int Model1_size; /* Size of the data allocated by the caller */
 union {
  /* {atm/eth/dsl}_settings anyone ? */
  Model1_raw_hdlc_proto *Model1_raw_hdlc;
  Model1_cisco_proto *Model1_cisco;
  Model1_fr_proto *Model1_fr;
  Model1_fr_proto_pvc *Model1_fr_pvc;
  Model1_fr_proto_pvc_info *Model1_fr_pvc_info;

  /* interface settings */
  Model1_sync_serial_settings *Model1_sync;
  Model1_te1_settings *Model1_te1;
 } Model1_ifs_ifsu;
};

/*
 * Interface request structure used for socket
 * ioctl's.  All interface ioctl's must have parameter
 * definitions which begin with ifr_name.  The
 * remainder may be interface specific.
 */

/* for compatibility with glibc net/if.h */

struct Model1_ifreq {

 union
 {
  char Model1_ifrn_name[16]; /* if name, e.g. "en0" */
 } Model1_ifr_ifrn;

 union {
  struct Model1_sockaddr Model1_ifru_addr;
  struct Model1_sockaddr Model1_ifru_dstaddr;
  struct Model1_sockaddr Model1_ifru_broadaddr;
  struct Model1_sockaddr Model1_ifru_netmask;
  struct Model1_sockaddr Model1_ifru_hwaddr;
  short Model1_ifru_flags;
  int Model1_ifru_ivalue;
  int Model1_ifru_mtu;
  struct Model1_ifmap Model1_ifru_map;
  char Model1_ifru_slave[16]; /* Just fits the size */
  char Model1_ifru_newname[16];
  void * Model1_ifru_data;
  struct Model1_if_settings Model1_ifru_settings;
 } Model1_ifr_ifru;
};
/*
 * Structure used in SIOCGIFCONF request.
 * Used to retrieve interface configuration
 * for machine (useful for programs which
 * must know all networks accessible).
 */

/* for compatibility with glibc net/if.h */

struct Model1_ifconf {
 int Model1_ifc_len; /* size of buffer	*/
 union {
  char *Model1_ifcu_buf;
  struct Model1_ifreq *Model1_ifcu_req;
 } Model1_ifc_ifcu;
};








/* acceptable for old filesystems */
static inline __attribute__((no_instrument_function)) bool Model1_old_valid_dev(Model1_dev_t Model1_dev)
{
 return ((unsigned int) ((Model1_dev) >> 20)) < 256 && ((unsigned int) ((Model1_dev) & ((1U << 20) - 1))) < 256;
}

static inline __attribute__((no_instrument_function)) Model1_u16 Model1_old_encode_dev(Model1_dev_t Model1_dev)
{
 return (((unsigned int) ((Model1_dev) >> 20)) << 8) | ((unsigned int) ((Model1_dev) & ((1U << 20) - 1)));
}

static inline __attribute__((no_instrument_function)) Model1_dev_t Model1_old_decode_dev(Model1_u16 Model1_val)
{
 return ((((Model1_val >> 8) & 255) << 20) | (Model1_val & 255));
}

static inline __attribute__((no_instrument_function)) Model1_u32 Model1_new_encode_dev(Model1_dev_t Model1_dev)
{
 unsigned Model1_major = ((unsigned int) ((Model1_dev) >> 20));
 unsigned Model1_minor = ((unsigned int) ((Model1_dev) & ((1U << 20) - 1)));
 return (Model1_minor & 0xff) | (Model1_major << 8) | ((Model1_minor & ~0xff) << 12);
}

static inline __attribute__((no_instrument_function)) Model1_dev_t Model1_new_decode_dev(Model1_u32 Model1_dev)
{
 unsigned Model1_major = (Model1_dev & 0xfff00) >> 8;
 unsigned Model1_minor = (Model1_dev & 0xff) | ((Model1_dev >> 12) & 0xfff00);
 return (((Model1_major) << 20) | (Model1_minor));
}

static inline __attribute__((no_instrument_function)) Model1_u64 Model1_huge_encode_dev(Model1_dev_t Model1_dev)
{
 return Model1_new_encode_dev(Model1_dev);
}

static inline __attribute__((no_instrument_function)) Model1_dev_t Model1_huge_decode_dev(Model1_u64 Model1_dev)
{
 return Model1_new_decode_dev(Model1_dev);
}

static inline __attribute__((no_instrument_function)) int Model1_sysv_valid_dev(Model1_dev_t Model1_dev)
{
 return ((unsigned int) ((Model1_dev) >> 20)) < (1<<14) && ((unsigned int) ((Model1_dev) & ((1U << 20) - 1))) < (1<<18);
}

static inline __attribute__((no_instrument_function)) Model1_u32 Model1_sysv_encode_dev(Model1_dev_t Model1_dev)
{
 return ((unsigned int) ((Model1_dev) & ((1U << 20) - 1))) | (((unsigned int) ((Model1_dev) >> 20)) << 18);
}

static inline __attribute__((no_instrument_function)) unsigned Model1_sysv_major(Model1_u32 Model1_dev)
{
 return (Model1_dev >> 18) & 0x3fff;
}

static inline __attribute__((no_instrument_function)) unsigned Model1_sysv_minor(Model1_u32 Model1_dev)
{
 return Model1_dev & 0x3ffff;
}









/*
 * RCU-protected bl list version. See include/linux/list_bl.h.
 */







/*
 * Special version of lists, where head of the list has a lock in the lowest
 * bit. This is useful for scalable hash tables without increasing memory
 * footprint overhead.
 *
 * For modification operations, the 0 bit of hlist_bl_head->first
 * pointer must be set.
 *
 * With some small modifications, this can easily be adapted to store several
 * arbitrary bits (not just a single lock bit), if the need arises to store
 * some fast and compact auxiliary data.
 */
struct Model1_hlist_bl_head {
 struct Model1_hlist_bl_node *Model1_first;
};

struct Model1_hlist_bl_node {
 struct Model1_hlist_bl_node *Model1_next, **Model1_pprev;
};



static inline __attribute__((no_instrument_function)) void Model1_INIT_HLIST_BL_NODE(struct Model1_hlist_bl_node *Model1_h)
{
 Model1_h->Model1_next = ((void *)0);
 Model1_h->Model1_pprev = ((void *)0);
}



static inline __attribute__((no_instrument_function)) bool Model1_hlist_bl_unhashed(const struct Model1_hlist_bl_node *Model1_h)
{
 return !Model1_h->Model1_pprev;
}

static inline __attribute__((no_instrument_function)) struct Model1_hlist_bl_node *Model1_hlist_bl_first(struct Model1_hlist_bl_head *Model1_h)
{
 return (struct Model1_hlist_bl_node *)
  ((unsigned long)Model1_h->Model1_first & ~1UL);
}

static inline __attribute__((no_instrument_function)) void Model1_hlist_bl_set_first(struct Model1_hlist_bl_head *Model1_h,
     struct Model1_hlist_bl_node *Model1_n)
{
                                                    ;

                        ;
 Model1_h->Model1_first = (struct Model1_hlist_bl_node *)((unsigned long)Model1_n | 1UL);
}

static inline __attribute__((no_instrument_function)) bool Model1_hlist_bl_empty(const struct Model1_hlist_bl_head *Model1_h)
{
 return !((unsigned long)({ union { typeof(Model1_h->Model1_first) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&(Model1_h->Model1_first), Model1___u.Model1___c, sizeof(Model1_h->Model1_first)); else Model1___read_once_size_nocheck(&(Model1_h->Model1_first), Model1___u.Model1___c, sizeof(Model1_h->Model1_first)); Model1___u.Model1___val; }) & ~1UL);
}

static inline __attribute__((no_instrument_function)) void Model1_hlist_bl_add_head(struct Model1_hlist_bl_node *Model1_n,
     struct Model1_hlist_bl_head *Model1_h)
{
 struct Model1_hlist_bl_node *Model1_first = Model1_hlist_bl_first(Model1_h);

 Model1_n->Model1_next = Model1_first;
 if (Model1_first)
  Model1_first->Model1_pprev = &Model1_n->Model1_next;
 Model1_n->Model1_pprev = &Model1_h->Model1_first;
 Model1_hlist_bl_set_first(Model1_h, Model1_n);
}

static inline __attribute__((no_instrument_function)) void Model1___hlist_bl_del(struct Model1_hlist_bl_node *Model1_n)
{
 struct Model1_hlist_bl_node *Model1_next = Model1_n->Model1_next;
 struct Model1_hlist_bl_node **Model1_pprev = Model1_n->Model1_pprev;

                                                    ;

 /* pprev may be `first`, so be careful not to lose the lock bit */
 ({ union { typeof(*Model1_pprev) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(*Model1_pprev)) ((struct Model1_hlist_bl_node *) ((unsigned long)Model1_next | ((unsigned long)*Model1_pprev & 1UL))) }; Model1___write_once_size(&(*Model1_pprev), Model1___u.Model1___c, sizeof(*Model1_pprev)); Model1___u.Model1___val; });



 if (Model1_next)
  Model1_next->Model1_pprev = Model1_pprev;
}

static inline __attribute__((no_instrument_function)) void Model1_hlist_bl_del(struct Model1_hlist_bl_node *Model1_n)
{
 Model1___hlist_bl_del(Model1_n);
 Model1_n->Model1_next = ((void *) 0x100 + (0xdead000000000000UL));
 Model1_n->Model1_pprev = ((void *) 0x200 + (0xdead000000000000UL));
}

static inline __attribute__((no_instrument_function)) void Model1_hlist_bl_del_init(struct Model1_hlist_bl_node *Model1_n)
{
 if (!Model1_hlist_bl_unhashed(Model1_n)) {
  Model1___hlist_bl_del(Model1_n);
  Model1_INIT_HLIST_BL_NODE(Model1_n);
 }
}

static inline __attribute__((no_instrument_function)) void Model1_hlist_bl_lock(struct Model1_hlist_bl_head *Model1_b)
{
 Model1_bit_spin_lock(0, (unsigned long *)Model1_b);
}

static inline __attribute__((no_instrument_function)) void Model1_hlist_bl_unlock(struct Model1_hlist_bl_head *Model1_b)
{
 Model1___bit_spin_unlock(0, (unsigned long *)Model1_b);
}

static inline __attribute__((no_instrument_function)) bool Model1_hlist_bl_is_locked(struct Model1_hlist_bl_head *Model1_b)
{
 return Model1_bit_spin_is_locked(0, (unsigned long *)Model1_b);
}

/**
 * hlist_bl_for_each_entry	- iterate over list of given type
 * @tpos:	the type * to use as a loop cursor.
 * @pos:	the &struct hlist_node to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the hlist_node within the struct.
 *
 */






/**
 * hlist_bl_for_each_entry_safe - iterate over list of given type safe against removal of list entry
 * @tpos:	the type * to use as a loop cursor.
 * @pos:	the &struct hlist_node to use as a loop cursor.
 * @n:		another &struct hlist_node to use as temporary storage
 * @head:	the head for your list.
 * @member:	the name of the hlist_node within the struct.
 */


static inline __attribute__((no_instrument_function)) void Model1_hlist_bl_set_first_rcu(struct Model1_hlist_bl_head *Model1_h,
     struct Model1_hlist_bl_node *Model1_n)
{
                                                    ;

                        ;
 ({ Model1_uintptr_t Model1__r_a_p__v = (Model1_uintptr_t)((struct Model1_hlist_bl_node *)((unsigned long)Model1_n | 1UL)); if (__builtin_constant_p((struct Model1_hlist_bl_node *)((unsigned long)Model1_n | 1UL)) && (Model1__r_a_p__v) == (Model1_uintptr_t)((void *)0)) ({ union { typeof((Model1_h->Model1_first)) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof((Model1_h->Model1_first))) ((typeof(Model1_h->Model1_first))(Model1__r_a_p__v)) }; Model1___write_once_size(&((Model1_h->Model1_first)), Model1___u.Model1___c, sizeof((Model1_h->Model1_first))); Model1___u.Model1___val; }); else do { do { bool Model1___cond = !((sizeof(*&Model1_h->Model1_first) == sizeof(char) || sizeof(*&Model1_h->Model1_first) == sizeof(short) || sizeof(*&Model1_h->Model1_first) == sizeof(int) || sizeof(*&Model1_h->Model1_first) == sizeof(long))); extern void Model1___compiletime_assert_17(void) ; if (Model1___cond) Model1___compiletime_assert_17(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&Model1_h->Model1_first) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(*&Model1_h->Model1_first)) ((typeof(*((typeof(Model1_h->Model1_first))Model1__r_a_p__v)) *)((typeof(Model1_h->Model1_first))Model1__r_a_p__v)) }; Model1___write_once_size(&(*&Model1_h->Model1_first), Model1___u.Model1___c, sizeof(*&Model1_h->Model1_first)); Model1___u.Model1___val; }); } while (0); Model1__r_a_p__v; });

}

static inline __attribute__((no_instrument_function)) struct Model1_hlist_bl_node *Model1_hlist_bl_first_rcu(struct Model1_hlist_bl_head *Model1_h)
{
 return (struct Model1_hlist_bl_node *)
  ((unsigned long)({ typeof(*(Model1_h->Model1_first)) *Model1_________p1 = (typeof(*(Model1_h->Model1_first)) *)({ typeof((Model1_h->Model1_first)) Model1__________p1 = ({ union { typeof((Model1_h->Model1_first)) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&((Model1_h->Model1_first)), Model1___u.Model1___c, sizeof((Model1_h->Model1_first))); else Model1___read_once_size_nocheck(&((Model1_h->Model1_first)), Model1___u.Model1___c, sizeof((Model1_h->Model1_first))); Model1___u.Model1___val; }); typeof(*((Model1_h->Model1_first))) *Model1____typecheck_p __attribute__((unused)); do { } while (0); (Model1__________p1); }); do { } while (0); ; ((typeof(*(Model1_h->Model1_first)) *)(Model1_________p1)); }) & ~1UL);
}

/**
 * hlist_bl_del_init_rcu - deletes entry from hash list with re-initialization
 * @n: the element to delete from the hash list.
 *
 * Note: hlist_bl_unhashed() on the node returns true after this. It is
 * useful for RCU based read lockfree traversal if the writer side
 * must know if the list entry is still hashed or already unhashed.
 *
 * In particular, it means that we can not poison the forward pointers
 * that may still be used for walking the hash list and we can only
 * zero the pprev pointer so list_unhashed() will return true after
 * this.
 *
 * The caller must take whatever precautions are necessary (such as
 * holding appropriate locks) to avoid racing with another
 * list-mutation primitive, such as hlist_bl_add_head_rcu() or
 * hlist_bl_del_rcu(), running on this same list.  However, it is
 * perfectly legal to run concurrently with the _rcu list-traversal
 * primitives, such as hlist_bl_for_each_entry_rcu().
 */
static inline __attribute__((no_instrument_function)) void Model1_hlist_bl_del_init_rcu(struct Model1_hlist_bl_node *Model1_n)
{
 if (!Model1_hlist_bl_unhashed(Model1_n)) {
  Model1___hlist_bl_del(Model1_n);
  Model1_n->Model1_pprev = ((void *)0);
 }
}

/**
 * hlist_bl_del_rcu - deletes entry from hash list without re-initialization
 * @n: the element to delete from the hash list.
 *
 * Note: hlist_bl_unhashed() on entry does not return true after this,
 * the entry is in an undefined state. It is useful for RCU based
 * lockfree traversal.
 *
 * In particular, it means that we can not poison the forward
 * pointers that may still be used for walking the hash list.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as hlist_bl_add_head_rcu()
 * or hlist_bl_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * hlist_bl_for_each_entry().
 */
static inline __attribute__((no_instrument_function)) void Model1_hlist_bl_del_rcu(struct Model1_hlist_bl_node *Model1_n)
{
 Model1___hlist_bl_del(Model1_n);
 Model1_n->Model1_pprev = ((void *) 0x200 + (0xdead000000000000UL));
}

/**
 * hlist_bl_add_head_rcu
 * @n: the element to add to the hash list.
 * @h: the list to add to.
 *
 * Description:
 * Adds the specified element to the specified hlist_bl,
 * while permitting racing traversals.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as hlist_bl_add_head_rcu()
 * or hlist_bl_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * hlist_bl_for_each_entry_rcu(), used to prevent memory-consistency
 * problems on Alpha CPUs.  Regardless of the type of CPU, the
 * list-traversal primitive must be guarded by rcu_read_lock().
 */
static inline __attribute__((no_instrument_function)) void Model1_hlist_bl_add_head_rcu(struct Model1_hlist_bl_node *Model1_n,
     struct Model1_hlist_bl_head *Model1_h)
{
 struct Model1_hlist_bl_node *Model1_first;

 /* don't need hlist_bl_first_rcu because we're under lock */
 Model1_first = Model1_hlist_bl_first(Model1_h);

 Model1_n->Model1_next = Model1_first;
 if (Model1_first)
  Model1_first->Model1_pprev = &Model1_n->Model1_next;
 Model1_n->Model1_pprev = &Model1_h->Model1_first;

 /* need _rcu because we can have concurrent lock free readers */
 Model1_hlist_bl_set_first_rcu(Model1_h, Model1_n);
}
/**
 * hlist_bl_for_each_entry_rcu - iterate over rcu list of given type
 * @tpos:	the type * to use as a loop cursor.
 * @pos:	the &struct hlist_bl_node to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the hlist_bl_node within the struct.
 *
 */







/*
 * Locked reference counts.
 *
 * These are different from just plain atomic refcounts in that they
 * are atomic with respect to the spinlock that goes with them.  In
 * particular, there can be implementations that don't actually get
 * the spinlock for the common decrement/increment operations, but they
 * still have to check that the operation is done semantically as if
 * the spinlock had been taken (using a cmpxchg operation that covers
 * both the lock and the count word, or using memory transactions, for
 * example).
 */
struct Model1_lockref {
 union {

  __u64 __attribute__((aligned(8))) Model1_lock_count;

  struct {
   Model1_spinlock_t Model1_lock;
   int Model1_count;
  };
 };
};

extern void Model1_lockref_get(struct Model1_lockref *);
extern int Model1_lockref_put_return(struct Model1_lockref *);
extern int Model1_lockref_get_not_zero(struct Model1_lockref *);
extern int Model1_lockref_get_or_lock(struct Model1_lockref *);
extern int Model1_lockref_put_or_lock(struct Model1_lockref *);

extern void Model1_lockref_mark_dead(struct Model1_lockref *);
extern int Model1_lockref_get_not_dead(struct Model1_lockref *);

/* Must be called under spinlock for reliable results */
static inline __attribute__((no_instrument_function)) int Model1___lockref_is_dead(const struct Model1_lockref *Model1_l)
{
 return ((int)Model1_l->Model1_count < 0);
}







/* Fast hashing routine for ints,  longs and pointers.
   (C) 2002 Nadia Yvette Chambers, IBM */




/*
 * The "GOLDEN_RATIO_PRIME" is used in ifs/btrfs/brtfs_inode.h and
 * fs/inode.c.  It's not actually prime any more (the previous primes
 * were actively bad for hashing), but the name remains.
 */
/*
 * This hash multiplies the input by a large odd number and takes the
 * high bits.  Since multiplication propagates changes to the most
 * significant end only, it is essential that the high bits of the
 * product be used for the hash value.
 *
 * Chuck Lever verified the effectiveness of this technique:
 * http://www.citi.umich.edu/techreports/reports/citi-tr-00-1.pdf
 *
 * Although a random odd number will do, it turns out that the golden
 * ratio phi = (sqrt(5)-1)/2, or its negative, has particularly nice
 * properties.  (See Knuth vol 3, section 6.4, exercise 9.)
 *
 * These are the negative, (1 - phi) = phi**2 = (3 - sqrt(5))/2,
 * which is very slightly easier to multiply by and makes no
 * difference to the hash distribution.
 */
/*
 * The _generic versions exist only so lib/test_hash.c can compare
 * the arch-optimized versions with the generic.
 *
 * Note that if you change these, any <asm/hash.h> that aren't updated
 * to match need to have their HAVE_ARCH_* define values updated so the
 * self-test will not false-positive.
 */



static inline __attribute__((no_instrument_function)) Model1_u32 Model1___hash_32_generic(Model1_u32 Model1_val)
{
 return Model1_val * 0x61C88647;
}




static inline __attribute__((no_instrument_function)) Model1_u32 Model1_hash_32_generic(Model1_u32 Model1_val, unsigned int Model1_bits)
{
 /* High bits are more random, so use them. */
 return Model1___hash_32_generic(Model1_val) >> (32 - Model1_bits);
}




static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model1_u32 Model1_hash_64_generic(Model1_u64 Model1_val, unsigned int Model1_bits)
{

 /* 64x64-bit multiply is efficient on all 64-bit processors */
 return Model1_val * 0x61C8864680B583EBull >> (64 - Model1_bits);




}

static inline __attribute__((no_instrument_function)) Model1_u32 Model1_hash_ptr(const void *Model1_ptr, unsigned int Model1_bits)
{
 return Model1_hash_64_generic((unsigned long)Model1_ptr, Model1_bits);
}

/* This really should be called fold32_ptr; it does no hashing to speak of. */
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_hash32_ptr(const void *Model1_ptr)
{
 unsigned long Model1_val = (unsigned long)Model1_ptr;


 Model1_val ^= (Model1_val >> 32);

 return (Model1_u32)Model1_val;
}

/*
 * Routines for hashing strings of bytes to a 32-bit hash value.
 *
 * These hash functions are NOT GUARANTEED STABLE between kernel
 * versions, architectures, or even repeated boots of the same kernel.
 * (E.g. they may depend on boot-time hardware detection or be
 * deliberately randomized.)
 *
 * They are also not intended to be secure against collisions caused by
 * malicious inputs; much slower hash functions are required for that.
 *
 * They are optimized for pathname components, meaning short strings.
 * Even if a majority of files have longer names, the dynamic profile of
 * pathname components skews short due to short directory names.
 * (E.g. /usr/lib/libsesquipedalianism.so.3.141.)
 */

/*
 * Version 1: one byte at a time.  Example of use:
 *
 * unsigned long hash = init_name_hash;
 * while (*p)
 *	hash = partial_name_hash(tolower(*p++), hash);
 * hash = end_name_hash(hash);
 *
 * Although this is designed for bytes, fs/hfsplus/unicode.c
 * abuses it to hash 16-bit values.
 */

/* Hash courtesy of the R5 hash in reiserfs modulo sign bits */


/* partial hash update function. Assume roughly 4 bits per character */
static inline __attribute__((no_instrument_function)) unsigned long
Model1_partial_name_hash(unsigned long Model1_c, unsigned long Model1_prevhash)
{
 return (Model1_prevhash + (Model1_c << 4) + (Model1_c >> 4)) * 11;
}

/*
 * Finally: cut down the number of bits to a int value (and try to avoid
 * losing bits).  This also has the property (wanted by the dcache)
 * that the msbits make a good hash table index.
 */
static inline __attribute__((no_instrument_function)) unsigned long Model1_end_name_hash(unsigned long Model1_hash)
{
 return Model1___hash_32_generic((unsigned int)Model1_hash);
}

/*
 * Version 2: One word (32 or 64 bits) at a time.
 * If CONFIG_DCACHE_WORD_ACCESS is defined (meaning <asm/word-at-a-time.h>
 * exists, which describes major Linux platforms like x86 and ARM), then
 * this computes a different hash function much faster.
 *
 * If not set, this falls back to a wrapper around the preceding.
 */
extern unsigned int __attribute__((pure)) Model1_full_name_hash(const void *Model1_salt, const char *, unsigned int);

/*
 * A hash_len is a u64 with the hash of a string in the low
 * half and the length in the high half.
 */




/* Return the "hash_len" (hash and length) of a null-terminated string */
extern Model1_u64 __attribute__((pure)) Model1_hashlen_string(const void *Model1_salt, const char *Model1_name);

struct Model1_path;
struct Model1_vfsmount;

/*
 * linux/include/linux/dcache.h
 *
 * Dirent cache data structures
 *
 * (C) Copyright 1997 Thomas Schoebel-Theuer,
 * with heavy changes by Linus Torvalds
 */



/* The hash is always the low bits of hash_len */
/*
 * "quick string" -- eases parameter passing, but more importantly
 * saves "metadata" about the string (ie length and the hash).
 *
 * hash comes first so it snuggles against d_parent in the
 * dentry.
 */
struct Model1_qstr {
 union {
  struct {
   Model1_u32 Model1_hash; Model1_u32 Model1_len;
  };
  Model1_u64 Model1_hash_len;
 };
 const unsigned char *Model1_name;
};



struct Model1_dentry_stat_t {
 long Model1_nr_dentry;
 long Model1_nr_unused;
 long Model1_age_limit; /* age in seconds */
 long Model1_want_pages; /* pages requested by system */
 long Model1_dummy[2];
};
extern struct Model1_dentry_stat_t Model1_dentry_stat;

/*
 * Try to keep struct dentry aligned on 64 byte cachelines (this will
 * give reasonable cacheline footprint with larger lines without the
 * large memory footprint increase).
 */
struct Model1_dentry {
 /* RCU lookup touched fields */
 unsigned int Model1_d_flags; /* protected by d_lock */
 Model1_seqcount_t Model1_d_seq; /* per dentry seqlock */
 struct Model1_hlist_bl_node Model1_d_hash; /* lookup hash list */
 struct Model1_dentry *Model1_d_parent; /* parent directory */
 struct Model1_qstr Model1_d_name;
 struct Model1_inode *Model1_d_inode; /* Where the name belongs to - NULL is
					 * negative */
 unsigned char Model1_d_iname[32]; /* small names */

 /* Ref lookup also touches following */
 struct Model1_lockref Model1_d_lockref; /* per-dentry lock and refcount */
 const struct Model1_dentry_operations *Model1_d_op;
 struct Model1_super_block *Model1_d_sb; /* The root of the dentry tree */
 unsigned long Model1_d_time; /* used by d_revalidate */
 void *Model1_d_fsdata; /* fs-specific data */

 union {
  struct Model1_list_head Model1_d_lru; /* LRU list */
  Model1_wait_queue_head_t *Model1_d_wait; /* in-lookup ones only */
 };
 struct Model1_list_head Model1_d_child; /* child of parent list */
 struct Model1_list_head Model1_d_subdirs; /* our children */
 /*
	 * d_alias and d_rcu can share memory
	 */
 union {
  struct Model1_hlist_node Model1_d_alias; /* inode alias list */
  struct Model1_hlist_bl_node Model1_d_in_lookup_hash; /* only for in-lookup ones */
   struct Model1_callback_head Model1_d_rcu;
 } Model1_d_u;
};

/*
 * dentry->d_lock spinlock nesting subclasses:
 *
 * 0: normal
 * 1: nested
 */
enum Model1_dentry_d_lock_class
{
 Model1_DENTRY_D_LOCK_NORMAL, /* implicitly used by plain spin_lock() APIs. */
 Model1_DENTRY_D_LOCK_NESTED
};

struct Model1_dentry_operations {
 int (*Model1_d_revalidate)(struct Model1_dentry *, unsigned int);
 int (*Model1_d_weak_revalidate)(struct Model1_dentry *, unsigned int);
 int (*Model1_d_hash)(const struct Model1_dentry *, struct Model1_qstr *);
 int (*Model1_d_compare)(const struct Model1_dentry *,
   unsigned int, const char *, const struct Model1_qstr *);
 int (*Model1_d_delete)(const struct Model1_dentry *);
 int (*Model1_d_init)(struct Model1_dentry *);
 void (*Model1_d_release)(struct Model1_dentry *);
 void (*Model1_d_prune)(struct Model1_dentry *);
 void (*Model1_d_iput)(struct Model1_dentry *, struct Model1_inode *);
 char *(*Model1_d_dname)(struct Model1_dentry *, char *, int);
 struct Model1_vfsmount *(*Model1_d_automount)(struct Model1_path *);
 int (*Model1_d_manage)(struct Model1_dentry *, bool);
 struct Model1_dentry *(*Model1_d_real)(struct Model1_dentry *, const struct Model1_inode *,
     unsigned int);
} __attribute__((__aligned__((1 << (6)))));

/*
 * Locking rules for dentry_operations callbacks are to be found in
 * Documentation/filesystems/Locking. Keep it updated!
 *
 * FUrther descriptions are found in Documentation/filesystems/vfs.txt.
 * Keep it updated too!
 */

/* d_flags entries */







     /* This dentry is possibly not currently connected to the dcache tree, in
      * which case its parent will either be itself, or will have this flag as
      * well.  nfsd will not use a dentry with this bit set, but will first
      * endeavour to clear the bit either by discovering that it is connected,
      * or by performing lookup operations.   Any filesystem which supports
      * nfsd_operations MUST have a lookup function which, if it finds a
      * directory inode with a DCACHE_DISCONNECTED dentry, will d_move that
      * dentry into place and return that dentry rather than the passed one,
      * typically using d_splice_alias. */
     /* this dentry has been "silly renamed" and has to be deleted on the last
      * dput() */


     /* Parent inode is watched by some fsnotify listener */
extern Model1_seqlock_t Model1_rename_lock;

/*
 * These are the low-level FS interfaces to the dcache..
 */
extern void Model1_d_instantiate(struct Model1_dentry *, struct Model1_inode *);
extern struct Model1_dentry * Model1_d_instantiate_unique(struct Model1_dentry *, struct Model1_inode *);
extern int Model1_d_instantiate_no_diralias(struct Model1_dentry *, struct Model1_inode *);
extern void Model1___d_drop(struct Model1_dentry *Model1_dentry);
extern void Model1_d_drop(struct Model1_dentry *Model1_dentry);
extern void Model1_d_delete(struct Model1_dentry *);
extern void Model1_d_set_d_op(struct Model1_dentry *Model1_dentry, const struct Model1_dentry_operations *Model1_op);

/* allocate/de-allocate */
extern struct Model1_dentry * Model1_d_alloc(struct Model1_dentry *, const struct Model1_qstr *);
extern struct Model1_dentry * Model1_d_alloc_pseudo(struct Model1_super_block *, const struct Model1_qstr *);
extern struct Model1_dentry * Model1_d_alloc_parallel(struct Model1_dentry *, const struct Model1_qstr *,
     Model1_wait_queue_head_t *);
extern struct Model1_dentry * Model1_d_splice_alias(struct Model1_inode *, struct Model1_dentry *);
extern struct Model1_dentry * Model1_d_add_ci(struct Model1_dentry *, struct Model1_inode *, struct Model1_qstr *);
extern struct Model1_dentry * Model1_d_exact_alias(struct Model1_dentry *, struct Model1_inode *);
extern struct Model1_dentry *Model1_d_find_any_alias(struct Model1_inode *Model1_inode);
extern struct Model1_dentry * Model1_d_obtain_alias(struct Model1_inode *);
extern struct Model1_dentry * Model1_d_obtain_root(struct Model1_inode *);
extern void Model1_shrink_dcache_sb(struct Model1_super_block *);
extern void Model1_shrink_dcache_parent(struct Model1_dentry *);
extern void Model1_shrink_dcache_for_umount(struct Model1_super_block *);
extern void Model1_d_invalidate(struct Model1_dentry *);

/* only used at mount-time */
extern struct Model1_dentry * Model1_d_make_root(struct Model1_inode *);

/* <clickety>-<click> the ramfs-type tree */
extern void Model1_d_genocide(struct Model1_dentry *);

extern void Model1_d_tmpfile(struct Model1_dentry *, struct Model1_inode *);

extern struct Model1_dentry *Model1_d_find_alias(struct Model1_inode *);
extern void Model1_d_prune_aliases(struct Model1_inode *);

/* test whether we have any submounts in a subdir tree */
extern int Model1_have_submounts(struct Model1_dentry *);

/*
 * This adds the entry to the hash queues.
 */
extern void Model1_d_rehash(struct Model1_dentry *);

extern void Model1_d_add(struct Model1_dentry *, struct Model1_inode *);

extern void Model1_dentry_update_name_case(struct Model1_dentry *, const struct Model1_qstr *);

/* used for rename() and baskets */
extern void Model1_d_move(struct Model1_dentry *, struct Model1_dentry *);
extern void Model1_d_exchange(struct Model1_dentry *, struct Model1_dentry *);
extern struct Model1_dentry *Model1_d_ancestor(struct Model1_dentry *, struct Model1_dentry *);

/* appendix may either be NULL or be used for transname suffixes */
extern struct Model1_dentry *Model1_d_lookup(const struct Model1_dentry *, const struct Model1_qstr *);
extern struct Model1_dentry *Model1_d_hash_and_lookup(struct Model1_dentry *, struct Model1_qstr *);
extern struct Model1_dentry *Model1___d_lookup(const struct Model1_dentry *, const struct Model1_qstr *);
extern struct Model1_dentry *Model1___d_lookup_rcu(const struct Model1_dentry *Model1_parent,
    const struct Model1_qstr *Model1_name, unsigned *Model1_seq);

static inline __attribute__((no_instrument_function)) unsigned Model1_d_count(const struct Model1_dentry *Model1_dentry)
{
 return Model1_dentry->Model1_d_lockref.Model1_count;
}

/*
 * helper function for dentry_operations.d_dname() members
 */
extern __attribute__((format(printf, 4, 5)))
char *Model1_dynamic_dname(struct Model1_dentry *, char *, int, const char *, ...);
extern char *Model1_simple_dname(struct Model1_dentry *, char *, int);

extern char *Model1___d_path(const struct Model1_path *, const struct Model1_path *, char *, int);
extern char *Model1_d_absolute_path(const struct Model1_path *, char *, int);
extern char *Model1_d_path(const struct Model1_path *, char *, int);
extern char *Model1_dentry_path_raw(struct Model1_dentry *, char *, int);
extern char *Model1_dentry_path(struct Model1_dentry *, char *, int);

/* Allocation counts.. */

/**
 *	dget, dget_dlock -	get a reference to a dentry
 *	@dentry: dentry to get a reference to
 *
 *	Given a dentry or %NULL pointer increment the reference count
 *	if appropriate and return the dentry. A dentry will not be 
 *	destroyed when it has references.
 */
static inline __attribute__((no_instrument_function)) struct Model1_dentry *Model1_dget_dlock(struct Model1_dentry *Model1_dentry)
{
 if (Model1_dentry)
  Model1_dentry->Model1_d_lockref.Model1_count++;
 return Model1_dentry;
}

static inline __attribute__((no_instrument_function)) struct Model1_dentry *Model1_dget(struct Model1_dentry *Model1_dentry)
{
 if (Model1_dentry)
  Model1_lockref_get(&Model1_dentry->Model1_d_lockref);
 return Model1_dentry;
}

extern struct Model1_dentry *Model1_dget_parent(struct Model1_dentry *Model1_dentry);

/**
 *	d_unhashed -	is dentry hashed
 *	@dentry: entry to check
 *
 *	Returns true if the dentry passed is not currently hashed.
 */

static inline __attribute__((no_instrument_function)) int Model1_d_unhashed(const struct Model1_dentry *Model1_dentry)
{
 return Model1_hlist_bl_unhashed(&Model1_dentry->Model1_d_hash);
}

static inline __attribute__((no_instrument_function)) int Model1_d_unlinked(const struct Model1_dentry *Model1_dentry)
{
 return Model1_d_unhashed(Model1_dentry) && !((Model1_dentry) == (Model1_dentry)->Model1_d_parent);
}

static inline __attribute__((no_instrument_function)) int Model1_cant_mount(const struct Model1_dentry *Model1_dentry)
{
 return (Model1_dentry->Model1_d_flags & 0x00000100);
}

static inline __attribute__((no_instrument_function)) void Model1_dont_mount(struct Model1_dentry *Model1_dentry)
{
 Model1_spin_lock(&Model1_dentry->Model1_d_lockref.Model1_lock);
 Model1_dentry->Model1_d_flags |= 0x00000100;
 Model1_spin_unlock(&Model1_dentry->Model1_d_lockref.Model1_lock);
}

extern void Model1___d_lookup_done(struct Model1_dentry *);

static inline __attribute__((no_instrument_function)) int Model1_d_in_lookup(struct Model1_dentry *Model1_dentry)
{
 return Model1_dentry->Model1_d_flags & 0x10000000;
}

static inline __attribute__((no_instrument_function)) void Model1_d_lookup_done(struct Model1_dentry *Model1_dentry)
{
 if (__builtin_expect(!!(Model1_d_in_lookup(Model1_dentry)), 0)) {
  Model1_spin_lock(&Model1_dentry->Model1_d_lockref.Model1_lock);
  Model1___d_lookup_done(Model1_dentry);
  Model1_spin_unlock(&Model1_dentry->Model1_d_lockref.Model1_lock);
 }
}

extern void Model1_dput(struct Model1_dentry *);

static inline __attribute__((no_instrument_function)) bool Model1_d_managed(const struct Model1_dentry *Model1_dentry)
{
 return Model1_dentry->Model1_d_flags & (0x00010000|0x00020000|0x00040000);
}

static inline __attribute__((no_instrument_function)) bool Model1_d_mountpoint(const struct Model1_dentry *Model1_dentry)
{
 return Model1_dentry->Model1_d_flags & 0x00010000;
}

/*
 * Directory cache entry type accessor functions.
 */
static inline __attribute__((no_instrument_function)) unsigned Model1___d_entry_type(const struct Model1_dentry *Model1_dentry)
{
 return Model1_dentry->Model1_d_flags & 0x00700000;
}

static inline __attribute__((no_instrument_function)) bool Model1_d_is_miss(const struct Model1_dentry *Model1_dentry)
{
 return Model1___d_entry_type(Model1_dentry) == 0x00000000;
}

static inline __attribute__((no_instrument_function)) bool Model1_d_is_whiteout(const struct Model1_dentry *Model1_dentry)
{
 return Model1___d_entry_type(Model1_dentry) == 0x00100000;
}

static inline __attribute__((no_instrument_function)) bool Model1_d_can_lookup(const struct Model1_dentry *Model1_dentry)
{
 return Model1___d_entry_type(Model1_dentry) == 0x00200000;
}

static inline __attribute__((no_instrument_function)) bool Model1_d_is_autodir(const struct Model1_dentry *Model1_dentry)
{
 return Model1___d_entry_type(Model1_dentry) == 0x00300000;
}

static inline __attribute__((no_instrument_function)) bool Model1_d_is_dir(const struct Model1_dentry *Model1_dentry)
{
 return Model1_d_can_lookup(Model1_dentry) || Model1_d_is_autodir(Model1_dentry);
}

static inline __attribute__((no_instrument_function)) bool Model1_d_is_symlink(const struct Model1_dentry *Model1_dentry)
{
 return Model1___d_entry_type(Model1_dentry) == 0x00600000;
}

static inline __attribute__((no_instrument_function)) bool Model1_d_is_reg(const struct Model1_dentry *Model1_dentry)
{
 return Model1___d_entry_type(Model1_dentry) == 0x00400000;
}

static inline __attribute__((no_instrument_function)) bool Model1_d_is_special(const struct Model1_dentry *Model1_dentry)
{
 return Model1___d_entry_type(Model1_dentry) == 0x00500000;
}

static inline __attribute__((no_instrument_function)) bool Model1_d_is_file(const struct Model1_dentry *Model1_dentry)
{
 return Model1_d_is_reg(Model1_dentry) || Model1_d_is_special(Model1_dentry);
}

static inline __attribute__((no_instrument_function)) bool Model1_d_is_negative(const struct Model1_dentry *Model1_dentry)
{
 // TODO: check d_is_whiteout(dentry) also.
 return Model1_d_is_miss(Model1_dentry);
}

static inline __attribute__((no_instrument_function)) bool Model1_d_is_positive(const struct Model1_dentry *Model1_dentry)
{
 return !Model1_d_is_negative(Model1_dentry);
}

/**
 * d_really_is_negative - Determine if a dentry is really negative (ignoring fallthroughs)
 * @dentry: The dentry in question
 *
 * Returns true if the dentry represents either an absent name or a name that
 * doesn't map to an inode (ie. ->d_inode is NULL).  The dentry could represent
 * a true miss, a whiteout that isn't represented by a 0,0 chardev or a
 * fallthrough marker in an opaque directory.
 *
 * Note!  (1) This should be used *only* by a filesystem to examine its own
 * dentries.  It should not be used to look at some other filesystem's
 * dentries.  (2) It should also be used in combination with d_inode() to get
 * the inode.  (3) The dentry may have something attached to ->d_lower and the
 * type field of the flags may be set to something other than miss or whiteout.
 */
static inline __attribute__((no_instrument_function)) bool Model1_d_really_is_negative(const struct Model1_dentry *Model1_dentry)
{
 return Model1_dentry->Model1_d_inode == ((void *)0);
}

/**
 * d_really_is_positive - Determine if a dentry is really positive (ignoring fallthroughs)
 * @dentry: The dentry in question
 *
 * Returns true if the dentry represents a name that maps to an inode
 * (ie. ->d_inode is not NULL).  The dentry might still represent a whiteout if
 * that is represented on medium as a 0,0 chardev.
 *
 * Note!  (1) This should be used *only* by a filesystem to examine its own
 * dentries.  It should not be used to look at some other filesystem's
 * dentries.  (2) It should also be used in combination with d_inode() to get
 * the inode.
 */
static inline __attribute__((no_instrument_function)) bool Model1_d_really_is_positive(const struct Model1_dentry *Model1_dentry)
{
 return Model1_dentry->Model1_d_inode != ((void *)0);
}

static inline __attribute__((no_instrument_function)) int Model1_simple_positive(struct Model1_dentry *Model1_dentry)
{
 return Model1_d_really_is_positive(Model1_dentry) && !Model1_d_unhashed(Model1_dentry);
}

extern void Model1_d_set_fallthru(struct Model1_dentry *Model1_dentry);

static inline __attribute__((no_instrument_function)) bool Model1_d_is_fallthru(const struct Model1_dentry *Model1_dentry)
{
 return Model1_dentry->Model1_d_flags & 0x01000000;
}


extern int Model1_sysctl_vfs_cache_pressure;

static inline __attribute__((no_instrument_function)) unsigned long Model1_vfs_pressure_ratio(unsigned long Model1_val)
{
 return ( { typeof(Model1_val) Model1_quot = (Model1_val) / (100); typeof(Model1_val) Model1_rem = (Model1_val) % (100); (Model1_quot * (Model1_sysctl_vfs_cache_pressure)) + ((Model1_rem * (Model1_sysctl_vfs_cache_pressure)) / (100)); } );
}

/**
 * d_inode - Get the actual inode of this dentry
 * @dentry: The dentry to query
 *
 * This is the helper normal filesystems should use to get at their own inodes
 * in their own dentries and ignore the layering superimposed upon them.
 */
static inline __attribute__((no_instrument_function)) struct Model1_inode *Model1_d_inode(const struct Model1_dentry *Model1_dentry)
{
 return Model1_dentry->Model1_d_inode;
}

/**
 * d_inode_rcu - Get the actual inode of this dentry with ACCESS_ONCE()
 * @dentry: The dentry to query
 *
 * This is the helper normal filesystems should use to get at their own inodes
 * in their own dentries and ignore the layering superimposed upon them.
 */
static inline __attribute__((no_instrument_function)) struct Model1_inode *Model1_d_inode_rcu(const struct Model1_dentry *Model1_dentry)
{
 return (*({ __attribute__((unused)) typeof(Model1_dentry->Model1_d_inode) Model1___var = ( typeof(Model1_dentry->Model1_d_inode)) 0; (volatile typeof(Model1_dentry->Model1_d_inode) *)&(Model1_dentry->Model1_d_inode); }));
}

/**
 * d_backing_inode - Get upper or lower inode we should be using
 * @upper: The upper layer
 *
 * This is the helper that should be used to get at the inode that will be used
 * if this dentry were to be opened as a file.  The inode may be on the upper
 * dentry or it may be on a lower dentry pinned by the upper.
 *
 * Normal filesystems should not use this to access their own inodes.
 */
static inline __attribute__((no_instrument_function)) struct Model1_inode *Model1_d_backing_inode(const struct Model1_dentry *Model1_upper)
{
 struct Model1_inode *Model1_inode = Model1_upper->Model1_d_inode;

 return Model1_inode;
}

/**
 * d_backing_dentry - Get upper or lower dentry we should be using
 * @upper: The upper layer
 *
 * This is the helper that should be used to get the dentry of the inode that
 * will be used if this dentry were opened as a file.  It may be the upper
 * dentry or it may be a lower dentry pinned by the upper.
 *
 * Normal filesystems should not use this to access their own dentries.
 */
static inline __attribute__((no_instrument_function)) struct Model1_dentry *Model1_d_backing_dentry(struct Model1_dentry *Model1_upper)
{
 return Model1_upper;
}

/**
 * d_real - Return the real dentry
 * @dentry: the dentry to query
 * @inode: inode to select the dentry from multiple layers (can be NULL)
 * @flags: open flags to control copy-up behavior
 *
 * If dentry is on an union/overlay, then return the underlying, real dentry.
 * Otherwise return the dentry itself.
 *
 * See also: Documentation/filesystems/vfs.txt
 */
static inline __attribute__((no_instrument_function)) struct Model1_dentry *Model1_d_real(struct Model1_dentry *Model1_dentry,
        const struct Model1_inode *Model1_inode,
        unsigned int Model1_flags)
{
 if (__builtin_expect(!!(Model1_dentry->Model1_d_flags & 0x04000000), 0))
  return Model1_dentry->Model1_d_op->Model1_d_real(Model1_dentry, Model1_inode, Model1_flags);
 else
  return Model1_dentry;
}

/**
 * d_real_inode - Return the real inode
 * @dentry: The dentry to query
 *
 * If dentry is on an union/overlay, then return the underlying, real inode.
 * Otherwise return d_inode().
 */
static inline __attribute__((no_instrument_function)) struct Model1_inode *Model1_d_real_inode(struct Model1_dentry *Model1_dentry)
{
 return Model1_d_backing_inode(Model1_d_real(Model1_dentry, ((void *)0), 0));
}



struct Model1_dentry;
struct Model1_vfsmount;

struct Model1_path {
 struct Model1_vfsmount *Model1_mnt;
 struct Model1_dentry *Model1_dentry;
};

extern void Model1_path_get(const struct Model1_path *);
extern void Model1_path_put(const struct Model1_path *);

static inline __attribute__((no_instrument_function)) int Model1_path_equal(const struct Model1_path *Model1_path1, const struct Model1_path *Model1_path2)
{
 return Model1_path1->Model1_mnt == Model1_path2->Model1_mnt && Model1_path1->Model1_dentry == Model1_path2->Model1_dentry;
}



/*
 * Copyright (c) 2013 Red Hat, Inc. and Parallels Inc. All rights reserved.
 * Authors: David Chinner and Glauber Costa
 *
 * Generic LRU infrastructure
 */







struct Model1_mem_cgroup;

/* list_lru_walk_cb has to always return one of those */
enum Model1_lru_status {
 Model1_LRU_REMOVED, /* item removed from list */
 Model1_LRU_REMOVED_RETRY, /* item removed, but lock has been
				   dropped and reacquired */
 Model1_LRU_ROTATE, /* item referenced, give another pass */
 Model1_LRU_SKIP, /* item cannot be locked, skip */
 Model1_LRU_RETRY, /* item not freeable. May drop the lock
				   internally, but has to return locked. */
};

struct Model1_list_lru_one {
 struct Model1_list_head Model1_list;
 /* may become negative during memcg reparenting */
 long Model1_nr_items;
};

struct Model1_list_lru_memcg {
 /* array of per cgroup lists, indexed by memcg_cache_id */
 struct Model1_list_lru_one *Model1_lru[0];
};

struct Model1_list_lru_node {
 /* protects all lists on the node, including per cgroup */
 Model1_spinlock_t Model1_lock;
 /* global list, used for the root cgroup in cgroup aware lrus */
 struct Model1_list_lru_one Model1_lru;




} __attribute__((__aligned__((1 << (6)))));

struct Model1_list_lru {
 struct Model1_list_lru_node *Model1_node;



};

void Model1_list_lru_destroy(struct Model1_list_lru *Model1_lru);
int Model1___list_lru_init(struct Model1_list_lru *Model1_lru, bool Model1_memcg_aware,
      struct Model1_lock_class_key *Model1_key);





int Model1_memcg_update_all_list_lrus(int Model1_num_memcgs);
void Model1_memcg_drain_all_list_lrus(int Model1_src_idx, int Model1_dst_idx);

/**
 * list_lru_add: add an element to the lru list's tail
 * @list_lru: the lru pointer
 * @item: the item to be added.
 *
 * If the element is already part of a list, this function returns doing
 * nothing. Therefore the caller does not need to keep state about whether or
 * not the element already belongs in the list and is allowed to lazy update
 * it. Note however that this is valid for *a* list, not *this* list. If
 * the caller organize itself in a way that elements can be in more than
 * one type of list, it is up to the caller to fully remove the item from
 * the previous list (with list_lru_del() for instance) before moving it
 * to @list_lru
 *
 * Return value: true if the list was updated, false otherwise
 */
bool Model1_list_lru_add(struct Model1_list_lru *Model1_lru, struct Model1_list_head *Model1_item);

/**
 * list_lru_del: delete an element to the lru list
 * @list_lru: the lru pointer
 * @item: the item to be deleted.
 *
 * This function works analogously as list_lru_add in terms of list
 * manipulation. The comments about an element already pertaining to
 * a list are also valid for list_lru_del.
 *
 * Return value: true if the list was updated, false otherwise
 */
bool Model1_list_lru_del(struct Model1_list_lru *Model1_lru, struct Model1_list_head *Model1_item);

/**
 * list_lru_count_one: return the number of objects currently held by @lru
 * @lru: the lru pointer.
 * @nid: the node id to count from.
 * @memcg: the cgroup to count from.
 *
 * Always return a non-negative number, 0 for empty lists. There is no
 * guarantee that the list is not updated while the count is being computed.
 * Callers that want such a guarantee need to provide an outer lock.
 */
unsigned long Model1_list_lru_count_one(struct Model1_list_lru *Model1_lru,
     int Model1_nid, struct Model1_mem_cgroup *Model1_memcg);
unsigned long Model1_list_lru_count_node(struct Model1_list_lru *Model1_lru, int Model1_nid);

static inline __attribute__((no_instrument_function)) unsigned long Model1_list_lru_shrink_count(struct Model1_list_lru *Model1_lru,
        struct Model1_shrink_control *Model1_sc)
{
 return Model1_list_lru_count_one(Model1_lru, Model1_sc->Model1_nid, Model1_sc->Model1_memcg);
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_list_lru_count(struct Model1_list_lru *Model1_lru)
{
 long Model1_count = 0;
 int Model1_nid;

 for (((Model1_nid)) = Model1___first_node(&(Model1_node_states[Model1_N_NORMAL_MEMORY])); ((Model1_nid)) < (1 << 6); ((Model1_nid)) = Model1___next_node((((Model1_nid))), &((Model1_node_states[Model1_N_NORMAL_MEMORY]))))
  Model1_count += Model1_list_lru_count_node(Model1_lru, Model1_nid);

 return Model1_count;
}

void Model1_list_lru_isolate(struct Model1_list_lru_one *Model1_list, struct Model1_list_head *Model1_item);
void Model1_list_lru_isolate_move(struct Model1_list_lru_one *Model1_list, struct Model1_list_head *Model1_item,
      struct Model1_list_head *Model1_head);

typedef enum Model1_lru_status (*Model1_list_lru_walk_cb)(struct Model1_list_head *Model1_item,
  struct Model1_list_lru_one *Model1_list, Model1_spinlock_t *Model1_lock, void *Model1_cb_arg);

/**
 * list_lru_walk_one: walk a list_lru, isolating and disposing freeable items.
 * @lru: the lru pointer.
 * @nid: the node id to scan from.
 * @memcg: the cgroup to scan from.
 * @isolate: callback function that is resposible for deciding what to do with
 *  the item currently being scanned
 * @cb_arg: opaque type that will be passed to @isolate
 * @nr_to_walk: how many items to scan.
 *
 * This function will scan all elements in a particular list_lru, calling the
 * @isolate callback for each of those items, along with the current list
 * spinlock and a caller-provided opaque. The @isolate callback can choose to
 * drop the lock internally, but *must* return with the lock held. The callback
 * will return an enum lru_status telling the list_lru infrastructure what to
 * do with the object being scanned.
 *
 * Please note that nr_to_walk does not mean how many objects will be freed,
 * just how many objects will be scanned.
 *
 * Return value: the number of objects effectively removed from the LRU.
 */
unsigned long Model1_list_lru_walk_one(struct Model1_list_lru *Model1_lru,
    int Model1_nid, struct Model1_mem_cgroup *Model1_memcg,
    Model1_list_lru_walk_cb Model1_isolate, void *Model1_cb_arg,
    unsigned long *Model1_nr_to_walk);
unsigned long Model1_list_lru_walk_node(struct Model1_list_lru *Model1_lru, int Model1_nid,
     Model1_list_lru_walk_cb Model1_isolate, void *Model1_cb_arg,
     unsigned long *Model1_nr_to_walk);

static inline __attribute__((no_instrument_function)) unsigned long
Model1_list_lru_shrink_walk(struct Model1_list_lru *Model1_lru, struct Model1_shrink_control *Model1_sc,
       Model1_list_lru_walk_cb Model1_isolate, void *Model1_cb_arg)
{
 return Model1_list_lru_walk_one(Model1_lru, Model1_sc->Model1_nid, Model1_sc->Model1_memcg, Model1_isolate, Model1_cb_arg,
     &Model1_sc->Model1_nr_to_scan);
}

static inline __attribute__((no_instrument_function)) unsigned long
Model1_list_lru_walk(struct Model1_list_lru *Model1_lru, Model1_list_lru_walk_cb Model1_isolate,
       void *Model1_cb_arg, unsigned long Model1_nr_to_walk)
{
 long Model1_isolated = 0;
 int Model1_nid;

 for (((Model1_nid)) = Model1___first_node(&(Model1_node_states[Model1_N_NORMAL_MEMORY])); ((Model1_nid)) < (1 << 6); ((Model1_nid)) = Model1___next_node((((Model1_nid))), &((Model1_node_states[Model1_N_NORMAL_MEMORY])))) {
  Model1_isolated += Model1_list_lru_walk_node(Model1_lru, Model1_nid, Model1_isolate,
            Model1_cb_arg, &Model1_nr_to_walk);
  if (Model1_nr_to_walk <= 0)
   break;
 }
 return Model1_isolated;
}

/*
 * Copyright (C) 2001 Momchil Velikov
 * Portions Copyright (C) 2001 Christoph Hellwig
 * Copyright (C) 2006 Nick Piggin
 * Copyright (C) 2012 Konstantin Khlebnikov
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public License as
 * published by the Free Software Foundation; either version 2, or (at
 * your option) any later version.
 * 
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 * General Public License for more details.
 * 
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
 */
/*
 * The bottom two bits of the slot determine how the remaining bits in the
 * slot are interpreted:
 *
 * 00 - data pointer
 * 01 - internal entry
 * 10 - exceptional entry
 * 11 - this bit combination is currently unused/reserved
 *
 * The internal entry may be a pointer to the next level in the tree, a
 * sibling entry, or an indicator that the entry in this slot has been moved
 * to another location in the tree and the lookup should be restarted.  While
 * NULL fits the 'data pointer' pattern, it means that there is no entry in
 * the tree for this index (no matter what level of the tree it is found at).
 * This means that you cannot store NULL in the tree as a value for the index.
 */



/*
 * Most users of the radix tree store pointers but shmem/tmpfs stores swap
 * entries in the same tree.  They are marked as exceptional entries to
 * distinguish them from pointers to struct page.
 * EXCEPTIONAL_ENTRY tests the bit, EXCEPTIONAL_SHIFT shifts content past it.
 */



static inline __attribute__((no_instrument_function)) bool Model1_radix_tree_is_internal_node(void *Model1_ptr)
{
 return ((unsigned long)Model1_ptr & 3UL) ==
    1UL;
}

/*** radix-tree API starts here ***/
/* Internally used bits of node->count */



struct Model1_radix_tree_node {
 unsigned char Model1_shift; /* Bits remaining in each slot */
 unsigned char Model1_offset; /* Slot offset in parent */
 unsigned int Model1_count;
 union {
  struct {
   /* Used when ascending tree */
   struct Model1_radix_tree_node *Model1_parent;
   /* For tree user */
   void *Model1_private_data;
  };
  /* Used when freeing node */
  struct Model1_callback_head Model1_callback_head;
 };
 /* For tree user */
 struct Model1_list_head Model1_private_list;
 void *Model1_slots[(1UL << (0 ? 4 : 6))];
 unsigned long Model1_tags[3][(((1UL << (0 ? 4 : 6)) + 64 - 1) / 64)];
};

/* root tags are stored in gfp_mask, shifted by __GFP_BITS_SHIFT */
struct Model1_radix_tree_root {
 Model1_gfp_t Model1_gfp_mask;
 struct Model1_radix_tree_node *Model1_rnode;
};
static inline __attribute__((no_instrument_function)) bool Model1_radix_tree_empty(struct Model1_radix_tree_root *Model1_root)
{
 return Model1_root->Model1_rnode == ((void *)0);
}

/**
 * Radix-tree synchronization
 *
 * The radix-tree API requires that users provide all synchronisation (with
 * specific exceptions, noted below).
 *
 * Synchronization of access to the data items being stored in the tree, and
 * management of their lifetimes must be completely managed by API users.
 *
 * For API usage, in general,
 * - any function _modifying_ the tree or tags (inserting or deleting
 *   items, setting or clearing tags) must exclude other modifications, and
 *   exclude any functions reading the tree.
 * - any function _reading_ the tree or tags (looking up items or tags,
 *   gang lookups) must exclude modifications to the tree, but may occur
 *   concurrently with other readers.
 *
 * The notable exceptions to this rule are the following functions:
 * __radix_tree_lookup
 * radix_tree_lookup
 * radix_tree_lookup_slot
 * radix_tree_tag_get
 * radix_tree_gang_lookup
 * radix_tree_gang_lookup_slot
 * radix_tree_gang_lookup_tag
 * radix_tree_gang_lookup_tag_slot
 * radix_tree_tagged
 *
 * The first 8 functions are able to be called locklessly, using RCU. The
 * caller must ensure calls to these functions are made within rcu_read_lock()
 * regions. Other readers (lock-free or otherwise) and modifications may be
 * running concurrently.
 *
 * It is still required that the caller manage the synchronization and lifetimes
 * of the items. So if RCU lock-free lookups are used, typically this would mean
 * that the items have their own locks, or are amenable to lock-free access; and
 * that the items are freed by RCU (or only freed after having been deleted from
 * the radix tree *and* a synchronize_rcu() grace period).
 *
 * (Note, rcu_assign_pointer and rcu_dereference are not needed to control
 * access to data items when inserting into or looking up from the radix tree)
 *
 * Note that the value returned by radix_tree_tag_get() may not be relied upon
 * if only the RCU read lock is held.  Functions to set/clear tags and to
 * delete nodes running concurrently with it may affect its result such that
 * two consecutive reads in the same locked section may return different
 * values.  If reliability is required, modification functions must also be
 * excluded from concurrency.
 *
 * radix_tree_tagged is able to be called without locking or RCU.
 */

/**
 * radix_tree_deref_slot	- dereference a slot
 * @pslot:	pointer to slot, returned by radix_tree_lookup_slot
 * Returns:	item that was stored in that slot with any direct pointer flag
 *		removed.
 *
 * For use with radix_tree_lookup_slot().  Caller must hold tree at least read
 * locked across slot lookup and dereference. Not required if write lock is
 * held (ie. items cannot be concurrently inserted).
 *
 * radix_tree_deref_retry must be used to confirm validity of the pointer if
 * only the read lock is held.
 */
static inline __attribute__((no_instrument_function)) void *Model1_radix_tree_deref_slot(void **Model1_pslot)
{
 return ({ typeof(*(*Model1_pslot)) *Model1_________p1 = (typeof(*(*Model1_pslot)) *)({ typeof((*Model1_pslot)) Model1__________p1 = ({ union { typeof((*Model1_pslot)) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&((*Model1_pslot)), Model1___u.Model1___c, sizeof((*Model1_pslot))); else Model1___read_once_size_nocheck(&((*Model1_pslot)), Model1___u.Model1___c, sizeof((*Model1_pslot))); Model1___u.Model1___val; }); typeof(*((*Model1_pslot))) *Model1____typecheck_p __attribute__((unused)); do { } while (0); (Model1__________p1); }); do { } while (0); ; ((typeof(*(*Model1_pslot)) *)(Model1_________p1)); });
}

/**
 * radix_tree_deref_slot_protected	- dereference a slot without RCU lock but with tree lock held
 * @pslot:	pointer to slot, returned by radix_tree_lookup_slot
 * Returns:	item that was stored in that slot with any direct pointer flag
 *		removed.
 *
 * Similar to radix_tree_deref_slot but only used during migration when a pages
 * mapping is being moved. The caller does not hold the RCU read lock but it
 * must hold the tree lock to prevent parallel updates.
 */
static inline __attribute__((no_instrument_function)) void *Model1_radix_tree_deref_slot_protected(void **Model1_pslot,
       Model1_spinlock_t *Model1_treelock)
{
 return ({ do { } while (0); ; ((typeof(*(*Model1_pslot)) *)((*Model1_pslot))); });
}

/**
 * radix_tree_deref_retry	- check radix_tree_deref_slot
 * @arg:	pointer returned by radix_tree_deref_slot
 * Returns:	0 if retry is not required, otherwise retry is required
 *
 * radix_tree_deref_retry must be used with radix_tree_deref_slot.
 */
static inline __attribute__((no_instrument_function)) int Model1_radix_tree_deref_retry(void *Model1_arg)
{
 return __builtin_expect(!!(Model1_radix_tree_is_internal_node(Model1_arg)), 0);
}

/**
 * radix_tree_exceptional_entry	- radix_tree_deref_slot gave exceptional entry?
 * @arg:	value returned by radix_tree_deref_slot
 * Returns:	0 if well-aligned pointer, non-0 if exceptional entry.
 */
static inline __attribute__((no_instrument_function)) int Model1_radix_tree_exceptional_entry(void *Model1_arg)
{
 /* Not unlikely because radix_tree_exception often tested first */
 return (unsigned long)Model1_arg & 2;
}

/**
 * radix_tree_exception	- radix_tree_deref_slot returned either exception?
 * @arg:	value returned by radix_tree_deref_slot
 * Returns:	0 if well-aligned pointer, non-0 if either kind of exception.
 */
static inline __attribute__((no_instrument_function)) int Model1_radix_tree_exception(void *Model1_arg)
{
 return __builtin_expect(!!((unsigned long)Model1_arg & 3UL), 0);
}

/**
 * radix_tree_replace_slot	- replace item in a slot
 * @pslot:	pointer to slot, returned by radix_tree_lookup_slot
 * @item:	new item to store in the slot.
 *
 * For use with radix_tree_lookup_slot().  Caller must hold tree write locked
 * across slot lookup and replacement.
 */
static inline __attribute__((no_instrument_function)) void Model1_radix_tree_replace_slot(void **Model1_pslot, void *Model1_item)
{
 do { if (__builtin_expect(!!(Model1_radix_tree_is_internal_node(Model1_item)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/radix-tree.h"), "i" (261), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);
 ({ Model1_uintptr_t Model1__r_a_p__v = (Model1_uintptr_t)(Model1_item); if (__builtin_constant_p(Model1_item) && (Model1__r_a_p__v) == (Model1_uintptr_t)((void *)0)) ({ union { typeof((*Model1_pslot)) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof((*Model1_pslot))) ((typeof(*Model1_pslot))(Model1__r_a_p__v)) }; Model1___write_once_size(&((*Model1_pslot)), Model1___u.Model1___c, sizeof((*Model1_pslot))); Model1___u.Model1___val; }); else do { do { bool Model1___cond = !((sizeof(*&*Model1_pslot) == sizeof(char) || sizeof(*&*Model1_pslot) == sizeof(short) || sizeof(*&*Model1_pslot) == sizeof(int) || sizeof(*&*Model1_pslot) == sizeof(long))); extern void Model1___compiletime_assert_262(void) ; if (Model1___cond) Model1___compiletime_assert_262(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&*Model1_pslot) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(*&*Model1_pslot)) ((typeof(*((typeof(*Model1_pslot))Model1__r_a_p__v)) *)((typeof(*Model1_pslot))Model1__r_a_p__v)) }; Model1___write_once_size(&(*&*Model1_pslot), Model1___u.Model1___c, sizeof(*&*Model1_pslot)); Model1___u.Model1___val; }); } while (0); Model1__r_a_p__v; });
}

int Model1___radix_tree_create(struct Model1_radix_tree_root *Model1_root, unsigned long Model1_index,
   unsigned Model1_order, struct Model1_radix_tree_node **Model1_nodep,
   void ***Model1_slotp);
int Model1___radix_tree_insert(struct Model1_radix_tree_root *, unsigned long Model1_index,
   unsigned Model1_order, void *);
static inline __attribute__((no_instrument_function)) int Model1_radix_tree_insert(struct Model1_radix_tree_root *Model1_root,
   unsigned long Model1_index, void *Model1_entry)
{
 return Model1___radix_tree_insert(Model1_root, Model1_index, 0, Model1_entry);
}
void *Model1___radix_tree_lookup(struct Model1_radix_tree_root *Model1_root, unsigned long Model1_index,
     struct Model1_radix_tree_node **Model1_nodep, void ***Model1_slotp);
void *Model1_radix_tree_lookup(struct Model1_radix_tree_root *, unsigned long);
void **Model1_radix_tree_lookup_slot(struct Model1_radix_tree_root *, unsigned long);
bool Model1___radix_tree_delete_node(struct Model1_radix_tree_root *Model1_root,
         struct Model1_radix_tree_node *Model1_node);
void *Model1_radix_tree_delete_item(struct Model1_radix_tree_root *, unsigned long, void *);
void *Model1_radix_tree_delete(struct Model1_radix_tree_root *, unsigned long);
struct Model1_radix_tree_node *Model1_radix_tree_replace_clear_tags(
    struct Model1_radix_tree_root *Model1_root,
    unsigned long Model1_index, void *Model1_entry);
unsigned int Model1_radix_tree_gang_lookup(struct Model1_radix_tree_root *Model1_root,
   void **Model1_results, unsigned long Model1_first_index,
   unsigned int Model1_max_items);
unsigned int Model1_radix_tree_gang_lookup_slot(struct Model1_radix_tree_root *Model1_root,
   void ***Model1_results, unsigned long *Model1_indices,
   unsigned long Model1_first_index, unsigned int Model1_max_items);
int Model1_radix_tree_preload(Model1_gfp_t Model1_gfp_mask);
int Model1_radix_tree_maybe_preload(Model1_gfp_t Model1_gfp_mask);
int Model1_radix_tree_maybe_preload_order(Model1_gfp_t Model1_gfp_mask, int Model1_order);
void Model1_radix_tree_init(void);
void *Model1_radix_tree_tag_set(struct Model1_radix_tree_root *Model1_root,
   unsigned long Model1_index, unsigned int Model1_tag);
void *Model1_radix_tree_tag_clear(struct Model1_radix_tree_root *Model1_root,
   unsigned long Model1_index, unsigned int Model1_tag);
int Model1_radix_tree_tag_get(struct Model1_radix_tree_root *Model1_root,
   unsigned long Model1_index, unsigned int Model1_tag);
unsigned int
Model1_radix_tree_gang_lookup_tag(struct Model1_radix_tree_root *Model1_root, void **Model1_results,
  unsigned long Model1_first_index, unsigned int Model1_max_items,
  unsigned int Model1_tag);
unsigned int
Model1_radix_tree_gang_lookup_tag_slot(struct Model1_radix_tree_root *Model1_root, void ***Model1_results,
  unsigned long Model1_first_index, unsigned int Model1_max_items,
  unsigned int Model1_tag);
unsigned long Model1_radix_tree_range_tag_if_tagged(struct Model1_radix_tree_root *Model1_root,
  unsigned long *Model1_first_indexp, unsigned long Model1_last_index,
  unsigned long Model1_nr_to_tag,
  unsigned int Model1_fromtag, unsigned int Model1_totag);
int Model1_radix_tree_tagged(struct Model1_radix_tree_root *Model1_root, unsigned int Model1_tag);
unsigned long Model1_radix_tree_locate_item(struct Model1_radix_tree_root *Model1_root, void *Model1_item);

static inline __attribute__((no_instrument_function)) void Model1_radix_tree_preload_end(void)
{
 __asm__ __volatile__("": : :"memory");
}

/**
 * struct radix_tree_iter - radix tree iterator state
 *
 * @index:	index of current slot
 * @next_index:	one beyond the last index for this chunk
 * @tags:	bit-mask for tag-iterating
 * @shift:	shift for the node that holds our slots
 *
 * This radix tree iterator works in terms of "chunks" of slots.  A chunk is a
 * subinterval of slots contained within one radix tree leaf node.  It is
 * described by a pointer to its first slot and a struct radix_tree_iter
 * which holds the chunk's position in the tree and its size.  For tagged
 * iteration radix_tree_iter also holds the slots' bit-mask for one chosen
 * radix tree tag.
 */
struct Model1_radix_tree_iter {
 unsigned long Model1_index;
 unsigned long Model1_next_index;
 unsigned long Model1_tags;



};

static inline __attribute__((no_instrument_function)) unsigned int Model1_iter_shift(struct Model1_radix_tree_iter *Model1_iter)
{



 return 0;

}





/**
 * radix_tree_iter_init - initialize radix tree iterator
 *
 * @iter:	pointer to iterator state
 * @start:	iteration starting index
 * Returns:	NULL
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void **
Model1_radix_tree_iter_init(struct Model1_radix_tree_iter *Model1_iter, unsigned long Model1_start)
{
 /*
	 * Leave iter->tags uninitialized. radix_tree_next_chunk() will fill it
	 * in the case of a successful tagged chunk lookup.  If the lookup was
	 * unsuccessful or non-tagged then nobody cares about ->tags.
	 *
	 * Set index to zero to bypass next_index overflow protection.
	 * See the comment in radix_tree_next_chunk() for details.
	 */
 Model1_iter->Model1_index = 0;
 Model1_iter->Model1_next_index = Model1_start;
 return ((void *)0);
}

/**
 * radix_tree_next_chunk - find next chunk of slots for iteration
 *
 * @root:	radix tree root
 * @iter:	iterator state
 * @flags:	RADIX_TREE_ITER_* flags and tag index
 * Returns:	pointer to chunk first slot, or NULL if there no more left
 *
 * This function looks up the next chunk in the radix tree starting from
 * @iter->next_index.  It returns a pointer to the chunk's first slot.
 * Also it fills @iter with data about chunk: position in the tree (index),
 * its end (next_index), and constructs a bit mask for tagged iterating (tags).
 */
void **Model1_radix_tree_next_chunk(struct Model1_radix_tree_root *Model1_root,
        struct Model1_radix_tree_iter *Model1_iter, unsigned Model1_flags);

/**
 * radix_tree_iter_retry - retry this chunk of the iteration
 * @iter:	iterator state
 *
 * If we iterate over a tree protected only by the RCU lock, a race
 * against deletion or creation may result in seeing a slot for which
 * radix_tree_deref_retry() returns true.  If so, call this function
 * and continue the iteration.
 */
static inline __attribute__((no_instrument_function)) __attribute__((warn_unused_result))
void **Model1_radix_tree_iter_retry(struct Model1_radix_tree_iter *Model1_iter)
{
 Model1_iter->Model1_next_index = Model1_iter->Model1_index;
 Model1_iter->Model1_tags = 0;
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) unsigned long
Model1___radix_tree_iter_add(struct Model1_radix_tree_iter *Model1_iter, unsigned long Model1_slots)
{
 return Model1_iter->Model1_index + (Model1_slots << Model1_iter_shift(Model1_iter));
}

/**
 * radix_tree_iter_next - resume iterating when the chunk may be invalid
 * @iter:	iterator state
 *
 * If the iterator needs to release then reacquire a lock, the chunk may
 * have been invalidated by an insertion or deletion.  Call this function
 * to continue the iteration from the next index.
 */
static inline __attribute__((no_instrument_function)) __attribute__((warn_unused_result))
void **Model1_radix_tree_iter_next(struct Model1_radix_tree_iter *Model1_iter)
{
 Model1_iter->Model1_next_index = Model1___radix_tree_iter_add(Model1_iter, 1);
 Model1_iter->Model1_tags = 0;
 return ((void *)0);
}

/**
 * radix_tree_chunk_size - get current chunk size
 *
 * @iter:	pointer to radix tree iterator
 * Returns:	current chunk size
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) long
Model1_radix_tree_chunk_size(struct Model1_radix_tree_iter *Model1_iter)
{
 return (Model1_iter->Model1_next_index - Model1_iter->Model1_index) >> Model1_iter_shift(Model1_iter);
}

static inline __attribute__((no_instrument_function)) struct Model1_radix_tree_node *Model1_entry_to_node(void *Model1_ptr)
{
 return (void *)((unsigned long)Model1_ptr & ~1UL);
}

/**
 * radix_tree_next_slot - find next slot in chunk
 *
 * @slot:	pointer to current slot
 * @iter:	pointer to interator state
 * @flags:	RADIX_TREE_ITER_*, should be constant
 * Returns:	pointer to next slot, or NULL if there no more left
 *
 * This function updates @iter->index in the case of a successful lookup.
 * For tagged lookup it also eats @iter->tags.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void **
Model1_radix_tree_next_slot(void **Model1_slot, struct Model1_radix_tree_iter *Model1_iter, unsigned Model1_flags)
{
 if (Model1_flags & 0x0100) {
  void *Model1_canon = Model1_slot;

  Model1_iter->Model1_tags >>= 1;
  if (__builtin_expect(!!(!Model1_iter->Model1_tags), 0))
   return ((void *)0);
  while (0 &&
     Model1_radix_tree_is_internal_node(Model1_slot[1])) {
   if (Model1_entry_to_node(Model1_slot[1]) == Model1_canon) {
    Model1_iter->Model1_tags >>= 1;
    Model1_iter->Model1_index = Model1___radix_tree_iter_add(Model1_iter, 1);
    Model1_slot++;
    continue;
   }
   Model1_iter->Model1_next_index = Model1___radix_tree_iter_add(Model1_iter, 1);
   return ((void *)0);
  }
  if (__builtin_expect(!!(Model1_iter->Model1_tags & 1ul), 1)) {
   Model1_iter->Model1_index = Model1___radix_tree_iter_add(Model1_iter, 1);
   return Model1_slot + 1;
  }
  if (!(Model1_flags & 0x0200)) {
   unsigned Model1_offset = Model1___ffs(Model1_iter->Model1_tags);

   Model1_iter->Model1_tags >>= Model1_offset;
   Model1_iter->Model1_index = Model1___radix_tree_iter_add(Model1_iter, Model1_offset + 1);
   return Model1_slot + Model1_offset + 1;
  }
 } else {
  long Model1_count = Model1_radix_tree_chunk_size(Model1_iter);
  void *Model1_canon = Model1_slot;

  while (--Model1_count > 0) {
   Model1_slot++;
   Model1_iter->Model1_index = Model1___radix_tree_iter_add(Model1_iter, 1);

   if (0 &&
       Model1_radix_tree_is_internal_node(*Model1_slot)) {
    if (Model1_entry_to_node(*Model1_slot) == Model1_canon)
     continue;
    Model1_iter->Model1_next_index = Model1_iter->Model1_index;
    break;
   }

   if (__builtin_expect(!!(*Model1_slot), 1))
    return Model1_slot;
   if (Model1_flags & 0x0200) {
    /* forbid switching to the next chunk */
    Model1_iter->Model1_next_index = 0;
    break;
   }
  }
 }
 return ((void *)0);
}

/**
 * radix_tree_for_each_slot - iterate over non-empty slots
 *
 * @slot:	the void** variable for pointer to slot
 * @root:	the struct radix_tree_root pointer
 * @iter:	the struct radix_tree_iter pointer
 * @start:	iteration starting index
 *
 * @slot points to radix tree slot, @iter->index contains its index.
 */





/**
 * radix_tree_for_each_contig - iterate over contiguous slots
 *
 * @slot:	the void** variable for pointer to slot
 * @root:	the struct radix_tree_root pointer
 * @iter:	the struct radix_tree_iter pointer
 * @start:	iteration starting index
 *
 * @slot points to radix tree slot, @iter->index contains its index.
 */







/**
 * radix_tree_for_each_tagged - iterate over tagged slots
 *
 * @slot:	the void** variable for pointer to slot
 * @root:	the struct radix_tree_root pointer
 * @iter:	the struct radix_tree_iter pointer
 * @start:	iteration starting index
 * @tag:	tag index
 *
 * @slot points to radix tree slot, @iter->index contains its index.
 */







/*
 * Copyright (c) 2008 Intel Corporation
 * Author: Matthew Wilcox <willy@linux.intel.com>
 *
 * Distributed under the terms of the GNU GPL, version 2
 *
 * Please see kernel/semaphore.c for documentation of these functions
 */






/* Please don't access any members of this structure directly */
struct Model1_semaphore {
 Model1_raw_spinlock_t Model1_lock;
 unsigned int Model1_count;
 struct Model1_list_head Model1_wait_list;
};
static inline __attribute__((no_instrument_function)) void Model1_sema_init(struct Model1_semaphore *Model1_sem, int Model1_val)
{
 static struct Model1_lock_class_key Model1___key;
 *Model1_sem = (struct Model1_semaphore) { .Model1_lock = (Model1_raw_spinlock_t) { .Model1_raw_lock = { { (0) } }, }, .Model1_count = Model1_val, .Model1_wait_list = { &((*Model1_sem).Model1_wait_list), &((*Model1_sem).Model1_wait_list) }, };
 do { (void)("semaphore->lock"); (void)(&Model1___key); } while (0);
}

extern void Model1_down(struct Model1_semaphore *Model1_sem);
extern int __attribute__((warn_unused_result)) Model1_down_interruptible(struct Model1_semaphore *Model1_sem);
extern int __attribute__((warn_unused_result)) Model1_down_killable(struct Model1_semaphore *Model1_sem);
extern int __attribute__((warn_unused_result)) Model1_down_trylock(struct Model1_semaphore *Model1_sem);
extern int __attribute__((warn_unused_result)) Model1_down_timeout(struct Model1_semaphore *Model1_sem, long Model1_jiffies);
extern void Model1_up(struct Model1_semaphore *Model1_sem);
/*
 * FS_IOC_FIEMAP ioctl infrastructure.
 *
 * Some portions copyright (C) 2007 Cluster File Systems, Inc
 *
 * Authors: Mark Fasheh <mfasheh@suse.com>
 *          Kalpak Shah <kalpak.shah@sun.com>
 *          Andreas Dilger <adilger@sun.com>
 */






struct Model1_fiemap_extent {
 __u64 Model1_fe_logical; /* logical offset in bytes for the start of
			    * the extent from the beginning of the file */
 __u64 Model1_fe_physical; /* physical offset in bytes for the start
			    * of the extent from the beginning of the disk */
 __u64 Model1_fe_length; /* length in bytes for this extent */
 __u64 Model1_fe_reserved64[2];
 __u32 Model1_fe_flags; /* FIEMAP_EXTENT_* flags for this extent */
 __u32 Model1_fe_reserved[3];
};

struct Model1_fiemap {
 __u64 Model1_fm_start; /* logical offset (inclusive) at
				 * which to start mapping (in) */
 __u64 Model1_fm_length; /* logical length of mapping which
				 * userspace wants (in) */
 __u32 Model1_fm_flags; /* FIEMAP_FLAG_* flags for request (in/out) */
 __u32 Model1_fm_mapped_extents;/* number of extents that were mapped (out) */
 __u32 Model1_fm_extent_count; /* size of fm_extents array (in) */
 __u32 Model1_fm_reserved;
 struct Model1_fiemap_extent Model1_fm_extents[0]; /* array of mapped extents (out) */
};





/*
 * MIGRATE_ASYNC means never block
 * MIGRATE_SYNC_LIGHT in the current implementation means to allow blocking
 *	on most operations but not ->writepage as the potential stall time
 *	is too significant
 * MIGRATE_SYNC will block when migrating pages
 */
enum Model1_migrate_mode {
 Model1_MIGRATE_ASYNC,
 Model1_MIGRATE_SYNC_LIGHT,
 Model1_MIGRATE_SYNC,
};



/*
 * Block data types and constants.  Directly include this file only to
 * break include dependency loop.
 */





/*
 * bvec iterator
 *
 * Copyright (C) 2001 Ming Lei <ming.lei@canonical.com>
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2 as
 * published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 *
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public Licens
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-
 */






/*
 * was unsigned short, but we might as well be ready for > 64kB I/O pages
 */
struct Model1_bio_vec {
 struct Model1_page *Model1_bv_page;
 unsigned int Model1_bv_len;
 unsigned int Model1_bv_offset;
};

struct Model1_bvec_iter {
 Model1_sector_t Model1_bi_sector; /* device address in 512 byte
						   sectors */
 unsigned int Model1_bi_size; /* residual I/O count */

 unsigned int Model1_bi_idx; /* current index into bvl_vec */

 unsigned int Model1_bi_bvec_done; /* number of bytes completed in
						   current bvec */
};

/*
 * various member access, note that bio_data should of course not be used
 * on highmem page vectors
 */
static inline __attribute__((no_instrument_function)) void Model1_bvec_iter_advance(const struct Model1_bio_vec *Model1_bv,
         struct Model1_bvec_iter *Model1_iter,
         unsigned Model1_bytes)
{
 ({ static bool __attribute__ ((__section__(".data.unlikely"))) Model1___warned; int Model1___ret_warn_once = !!(Model1_bytes > Model1_iter->Model1_bi_size); if (__builtin_expect(!!(Model1___ret_warn_once && !Model1___warned), 0)) { Model1___warned = true; ({ int Model1___ret_warn_on = !!(1); if (__builtin_expect(!!(Model1___ret_warn_on), 0)) Model1_warn_slowpath_fmt("./include/linux/bvec.h", 74, "Attempted to advance past end of bvec iter\n"); __builtin_expect(!!(Model1___ret_warn_on), 0); }); } __builtin_expect(!!(Model1___ret_warn_once), 0); });


 while (Model1_bytes) {
  unsigned Model1_iter_len = ({ typeof((*Model1_iter).Model1_bi_size) Model1__min1 = ((*Model1_iter).Model1_bi_size); typeof((&((Model1_bv))[((*Model1_iter)).Model1_bi_idx])->Model1_bv_len - (*Model1_iter).Model1_bi_bvec_done) Model1__min2 = ((&((Model1_bv))[((*Model1_iter)).Model1_bi_idx])->Model1_bv_len - (*Model1_iter).Model1_bi_bvec_done); (void) (&Model1__min1 == &Model1__min2); Model1__min1 < Model1__min2 ? Model1__min1 : Model1__min2; });
  unsigned Model1_len = ({ typeof(Model1_bytes) Model1__min1 = (Model1_bytes); typeof(Model1_iter_len) Model1__min2 = (Model1_iter_len); (void) (&Model1__min1 == &Model1__min2); Model1__min1 < Model1__min2 ? Model1__min1 : Model1__min2; });

  Model1_bytes -= Model1_len;
  Model1_iter->Model1_bi_size -= Model1_len;
  Model1_iter->Model1_bi_bvec_done += Model1_len;

  if (Model1_iter->Model1_bi_bvec_done == (&(Model1_bv)[(*Model1_iter).Model1_bi_idx])->Model1_bv_len) {
   Model1_iter->Model1_bi_bvec_done = 0;
   Model1_iter->Model1_bi_idx++;
  }
 }
}

struct Model1_bio_set;
struct Model1_bio;
struct Model1_bio_integrity_payload;
struct Model1_page;
struct Model1_block_device;
struct Model1_io_context;
struct Model1_cgroup_subsys_state;
typedef void (Model1_bio_end_io_t) (struct Model1_bio *);
typedef void (Model1_bio_destructor_t) (struct Model1_bio *);


/*
 * main unit of I/O for the block layer and lower layers (ie drivers and
 * stacking drivers)
 */
struct Model1_bio {
 struct Model1_bio *Model1_bi_next; /* request queue link */
 struct Model1_block_device *Model1_bi_bdev;
 int Model1_bi_error;
 unsigned int Model1_bi_opf; /* bottom bits req flags,
						 * top bits REQ_OP. Use
						 * accessors.
						 */
 unsigned short Model1_bi_flags; /* status, command, etc */
 unsigned short Model1_bi_ioprio;

 struct Model1_bvec_iter Model1_bi_iter;

 /* Number of segments in this BIO after
	 * physical address coalescing is performed.
	 */
 unsigned int Model1_bi_phys_segments;

 /*
	 * To keep track of the max segment size, we account for the
	 * sizes of the first and last mergeable segments in this bio.
	 */
 unsigned int Model1_bi_seg_front_size;
 unsigned int Model1_bi_seg_back_size;

 Model1_atomic_t Model1___bi_remaining;

 Model1_bio_end_io_t *Model1_bi_end_io;

 void *Model1_bi_private;
 union {



 };

 unsigned short Model1_bi_vcnt; /* how many bio_vec's */

 /*
	 * Everything starting with bi_max_vecs will be preserved by bio_reset()
	 */

 unsigned short Model1_bi_max_vecs; /* max bvl_vecs we can hold */

 Model1_atomic_t Model1___bi_cnt; /* pin count */

 struct Model1_bio_vec *Model1_bi_io_vec; /* the actual vec list */

 struct Model1_bio_set *Model1_bi_pool;

 /*
	 * We can inline a number of vecs at the end of the bio, to avoid
	 * double allocations for a small number of bio_vecs. This member
	 * MUST obviously be kept at the very end of the bio.
	 */
 struct Model1_bio_vec Model1_bi_inline_vecs[0];
};
/*
 * bio flags
 */
/*
 * Flags starting here get preserved by bio_reset() - this includes
 * BVEC_POOL_IDX()
 */


/*
 * We support 6 different bvec pools, the last one is magic in that it
 * is backed by a mempool.
 */



/*
 * Top 4 bits of bio flags indicate the pool the bvecs came from.  We add
 * 1 to the actual index so that 0 indicates that there are no bvecs to be
 * freed.
 */






/*
 * Request flags.  For use in the cmd_flags field of struct request, and in
 * bi_opf of struct bio.  Note that some flags are only valid in either one.
 */
enum Model1_rq_flag_bits {
 /* common flags */
 Model1___REQ_FAILFAST_DEV, /* no driver retries of device errors */
 Model1___REQ_FAILFAST_TRANSPORT, /* no driver retries of transport errors */
 Model1___REQ_FAILFAST_DRIVER, /* no driver retries of driver errors */

 Model1___REQ_SYNC, /* request is sync (sync write or read) */
 Model1___REQ_META, /* metadata io request */
 Model1___REQ_PRIO, /* boost priority in cfq */

 Model1___REQ_NOIDLE, /* don't anticipate more IO after this one */
 Model1___REQ_INTEGRITY, /* I/O includes block integrity payload */
 Model1___REQ_FUA, /* forced unit access */
 Model1___REQ_PREFLUSH, /* request for cache flush */

 /* bio only flags */
 Model1___REQ_RAHEAD, /* read ahead, can fail anytime */
 Model1___REQ_THROTTLED, /* This bio has already been subjected to
				 * throttling rules. Don't do it again. */

 /* request only flags */
 Model1___REQ_SORTED, /* elevator knows about this request */
 Model1___REQ_SOFTBARRIER, /* may not be passed by ioscheduler */
 Model1___REQ_NOMERGE, /* don't touch this for merging */
 Model1___REQ_STARTED, /* drive already may have started this one */
 Model1___REQ_DONTPREP, /* don't call prep for this one */
 Model1___REQ_QUEUED, /* uses queueing */
 Model1___REQ_ELVPRIV, /* elevator private data attached */
 Model1___REQ_FAILED, /* set if the request failed */
 Model1___REQ_QUIET, /* don't worry about errors */
 Model1___REQ_PREEMPT, /* set for "ide_preempt" requests and also
				   for requests for which the SCSI "quiesce"
				   state must be ignored. */
 Model1___REQ_ALLOCED, /* request came from our alloc pool */
 Model1___REQ_COPY_USER, /* contains copies of user pages */
 Model1___REQ_FLUSH_SEQ, /* request for flush sequence */
 Model1___REQ_IO_STAT, /* account I/O stat */
 Model1___REQ_MIXED_MERGE, /* merge of different types, fail separately */
 Model1___REQ_PM, /* runtime pm request */
 Model1___REQ_HASHED, /* on IO scheduler merge hash */
 Model1___REQ_MQ_INFLIGHT, /* track inflight for MQ */
 Model1___REQ_NR_BITS, /* stops here */
};
/* This mask is used for both bio and request merge checking */
enum Model1_req_op {
 Model1_REQ_OP_READ,
 Model1_REQ_OP_WRITE,
 Model1_REQ_OP_DISCARD, /* request to discard sectors */
 Model1_REQ_OP_SECURE_ERASE, /* request to securely erase sectors */
 Model1_REQ_OP_WRITE_SAME, /* write same block many times */
 Model1_REQ_OP_FLUSH, /* request for cache flush */
};



typedef unsigned int Model1_blk_qc_t;



static inline __attribute__((no_instrument_function)) bool Model1_blk_qc_t_valid(Model1_blk_qc_t Model1_cookie)
{
 return Model1_cookie != -1U;
}

static inline __attribute__((no_instrument_function)) Model1_blk_qc_t Model1_blk_tag_to_qc_t(unsigned int Model1_tag, unsigned int Model1_queue_num)
{
 return Model1_tag | (Model1_queue_num << 16);
}

static inline __attribute__((no_instrument_function)) unsigned int Model1_blk_qc_t_to_queue_num(Model1_blk_qc_t Model1_cookie)
{
 return Model1_cookie >> 16;
}

static inline __attribute__((no_instrument_function)) unsigned int Model1_blk_qc_t_to_tag(Model1_blk_qc_t Model1_cookie)
{
 return Model1_cookie & ((1u << 16) - 1);
}





/*
 * Poor man's closures; I wish we could've done them sanely polymorphic,
 * but...
 */

struct Model1_delayed_call {
 void (*Model1_fn)(void *);
 void *Model1_arg;
};



/* I really wish we had closures with sane typechecking... */
static inline __attribute__((no_instrument_function)) void Model1_set_delayed_call(struct Model1_delayed_call *Model1_call,
  void (*Model1_fn)(void *), void *Model1_arg)
{
 Model1_call->Model1_fn = Model1_fn;
 Model1_call->Model1_arg = Model1_arg;
}

static inline __attribute__((no_instrument_function)) void Model1_do_delayed_call(struct Model1_delayed_call *Model1_call)
{
 if (Model1_call->Model1_fn)
  Model1_call->Model1_fn(Model1_call->Model1_arg);
}

static inline __attribute__((no_instrument_function)) void Model1_clear_delayed_call(struct Model1_delayed_call *Model1_call)
{
 Model1_call->Model1_fn = ((void *)0);
}





/*
 * This file has definitions for some important file table structures
 * and constants and structures used by various generic file system
 * ioctl's.  Please do not make any changes in this file before
 * sending patches for review to linux-fsdevel@vger.kernel.org and
 * linux-api@vger.kernel.org.
 */





/*
 * It's silly to have NR_OPEN bigger than NR_FILE, but you can change
 * the file limit at runtime and only root can increase the per-process
 * nr_file rlimit, so it's safe to set up a ridiculously high absolute
 * upper limit on files-per-process.
 *
 * Some programs (notably those using select()) may have to be 
 * recompiled to take full advantage of the new limits..  
 */

/* Fixed constants first: */
struct Model1_file_clone_range {
 Model1___s64 Model1_src_fd;
 __u64 Model1_src_offset;
 __u64 Model1_src_length;
 __u64 Model1_dest_offset;
};

struct Model1_fstrim_range {
 __u64 Model1_start;
 __u64 Model1_len;
 __u64 Model1_minlen;
};

/* extent-same (dedupe) ioctls; these MUST match the btrfs ioctl definitions */



/* from struct btrfs_ioctl_file_extent_same_info */
struct Model1_file_dedupe_range_info {
 Model1___s64 Model1_dest_fd; /* in - destination file */
 __u64 Model1_dest_offset; /* in - start of extent in destination */
 __u64 Model1_bytes_deduped; /* out - total # of bytes we were able
				 * to dedupe from this file. */
 /* status of this dedupe operation:
	 * < 0 for error
	 * == FILE_DEDUPE_RANGE_SAME if dedupe succeeds
	 * == FILE_DEDUPE_RANGE_DIFFERS if data differs
	 */
 Model1___s32 Model1_status; /* out - see above description */
 __u32 Model1_reserved; /* must be zero */
};

/* from struct btrfs_ioctl_file_extent_same_args */
struct Model1_file_dedupe_range {
 __u64 Model1_src_offset; /* in - start of extent in source */
 __u64 Model1_src_length; /* in - length of extent */
 Model1___u16 Model1_dest_count; /* in - total elements in info array */
 Model1___u16 Model1_reserved1; /* must be zero */
 __u32 Model1_reserved2; /* must be zero */
 struct Model1_file_dedupe_range_info Model1_info[0];
};

/* And dynamically-tunable limits and defaults: */
struct Model1_files_stat_struct {
 unsigned long Model1_nr_files; /* read only */
 unsigned long Model1_nr_free_files; /* read only */
 unsigned long Model1_max_files; /* tunable */
};

struct Model1_inodes_stat_t {
 long Model1_nr_inodes;
 long Model1_nr_unused;
 long Model1_dummy[5]; /* padding for sysctl ABI compatibility */
};





/*
 * These are the fs-independent mount-flags: up to 32 flags are supported
 */
/* These sb flags are internal to the kernel */





/*
 * Superblock flags that can be altered by MS_REMOUNT
 */



/*
 * Old magic mount flag and mask
 */



/*
 * Structure for FS_IOC_FSGETXATTR[A] and FS_IOC_FSSETXATTR.
 */
struct Model1_fsxattr {
 __u32 Model1_fsx_xflags; /* xflags field value (get/set) */
 __u32 Model1_fsx_extsize; /* extsize field value (get/set)*/
 __u32 Model1_fsx_nextents; /* nextents field value (get)	*/
 __u32 Model1_fsx_projid; /* project identifier (get/set) */
 unsigned char Model1_fsx_pad[12];
};

/*
 * Flags for the fsx_xflags field
 */
/* the read-only stuff doesn't really belong here, but any other place is
   probably as bad and I don't want to create yet another include file. */
/* A jump here: 108-111 have been used for various private purposes. */
/*
 * File system encryption support
 */
/* Policy provided via an ioctl on the topmost directory */


struct Model1_fscrypt_policy {
 __u8 Model1_version;
 __u8 Model1_contents_encryption_mode;
 __u8 Model1_filenames_encryption_mode;
 __u8 Model1_flags;
 __u8 Model1_master_key_descriptor[8];
} __attribute__((packed));





/*
 * Inode flags (FS_IOC_GETFLAGS / FS_IOC_SETFLAGS)
 *
 * Note: for historical reasons, these flags were originally used and
 * defined for use by ext2/ext3, and then other file systems started
 * using these flags so they wouldn't need to write their own version
 * of chattr/lsattr (which was shipped as part of e2fsprogs).  You
 * should think twice before trying to use these flags in new
 * contexts, or trying to assign these flags, since they are used both
 * as the UAPI and the on-disk encoding for ext2/3/4.  Also, we are
 * almost out of 32-bit flags.  :-)
 *
 * We have recently hoisted FS_IOC_FSGETXATTR / FS_IOC_FSSETXATTR from
 * XFS to the generic FS level interface.  This uses a structure that
 * has padding and hence has more room to grow, so it may be more
 * appropriate for many new use cases.
 *
 * Please do not change these flags or interfaces before checking with
 * linux-fsdevel@vger.kernel.org and linux-api@vger.kernel.org.
 */
/* Reserved for compression usage... */



/* End compression flags --- maybe not all used */
/* flags for preadv2/pwritev2: */

struct Model1_backing_dev_info;
struct Model1_bdi_writeback;
struct Model1_export_operations;
struct Model1_hd_geometry;
struct Model1_iovec;
struct Model1_kiocb;
struct Model1_kobject;
struct Model1_pipe_inode_info;
struct Model1_poll_table_struct;
struct Model1_kstatfs;
struct Model1_vm_area_struct;
struct Model1_vfsmount;
struct Model1_cred;
struct Model1_swap_info_struct;
struct Model1_seq_file;
struct Model1_workqueue_struct;
struct Model1_iov_iter;
struct Model1_fscrypt_info;
struct Model1_fscrypt_operations;

extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_inode_init(void);
extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_inode_init_early(void);
extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_files_init(void);
extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_files_maxfiles_init(void);

extern struct Model1_files_stat_struct Model1_files_stat;
extern unsigned long Model1_get_max_files(void);
extern int Model1_sysctl_nr_open;
extern struct Model1_inodes_stat_t Model1_inodes_stat;
extern int Model1_leases_enable, Model1_lease_break_time;
extern int Model1_sysctl_protected_symlinks;
extern int Model1_sysctl_protected_hardlinks;

struct Model1_buffer_head;
typedef int (Model1_get_block_t)(struct Model1_inode *Model1_inode, Model1_sector_t Model1_iblock,
   struct Model1_buffer_head *Model1_bh_result, int Model1_create);
typedef int (Model1_dio_iodone_t)(struct Model1_kiocb *Model1_iocb, Model1_loff_t Model1_offset,
   Model1_ssize_t Model1_bytes, void *Model1_private);
/* called from RCU mode, don't block */


/*
 * flags in file.f_mode.  Note that FMODE_READ and FMODE_WRITE must correspond
 * to O_WRONLY and O_RDWR via the strange trick in __dentry_open()
 */

/* file is open for reading */

/* file is open for writing */

/* file is seekable */

/* file can be accessed using pread */

/* file can be accessed using pwrite */

/* File is opened for execution with sys_execve / sys_uselib */

/* File is opened with O_NDELAY (only set for block devices) */

/* File is opened with O_EXCL (only set for block devices) */

/* File is opened using open(.., 3, ..) and is writeable only for ioctls
   (specialy hack for floppy.c) */

/* 32bit hashes as llseek() offset (for directories) */

/* 64bit hashes as llseek() offset (for directories) */


/*
 * Don't update ctime and mtime.
 *
 * Currently a special hack for the XFS open_by_handle ioctl, but we'll
 * hopefully graduate it to a proper O_CMTIME flag supported by open(2) soon.
 */


/* Expect random access pattern */


/* File is huge (eg. /dev/kmem): treat loff_t as unsigned */


/* File is opened with O_PATH; almost nothing can be done with it */


/* File needs atomic accesses to f_pos */

/* Write access to underlying fs */

/* Has read method(s) */

/* Has write method(s) */


/* File was opened by fanotify and shouldn't generate fanotify events */


/*
 * Flag for rw_copy_check_uvector and compat_rw_copy_check_uvector
 * that indicates that they should check the contents of the iovec are
 * valid, but not check the memory that the iovec elements
 * points too.
 */


/*
 * The below are the various read and write flags that we support. Some of
 * them include behavioral modifiers that send information down to the
 * block layer and IO scheduler. They should be used along with a req_op.
 * Terminology:
 *
 *	The block layer uses device plugging to defer IO a little bit, in
 *	the hope that we will see more IO very shortly. This increases
 *	coalescing of adjacent IO and thus reduces the number of IOs we
 *	have to send to the device. It also allows for better queuing,
 *	if the IO isn't mergeable. If the caller is going to be waiting
 *	for the IO, then he must ensure that the device is unplugged so
 *	that the IO is dispatched to the driver.
 *
 *	All IO is handled async in Linux. This is fine for background
 *	writes, but for reads or writes that someone waits for completion
 *	on, we want to notify the block layer and IO scheduler so that they
 *	know about it. That allows them to make better scheduling
 *	decisions. So when the below references 'sync' and 'async', it
 *	is referencing this priority hint.
 *
 * With that in mind, the available types are:
 *
 * READ			A normal read operation. Device will be plugged.
 * READ_SYNC		A synchronous read. Device is not plugged, caller can
 *			immediately wait on this read without caring about
 *			unplugging.
 * WRITE		A normal async write. Device will be plugged.
 * WRITE_SYNC		Synchronous write. Identical to WRITE, but passes down
 *			the hint that someone will be waiting on this IO
 *			shortly. The write equivalent of READ_SYNC.
 * WRITE_ODIRECT	Special case write for O_DIRECT only.
 * WRITE_FLUSH		Like WRITE_SYNC but with preceding cache flush.
 * WRITE_FUA		Like WRITE_SYNC but data is guaranteed to be on
 *			non-volatile media on completion.
 * WRITE_FLUSH_FUA	Combination of WRITE_FLUSH and FUA. The IO is preceded
 *			by a cache flush and data is guaranteed to be on
 *			non-volatile media on completion.
 *
 */
/*
 * Attribute flags.  These should be or-ed together to figure out what
 * has been changed!
 */
/*
 * Whiteout is represented by a char device.  The following constants define the
 * mode and device number to use.
 */



/*
 * This is the Inode Attributes structure, used for notify_change().  It
 * uses the above definitions as flags, to know which values have changed.
 * Also, in this manner, a Filesystem can look at only the values it cares
 * about.  Basically, these are the attributes that the VFS layer can
 * request to change from the FS layer.
 *
 * Derek Atkins <warlord@MIT.EDU> 94-10-20
 */
struct Model1_iattr {
 unsigned int Model1_ia_valid;
 Model1_umode_t Model1_ia_mode;
 Model1_kuid_t Model1_ia_uid;
 Model1_kgid_t Model1_ia_gid;
 Model1_loff_t Model1_ia_size;
 struct Model1_timespec Model1_ia_atime;
 struct Model1_timespec Model1_ia_mtime;
 struct Model1_timespec Model1_ia_ctime;

 /*
	 * Not an attribute, but an auxiliary info for filesystems wanting to
	 * implement an ftruncate() like method.  NOTE: filesystem should
	 * check for (ia_valid & ATTR_FILE), and not for (ia_file != NULL).
	 */
 struct Model1_file *Model1_ia_file;
};

/*
 * Includes for diskquotas.
 */

/*
 * Copyright (c) 1982, 1986 Regents of the University of California.
 * All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * Robert Elz at The University of Melbourne.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 */
/*
 * Copyright (c) 1995-2001,2004 Silicon Graphics, Inc.  All Rights Reserved.
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU Lesser General Public License
 * as published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU Lesser General Public License for more details.
 *
 * You should have received a copy of the GNU Lesset General Public License
 * along with this program; if not, write to the Free Software Foundation,
 * Inc.,  51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
 */





/*
 * Disk quota - quotactl(2) commands for the XFS Quota Manager (XQM).
 */
/*
 * fs_disk_quota structure:
 *
 * This contains the current quota information regarding a user/proj/group.
 * It is 64-bit aligned, and all the blk units are in BBs (Basic Blocks) of
 * 512 bytes.
 */

typedef struct Model1_fs_disk_quota {
 Model1___s8 Model1_d_version; /* version of this structure */
 Model1___s8 Model1_d_flags; /* FS_{USER,PROJ,GROUP}_QUOTA */
 Model1___u16 Model1_d_fieldmask; /* field specifier */
 __u32 Model1_d_id; /* user, project, or group ID */
 __u64 Model1_d_blk_hardlimit;/* absolute limit on disk blks */
 __u64 Model1_d_blk_softlimit;/* preferred limit on disk blks */
 __u64 Model1_d_ino_hardlimit;/* maximum # allocated inodes */
 __u64 Model1_d_ino_softlimit;/* preferred inode limit */
 __u64 Model1_d_bcount; /* # disk blocks owned by the user */
 __u64 Model1_d_icount; /* # inodes owned by the user */
 Model1___s32 Model1_d_itimer; /* zero if within inode limits */
     /* if not, we refuse service */
 Model1___s32 Model1_d_btimer; /* similar to above; for disk blocks */
 Model1___u16 Model1_d_iwarns; /* # warnings issued wrt num inodes */
 Model1___u16 Model1_d_bwarns; /* # warnings issued wrt disk blocks */
 Model1___s32 Model1_d_padding2; /* padding2 - for future use */
 __u64 Model1_d_rtb_hardlimit;/* absolute limit on realtime blks */
 __u64 Model1_d_rtb_softlimit;/* preferred limit on RT disk blks */
 __u64 Model1_d_rtbcount; /* # realtime blocks owned */
 Model1___s32 Model1_d_rtbtimer; /* similar to above; for RT disk blks */
 Model1___u16 Model1_d_rtbwarns; /* # warnings issued wrt RT disk blks */
 Model1___s16 Model1_d_padding3; /* padding3 - for future use */
 char Model1_d_padding4[8]; /* yet more padding */
} Model1_fs_disk_quota_t;

/*
 * These fields are sent to Q_XSETQLIM to specify fields that need to change.
 */
/*
 * These timers can only be set in super user's dquot. For others, timers are
 * automatically started and stopped. Superusers timer values set the limits
 * for the rest.  In case these values are zero, the DQ_{F,B}TIMELIMIT values
 * defined below are used. 
 * These values also apply only to the d_fieldmask field for Q_XSETQLIM.
 */





/*
 * Warning counts are set in both super user's dquot and others. For others,
 * warnings are set/cleared by the administrators (or automatically by going
 * below the soft limit).  Superusers warning values set the warning limits
 * for the rest.  In case these values are zero, the DQ_{F,B}WARNLIMIT values
 * defined below are used. 
 * These values also apply only to the d_fieldmask field for Q_XSETQLIM.
 */





/*
 * Accounting values.  These can only be set for filesystem with
 * non-transactional quotas that require quotacheck(8) in userspace.
 */





/*
 * Various flags related to quotactl(2).
 */
/*
 * fs_quota_stat is the struct returned in Q_XGETQSTAT for a given file system.
 * Provides a centralized way to get meta information about the quota subsystem.
 * eg. space taken up for user and group quotas, number of dquots currently
 * incore.
 */


/*
 * Some basic information about 'quota files'.
 */
typedef struct Model1_fs_qfilestat {
 __u64 Model1_qfs_ino; /* inode number */
 __u64 Model1_qfs_nblks; /* number of BBs 512-byte-blks */
 __u32 Model1_qfs_nextents; /* number of extents */
} Model1_fs_qfilestat_t;

typedef struct Model1_fs_quota_stat {
 Model1___s8 Model1_qs_version; /* version number for future changes */
 Model1___u16 Model1_qs_flags; /* FS_QUOTA_{U,P,G}DQ_{ACCT,ENFD} */
 Model1___s8 Model1_qs_pad; /* unused */
 Model1_fs_qfilestat_t Model1_qs_uquota; /* user quota storage information */
 Model1_fs_qfilestat_t Model1_qs_gquota; /* group quota storage information */
 __u32 Model1_qs_incoredqs; /* number of dquots incore */
 Model1___s32 Model1_qs_btimelimit; /* limit for blks timer */
 Model1___s32 Model1_qs_itimelimit; /* limit for inodes timer */
 Model1___s32 Model1_qs_rtbtimelimit;/* limit for rt blks timer */
 Model1___u16 Model1_qs_bwarnlimit; /* limit for num warnings */
 Model1___u16 Model1_qs_iwarnlimit; /* limit for num warnings */
} Model1_fs_quota_stat_t;

/*
 * fs_quota_statv is used by Q_XGETQSTATV for a given file system. It provides
 * a centralized way to get meta information about the quota subsystem. eg.
 * space taken up for user, group, and project quotas, number of dquots
 * currently incore.
 *
 * This version has proper versioning support with appropriate padding for
 * future expansions, and ability to expand for future without creating any
 * backward compatibility issues.
 *
 * Q_XGETQSTATV uses the passed in value of the requested version via
 * fs_quota_statv.qs_version to determine the return data layout of
 * fs_quota_statv.  The kernel will fill the data fields relevant to that
 * version.
 *
 * If kernel does not support user space caller specified version, EINVAL will
 * be returned. User space caller can then reduce the version number and retry
 * the same command.
 */

/*
 * Some basic information about 'quota files' for Q_XGETQSTATV command
 */
struct Model1_fs_qfilestatv {
 __u64 Model1_qfs_ino; /* inode number */
 __u64 Model1_qfs_nblks; /* number of BBs 512-byte-blks */
 __u32 Model1_qfs_nextents; /* number of extents */
 __u32 Model1_qfs_pad; /* pad for 8-byte alignment */
};

struct Model1_fs_quota_statv {
 Model1___s8 Model1_qs_version; /* version for future changes */
 __u8 Model1_qs_pad1; /* pad for 16bit alignment */
 Model1___u16 Model1_qs_flags; /* FS_QUOTA_.* flags */
 __u32 Model1_qs_incoredqs; /* number of dquots incore */
 struct Model1_fs_qfilestatv Model1_qs_uquota; /* user quota information */
 struct Model1_fs_qfilestatv Model1_qs_gquota; /* group quota information */
 struct Model1_fs_qfilestatv Model1_qs_pquota; /* project quota information */
 Model1___s32 Model1_qs_btimelimit; /* limit for blks timer */
 Model1___s32 Model1_qs_itimelimit; /* limit for inodes timer */
 Model1___s32 Model1_qs_rtbtimelimit;/* limit for rt blks timer */
 Model1___u16 Model1_qs_bwarnlimit; /* limit for num warnings */
 Model1___u16 Model1_qs_iwarnlimit; /* limit for num warnings */
 __u64 Model1_qs_pad2[8]; /* for future proofing */
};
/*
 *	File with in-memory structures of old quota format
 */




/* Numbers of blocks needed for updates */
/*
 *  Definitions for vfsv0 quota format
 */





/*
 *	Definitions of structures and functions for quota formats using trie
 */






/* Numbers of blocks needed for updates - we count with the smallest
 * possible block size (1024) */





struct Model1_dquot;
struct Model1_kqid;

/* Operations */
struct Model1_qtree_fmt_operations {
 void (*Model1_mem2disk_dqblk)(void *Model1_disk, struct Model1_dquot *Model1_dquot); /* Convert given entry from in memory format to disk one */
 void (*Model1_disk2mem_dqblk)(struct Model1_dquot *Model1_dquot, void *Model1_disk); /* Convert given entry from disk format to in memory one */
 int (*Model1_is_id)(void *Model1_disk, struct Model1_dquot *Model1_dquot); /* Is this structure for given id? */
};

/* Inmemory copy of version specific information */
struct Model1_qtree_mem_dqinfo {
 struct Model1_super_block *Model1_dqi_sb; /* Sb quota is on */
 int Model1_dqi_type; /* Quota type */
 unsigned int Model1_dqi_blocks; /* # of blocks in quota file */
 unsigned int Model1_dqi_free_blk; /* First block in list of free blocks */
 unsigned int Model1_dqi_free_entry; /* First block with free entry */
 unsigned int Model1_dqi_blocksize_bits; /* Block size of quota file */
 unsigned int Model1_dqi_entry_size; /* Size of quota entry in quota file */
 unsigned int Model1_dqi_usable_bs; /* Space usable in block for quota data */
 unsigned int Model1_dqi_qtree_depth; /* Precomputed depth of quota tree */
 const struct Model1_qtree_fmt_operations *Model1_dqi_ops; /* Operations for entry manipulation */
};

int Model1_qtree_write_dquot(struct Model1_qtree_mem_dqinfo *Model1_info, struct Model1_dquot *Model1_dquot);
int Model1_qtree_read_dquot(struct Model1_qtree_mem_dqinfo *Model1_info, struct Model1_dquot *Model1_dquot);
int Model1_qtree_delete_dquot(struct Model1_qtree_mem_dqinfo *Model1_info, struct Model1_dquot *Model1_dquot);
int Model1_qtree_release_dquot(struct Model1_qtree_mem_dqinfo *Model1_info, struct Model1_dquot *Model1_dquot);
int Model1_qtree_entry_unused(struct Model1_qtree_mem_dqinfo *Model1_info, char *Model1_disk);
static inline __attribute__((no_instrument_function)) int Model1_qtree_depth(struct Model1_qtree_mem_dqinfo *Model1_info)
{
 unsigned int Model1_epb = Model1_info->Model1_dqi_usable_bs >> 2;
 unsigned long long Model1_entries = Model1_epb;
 int Model1_i;

 for (Model1_i = 1; Model1_entries < (1ULL << 32); Model1_i++)
  Model1_entries *= Model1_epb;
 return Model1_i;
}
int Model1_qtree_get_next_id(struct Model1_qtree_mem_dqinfo *Model1_info, struct Model1_kqid *Model1_qid);

/* Numbers of blocks needed for updates */






/*
 * A set of types for the internal kernel types representing project ids.
 *
 * The types defined in this header allow distinguishing which project ids in
 * the kernel are values used by userspace and which project id values are
 * the internal kernel values.  With the addition of user namespaces the values
 * can be different.  Using the type system makes it possible for the compiler
 * to detect when we overlook these differences.
 *
 */


struct Model1_user_namespace;
extern struct Model1_user_namespace Model1_init_user_ns;

typedef Model1___kernel_uid32_t Model1_projid_t;

typedef struct {
 Model1_projid_t Model1_val;
} Model1_kprojid_t;

static inline __attribute__((no_instrument_function)) Model1_projid_t Model1___kprojid_val(Model1_kprojid_t Model1_projid)
{
 return Model1_projid.Model1_val;
}






static inline __attribute__((no_instrument_function)) bool Model1_projid_eq(Model1_kprojid_t Model1_left, Model1_kprojid_t Model1_right)
{
 return Model1___kprojid_val(Model1_left) == Model1___kprojid_val(Model1_right);
}

static inline __attribute__((no_instrument_function)) bool Model1_projid_lt(Model1_kprojid_t Model1_left, Model1_kprojid_t Model1_right)
{
 return Model1___kprojid_val(Model1_left) < Model1___kprojid_val(Model1_right);
}

static inline __attribute__((no_instrument_function)) bool Model1_projid_valid(Model1_kprojid_t Model1_projid)
{
 return !Model1_projid_eq(Model1_projid, (Model1_kprojid_t){ -1 });
}
static inline __attribute__((no_instrument_function)) Model1_kprojid_t Model1_make_kprojid(struct Model1_user_namespace *Model1_from, Model1_projid_t Model1_projid)
{
 return (Model1_kprojid_t){ Model1_projid };
}

static inline __attribute__((no_instrument_function)) Model1_projid_t Model1_from_kprojid(struct Model1_user_namespace *Model1_to, Model1_kprojid_t Model1_kprojid)
{
 return Model1___kprojid_val(Model1_kprojid);
}

static inline __attribute__((no_instrument_function)) Model1_projid_t Model1_from_kprojid_munged(struct Model1_user_namespace *Model1_to, Model1_kprojid_t Model1_kprojid)
{
 Model1_projid_t Model1_projid = Model1_from_kprojid(Model1_to, Model1_kprojid);
 if (Model1_projid == (Model1_projid_t)-1)
  Model1_projid = 65534;
 return Model1_projid;
}

static inline __attribute__((no_instrument_function)) bool Model1_kprojid_has_mapping(struct Model1_user_namespace *Model1_ns, Model1_kprojid_t Model1_projid)
{
 return true;
}
/*
 * Copyright (c) 1982, 1986 Regents of the University of California.
 * All rights reserved.
 *
 * This code is derived from software contributed to Berkeley by
 * Robert Elz at The University of Melbourne.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 3. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 */
/*
 * Definitions for the default names of the quotas files.
 */







/*
 * Command definitions for the 'quotactl' system call.
 * The commands are broken into a main command defined below
 * and a subcommand that is used to convey the type of
 * quota that is being manipulated (see above).
 */
/* Quota format type IDs */





/* Size of block in which space limits are passed through the quota
 * interface */



/*
 * Quota structure used for communication with userspace via quotactl
 * Following flags are used to specify which fields are valid
 */
enum {
 Model1_QIF_BLIMITS_B = 0,
 Model1_QIF_SPACE_B,
 Model1_QIF_ILIMITS_B,
 Model1_QIF_INODES_B,
 Model1_QIF_BTIME_B,
 Model1_QIF_ITIME_B,
};
struct Model1_if_dqblk {
 __u64 Model1_dqb_bhardlimit;
 __u64 Model1_dqb_bsoftlimit;
 __u64 Model1_dqb_curspace;
 __u64 Model1_dqb_ihardlimit;
 __u64 Model1_dqb_isoftlimit;
 __u64 Model1_dqb_curinodes;
 __u64 Model1_dqb_btime;
 __u64 Model1_dqb_itime;
 __u32 Model1_dqb_valid;
};

struct Model1_if_nextdqblk {
 __u64 Model1_dqb_bhardlimit;
 __u64 Model1_dqb_bsoftlimit;
 __u64 Model1_dqb_curspace;
 __u64 Model1_dqb_ihardlimit;
 __u64 Model1_dqb_isoftlimit;
 __u64 Model1_dqb_curinodes;
 __u64 Model1_dqb_btime;
 __u64 Model1_dqb_itime;
 __u32 Model1_dqb_valid;
 __u32 Model1_dqb_id;
};

/*
 * Structure used for setting quota information about file via quotactl
 * Following flags are used to specify which fields are valid
 */





enum {
 Model1_DQF_ROOT_SQUASH_B = 0,
 Model1_DQF_SYS_FILE_B = 16,
 /* Kernel internal flags invisible to userspace */
 Model1_DQF_PRIVATE
};

/* Root squash enabled (for v1 quota format) */

/* Quota stored in a system file */


struct Model1_if_dqinfo {
 __u64 Model1_dqi_bgrace;
 __u64 Model1_dqi_igrace;
 __u32 Model1_dqi_flags; /* DFQ_* */
 __u32 Model1_dqi_valid;
};

/*
 * Definitions for quota netlink interface
 */
enum {
 Model1_QUOTA_NL_C_UNSPEC,
 Model1_QUOTA_NL_C_WARNING,
 Model1___QUOTA_NL_C_MAX,
};


enum {
 Model1_QUOTA_NL_A_UNSPEC,
 Model1_QUOTA_NL_A_QTYPE,
 Model1_QUOTA_NL_A_EXCESS_ID,
 Model1_QUOTA_NL_A_WARNING,
 Model1_QUOTA_NL_A_DEV_MAJOR,
 Model1_QUOTA_NL_A_DEV_MINOR,
 Model1_QUOTA_NL_A_CAUSED_ID,
 Model1_QUOTA_NL_A_PAD,
 Model1___QUOTA_NL_A_MAX,
};




enum Model1_quota_type {
 Model1_USRQUOTA = 0, /* element used for user quotas */
 Model1_GRPQUOTA = 1, /* element used for group quotas */
 Model1_PRJQUOTA = 2, /* element used for project quotas */
};

/* Masks for quota types when used as a bitmask */




typedef Model1___kernel_uid32_t Model1_qid_t; /* Type in which we store ids in memory */
typedef long long Model1_qsize_t; /* Type in which we store sizes */

struct Model1_kqid { /* Type in which we store the quota identifier */
 union {
  Model1_kuid_t Model1_uid;
  Model1_kgid_t Model1_gid;
  Model1_kprojid_t Model1_projid;
 };
 enum Model1_quota_type Model1_type; /* USRQUOTA (uid) or GRPQUOTA (gid) or PRJQUOTA (projid) */
};

extern bool Model1_qid_eq(struct Model1_kqid Model1_left, struct Model1_kqid Model1_right);
extern bool Model1_qid_lt(struct Model1_kqid Model1_left, struct Model1_kqid Model1_right);
extern Model1_qid_t Model1_from_kqid(struct Model1_user_namespace *Model1_to, struct Model1_kqid Model1_qid);
extern Model1_qid_t Model1_from_kqid_munged(struct Model1_user_namespace *Model1_to, struct Model1_kqid Model1_qid);
extern bool Model1_qid_valid(struct Model1_kqid Model1_qid);

/**
 *	make_kqid - Map a user-namespace, type, qid tuple into a kqid.
 *	@from: User namespace that the qid is in
 *	@type: The type of quota
 *	@qid: Quota identifier
 *
 *	Maps a user-namespace, type qid tuple into a kernel internal
 *	kqid, and returns that kqid.
 *
 *	When there is no mapping defined for the user-namespace, type,
 *	qid tuple an invalid kqid is returned.  Callers are expected to
 *	test for and handle handle invalid kqids being returned.
 *	Invalid kqids may be tested for using qid_valid().
 */
static inline __attribute__((no_instrument_function)) struct Model1_kqid Model1_make_kqid(struct Model1_user_namespace *Model1_from,
        enum Model1_quota_type Model1_type, Model1_qid_t Model1_qid)
{
 struct Model1_kqid Model1_kqid;

 Model1_kqid.Model1_type = Model1_type;
 switch (Model1_type) {
 case Model1_USRQUOTA:
  Model1_kqid.Model1_uid = Model1_make_kuid(Model1_from, Model1_qid);
  break;
 case Model1_GRPQUOTA:
  Model1_kqid.Model1_gid = Model1_make_kgid(Model1_from, Model1_qid);
  break;
 case Model1_PRJQUOTA:
  Model1_kqid.Model1_projid = Model1_make_kprojid(Model1_from, Model1_qid);
  break;
 default:
  do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/quota.h"), "i" (114), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0);
 }
 return Model1_kqid;
}

/**
 *	make_kqid_invalid - Explicitly make an invalid kqid
 *	@type: The type of quota identifier
 *
 *	Returns an invalid kqid with the specified type.
 */
static inline __attribute__((no_instrument_function)) struct Model1_kqid Model1_make_kqid_invalid(enum Model1_quota_type Model1_type)
{
 struct Model1_kqid Model1_kqid;

 Model1_kqid.Model1_type = Model1_type;
 switch (Model1_type) {
 case Model1_USRQUOTA:
  Model1_kqid.Model1_uid = (Model1_kuid_t){ -1 };
  break;
 case Model1_GRPQUOTA:
  Model1_kqid.Model1_gid = (Model1_kgid_t){ -1 };
  break;
 case Model1_PRJQUOTA:
  Model1_kqid.Model1_projid = (Model1_kprojid_t){ -1 };
  break;
 default:
  do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/quota.h"), "i" (141), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0);
 }
 return Model1_kqid;
}

/**
 *	make_kqid_uid - Make a kqid from a kuid
 *	@uid: The kuid to make the quota identifier from
 */
static inline __attribute__((no_instrument_function)) struct Model1_kqid Model1_make_kqid_uid(Model1_kuid_t Model1_uid)
{
 struct Model1_kqid Model1_kqid;
 Model1_kqid.Model1_type = Model1_USRQUOTA;
 Model1_kqid.Model1_uid = Model1_uid;
 return Model1_kqid;
}

/**
 *	make_kqid_gid - Make a kqid from a kgid
 *	@gid: The kgid to make the quota identifier from
 */
static inline __attribute__((no_instrument_function)) struct Model1_kqid Model1_make_kqid_gid(Model1_kgid_t Model1_gid)
{
 struct Model1_kqid Model1_kqid;
 Model1_kqid.Model1_type = Model1_GRPQUOTA;
 Model1_kqid.Model1_gid = Model1_gid;
 return Model1_kqid;
}

/**
 *	make_kqid_projid - Make a kqid from a projid
 *	@projid: The kprojid to make the quota identifier from
 */
static inline __attribute__((no_instrument_function)) struct Model1_kqid Model1_make_kqid_projid(Model1_kprojid_t Model1_projid)
{
 struct Model1_kqid Model1_kqid;
 Model1_kqid.Model1_type = Model1_PRJQUOTA;
 Model1_kqid.Model1_projid = Model1_projid;
 return Model1_kqid;
}

/**
 *	qid_has_mapping - Report if a qid maps into a user namespace.
 *	@ns:  The user namespace to see if a value maps into.
 *	@qid: The kernel internal quota identifier to test.
 */
static inline __attribute__((no_instrument_function)) bool Model1_qid_has_mapping(struct Model1_user_namespace *Model1_ns, struct Model1_kqid Model1_qid)
{
 return Model1_from_kqid(Model1_ns, Model1_qid) != (Model1_qid_t) -1;
}


extern Model1_spinlock_t Model1_dq_data_lock;

/* Maximal numbers of writes for quota operation (insert/delete/update)
 * (over VFS all formats) */





/*
 * Data for one user/group kept in memory
 */
struct Model1_mem_dqblk {
 Model1_qsize_t Model1_dqb_bhardlimit; /* absolute limit on disk blks alloc */
 Model1_qsize_t Model1_dqb_bsoftlimit; /* preferred limit on disk blks */
 Model1_qsize_t Model1_dqb_curspace; /* current used space */
 Model1_qsize_t Model1_dqb_rsvspace; /* current reserved space for delalloc*/
 Model1_qsize_t Model1_dqb_ihardlimit; /* absolute limit on allocated inodes */
 Model1_qsize_t Model1_dqb_isoftlimit; /* preferred inode limit */
 Model1_qsize_t Model1_dqb_curinodes; /* current # allocated inodes */
 Model1_time64_t Model1_dqb_btime; /* time limit for excessive disk use */
 Model1_time64_t Model1_dqb_itime; /* time limit for excessive inode use */
};

/*
 * Data for one quotafile kept in memory
 */
struct Model1_quota_format_type;

struct Model1_mem_dqinfo {
 struct Model1_quota_format_type *Model1_dqi_format;
 int Model1_dqi_fmt_id; /* Id of the dqi_format - used when turning
				 * quotas on after remount RW */
 struct Model1_list_head Model1_dqi_dirty_list; /* List of dirty dquots */
 unsigned long Model1_dqi_flags;
 unsigned int Model1_dqi_bgrace;
 unsigned int Model1_dqi_igrace;
 Model1_qsize_t Model1_dqi_max_spc_limit;
 Model1_qsize_t Model1_dqi_max_ino_limit;
 void *Model1_dqi_priv;
};

struct Model1_super_block;

/* Mask for flags passed to userspace */

/* Mask for flags modifiable from userspace */


enum {
 Model1_DQF_INFO_DIRTY_B = Model1_DQF_PRIVATE,
};


extern void Model1_mark_info_dirty(struct Model1_super_block *Model1_sb, int Model1_type);
static inline __attribute__((no_instrument_function)) int Model1_info_dirty(struct Model1_mem_dqinfo *Model1_info)
{
 return (__builtin_constant_p((Model1_DQF_INFO_DIRTY_B)) ? Model1_constant_test_bit((Model1_DQF_INFO_DIRTY_B), (&Model1_info->Model1_dqi_flags)) : Model1_variable_test_bit((Model1_DQF_INFO_DIRTY_B), (&Model1_info->Model1_dqi_flags)));
}

enum {
 Model1_DQST_LOOKUPS,
 Model1_DQST_DROPS,
 Model1_DQST_READS,
 Model1_DQST_WRITES,
 Model1_DQST_CACHE_HITS,
 Model1_DQST_ALLOC_DQUOTS,
 Model1_DQST_FREE_DQUOTS,
 Model1_DQST_SYNCS,
 Model1__DQST_DQSTAT_LAST
};

struct Model1_dqstats {
 int Model1_stat[Model1__DQST_DQSTAT_LAST];
 struct Model1_percpu_counter Model1_counter[Model1__DQST_DQSTAT_LAST];
};

extern struct Model1_dqstats *Model1_dqstats_pcpu;
extern struct Model1_dqstats Model1_dqstats;

static inline __attribute__((no_instrument_function)) void Model1_dqstats_inc(unsigned int Model1_type)
{
 Model1_percpu_counter_inc(&Model1_dqstats.Model1_counter[Model1_type]);
}

static inline __attribute__((no_instrument_function)) void Model1_dqstats_dec(unsigned int Model1_type)
{
 Model1_percpu_counter_dec(&Model1_dqstats.Model1_counter[Model1_type]);
}
struct Model1_dquot {
 struct Model1_hlist_node Model1_dq_hash; /* Hash list in memory */
 struct Model1_list_head Model1_dq_inuse; /* List of all quotas */
 struct Model1_list_head Model1_dq_free; /* Free list element */
 struct Model1_list_head Model1_dq_dirty; /* List of dirty dquots */
 struct Model1_mutex Model1_dq_lock; /* dquot IO lock */
 Model1_atomic_t Model1_dq_count; /* Use count */
 Model1_wait_queue_head_t Model1_dq_wait_unused; /* Wait queue for dquot to become unused */
 struct Model1_super_block *Model1_dq_sb; /* superblock this applies to */
 struct Model1_kqid Model1_dq_id; /* ID this applies to (uid, gid, projid) */
 Model1_loff_t Model1_dq_off; /* Offset of dquot on disk */
 unsigned long Model1_dq_flags; /* See DQ_* */
 struct Model1_mem_dqblk Model1_dq_dqb; /* Diskquota usage */
};

/* Operations which must be implemented by each quota format */
struct Model1_quota_format_ops {
 int (*Model1_check_quota_file)(struct Model1_super_block *Model1_sb, int Model1_type); /* Detect whether file is in our format */
 int (*Model1_read_file_info)(struct Model1_super_block *Model1_sb, int Model1_type); /* Read main info about file - called on quotaon() */
 int (*Model1_write_file_info)(struct Model1_super_block *Model1_sb, int Model1_type); /* Write main info about file */
 int (*Model1_free_file_info)(struct Model1_super_block *Model1_sb, int Model1_type); /* Called on quotaoff() */
 int (*Model1_read_dqblk)(struct Model1_dquot *Model1_dquot); /* Read structure for one user */
 int (*Model1_commit_dqblk)(struct Model1_dquot *Model1_dquot); /* Write structure for one user */
 int (*Model1_release_dqblk)(struct Model1_dquot *Model1_dquot); /* Called when last reference to dquot is being dropped */
 int (*Model1_get_next_id)(struct Model1_super_block *Model1_sb, struct Model1_kqid *Model1_qid); /* Get next ID with existing structure in the quota file */
};

/* Operations working with dquots */
struct Model1_dquot_operations {
 int (*Model1_write_dquot) (struct Model1_dquot *); /* Ordinary dquot write */
 struct Model1_dquot *(*Model1_alloc_dquot)(struct Model1_super_block *, int); /* Allocate memory for new dquot */
 void (*Model1_destroy_dquot)(struct Model1_dquot *); /* Free memory for dquot */
 int (*Model1_acquire_dquot) (struct Model1_dquot *); /* Quota is going to be created on disk */
 int (*Model1_release_dquot) (struct Model1_dquot *); /* Quota is going to be deleted from disk */
 int (*Model1_mark_dirty) (struct Model1_dquot *); /* Dquot is marked dirty */
 int (*Model1_write_info) (struct Model1_super_block *, int); /* Write of quota "superblock" */
 /* get reserved quota for delayed alloc, value returned is managed by
	 * quota code only */
 Model1_qsize_t *(*Model1_get_reserved_space) (struct Model1_inode *);
 int (*Model1_get_projid) (struct Model1_inode *, Model1_kprojid_t *);/* Get project ID */
 /* Get next ID with active quota structure */
 int (*Model1_get_next_id) (struct Model1_super_block *Model1_sb, struct Model1_kqid *Model1_qid);
};

struct Model1_path;

/* Structure for communicating via ->get_dqblk() & ->set_dqblk() */
struct Model1_qc_dqblk {
 int Model1_d_fieldmask; /* mask of fields to change in ->set_dqblk() */
 Model1_u64 Model1_d_spc_hardlimit; /* absolute limit on used space */
 Model1_u64 Model1_d_spc_softlimit; /* preferred limit on used space */
 Model1_u64 Model1_d_ino_hardlimit; /* maximum # allocated inodes */
 Model1_u64 Model1_d_ino_softlimit; /* preferred inode limit */
 Model1_u64 Model1_d_space; /* Space owned by the user */
 Model1_u64 Model1_d_ino_count; /* # inodes owned by the user */
 Model1_s64 Model1_d_ino_timer; /* zero if within inode limits */
    /* if not, we refuse service */
 Model1_s64 Model1_d_spc_timer; /* similar to above; for space */
 int Model1_d_ino_warns; /* # warnings issued wrt num inodes */
 int Model1_d_spc_warns; /* # warnings issued wrt used space */
 Model1_u64 Model1_d_rt_spc_hardlimit; /* absolute limit on realtime space */
 Model1_u64 Model1_d_rt_spc_softlimit; /* preferred limit on RT space */
 Model1_u64 Model1_d_rt_space; /* realtime space owned */
 Model1_s64 Model1_d_rt_spc_timer; /* similar to above; for RT space */
 int Model1_d_rt_spc_warns; /* # warnings issued wrt RT space */
};

/*
 * Field specifiers for ->set_dqblk() in struct qc_dqblk and also for
 * ->set_info() in struct qc_info
 */
/* Structures for communicating via ->get_state */
struct Model1_qc_type_state {
 unsigned int Model1_flags; /* Flags QCI_* */
 unsigned int Model1_spc_timelimit; /* Time after which space softlimit is
					 * enforced */
 unsigned int Model1_ino_timelimit; /* Ditto for inode softlimit */
 unsigned int Model1_rt_spc_timelimit; /* Ditto for real-time space */
 unsigned int Model1_spc_warnlimit; /* Limit for number of space warnings */
 unsigned int Model1_ino_warnlimit; /* Ditto for inodes */
 unsigned int Model1_rt_spc_warnlimit; /* Ditto for real-time space */
 unsigned long long Model1_ino; /* Inode number of quota file */
 Model1_blkcnt_t Model1_blocks; /* Number of 512-byte blocks in the file */
 Model1_blkcnt_t Model1_nextents; /* Number of extents in the file */
};

struct Model1_qc_state {
 unsigned int Model1_s_incoredqs; /* Number of dquots in core */
 /*
	 * Per quota type information. The array should really have
	 * max(MAXQUOTAS, XQM_MAXQUOTAS) entries. BUILD_BUG_ON in
	 * quota_getinfo() makes sure XQM_MAXQUOTAS is large enough.  Once VFS
	 * supports project quotas, this can be changed to MAXQUOTAS
	 */
 struct Model1_qc_type_state Model1_s_state[3];
};

/* Structure for communicating via ->set_info */
struct Model1_qc_info {
 int Model1_i_fieldmask; /* mask of fields to change in ->set_info() */
 unsigned int Model1_i_flags; /* Flags QCI_* */
 unsigned int Model1_i_spc_timelimit; /* Time after which space softlimit is
					 * enforced */
 unsigned int Model1_i_ino_timelimit; /* Ditto for inode softlimit */
 unsigned int Model1_i_rt_spc_timelimit;/* Ditto for real-time space */
 unsigned int Model1_i_spc_warnlimit; /* Limit for number of space warnings */
 unsigned int Model1_i_ino_warnlimit; /* Limit for number of inode warnings */
 unsigned int Model1_i_rt_spc_warnlimit; /* Ditto for real-time space */
};

/* Operations handling requests from userspace */
struct Model1_quotactl_ops {
 int (*Model1_quota_on)(struct Model1_super_block *, int, int, struct Model1_path *);
 int (*Model1_quota_off)(struct Model1_super_block *, int);
 int (*Model1_quota_enable)(struct Model1_super_block *, unsigned int);
 int (*Model1_quota_disable)(struct Model1_super_block *, unsigned int);
 int (*Model1_quota_sync)(struct Model1_super_block *, int);
 int (*Model1_set_info)(struct Model1_super_block *, int, struct Model1_qc_info *);
 int (*Model1_get_dqblk)(struct Model1_super_block *, struct Model1_kqid, struct Model1_qc_dqblk *);
 int (*Model1_get_nextdqblk)(struct Model1_super_block *, struct Model1_kqid *,
        struct Model1_qc_dqblk *);
 int (*Model1_set_dqblk)(struct Model1_super_block *, struct Model1_kqid, struct Model1_qc_dqblk *);
 int (*Model1_get_state)(struct Model1_super_block *, struct Model1_qc_state *);
 int (*Model1_rm_xquota)(struct Model1_super_block *, unsigned int);
};

struct Model1_quota_format_type {
 int Model1_qf_fmt_id; /* Quota format id */
 const struct Model1_quota_format_ops *Model1_qf_ops; /* Operations of format */
 struct Model1_module *Model1_qf_owner; /* Module implementing quota format */
 struct Model1_quota_format_type *Model1_qf_next;
};

/**
 * Quota state flags - they actually come in two flavors - for users and groups.
 *
 * Actual typed flags layout:
 *				USRQUOTA	GRPQUOTA
 *  DQUOT_USAGE_ENABLED		0x0001		0x0002
 *  DQUOT_LIMITS_ENABLED	0x0004		0x0008
 *  DQUOT_SUSPENDED		0x0010		0x0020
 *
 * Following bits are used for non-typed flags:
 *  DQUOT_QUOTA_SYS_FILE	0x0040
 *  DQUOT_NEGATIVE_USAGE	0x0080
 */
enum {
 Model1__DQUOT_USAGE_ENABLED = 0, /* Track disk usage for users */
 Model1__DQUOT_LIMITS_ENABLED, /* Enforce quota limits for users */
 Model1__DQUOT_SUSPENDED, /* User diskquotas are off, but
						 * we have necessary info in
						 * memory to turn them on */
 Model1__DQUOT_STATE_FLAGS
};





/* Other quota flags */


      /* Quota file is a special
						 * system file and user cannot
						 * touch it. Filesystem is
						 * responsible for setting
						 * S_NOQUOTA, S_NOATIME flags
						 */

            /* Allow negative quota usage */
static inline __attribute__((no_instrument_function)) unsigned int Model1_dquot_state_flag(unsigned int Model1_flags, int Model1_type)
{
 return Model1_flags << Model1_type;
}

static inline __attribute__((no_instrument_function)) unsigned int Model1_dquot_generic_flag(unsigned int Model1_flags, int Model1_type)
{
 return (Model1_flags >> Model1_type) & ((1 << Model1__DQUOT_USAGE_ENABLED * 3) | (1 << Model1__DQUOT_LIMITS_ENABLED * 3) | (1 << Model1__DQUOT_SUSPENDED * 3));
}

/* Bitmap of quota types where flag is set in flags */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) unsigned Model1_dquot_state_types(unsigned Model1_flags, unsigned Model1_flag)
{
 do { bool Model1___cond = !(!((Model1_flag) == 0 || (((Model1_flag) & ((Model1_flag) - 1)) != 0))); extern void Model1___compiletime_assert_505(void) ; if (Model1___cond) Model1___compiletime_assert_505(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
 return (Model1_flags / Model1_flag) & ((1 << 3) - 1);
}


extern void Model1_quota_send_warning(struct Model1_kqid Model1_qid, Model1_dev_t Model1_dev,
          const char Model1_warntype);
struct Model1_quota_info {
 unsigned int Model1_flags; /* Flags for diskquotas on this device */
 struct Model1_mutex Model1_dqio_mutex; /* lock device while I/O in progress */
 struct Model1_mutex Model1_dqonoff_mutex; /* Serialize quotaon & quotaoff */
 struct Model1_inode *Model1_files[3]; /* inodes of quotafiles */
 struct Model1_mem_dqinfo Model1_info[3]; /* Information for each quota type */
 const struct Model1_quota_format_ops *Model1_ops[3]; /* Operations for each type */
};

int Model1_register_quota_format(struct Model1_quota_format_type *Model1_fmt);
void Model1_unregister_quota_format(struct Model1_quota_format_type *Model1_fmt);

struct Model1_quota_module_name {
 int Model1_qm_fmt_id;
 char *Model1_qm_mod_name;
};

/*
 * Maximum number of layers of fs stack.  Needs to be limited to
 * prevent kernel stack overflow
 */


/** 
 * enum positive_aop_returns - aop return codes with specific semantics
 *
 * @AOP_WRITEPAGE_ACTIVATE: Informs the caller that page writeback has
 * 			    completed, that the page is still locked, and
 * 			    should be considered active.  The VM uses this hint
 * 			    to return the page to the active list -- it won't
 * 			    be a candidate for writeback again in the near
 * 			    future.  Other callers must be careful to unlock
 * 			    the page if they get this return.  Returned by
 * 			    writepage(); 
 *
 * @AOP_TRUNCATED_PAGE: The AOP method that was handed a locked page has
 *  			unlocked it and the page might have been truncated.
 *  			The caller should back up to acquiring a new page and
 *  			trying again.  The aop will be taking reasonable
 *  			precautions not to livelock.  If the caller held a page
 *  			reference, it should drop it before retrying.  Returned
 *  			by readpage().
 *
 * address_space_operation functions return these large constants to indicate
 * special semantics to the caller.  These are much larger than the bytes in a
 * page to allow for functions that return the number of bytes operated on in a
 * given page.
 */

enum Model1_positive_aop_returns {
 Model1_AOP_WRITEPAGE_ACTIVATE = 0x80000,
 Model1_AOP_TRUNCATED_PAGE = 0x80001,
};







/*
 * oh the beauties of C type declarations.
 */
struct Model1_page;
struct Model1_address_space;
struct Model1_writeback_control;
struct Model1_kiocb {
 struct Model1_file *Model1_ki_filp;
 Model1_loff_t Model1_ki_pos;
 void (*Model1_ki_complete)(struct Model1_kiocb *Model1_iocb, long Model1_ret, long Model1_ret2);
 void *Model1_private;
 int Model1_ki_flags;
};

static inline __attribute__((no_instrument_function)) bool Model1_is_sync_kiocb(struct Model1_kiocb *Model1_kiocb)
{
 return Model1_kiocb->Model1_ki_complete == ((void *)0);
}

static inline __attribute__((no_instrument_function)) int Model1_iocb_flags(struct Model1_file *Model1_file);

static inline __attribute__((no_instrument_function)) void Model1_init_sync_kiocb(struct Model1_kiocb *Model1_kiocb, struct Model1_file *Model1_filp)
{
 *Model1_kiocb = (struct Model1_kiocb) {
  .Model1_ki_filp = Model1_filp,
  .Model1_ki_flags = Model1_iocb_flags(Model1_filp),
 };
}

/*
 * "descriptor" for what we're up to with a read.
 * This allows us to use the same read code yet
 * have multiple different users of the data that
 * we read from a file.
 *
 * The simplest case just copies the data to user
 * mode.
 */
typedef struct {
 Model1_size_t Model1_written;
 Model1_size_t Model1_count;
 union {
  char *Model1_buf;
  void *Model1_data;
 } Model1_arg;
 int error;
} Model1_read_descriptor_t;

typedef int (*Model1_read_actor_t)(Model1_read_descriptor_t *, struct Model1_page *,
  unsigned long, unsigned long);

struct Model1_address_space_operations {
 int (*Model1_writepage)(struct Model1_page *Model1_page, struct Model1_writeback_control *Model1_wbc);
 int (*Model1_readpage)(struct Model1_file *, struct Model1_page *);

 /* Write back some dirty pages from this mapping. */
 int (*Model1_writepages)(struct Model1_address_space *, struct Model1_writeback_control *);

 /* Set a page dirty.  Return true if this dirtied it */
 int (*Model1_set_page_dirty)(struct Model1_page *Model1_page);

 int (*Model1_readpages)(struct Model1_file *Model1_filp, struct Model1_address_space *Model1_mapping,
   struct Model1_list_head *Model1_pages, unsigned Model1_nr_pages);

 int (*Model1_write_begin)(struct Model1_file *, struct Model1_address_space *Model1_mapping,
    Model1_loff_t Model1_pos, unsigned Model1_len, unsigned Model1_flags,
    struct Model1_page **Model1_pagep, void **Model1_fsdata);
 int (*Model1_write_end)(struct Model1_file *, struct Model1_address_space *Model1_mapping,
    Model1_loff_t Model1_pos, unsigned Model1_len, unsigned Model1_copied,
    struct Model1_page *Model1_page, void *Model1_fsdata);

 /* Unfortunately this kludge is needed for FIBMAP. Don't use it */
 Model1_sector_t (*Model1_bmap)(struct Model1_address_space *, Model1_sector_t);
 void (*Model1_invalidatepage) (struct Model1_page *, unsigned int, unsigned int);
 int (*Model1_releasepage) (struct Model1_page *, Model1_gfp_t);
 void (*Model1_freepage)(struct Model1_page *);
 Model1_ssize_t (*Model1_direct_IO)(struct Model1_kiocb *, struct Model1_iov_iter *Model1_iter);
 /*
	 * migrate the contents of a page to the specified target. If
	 * migrate_mode is MIGRATE_ASYNC, it must not block.
	 */
 int (*Model1_migratepage) (struct Model1_address_space *,
   struct Model1_page *, struct Model1_page *, enum Model1_migrate_mode);
 bool (*Model1_isolate_page)(struct Model1_page *, Model1_isolate_mode_t);
 void (*Model1_putback_page)(struct Model1_page *);
 int (*Model1_launder_page) (struct Model1_page *);
 int (*Model1_is_partially_uptodate) (struct Model1_page *, unsigned long,
     unsigned long);
 void (*Model1_is_dirty_writeback) (struct Model1_page *, bool *, bool *);
 int (*Model1_error_remove_page)(struct Model1_address_space *, struct Model1_page *);

 /* swapfile support */
 int (*Model1_swap_activate)(struct Model1_swap_info_struct *Model1_sis, struct Model1_file *Model1_file,
    Model1_sector_t *Model1_span);
 void (*Model1_swap_deactivate)(struct Model1_file *Model1_file);
};

extern const struct Model1_address_space_operations Model1_empty_aops;

/*
 * pagecache_write_begin/pagecache_write_end must be used by general code
 * to write into the pagecache.
 */
int Model1_pagecache_write_begin(struct Model1_file *, struct Model1_address_space *Model1_mapping,
    Model1_loff_t Model1_pos, unsigned Model1_len, unsigned Model1_flags,
    struct Model1_page **Model1_pagep, void **Model1_fsdata);

int Model1_pagecache_write_end(struct Model1_file *, struct Model1_address_space *Model1_mapping,
    Model1_loff_t Model1_pos, unsigned Model1_len, unsigned Model1_copied,
    struct Model1_page *Model1_page, void *Model1_fsdata);

struct Model1_address_space {
 struct Model1_inode *Model1_host; /* owner: inode, block_device */
 struct Model1_radix_tree_root Model1_page_tree; /* radix tree of all pages */
 Model1_spinlock_t Model1_tree_lock; /* and lock protecting it */
 Model1_atomic_t Model1_i_mmap_writable;/* count VM_SHARED mappings */
 struct Model1_rb_root Model1_i_mmap; /* tree of private and shared mappings */
 struct Model1_rw_semaphore Model1_i_mmap_rwsem; /* protect tree, count, list */
 /* Protected by tree_lock together with the radix tree */
 unsigned long Model1_nrpages; /* number of total pages */
 /* number of shadow or DAX exceptional entries */
 unsigned long Model1_nrexceptional;
 unsigned long Model1_writeback_index;/* writeback starts here */
 const struct Model1_address_space_operations *Model1_a_ops; /* methods */
 unsigned long Model1_flags; /* error bits/gfp mask */
 Model1_spinlock_t Model1_private_lock; /* for use by the address_space */
 struct Model1_list_head Model1_private_list; /* ditto */
 void *Model1_private_data; /* ditto */
} __attribute__((aligned(sizeof(long))));
 /*
	 * On most architectures that alignment is already the case; but
	 * must be enforced here for CRIS, to let the least significant bit
	 * of struct page's "mapping" pointer be used for PAGE_MAPPING_ANON.
	 */
struct Model1_request_queue;

struct Model1_block_device {
 Model1_dev_t Model1_bd_dev; /* not a kdev_t - it's a search key */
 int Model1_bd_openers;
 struct Model1_inode * Model1_bd_inode; /* will die */
 struct Model1_super_block * Model1_bd_super;
 struct Model1_mutex Model1_bd_mutex; /* open/close mutex */
 void * Model1_bd_claiming;
 void * Model1_bd_holder;
 int Model1_bd_holders;
 bool Model1_bd_write_holder;

 struct Model1_list_head Model1_bd_holder_disks;

 struct Model1_block_device * Model1_bd_contains;
 unsigned Model1_bd_block_size;
 struct Model1_hd_struct * Model1_bd_part;
 /* number of times partitions within this device have been opened. */
 unsigned Model1_bd_part_count;
 int Model1_bd_invalidated;
 struct Model1_gendisk * Model1_bd_disk;
 struct Model1_request_queue * Model1_bd_queue;
 struct Model1_list_head Model1_bd_list;
 /*
	 * Private data.  You must have bd_claim'ed the block_device
	 * to use this.  NOTE:  bd_claim allows an owner to claim
	 * the same device multiple times, the owner must take special
	 * care to not mess up bd_private for that case.
	 */
 unsigned long Model1_bd_private;

 /* The counter of freeze processes */
 int Model1_bd_fsfreeze_count;
 /* Mutex for freeze */
 struct Model1_mutex Model1_bd_fsfreeze_mutex;
};

/*
 * Radix-tree tags, for tagging dirty and writeback pages within the pagecache
 * radix trees
 */




int Model1_mapping_tagged(struct Model1_address_space *Model1_mapping, int Model1_tag);

static inline __attribute__((no_instrument_function)) void Model1_i_mmap_lock_write(struct Model1_address_space *Model1_mapping)
{
 Model1_down_write(&Model1_mapping->Model1_i_mmap_rwsem);
}

static inline __attribute__((no_instrument_function)) void Model1_i_mmap_unlock_write(struct Model1_address_space *Model1_mapping)
{
 Model1_up_write(&Model1_mapping->Model1_i_mmap_rwsem);
}

static inline __attribute__((no_instrument_function)) void Model1_i_mmap_lock_read(struct Model1_address_space *Model1_mapping)
{
 Model1_down_read(&Model1_mapping->Model1_i_mmap_rwsem);
}

static inline __attribute__((no_instrument_function)) void Model1_i_mmap_unlock_read(struct Model1_address_space *Model1_mapping)
{
 Model1_up_read(&Model1_mapping->Model1_i_mmap_rwsem);
}

/*
 * Might pages of this file be mapped into userspace?
 */
static inline __attribute__((no_instrument_function)) int Model1_mapping_mapped(struct Model1_address_space *Model1_mapping)
{
 return !(({ union { typeof((&Model1_mapping->Model1_i_mmap)->Model1_rb_node) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&((&Model1_mapping->Model1_i_mmap)->Model1_rb_node), Model1___u.Model1___c, sizeof((&Model1_mapping->Model1_i_mmap)->Model1_rb_node)); else Model1___read_once_size_nocheck(&((&Model1_mapping->Model1_i_mmap)->Model1_rb_node), Model1___u.Model1___c, sizeof((&Model1_mapping->Model1_i_mmap)->Model1_rb_node)); Model1___u.Model1___val; }) == ((void *)0));
}

/*
 * Might pages of this file have been modified in userspace?
 * Note that i_mmap_writable counts all VM_SHARED vmas: do_mmap_pgoff
 * marks vma as VM_SHARED if it is shared, and the file was opened for
 * writing i.e. vma may be mprotected writable even if now readonly.
 *
 * If i_mmap_writable is negative, no new writable mappings are allowed. You
 * can only deny writable mappings, if none exists right now.
 */
static inline __attribute__((no_instrument_function)) int Model1_mapping_writably_mapped(struct Model1_address_space *Model1_mapping)
{
 return Model1_atomic_read(&Model1_mapping->Model1_i_mmap_writable) > 0;
}

static inline __attribute__((no_instrument_function)) int Model1_mapping_map_writable(struct Model1_address_space *Model1_mapping)
{
 return Model1_atomic_inc_unless_negative(&Model1_mapping->Model1_i_mmap_writable) ?
  0 : -1;
}

static inline __attribute__((no_instrument_function)) void Model1_mapping_unmap_writable(struct Model1_address_space *Model1_mapping)
{
 Model1_atomic_dec(&Model1_mapping->Model1_i_mmap_writable);
}

static inline __attribute__((no_instrument_function)) int Model1_mapping_deny_writable(struct Model1_address_space *Model1_mapping)
{
 return Model1_atomic_dec_unless_positive(&Model1_mapping->Model1_i_mmap_writable) ?
  0 : -16;
}

static inline __attribute__((no_instrument_function)) void Model1_mapping_allow_writable(struct Model1_address_space *Model1_mapping)
{
 Model1_atomic_inc(&Model1_mapping->Model1_i_mmap_writable);
}

/*
 * Use sequence counter to get consistent i_size on 32-bit processors.
 */
struct Model1_posix_acl;



static inline __attribute__((no_instrument_function)) struct Model1_posix_acl *
Model1_uncached_acl_sentinel(struct Model1_task_struct *Model1_task)
{
 return (void *)Model1_task + 1;
}

static inline __attribute__((no_instrument_function)) bool
Model1_is_uncached_acl(struct Model1_posix_acl *Model1_acl)
{
 return (long)Model1_acl & 1;
}





/*
 * Keep mostly read-only and often accessed (especially for
 * the RCU path lookup and 'stat' data) fields at the beginning
 * of the 'struct inode'
 */
struct Model1_inode {
 Model1_umode_t Model1_i_mode;
 unsigned short Model1_i_opflags;
 Model1_kuid_t Model1_i_uid;
 Model1_kgid_t Model1_i_gid;
 unsigned int Model1_i_flags;


 struct Model1_posix_acl *Model1_i_acl;
 struct Model1_posix_acl *Model1_i_default_acl;


 const struct Model1_inode_operations *Model1_i_op;
 struct Model1_super_block *Model1_i_sb;
 struct Model1_address_space *Model1_i_mapping;


 void *Model1_i_security;


 /* Stat data, not accessed from path walking */
 unsigned long Model1_i_ino;
 /*
	 * Filesystems may only read i_nlink directly.  They shall use the
	 * following functions for modification:
	 *
	 *    (set|clear|inc|drop)_nlink
	 *    inode_(inc|dec)_link_count
	 */
 union {
  const unsigned int Model1_i_nlink;
  unsigned int Model1___i_nlink;
 };
 Model1_dev_t Model1_i_rdev;
 Model1_loff_t Model1_i_size;
 struct Model1_timespec Model1_i_atime;
 struct Model1_timespec Model1_i_mtime;
 struct Model1_timespec Model1_i_ctime;
 Model1_spinlock_t Model1_i_lock; /* i_blocks, i_bytes, maybe i_size */
 unsigned short Model1_i_bytes;
 unsigned int Model1_i_blkbits;
 Model1_blkcnt_t Model1_i_blocks;





 /* Misc */
 unsigned long Model1_i_state;
 struct Model1_rw_semaphore Model1_i_rwsem;

 unsigned long Model1_dirtied_when; /* jiffies of first dirtying */
 unsigned long Model1_dirtied_time_when;

 struct Model1_hlist_node Model1_i_hash;
 struct Model1_list_head Model1_i_io_list; /* backing dev IO list */
 struct Model1_list_head Model1_i_lru; /* inode LRU list */
 struct Model1_list_head Model1_i_sb_list;
 struct Model1_list_head Model1_i_wb_list; /* backing dev writeback list */
 union {
  struct Model1_hlist_head Model1_i_dentry;
  struct Model1_callback_head Model1_i_rcu;
 };
 Model1_u64 Model1_i_version;
 Model1_atomic_t Model1_i_count;
 Model1_atomic_t Model1_i_dio_count;
 Model1_atomic_t Model1_i_writecount;



 const struct Model1_file_operations *Model1_i_fop; /* former ->i_op->default_file_ops */
 struct Model1_file_lock_context *Model1_i_flctx;
 struct Model1_address_space Model1_i_data;
 struct Model1_list_head Model1_i_devices;
 union {
  struct Model1_pipe_inode_info *Model1_i_pipe;
  struct Model1_block_device *Model1_i_bdev;
  struct Model1_cdev *Model1_i_cdev;
  char *Model1_i_link;
  unsigned Model1_i_dir_seq;
 };

 __u32 Model1_i_generation;


 __u32 Model1_i_fsnotify_mask; /* all events this inode cares about */
 struct Model1_hlist_head Model1_i_fsnotify_marks;






 void *Model1_i_private; /* fs or device private pointer */
};

static inline __attribute__((no_instrument_function)) int Model1_inode_unhashed(struct Model1_inode *Model1_inode)
{
 return Model1_hlist_unhashed(&Model1_inode->Model1_i_hash);
}

/*
 * inode->i_mutex nesting subclasses for the lock validator:
 *
 * 0: the object of the current VFS operation
 * 1: parent
 * 2: child/target
 * 3: xattr
 * 4: second non-directory
 * 5: second parent (when locking independent directories in rename)
 *
 * I_MUTEX_NONDIR2 is for certain operations (such as rename) which lock two
 * non-directories at once.
 *
 * The locking order between these classes is
 * parent[2] -> child -> grandchild -> normal -> xattr -> second non-directory
 */
enum Model1_inode_i_mutex_lock_class
{
 Model1_I_MUTEX_NORMAL,
 Model1_I_MUTEX_PARENT,
 Model1_I_MUTEX_CHILD,
 Model1_I_MUTEX_XATTR,
 Model1_I_MUTEX_NONDIR2,
 Model1_I_MUTEX_PARENT2,
};

static inline __attribute__((no_instrument_function)) void Model1_inode_lock(struct Model1_inode *Model1_inode)
{
 Model1_down_write(&Model1_inode->Model1_i_rwsem);
}

static inline __attribute__((no_instrument_function)) void Model1_inode_unlock(struct Model1_inode *Model1_inode)
{
 Model1_up_write(&Model1_inode->Model1_i_rwsem);
}

static inline __attribute__((no_instrument_function)) void Model1_inode_lock_shared(struct Model1_inode *Model1_inode)
{
 Model1_down_read(&Model1_inode->Model1_i_rwsem);
}

static inline __attribute__((no_instrument_function)) void Model1_inode_unlock_shared(struct Model1_inode *Model1_inode)
{
 Model1_up_read(&Model1_inode->Model1_i_rwsem);
}

static inline __attribute__((no_instrument_function)) int Model1_inode_trylock(struct Model1_inode *Model1_inode)
{
 return Model1_down_write_trylock(&Model1_inode->Model1_i_rwsem);
}

static inline __attribute__((no_instrument_function)) int Model1_inode_trylock_shared(struct Model1_inode *Model1_inode)
{
 return Model1_down_read_trylock(&Model1_inode->Model1_i_rwsem);
}

static inline __attribute__((no_instrument_function)) int Model1_inode_is_locked(struct Model1_inode *Model1_inode)
{
 return Model1_rwsem_is_locked(&Model1_inode->Model1_i_rwsem);
}

static inline __attribute__((no_instrument_function)) void Model1_inode_lock_nested(struct Model1_inode *Model1_inode, unsigned Model1_subclass)
{
 Model1_down_write(&Model1_inode->Model1_i_rwsem);
}

void Model1_lock_two_nondirectories(struct Model1_inode *, struct Model1_inode*);
void Model1_unlock_two_nondirectories(struct Model1_inode *, struct Model1_inode*);

/*
 * NOTE: in a 32bit arch with a preemptable kernel and
 * an UP compile the i_size_read/write must be atomic
 * with respect to the local cpu (unlike with preempt disabled),
 * but they don't need to be atomic with respect to other cpus like in
 * true SMP (so they need either to either locally disable irq around
 * the read or for example on x86 they can be still implemented as a
 * cmpxchg8b without the need of the lock prefix). For SMP compiles
 * and 64bit archs it makes no difference if preempt is enabled or not.
 */
static inline __attribute__((no_instrument_function)) Model1_loff_t Model1_i_size_read(const struct Model1_inode *Model1_inode)
{
 return Model1_inode->Model1_i_size;

}

/*
 * NOTE: unlike i_size_read(), i_size_write() does need locking around it
 * (normally i_mutex), otherwise on 32bit/SMP an update of i_size_seqcount
 * can be lost, resulting in subsequent i_size_read() calls spinning forever.
 */
static inline __attribute__((no_instrument_function)) void Model1_i_size_write(struct Model1_inode *Model1_inode, Model1_loff_t Model1_i_size)
{
 Model1_inode->Model1_i_size = Model1_i_size;

}

static inline __attribute__((no_instrument_function)) unsigned Model1_iminor(const struct Model1_inode *Model1_inode)
{
 return ((unsigned int) ((Model1_inode->Model1_i_rdev) & ((1U << 20) - 1)));
}

static inline __attribute__((no_instrument_function)) unsigned Model1_imajor(const struct Model1_inode *Model1_inode)
{
 return ((unsigned int) ((Model1_inode->Model1_i_rdev) >> 20));
}

extern struct Model1_block_device *Model1_I_BDEV(struct Model1_inode *Model1_inode);

struct Model1_fown_struct {
 Model1_rwlock_t Model1_lock; /* protects pid, uid, euid fields */
 struct Model1_pid *Model1_pid; /* pid or -pgrp where SIGIO should be sent */
 enum Model1_pid_type Model1_pid_type; /* Kind of process group SIGIO should be sent to */
 Model1_kuid_t Model1_uid, Model1_euid; /* uid/euid of process setting the owner */
 int Model1_signum; /* posix.1b rt signal to be delivered on IO */
};

/*
 * Track a single file's readahead state
 */
struct Model1_file_ra_state {
 unsigned long Model1_start; /* where readahead started */
 unsigned int Model1_size; /* # of readahead pages */
 unsigned int Model1_async_size; /* do asynchronous readahead when
					   there are only # of pages ahead */

 unsigned int Model1_ra_pages; /* Maximum readahead window */
 unsigned int Model1_mmap_miss; /* Cache miss stat for mmap accesses */
 Model1_loff_t Model1_prev_pos; /* Cache last read() position */
};

/*
 * Check if @index falls in the readahead windows.
 */
static inline __attribute__((no_instrument_function)) int Model1_ra_has_index(struct Model1_file_ra_state *Model1_ra, unsigned long Model1_index)
{
 return (Model1_index >= Model1_ra->Model1_start &&
  Model1_index < Model1_ra->Model1_start + Model1_ra->Model1_size);
}

struct Model1_file {
 union {
  struct Model1_llist_node Model1_fu_llist;
  struct Model1_callback_head Model1_fu_rcuhead;
 } Model1_f_u;
 struct Model1_path Model1_f_path;
 struct Model1_inode *Model1_f_inode; /* cached value */
 const struct Model1_file_operations *Model1_f_op;

 /*
	 * Protects f_ep_links, f_flags.
	 * Must not be taken from IRQ context.
	 */
 Model1_spinlock_t Model1_f_lock;
 Model1_atomic_long_t Model1_f_count;
 unsigned int Model1_f_flags;
 Model1_fmode_t Model1_f_mode;
 struct Model1_mutex Model1_f_pos_lock;
 Model1_loff_t Model1_f_pos;
 struct Model1_fown_struct Model1_f_owner;
 const struct Model1_cred *Model1_f_cred;
 struct Model1_file_ra_state Model1_f_ra;

 Model1_u64 Model1_f_version;

 void *Model1_f_security;

 /* needed for tty driver, and maybe others */
 void *Model1_private_data;


 /* Used by fs/eventpoll.c to link all the hooks to this file */
 struct Model1_list_head Model1_f_ep_links;
 struct Model1_list_head Model1_f_tfile_llink;

 struct Model1_address_space *Model1_f_mapping;
} __attribute__((aligned(4))); /* lest something weird decides that 2 is OK */

struct Model1_file_handle {
 __u32 Model1_handle_bytes;
 int Model1_handle_type;
 /* file identifier */
 unsigned char Model1_f_handle[0];
};

static inline __attribute__((no_instrument_function)) struct Model1_file *Model1_get_file(struct Model1_file *Model1_f)
{
 Model1_atomic_long_inc(&Model1_f->Model1_f_count);
 return Model1_f;
}






/* Page cache limit. The filesystems should put that into their s_maxbytes 
   limits, otherwise bad things can happen in VM. */
/*
 * Special return value from posix_lock_file() and vfs_lock_file() for
 * asynchronous locking.
 */


/* legacy typedef, should eventually be removed */
typedef void *Model1_fl_owner_t;

struct Model1_file_lock;

struct Model1_file_lock_operations {
 void (*Model1_fl_copy_lock)(struct Model1_file_lock *, struct Model1_file_lock *);
 void (*Model1_fl_release_private)(struct Model1_file_lock *);
};

struct Model1_lock_manager_operations {
 int (*Model1_lm_compare_owner)(struct Model1_file_lock *, struct Model1_file_lock *);
 unsigned long (*Model1_lm_owner_key)(struct Model1_file_lock *);
 Model1_fl_owner_t (*Model1_lm_get_owner)(Model1_fl_owner_t);
 void (*Model1_lm_put_owner)(Model1_fl_owner_t);
 void (*Model1_lm_notify)(struct Model1_file_lock *); /* unblock callback */
 int (*Model1_lm_grant)(struct Model1_file_lock *, int);
 bool (*Model1_lm_break)(struct Model1_file_lock *);
 int (*Model1_lm_change)(struct Model1_file_lock *, int, struct Model1_list_head *);
 void (*Model1_lm_setup)(struct Model1_file_lock *, void **);
};

struct Model1_lock_manager {
 struct Model1_list_head Model1_list;
 /*
	 * NFSv4 and up also want opens blocked during the grace period;
	 * NLM doesn't care:
	 */
 bool Model1_block_opens;
};

struct Model1_net;
void Model1_locks_start_grace(struct Model1_net *, struct Model1_lock_manager *);
void Model1_locks_end_grace(struct Model1_lock_manager *);
int Model1_locks_in_grace(struct Model1_net *);
int Model1_opens_in_grace(struct Model1_net *);

/* that will die - we need it for nfs_lock_info */




struct Model1_nlm_lockowner;

/*
 * NFS lock info
 */
struct Model1_nfs_lock_info {
 Model1_u32 Model1_state;
 struct Model1_nlm_lockowner *Model1_owner;
 struct Model1_list_head Model1_list;
};

struct Model1_nfs4_lock_state;
struct Model1_nfs4_lock_info {
 struct Model1_nfs4_lock_state *Model1_owner;
};

/*
 * struct file_lock represents a generic "file lock". It's used to represent
 * POSIX byte range locks, BSD (flock) locks, and leases. It's important to
 * note that the same struct is used to represent both a request for a lock and
 * the lock itself, but the same object is never used for both.
 *
 * FIXME: should we create a separate "struct lock_request" to help distinguish
 * these two uses?
 *
 * The varous i_flctx lists are ordered by:
 *
 * 1) lock owner
 * 2) lock range start
 * 3) lock range end
 *
 * Obviously, the last two criteria only matter for POSIX locks.
 */
struct Model1_file_lock {
 struct Model1_file_lock *Model1_fl_next; /* singly linked list for this inode  */
 struct Model1_list_head Model1_fl_list; /* link into file_lock_context */
 struct Model1_hlist_node Model1_fl_link; /* node in global lists */
 struct Model1_list_head Model1_fl_block; /* circular list of blocked processes */
 Model1_fl_owner_t Model1_fl_owner;
 unsigned int Model1_fl_flags;
 unsigned char Model1_fl_type;
 unsigned int Model1_fl_pid;
 int Model1_fl_link_cpu; /* what cpu's list is this on? */
 struct Model1_pid *Model1_fl_nspid;
 Model1_wait_queue_head_t Model1_fl_wait;
 struct Model1_file *Model1_fl_file;
 Model1_loff_t Model1_fl_start;
 Model1_loff_t Model1_fl_end;

 struct Model1_fasync_struct * Model1_fl_fasync; /* for lease break notifications */
 /* for lease breaks: */
 unsigned long Model1_fl_break_time;
 unsigned long Model1_fl_downgrade_time;

 const struct Model1_file_lock_operations *Model1_fl_ops; /* Callbacks for filesystems */
 const struct Model1_lock_manager_operations *Model1_fl_lmops; /* Callbacks for lockmanagers */
 union {
  struct Model1_nfs_lock_info Model1_nfs_fl;
  struct Model1_nfs4_lock_info Model1_nfs4_fl;
  struct {
   struct Model1_list_head Model1_link; /* link in AFS vnode's pending_locks list */
   int Model1_state; /* state of grant or error if -ve */
  } Model1_afs;
 } Model1_fl_u;
};

struct Model1_file_lock_context {
 Model1_spinlock_t Model1_flc_lock;
 struct Model1_list_head Model1_flc_flock;
 struct Model1_list_head Model1_flc_posix;
 struct Model1_list_head Model1_flc_lease;
};

/* The following constant reflects the upper bound of the file/locking space */


















/*
 * FMODE_EXEC is 0x20
 * FMODE_NONOTIFY is 0x4000000
 * These cannot be used by userspace O_* until internal and external open
 * flags are split.
 * -Eric Paris
 */

/*
 * When introducing new O_* bits, please check its uniqueness in fcntl_init().
 */
/*
 * Before Linux 2.6.33 only O_DSYNC semantics were implemented, but using
 * the O_SYNC flag.  We continue to use the existing numerical value
 * for O_DSYNC semantics now, but using the correct symbolic name for it.
 * This new value is used to request true Posix O_SYNC semantics.  It is
 * defined in this strange way to make sure applications compiled against
 * new headers get at least O_DSYNC semantics on older kernels.
 *
 * This has the nice side-effect that we can simply test for O_DSYNC
 * wherever we do not care if O_DSYNC or O_SYNC is used.
 *
 * Note: __O_SYNC must never be used directly.
 */
/* a horrid kludge trying to make sure that this will fail on old kernels */
/*
 * Open File Description Locks
 *
 * Usually record locks held by a process are released on *any* close and are
 * not inherited across a fork().
 *
 * These cmd values will set locks that conflict with process-associated
 * record  locks, but are "owned" by the open file description, not the
 * process. This means that they are inherited across fork() like BSD (flock)
 * locks, and they are only released automatically when the last reference to
 * the the open file against which they were acquired is put.
 */
struct Model1_f_owner_ex {
 int Model1_type;
 Model1___kernel_pid_t Model1_pid;
};

/* for F_[GET|SET]FL */


/* for posix fcntl() and lockf() */






/* for old implementation of bsd flock () */





/* operations for bsd flock(), also used by the kernel implementation */
struct Model1_flock {
 short Model1_l_type;
 short Model1_l_whence;
 Model1___kernel_off_t Model1_l_start;
 Model1___kernel_off_t Model1_l_len;
 Model1___kernel_pid_t Model1_l_pid;

};







struct Model1_flock64 {
 short Model1_l_type;
 short Model1_l_whence;
 Model1___kernel_loff_t Model1_l_start;
 Model1___kernel_loff_t Model1_l_len;
 Model1___kernel_pid_t Model1_l_pid;

};




/*
 * Cancel a blocking posix lock; internal use only until we expose an
 * asynchronous lock api to userspace:
 */


/* Create a file descriptor with FD_CLOEXEC set. */


/*
 * Request nofications on a directory.
 * See below for events that may be notified.
 */


/*
 * Set and get of pipe page size array
 */



/*
 * Set/Get seals
 */



/*
 * Types of seals
 */




/* (1U << 31) is reserved for signed error codes */

/*
 * Types of directory notifications that may be requested.
 */

extern void Model1_send_sigio(struct Model1_fown_struct *Model1_fown, int Model1_fd, int Model1_band);


extern int Model1_fcntl_getlk(struct Model1_file *, unsigned int, struct Model1_flock *);
extern int Model1_fcntl_setlk(unsigned int, struct Model1_file *, unsigned int,
   struct Model1_flock *);







extern int Model1_fcntl_setlease(unsigned int Model1_fd, struct Model1_file *Model1_filp, long Model1_arg);
extern int Model1_fcntl_getlease(struct Model1_file *Model1_filp);

/* fs/locks.c */
void Model1_locks_free_lock_context(struct Model1_inode *Model1_inode);
void Model1_locks_free_lock(struct Model1_file_lock *Model1_fl);
extern void Model1_locks_init_lock(struct Model1_file_lock *);
extern struct Model1_file_lock * Model1_locks_alloc_lock(void);
extern void Model1_locks_copy_lock(struct Model1_file_lock *, struct Model1_file_lock *);
extern void Model1_locks_copy_conflock(struct Model1_file_lock *, struct Model1_file_lock *);
extern void Model1_locks_remove_posix(struct Model1_file *, Model1_fl_owner_t);
extern void Model1_locks_remove_file(struct Model1_file *);
extern void Model1_locks_release_private(struct Model1_file_lock *);
extern void Model1_posix_test_lock(struct Model1_file *, struct Model1_file_lock *);
extern int Model1_posix_lock_file(struct Model1_file *, struct Model1_file_lock *, struct Model1_file_lock *);
extern int Model1_posix_unblock_lock(struct Model1_file_lock *);
extern int Model1_vfs_test_lock(struct Model1_file *, struct Model1_file_lock *);
extern int Model1_vfs_lock_file(struct Model1_file *, unsigned int, struct Model1_file_lock *, struct Model1_file_lock *);
extern int Model1_vfs_cancel_lock(struct Model1_file *Model1_filp, struct Model1_file_lock *Model1_fl);
extern int Model1_locks_lock_inode_wait(struct Model1_inode *Model1_inode, struct Model1_file_lock *Model1_fl);
extern int Model1___break_lease(struct Model1_inode *Model1_inode, unsigned int Model1_flags, unsigned int Model1_type);
extern void Model1_lease_get_mtime(struct Model1_inode *, struct Model1_timespec *Model1_time);
extern int Model1_generic_setlease(struct Model1_file *, long, struct Model1_file_lock **, void **Model1_priv);
extern int Model1_vfs_setlease(struct Model1_file *, long, struct Model1_file_lock **, void **);
extern int Model1_lease_modify(struct Model1_file_lock *, int, struct Model1_list_head *);
struct Model1_files_struct;
extern void Model1_show_fd_locks(struct Model1_seq_file *Model1_f,
    struct Model1_file *Model1_filp, struct Model1_files_struct *Model1_files);
static inline __attribute__((no_instrument_function)) struct Model1_inode *Model1_file_inode(const struct Model1_file *Model1_f)
{
 return Model1_f->Model1_f_inode;
}

static inline __attribute__((no_instrument_function)) struct Model1_dentry *Model1_file_dentry(const struct Model1_file *Model1_file)
{
 return Model1_d_real(Model1_file->Model1_f_path.Model1_dentry, Model1_file_inode(Model1_file), 0);
}

static inline __attribute__((no_instrument_function)) int Model1_locks_lock_file_wait(struct Model1_file *Model1_filp, struct Model1_file_lock *Model1_fl)
{
 return Model1_locks_lock_inode_wait(Model1_file_inode(Model1_filp), Model1_fl);
}

struct Model1_fasync_struct {
 Model1_spinlock_t Model1_fa_lock;
 int Model1_magic;
 int Model1_fa_fd;
 struct Model1_fasync_struct *Model1_fa_next; /* singly linked list */
 struct Model1_file *Model1_fa_file;
 struct Model1_callback_head Model1_fa_rcu;
};



/* SMP safe fasync helpers: */
extern int Model1_fasync_helper(int, struct Model1_file *, int, struct Model1_fasync_struct **);
extern struct Model1_fasync_struct *Model1_fasync_insert_entry(int, struct Model1_file *, struct Model1_fasync_struct **, struct Model1_fasync_struct *);
extern int Model1_fasync_remove_entry(struct Model1_file *, struct Model1_fasync_struct **);
extern struct Model1_fasync_struct *Model1_fasync_alloc(void);
extern void Model1_fasync_free(struct Model1_fasync_struct *);

/* can be called from interrupts */
extern void Model1_kill_fasync(struct Model1_fasync_struct **, int, int);

extern void Model1___f_setown(struct Model1_file *Model1_filp, struct Model1_pid *, enum Model1_pid_type, int Model1_force);
extern void Model1_f_setown(struct Model1_file *Model1_filp, unsigned long Model1_arg, int Model1_force);
extern void Model1_f_delown(struct Model1_file *Model1_filp);
extern Model1_pid_t Model1_f_getown(struct Model1_file *Model1_filp);
extern int Model1_send_sigurg(struct Model1_fown_struct *Model1_fown);

struct Model1_mm_struct;

/*
 *	Umount options
 */







/* sb->s_iflags */




/* sb->s_iflags to limit user namespace mounts */


/* Possible states of 'frozen' field */
enum {
 Model1_SB_UNFROZEN = 0, /* FS is unfrozen */
 Model1_SB_FREEZE_WRITE = 1, /* Writes, dir ops, ioctls frozen */
 Model1_SB_FREEZE_PAGEFAULT = 2, /* Page faults stopped as well */
 Model1_SB_FREEZE_FS = 3, /* For internal FS use (e.g. to stop
					 * internal threads if needed) */
 Model1_SB_FREEZE_COMPLETE = 4, /* ->freeze_fs finished successfully */
};



struct Model1_sb_writers {
 int Model1_frozen; /* Is sb frozen? */
 Model1_wait_queue_head_t Model1_wait_unfrozen; /* for get_super_thawed() */
 struct Model1_percpu_rw_semaphore Model1_rw_sem[(Model1_SB_FREEZE_COMPLETE - 1)];
};

struct Model1_super_block {
 struct Model1_list_head Model1_s_list; /* Keep this first */
 Model1_dev_t Model1_s_dev; /* search index; _not_ kdev_t */
 unsigned char Model1_s_blocksize_bits;
 unsigned long Model1_s_blocksize;
 Model1_loff_t Model1_s_maxbytes; /* Max file size */
 struct Model1_file_system_type *Model1_s_type;
 const struct Model1_super_operations *Model1_s_op;
 const struct Model1_dquot_operations *Model1_dq_op;
 const struct Model1_quotactl_ops *Model1_s_qcop;
 const struct Model1_export_operations *Model1_s_export_op;
 unsigned long Model1_s_flags;
 unsigned long Model1_s_iflags; /* internal SB_I_* flags */
 unsigned long Model1_s_magic;
 struct Model1_dentry *Model1_s_root;
 struct Model1_rw_semaphore Model1_s_umount;
 int Model1_s_count;
 Model1_atomic_t Model1_s_active;

 void *Model1_s_security;

 const struct Model1_xattr_handler **Model1_s_xattr;

 const struct Model1_fscrypt_operations *Model1_s_cop;

 struct Model1_hlist_bl_head Model1_s_anon; /* anonymous dentries for (nfs) exporting */
 struct Model1_list_head Model1_s_mounts; /* list of mounts; _not_ for fs use */
 struct Model1_block_device *Model1_s_bdev;
 struct Model1_backing_dev_info *Model1_s_bdi;
 struct Model1_mtd_info *Model1_s_mtd;
 struct Model1_hlist_node Model1_s_instances;
 unsigned int Model1_s_quota_types; /* Bitmask of supported quota types */
 struct Model1_quota_info Model1_s_dquot; /* Diskquota specific options */

 struct Model1_sb_writers Model1_s_writers;

 char Model1_s_id[32]; /* Informational name */
 Model1_u8 Model1_s_uuid[16]; /* UUID */

 void *Model1_s_fs_info; /* Filesystem private info */
 unsigned int Model1_s_max_links;
 Model1_fmode_t Model1_s_mode;

 /* Granularity of c/m/atime in ns.
	   Cannot be worse than a second */
 Model1_u32 Model1_s_time_gran;

 /*
	 * The next field is for VFS *only*. No filesystems have any business
	 * even looking at it. You had been warned.
	 */
 struct Model1_mutex Model1_s_vfs_rename_mutex; /* Kludge */

 /*
	 * Filesystem subtype.  If non-empty the filesystem type field
	 * in /proc/mounts will be "type.subtype"
	 */
 char *Model1_s_subtype;

 /*
	 * Saved mount options for lazy filesystems using
	 * generic_show_options()
	 */
 char *Model1_s_options;
 const struct Model1_dentry_operations *Model1_s_d_op; /* default d_op for dentries */

 /*
	 * Saved pool identifier for cleancache (-1 means none)
	 */
 int Model1_cleancache_poolid;

 struct Model1_shrinker Model1_s_shrink; /* per-sb shrinker handle */

 /* Number of inodes with nlink == 0 but still referenced */
 Model1_atomic_long_t Model1_s_remove_count;

 /* Being remounted read-only */
 int Model1_s_readonly_remount;

 /* AIO completions deferred from interrupt context */
 struct Model1_workqueue_struct *Model1_s_dio_done_wq;
 struct Model1_hlist_head Model1_s_pins;

 /*
	 * Owning user namespace and default context in which to
	 * interpret filesystem uids, gids, quotas, device nodes,
	 * xattrs and security labels.
	 */
 struct Model1_user_namespace *Model1_s_user_ns;

 /*
	 * Keep the lru lists last in the structure so they always sit on their
	 * own individual cachelines.
	 */
 struct Model1_list_lru Model1_s_dentry_lru __attribute__((__aligned__((1 << (6)))));
 struct Model1_list_lru Model1_s_inode_lru __attribute__((__aligned__((1 << (6)))));
 struct Model1_callback_head Model1_rcu;
 struct Model1_work_struct Model1_destroy_work;

 struct Model1_mutex Model1_s_sync_lock; /* sync serialisation lock */

 /*
	 * Indicates how deep in a filesystem stack this SB is
	 */
 int Model1_s_stack_depth;

 /* s_inode_list_lock protects s_inodes */
 Model1_spinlock_t Model1_s_inode_list_lock __attribute__((__aligned__((1 << (6)))));
 struct Model1_list_head Model1_s_inodes; /* all inodes */

 Model1_spinlock_t Model1_s_inode_wblist_lock;
 struct Model1_list_head Model1_s_inodes_wb; /* writeback inodes */
};

/* Helper functions so that in most cases filesystems will
 * not need to deal directly with kuid_t and kgid_t and can
 * instead deal with the raw numeric values that are stored
 * in the filesystem.
 */
static inline __attribute__((no_instrument_function)) Model1_uid_t Model1_i_uid_read(const struct Model1_inode *Model1_inode)
{
 return Model1_from_kuid(Model1_inode->Model1_i_sb->Model1_s_user_ns, Model1_inode->Model1_i_uid);
}

static inline __attribute__((no_instrument_function)) Model1_gid_t Model1_i_gid_read(const struct Model1_inode *Model1_inode)
{
 return Model1_from_kgid(Model1_inode->Model1_i_sb->Model1_s_user_ns, Model1_inode->Model1_i_gid);
}

static inline __attribute__((no_instrument_function)) void Model1_i_uid_write(struct Model1_inode *Model1_inode, Model1_uid_t Model1_uid)
{
 Model1_inode->Model1_i_uid = Model1_make_kuid(Model1_inode->Model1_i_sb->Model1_s_user_ns, Model1_uid);
}

static inline __attribute__((no_instrument_function)) void Model1_i_gid_write(struct Model1_inode *Model1_inode, Model1_gid_t Model1_gid)
{
 Model1_inode->Model1_i_gid = Model1_make_kgid(Model1_inode->Model1_i_sb->Model1_s_user_ns, Model1_gid);
}

extern struct Model1_timespec Model1_current_fs_time(struct Model1_super_block *Model1_sb);

/*
 * Snapshotting support.
 */

void Model1___sb_end_write(struct Model1_super_block *Model1_sb, int Model1_level);
int Model1___sb_start_write(struct Model1_super_block *Model1_sb, int Model1_level, bool Model1_wait);






/**
 * sb_end_write - drop write access to a superblock
 * @sb: the super we wrote to
 *
 * Decrement number of writers to the filesystem. Wake up possible waiters
 * wanting to freeze the filesystem.
 */
static inline __attribute__((no_instrument_function)) void Model1_sb_end_write(struct Model1_super_block *Model1_sb)
{
 Model1___sb_end_write(Model1_sb, Model1_SB_FREEZE_WRITE);
}

/**
 * sb_end_pagefault - drop write access to a superblock from a page fault
 * @sb: the super we wrote to
 *
 * Decrement number of processes handling write page fault to the filesystem.
 * Wake up possible waiters wanting to freeze the filesystem.
 */
static inline __attribute__((no_instrument_function)) void Model1_sb_end_pagefault(struct Model1_super_block *Model1_sb)
{
 Model1___sb_end_write(Model1_sb, Model1_SB_FREEZE_PAGEFAULT);
}

/**
 * sb_end_intwrite - drop write access to a superblock for internal fs purposes
 * @sb: the super we wrote to
 *
 * Decrement fs-internal number of writers to the filesystem.  Wake up possible
 * waiters wanting to freeze the filesystem.
 */
static inline __attribute__((no_instrument_function)) void Model1_sb_end_intwrite(struct Model1_super_block *Model1_sb)
{
 Model1___sb_end_write(Model1_sb, Model1_SB_FREEZE_FS);
}

/**
 * sb_start_write - get write access to a superblock
 * @sb: the super we write to
 *
 * When a process wants to write data or metadata to a file system (i.e. dirty
 * a page or an inode), it should embed the operation in a sb_start_write() -
 * sb_end_write() pair to get exclusion against file system freezing. This
 * function increments number of writers preventing freezing. If the file
 * system is already frozen, the function waits until the file system is
 * thawed.
 *
 * Since freeze protection behaves as a lock, users have to preserve
 * ordering of freeze protection and other filesystem locks. Generally,
 * freeze protection should be the outermost lock. In particular, we have:
 *
 * sb_start_write
 *   -> i_mutex			(write path, truncate, directory ops, ...)
 *   -> s_umount		(freeze_super, thaw_super)
 */
static inline __attribute__((no_instrument_function)) void Model1_sb_start_write(struct Model1_super_block *Model1_sb)
{
 Model1___sb_start_write(Model1_sb, Model1_SB_FREEZE_WRITE, true);
}

static inline __attribute__((no_instrument_function)) int Model1_sb_start_write_trylock(struct Model1_super_block *Model1_sb)
{
 return Model1___sb_start_write(Model1_sb, Model1_SB_FREEZE_WRITE, false);
}

/**
 * sb_start_pagefault - get write access to a superblock from a page fault
 * @sb: the super we write to
 *
 * When a process starts handling write page fault, it should embed the
 * operation into sb_start_pagefault() - sb_end_pagefault() pair to get
 * exclusion against file system freezing. This is needed since the page fault
 * is going to dirty a page. This function increments number of running page
 * faults preventing freezing. If the file system is already frozen, the
 * function waits until the file system is thawed.
 *
 * Since page fault freeze protection behaves as a lock, users have to preserve
 * ordering of freeze protection and other filesystem locks. It is advised to
 * put sb_start_pagefault() close to mmap_sem in lock ordering. Page fault
 * handling code implies lock dependency:
 *
 * mmap_sem
 *   -> sb_start_pagefault
 */
static inline __attribute__((no_instrument_function)) void Model1_sb_start_pagefault(struct Model1_super_block *Model1_sb)
{
 Model1___sb_start_write(Model1_sb, Model1_SB_FREEZE_PAGEFAULT, true);
}

/*
 * sb_start_intwrite - get write access to a superblock for internal fs purposes
 * @sb: the super we write to
 *
 * This is the third level of protection against filesystem freezing. It is
 * free for use by a filesystem. The only requirement is that it must rank
 * below sb_start_pagefault.
 *
 * For example filesystem can call sb_start_intwrite() when starting a
 * transaction which somewhat eases handling of freezing for internal sources
 * of filesystem changes (internal fs threads, discarding preallocation on file
 * close, etc.).
 */
static inline __attribute__((no_instrument_function)) void Model1_sb_start_intwrite(struct Model1_super_block *Model1_sb)
{
 Model1___sb_start_write(Model1_sb, Model1_SB_FREEZE_FS, true);
}


extern bool Model1_inode_owner_or_capable(const struct Model1_inode *Model1_inode);

/*
 * VFS helper functions..
 */
extern int Model1_vfs_create(struct Model1_inode *, struct Model1_dentry *, Model1_umode_t, bool);
extern int Model1_vfs_mkdir(struct Model1_inode *, struct Model1_dentry *, Model1_umode_t);
extern int Model1_vfs_mknod(struct Model1_inode *, struct Model1_dentry *, Model1_umode_t, Model1_dev_t);
extern int Model1_vfs_symlink(struct Model1_inode *, struct Model1_dentry *, const char *);
extern int Model1_vfs_link(struct Model1_dentry *, struct Model1_inode *, struct Model1_dentry *, struct Model1_inode **);
extern int Model1_vfs_rmdir(struct Model1_inode *, struct Model1_dentry *);
extern int Model1_vfs_unlink(struct Model1_inode *, struct Model1_dentry *, struct Model1_inode **);
extern int Model1_vfs_rename(struct Model1_inode *, struct Model1_dentry *, struct Model1_inode *, struct Model1_dentry *, struct Model1_inode **, unsigned int);
extern int Model1_vfs_whiteout(struct Model1_inode *, struct Model1_dentry *);

/*
 * VFS file helper functions.
 */
extern void Model1_inode_init_owner(struct Model1_inode *Model1_inode, const struct Model1_inode *Model1_dir,
   Model1_umode_t Model1_mode);
extern bool Model1_may_open_dev(const struct Model1_path *Model1_path);
/*
 * VFS FS_IOC_FIEMAP helper definitions.
 */
struct Model1_fiemap_extent_info {
 unsigned int Model1_fi_flags; /* Flags as passed from user */
 unsigned int Model1_fi_extents_mapped; /* Number of mapped extents */
 unsigned int Model1_fi_extents_max; /* Size of fiemap_extent array */
 struct Model1_fiemap_extent *Model1_fi_extents_start; /* Start of
							fiemap_extent array */
};
int Model1_fiemap_fill_next_extent(struct Model1_fiemap_extent_info *Model1_info, Model1_u64 Model1_logical,
       Model1_u64 Model1_phys, Model1_u64 Model1_len, Model1_u32 Model1_flags);
int Model1_fiemap_check_flags(struct Model1_fiemap_extent_info *Model1_fieinfo, Model1_u32 Model1_fs_flags);

/*
 * File types
 *
 * NOTE! These match bits 12..15 of stat.st_mode
 * (ie "(i_mode >> 12) & 15").
 */
/*
 * This is the "filldir" function type, used by readdir() to let
 * the kernel specify what kind of dirent layout it wants to have.
 * This allows the kernel to read directories into kernel space or
 * to have different dirent layouts depending on the binary type.
 */
struct Model1_dir_context;
typedef int (*Model1_filldir_t)(struct Model1_dir_context *, const char *, int, Model1_loff_t, Model1_u64,
    unsigned);

struct Model1_dir_context {
 const Model1_filldir_t Model1_actor;
 Model1_loff_t Model1_pos;
};

struct Model1_block_device_operations;

/* These macros are for out of kernel modules to test that
 * the kernel supports the unlocked_ioctl and compat_ioctl
 * fields in struct file_operations. */



/*
 * These flags let !MMU mmap() govern direct device mapping vs immediate
 * copying more easily for MAP_PRIVATE, especially for ROM filesystems.
 *
 * NOMMU_MAP_COPY:	Copy can be mapped (MAP_PRIVATE)
 * NOMMU_MAP_DIRECT:	Can be mapped directly (MAP_SHARED)
 * NOMMU_MAP_READ:	Can be mapped for reading
 * NOMMU_MAP_WRITE:	Can be mapped for writing
 * NOMMU_MAP_EXEC:	Can be mapped for execution
 */
struct Model1_iov_iter;

struct Model1_file_operations {
 struct Model1_module *Model1_owner;
 Model1_loff_t (*Model1_llseek) (struct Model1_file *, Model1_loff_t, int);
 Model1_ssize_t (*Model1_read) (struct Model1_file *, char *, Model1_size_t, Model1_loff_t *);
 Model1_ssize_t (*Model1_write) (struct Model1_file *, const char *, Model1_size_t, Model1_loff_t *);
 Model1_ssize_t (*Model1_read_iter) (struct Model1_kiocb *, struct Model1_iov_iter *);
 Model1_ssize_t (*Model1_write_iter) (struct Model1_kiocb *, struct Model1_iov_iter *);
 int (*Model1_iterate) (struct Model1_file *, struct Model1_dir_context *);
 int (*Model1_iterate_shared) (struct Model1_file *, struct Model1_dir_context *);
 unsigned int (*Model1_poll) (struct Model1_file *, struct Model1_poll_table_struct *);
 long (*Model1_unlocked_ioctl) (struct Model1_file *, unsigned int, unsigned long);
 long (*Model1_compat_ioctl) (struct Model1_file *, unsigned int, unsigned long);
 int (*Model1_mmap) (struct Model1_file *, struct Model1_vm_area_struct *);
 int (*Model1_open) (struct Model1_inode *, struct Model1_file *);
 int (*Model1_flush) (struct Model1_file *, Model1_fl_owner_t Model1_id);
 int (*Model1_release) (struct Model1_inode *, struct Model1_file *);
 int (*Model1_fsync) (struct Model1_file *, Model1_loff_t, Model1_loff_t, int Model1_datasync);
 int (*Model1_aio_fsync) (struct Model1_kiocb *, int Model1_datasync);
 int (*Model1_fasync) (int, struct Model1_file *, int);
 int (*Model1_lock) (struct Model1_file *, int, struct Model1_file_lock *);
 Model1_ssize_t (*Model1_sendpage) (struct Model1_file *, struct Model1_page *, int, Model1_size_t, Model1_loff_t *, int);
 unsigned long (*Model1_get_unmapped_area)(struct Model1_file *, unsigned long, unsigned long, unsigned long, unsigned long);
 int (*Model1_check_flags)(int);
 int (*Model1_flock) (struct Model1_file *, int, struct Model1_file_lock *);
 Model1_ssize_t (*Model1_splice_write)(struct Model1_pipe_inode_info *, struct Model1_file *, Model1_loff_t *, Model1_size_t, unsigned int);
 Model1_ssize_t (*Model1_splice_read)(struct Model1_file *, Model1_loff_t *, struct Model1_pipe_inode_info *, Model1_size_t, unsigned int);
 int (*Model1_setlease)(struct Model1_file *, long, struct Model1_file_lock **, void **);
 long (*Model1_fallocate)(struct Model1_file *Model1_file, int Model1_mode, Model1_loff_t Model1_offset,
     Model1_loff_t Model1_len);
 void (*Model1_show_fdinfo)(struct Model1_seq_file *Model1_m, struct Model1_file *Model1_f);



 Model1_ssize_t (*Model1_copy_file_range)(struct Model1_file *, Model1_loff_t, struct Model1_file *,
   Model1_loff_t, Model1_size_t, unsigned int);
 int (*Model1_clone_file_range)(struct Model1_file *, Model1_loff_t, struct Model1_file *, Model1_loff_t,
   Model1_u64);
 Model1_ssize_t (*Model1_dedupe_file_range)(struct Model1_file *, Model1_u64, Model1_u64, struct Model1_file *,
   Model1_u64);
};

struct Model1_inode_operations {
 struct Model1_dentry * (*Model1_lookup) (struct Model1_inode *,struct Model1_dentry *, unsigned int);
 const char * (*Model1_get_link) (struct Model1_dentry *, struct Model1_inode *, struct Model1_delayed_call *);
 int (*Model1_permission) (struct Model1_inode *, int);
 struct Model1_posix_acl * (*Model1_get_acl)(struct Model1_inode *, int);

 int (*Model1_readlink) (struct Model1_dentry *, char *,int);

 int (*Model1_create) (struct Model1_inode *,struct Model1_dentry *, Model1_umode_t, bool);
 int (*Model1_link) (struct Model1_dentry *,struct Model1_inode *,struct Model1_dentry *);
 int (*Model1_unlink) (struct Model1_inode *,struct Model1_dentry *);
 int (*Model1_symlink) (struct Model1_inode *,struct Model1_dentry *,const char *);
 int (*Model1_mkdir) (struct Model1_inode *,struct Model1_dentry *,Model1_umode_t);
 int (*Model1_rmdir) (struct Model1_inode *,struct Model1_dentry *);
 int (*Model1_mknod) (struct Model1_inode *,struct Model1_dentry *,Model1_umode_t,Model1_dev_t);
 int (*Model1_rename) (struct Model1_inode *, struct Model1_dentry *,
   struct Model1_inode *, struct Model1_dentry *);
 int (*Model1_rename2) (struct Model1_inode *, struct Model1_dentry *,
   struct Model1_inode *, struct Model1_dentry *, unsigned int);
 int (*Model1_setattr) (struct Model1_dentry *, struct Model1_iattr *);
 int (*Model1_getattr) (struct Model1_vfsmount *Model1_mnt, struct Model1_dentry *, struct Model1_kstat *);
 int (*Model1_setxattr) (struct Model1_dentry *, struct Model1_inode *,
    const char *, const void *, Model1_size_t, int);
 Model1_ssize_t (*Model1_getxattr) (struct Model1_dentry *, struct Model1_inode *,
        const char *, void *, Model1_size_t);
 Model1_ssize_t (*Model1_listxattr) (struct Model1_dentry *, char *, Model1_size_t);
 int (*Model1_removexattr) (struct Model1_dentry *, const char *);
 int (*Model1_fiemap)(struct Model1_inode *, struct Model1_fiemap_extent_info *, Model1_u64 Model1_start,
        Model1_u64 Model1_len);
 int (*Model1_update_time)(struct Model1_inode *, struct Model1_timespec *, int);
 int (*Model1_atomic_open)(struct Model1_inode *, struct Model1_dentry *,
      struct Model1_file *, unsigned Model1_open_flag,
      Model1_umode_t Model1_create_mode, int *Model1_opened);
 int (*Model1_tmpfile) (struct Model1_inode *, struct Model1_dentry *, Model1_umode_t);
 int (*Model1_set_acl)(struct Model1_inode *, struct Model1_posix_acl *, int);
} __attribute__((__aligned__((1 << (6)))));

Model1_ssize_t Model1_rw_copy_check_uvector(int Model1_type, const struct Model1_iovec * Model1_uvector,
         unsigned long Model1_nr_segs, unsigned long Model1_fast_segs,
         struct Model1_iovec *Model1_fast_pointer,
         struct Model1_iovec **Model1_ret_pointer);

extern Model1_ssize_t Model1___vfs_read(struct Model1_file *, char *, Model1_size_t, Model1_loff_t *);
extern Model1_ssize_t Model1___vfs_write(struct Model1_file *, const char *, Model1_size_t, Model1_loff_t *);
extern Model1_ssize_t Model1_vfs_read(struct Model1_file *, char *, Model1_size_t, Model1_loff_t *);
extern Model1_ssize_t Model1_vfs_write(struct Model1_file *, const char *, Model1_size_t, Model1_loff_t *);
extern Model1_ssize_t Model1_vfs_readv(struct Model1_file *, const struct Model1_iovec *,
  unsigned long, Model1_loff_t *, int);
extern Model1_ssize_t Model1_vfs_writev(struct Model1_file *, const struct Model1_iovec *,
  unsigned long, Model1_loff_t *, int);
extern Model1_ssize_t Model1_vfs_copy_file_range(struct Model1_file *, Model1_loff_t , struct Model1_file *,
       Model1_loff_t, Model1_size_t, unsigned int);
extern int Model1_vfs_clone_file_range(struct Model1_file *Model1_file_in, Model1_loff_t Model1_pos_in,
  struct Model1_file *Model1_file_out, Model1_loff_t Model1_pos_out, Model1_u64 Model1_len);
extern int Model1_vfs_dedupe_file_range(struct Model1_file *Model1_file,
     struct Model1_file_dedupe_range *Model1_same);

struct Model1_super_operations {
    struct Model1_inode *(*Model1_alloc_inode)(struct Model1_super_block *Model1_sb);
 void (*Model1_destroy_inode)(struct Model1_inode *);

    void (*Model1_dirty_inode) (struct Model1_inode *, int Model1_flags);
 int (*Model1_write_inode) (struct Model1_inode *, struct Model1_writeback_control *Model1_wbc);
 int (*Model1_drop_inode) (struct Model1_inode *);
 void (*Model1_evict_inode) (struct Model1_inode *);
 void (*Model1_put_super) (struct Model1_super_block *);
 int (*Model1_sync_fs)(struct Model1_super_block *Model1_sb, int Model1_wait);
 int (*Model1_freeze_super) (struct Model1_super_block *);
 int (*Model1_freeze_fs) (struct Model1_super_block *);
 int (*Model1_thaw_super) (struct Model1_super_block *);
 int (*Model1_unfreeze_fs) (struct Model1_super_block *);
 int (*Model1_statfs) (struct Model1_dentry *, struct Model1_kstatfs *);
 int (*Model1_remount_fs) (struct Model1_super_block *, int *, char *);
 void (*Model1_umount_begin) (struct Model1_super_block *);

 int (*Model1_show_options)(struct Model1_seq_file *, struct Model1_dentry *);
 int (*Model1_show_devname)(struct Model1_seq_file *, struct Model1_dentry *);
 int (*Model1_show_path)(struct Model1_seq_file *, struct Model1_dentry *);
 int (*Model1_show_stats)(struct Model1_seq_file *, struct Model1_dentry *);

 Model1_ssize_t (*Model1_quota_read)(struct Model1_super_block *, int, char *, Model1_size_t, Model1_loff_t);
 Model1_ssize_t (*Model1_quota_write)(struct Model1_super_block *, int, const char *, Model1_size_t, Model1_loff_t);
 struct Model1_dquot **(*Model1_get_dquots)(struct Model1_inode *);

 int (*Model1_bdev_try_to_free_page)(struct Model1_super_block*, struct Model1_page*, Model1_gfp_t);
 long (*Model1_nr_cached_objects)(struct Model1_super_block *,
      struct Model1_shrink_control *);
 long (*Model1_free_cached_objects)(struct Model1_super_block *,
        struct Model1_shrink_control *);
};

/*
 * Inode flags - they have no relation to superblock flags now
 */
/*
 * Note that nosuid etc flags are inode-specific: setting some file-system
 * flags just means all the inodes inherit those flags by default. It might be
 * possible to override it selectively if you really wanted to with some
 * ioctl() that is not currently implemented.
 *
 * Exception: MS_RDONLY is always applied to the entire file system.
 *
 * Unfortunately, it is possible to change a filesystems flags with it mounted
 * with files in use.  This means that all of the inodes will not have their
 * i_flags updated.  Hence, i_flags no longer inherit the superblock mount
 * flags, so these have to be checked separately. -- rmk@arm.uk.linux.org
 */
static inline __attribute__((no_instrument_function)) bool Model1_HAS_UNMAPPED_ID(struct Model1_inode *Model1_inode)
{
 return !Model1_uid_valid(Model1_inode->Model1_i_uid) || !Model1_gid_valid(Model1_inode->Model1_i_gid);
}

/*
 * Inode state bits.  Protected by inode->i_lock
 *
 * Three bits determine the dirty state of the inode, I_DIRTY_SYNC,
 * I_DIRTY_DATASYNC and I_DIRTY_PAGES.
 *
 * Four bits define the lifetime of an inode.  Initially, inodes are I_NEW,
 * until that flag is cleared.  I_WILL_FREE, I_FREEING and I_CLEAR are set at
 * various stages of removing an inode.
 *
 * Two bits are used for locking and completion notification, I_NEW and I_SYNC.
 *
 * I_DIRTY_SYNC		Inode is dirty, but doesn't have to be written on
 *			fdatasync().  i_atime is the usual cause.
 * I_DIRTY_DATASYNC	Data-related inode changes pending. We keep track of
 *			these changes separately from I_DIRTY_SYNC so that we
 *			don't have to write inode on fdatasync() when only
 *			mtime has changed in it.
 * I_DIRTY_PAGES	Inode has dirty pages.  Inode itself may be clean.
 * I_NEW		Serves as both a mutex and completion notification.
 *			New inodes set I_NEW.  If two processes both create
 *			the same inode, one of them will release its inode and
 *			wait for I_NEW to be released before returning.
 *			Inodes in I_WILL_FREE, I_FREEING or I_CLEAR state can
 *			also cause waiting on I_NEW, without I_NEW actually
 *			being set.  find_inode() uses this to prevent returning
 *			nearly-dead inodes.
 * I_WILL_FREE		Must be set when calling write_inode_now() if i_count
 *			is zero.  I_FREEING must be set when I_WILL_FREE is
 *			cleared.
 * I_FREEING		Set when inode is about to be freed but still has dirty
 *			pages or buffers attached or the inode itself is still
 *			dirty.
 * I_CLEAR		Added by clear_inode().  In this state the inode is
 *			clean and can be destroyed.  Inode keeps I_FREEING.
 *
 *			Inodes that are I_WILL_FREE, I_FREEING or I_CLEAR are
 *			prohibited for many purposes.  iget() must wait for
 *			the inode to be completely released, then create it
 *			anew.  Other functions will just ignore such inodes,
 *			if appropriate.  I_NEW is used for waiting.
 *
 * I_SYNC		Writeback of inode is running. The bit is set during
 *			data writeback, and cleared with a wakeup on the bit
 *			address once it is done. The bit is also used to pin
 *			the inode in memory for flusher thread.
 *
 * I_REFERENCED		Marks the inode as recently references on the LRU list.
 *
 * I_DIO_WAKEUP		Never set.  Only used as a key for wait_on_bit().
 *
 * I_WB_SWITCH		Cgroup bdi_writeback switching in progress.  Used to
 *			synchronize competing switching instances and to tell
 *			wb stat updates to grab mapping->tree_lock.  See
 *			inode_switch_wb_work_fn() for details.
 *
 * Q: What is the difference between I_WILL_FREE and I_FREEING?
 */
extern void Model1___mark_inode_dirty(struct Model1_inode *, int);
static inline __attribute__((no_instrument_function)) void Model1_mark_inode_dirty(struct Model1_inode *Model1_inode)
{
 Model1___mark_inode_dirty(Model1_inode, ((1 << 0) | (1 << 1) | (1 << 2)));
}

static inline __attribute__((no_instrument_function)) void Model1_mark_inode_dirty_sync(struct Model1_inode *Model1_inode)
{
 Model1___mark_inode_dirty(Model1_inode, (1 << 0));
}

extern void Model1_inc_nlink(struct Model1_inode *Model1_inode);
extern void Model1_drop_nlink(struct Model1_inode *Model1_inode);
extern void Model1_clear_nlink(struct Model1_inode *Model1_inode);
extern void Model1_set_nlink(struct Model1_inode *Model1_inode, unsigned int Model1_nlink);

static inline __attribute__((no_instrument_function)) void Model1_inode_inc_link_count(struct Model1_inode *Model1_inode)
{
 Model1_inc_nlink(Model1_inode);
 Model1_mark_inode_dirty(Model1_inode);
}

static inline __attribute__((no_instrument_function)) void Model1_inode_dec_link_count(struct Model1_inode *Model1_inode)
{
 Model1_drop_nlink(Model1_inode);
 Model1_mark_inode_dirty(Model1_inode);
}

/**
 * inode_inc_iversion - increments i_version
 * @inode: inode that need to be updated
 *
 * Every time the inode is modified, the i_version field will be incremented.
 * The filesystem has to be mounted with i_version flag
 */

static inline __attribute__((no_instrument_function)) void Model1_inode_inc_iversion(struct Model1_inode *Model1_inode)
{
       Model1_spin_lock(&Model1_inode->Model1_i_lock);
       Model1_inode->Model1_i_version++;
       Model1_spin_unlock(&Model1_inode->Model1_i_lock);
}

enum Model1_file_time_flags {
 Model1_S_ATIME = 1,
 Model1_S_MTIME = 2,
 Model1_S_CTIME = 4,
 Model1_S_VERSION = 8,
};

extern bool Model1_atime_needs_update(const struct Model1_path *, struct Model1_inode *);
extern void Model1_touch_atime(const struct Model1_path *);
static inline __attribute__((no_instrument_function)) void Model1_file_accessed(struct Model1_file *Model1_file)
{
 if (!(Model1_file->Model1_f_flags & 01000000))
  Model1_touch_atime(&Model1_file->Model1_f_path);
}

int Model1_sync_inode(struct Model1_inode *Model1_inode, struct Model1_writeback_control *Model1_wbc);
int Model1_sync_inode_metadata(struct Model1_inode *Model1_inode, int Model1_wait);

struct Model1_file_system_type {
 const char *Model1_name;
 int Model1_fs_flags;





 struct Model1_dentry *(*Model1_mount) (struct Model1_file_system_type *, int,
         const char *, void *);
 void (*Model1_kill_sb) (struct Model1_super_block *);
 struct Model1_module *Model1_owner;
 struct Model1_file_system_type * Model1_next;
 struct Model1_hlist_head Model1_fs_supers;

 struct Model1_lock_class_key Model1_s_lock_key;
 struct Model1_lock_class_key Model1_s_umount_key;
 struct Model1_lock_class_key Model1_s_vfs_rename_key;
 struct Model1_lock_class_key Model1_s_writers_key[(Model1_SB_FREEZE_COMPLETE - 1)];

 struct Model1_lock_class_key Model1_i_lock_key;
 struct Model1_lock_class_key Model1_i_mutex_key;
 struct Model1_lock_class_key Model1_i_mutex_dir_key;
};



extern struct Model1_dentry *Model1_mount_ns(struct Model1_file_system_type *Model1_fs_type,
 int Model1_flags, void *Model1_data, void *Model1_ns, struct Model1_user_namespace *Model1_user_ns,
 int (*Model1_fill_super)(struct Model1_super_block *, void *, int));
extern struct Model1_dentry *Model1_mount_bdev(struct Model1_file_system_type *Model1_fs_type,
 int Model1_flags, const char *Model1_dev_name, void *Model1_data,
 int (*Model1_fill_super)(struct Model1_super_block *, void *, int));
extern struct Model1_dentry *Model1_mount_single(struct Model1_file_system_type *Model1_fs_type,
 int Model1_flags, void *Model1_data,
 int (*Model1_fill_super)(struct Model1_super_block *, void *, int));
extern struct Model1_dentry *Model1_mount_nodev(struct Model1_file_system_type *Model1_fs_type,
 int Model1_flags, void *Model1_data,
 int (*Model1_fill_super)(struct Model1_super_block *, void *, int));
extern struct Model1_dentry *Model1_mount_subtree(struct Model1_vfsmount *Model1_mnt, const char *Model1_path);
void Model1_generic_shutdown_super(struct Model1_super_block *Model1_sb);
void Model1_kill_block_super(struct Model1_super_block *Model1_sb);
void Model1_kill_anon_super(struct Model1_super_block *Model1_sb);
void Model1_kill_litter_super(struct Model1_super_block *Model1_sb);
void Model1_deactivate_super(struct Model1_super_block *Model1_sb);
void Model1_deactivate_locked_super(struct Model1_super_block *Model1_sb);
int Model1_set_anon_super(struct Model1_super_block *Model1_s, void *Model1_data);
int Model1_get_anon_bdev(Model1_dev_t *);
void Model1_free_anon_bdev(Model1_dev_t);
struct Model1_super_block *Model1_sget_userns(struct Model1_file_system_type *Model1_type,
   int (*Model1_test)(struct Model1_super_block *,void *),
   int (*Model1_set)(struct Model1_super_block *,void *),
   int Model1_flags, struct Model1_user_namespace *Model1_user_ns,
   void *Model1_data);
struct Model1_super_block *Model1_sget(struct Model1_file_system_type *Model1_type,
   int (*Model1_test)(struct Model1_super_block *,void *),
   int (*Model1_set)(struct Model1_super_block *,void *),
   int Model1_flags, void *Model1_data);
extern struct Model1_dentry *Model1_mount_pseudo(struct Model1_file_system_type *, char *,
 const struct Model1_super_operations *Model1_ops,
 const struct Model1_dentry_operations *Model1_dops,
 unsigned long);

/* Alas, no aliases. Too much hassle with bringing module.h everywhere */




/*
 * This one is to be used *ONLY* from ->open() instances.
 * fops must be non-NULL, pinned down *and* module dependencies
 * should be sufficient to pin the caller down as well.
 */







extern int Model1_register_filesystem(struct Model1_file_system_type *);
extern int Model1_unregister_filesystem(struct Model1_file_system_type *);
extern struct Model1_vfsmount *Model1_kern_mount_data(struct Model1_file_system_type *, void *Model1_data);

extern void Model1_kern_unmount(struct Model1_vfsmount *Model1_mnt);
extern int Model1_may_umount_tree(struct Model1_vfsmount *);
extern int Model1_may_umount(struct Model1_vfsmount *);
extern long Model1_do_mount(const char *, const char *,
       const char *, unsigned long, void *);
extern struct Model1_vfsmount *Model1_collect_mounts(struct Model1_path *);
extern void Model1_drop_collected_mounts(struct Model1_vfsmount *);
extern int Model1_iterate_mounts(int (*)(struct Model1_vfsmount *, void *), void *,
     struct Model1_vfsmount *);
extern int Model1_vfs_statfs(struct Model1_path *, struct Model1_kstatfs *);
extern int Model1_user_statfs(const char *, struct Model1_kstatfs *);
extern int Model1_fd_statfs(int, struct Model1_kstatfs *);
extern int Model1_vfs_ustat(Model1_dev_t, struct Model1_kstatfs *);
extern int Model1_freeze_super(struct Model1_super_block *Model1_super);
extern int Model1_thaw_super(struct Model1_super_block *Model1_super);
extern bool Model1_our_mnt(struct Model1_vfsmount *Model1_mnt);

extern int Model1_current_umask(void);

extern void Model1_ihold(struct Model1_inode * Model1_inode);
extern void Model1_iput(struct Model1_inode *);
extern int Model1_generic_update_time(struct Model1_inode *, struct Model1_timespec *, int);

/* /sys/fs */
extern struct Model1_kobject *Model1_fs_kobj;




extern int Model1_locks_mandatory_locked(struct Model1_file *);
extern int Model1_locks_mandatory_area(struct Model1_inode *, struct Model1_file *, Model1_loff_t, Model1_loff_t, unsigned char);

/*
 * Candidates for mandatory locking have the setgid bit set
 * but no group execute bit -  an otherwise meaningless combination.
 */

static inline __attribute__((no_instrument_function)) int Model1___mandatory_lock(struct Model1_inode *Model1_ino)
{
 return (Model1_ino->Model1_i_mode & (0002000 | 00010)) == 0002000;
}

/*
 * ... and these candidates should be on MS_MANDLOCK mounted fs,
 * otherwise these will be advisory locks
 */

static inline __attribute__((no_instrument_function)) int Model1_mandatory_lock(struct Model1_inode *Model1_ino)
{
 return ((Model1_ino)->Model1_i_sb->Model1_s_flags & (64)) && Model1___mandatory_lock(Model1_ino);
}

static inline __attribute__((no_instrument_function)) int Model1_locks_verify_locked(struct Model1_file *Model1_file)
{
 if (Model1_mandatory_lock(Model1_file_inode(Model1_file)))
  return Model1_locks_mandatory_locked(Model1_file);
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_locks_verify_truncate(struct Model1_inode *Model1_inode,
        struct Model1_file *Model1_f,
        Model1_loff_t Model1_size)
{
 if (!Model1_inode->Model1_i_flctx || !Model1_mandatory_lock(Model1_inode))
  return 0;

 if (Model1_size < Model1_inode->Model1_i_size) {
  return Model1_locks_mandatory_area(Model1_inode, Model1_f, Model1_size, Model1_inode->Model1_i_size - 1,
    1);
 } else {
  return Model1_locks_mandatory_area(Model1_inode, Model1_f, Model1_inode->Model1_i_size, Model1_size - 1,
    1);
 }
}
static inline __attribute__((no_instrument_function)) int Model1_break_lease(struct Model1_inode *Model1_inode, unsigned int Model1_mode)
{
 /*
	 * Since this check is lockless, we must ensure that any refcounts
	 * taken are done before checking i_flctx->flc_lease. Otherwise, we
	 * could end up racing with tasks trying to set a new lease on this
	 * file.
	 */
 asm volatile("mfence":::"memory");
 if (Model1_inode->Model1_i_flctx && !Model1_list_empty_careful(&Model1_inode->Model1_i_flctx->Model1_flc_lease))
  return Model1___break_lease(Model1_inode, Model1_mode, 32);
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_break_deleg(struct Model1_inode *Model1_inode, unsigned int Model1_mode)
{
 /*
	 * Since this check is lockless, we must ensure that any refcounts
	 * taken are done before checking i_flctx->flc_lease. Otherwise, we
	 * could end up racing with tasks trying to set a new lease on this
	 * file.
	 */
 asm volatile("mfence":::"memory");
 if (Model1_inode->Model1_i_flctx && !Model1_list_empty_careful(&Model1_inode->Model1_i_flctx->Model1_flc_lease))
  return Model1___break_lease(Model1_inode, Model1_mode, 4);
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_try_break_deleg(struct Model1_inode *Model1_inode, struct Model1_inode **Model1_delegated_inode)
{
 int Model1_ret;

 Model1_ret = Model1_break_deleg(Model1_inode, 00000001|00004000);
 if (Model1_ret == -11 && Model1_delegated_inode) {
  *Model1_delegated_inode = Model1_inode;
  Model1_ihold(Model1_inode);
 }
 return Model1_ret;
}

static inline __attribute__((no_instrument_function)) int Model1_break_deleg_wait(struct Model1_inode **Model1_delegated_inode)
{
 int Model1_ret;

 Model1_ret = Model1_break_deleg(*Model1_delegated_inode, 00000001);
 Model1_iput(*Model1_delegated_inode);
 *Model1_delegated_inode = ((void *)0);
 return Model1_ret;
}

static inline __attribute__((no_instrument_function)) int Model1_break_layout(struct Model1_inode *Model1_inode, bool Model1_wait)
{
 asm volatile("mfence":::"memory");
 if (Model1_inode->Model1_i_flctx && !Model1_list_empty_careful(&Model1_inode->Model1_i_flctx->Model1_flc_lease))
  return Model1___break_lease(Model1_inode,
    Model1_wait ? 00000001 : 00000001 | 00004000,
    2048);
 return 0;
}
/* fs/open.c */
struct Model1_audit_names;
struct Model1_filename {
 const char *Model1_name; /* pointer to actual string */
 const char *Model1_uptr; /* original userland pointer */
 struct Model1_audit_names *Model1_aname;
 int Model1_refcnt;
 const char Model1_iname[];
};

extern long Model1_vfs_truncate(const struct Model1_path *, Model1_loff_t);
extern int Model1_do_truncate(struct Model1_dentry *, Model1_loff_t Model1_start, unsigned int Model1_time_attrs,
         struct Model1_file *Model1_filp);
extern int Model1_vfs_fallocate(struct Model1_file *Model1_file, int Model1_mode, Model1_loff_t Model1_offset,
   Model1_loff_t Model1_len);
extern long Model1_do_sys_open(int Model1_dfd, const char *Model1_filename, int Model1_flags,
   Model1_umode_t Model1_mode);
extern struct Model1_file *Model1_file_open_name(struct Model1_filename *, int, Model1_umode_t);
extern struct Model1_file *Model1_filp_open(const char *, int, Model1_umode_t);
extern struct Model1_file *Model1_file_open_root(struct Model1_dentry *, struct Model1_vfsmount *,
       const char *, int, Model1_umode_t);
extern struct Model1_file * Model1_dentry_open(const struct Model1_path *, int, const struct Model1_cred *);
extern int Model1_filp_close(struct Model1_file *, Model1_fl_owner_t Model1_id);

extern struct Model1_filename *Model1_getname_flags(const char *, int, int *);
extern struct Model1_filename *Model1_getname(const char *);
extern struct Model1_filename *Model1_getname_kernel(const char *);
extern void Model1_putname(struct Model1_filename *Model1_name);

enum {
 Model1_FILE_CREATED = 1,
 Model1_FILE_OPENED = 2
};
extern int Model1_finish_open(struct Model1_file *Model1_file, struct Model1_dentry *Model1_dentry,
   int (*Model1_open)(struct Model1_inode *, struct Model1_file *),
   int *Model1_opened);
extern int Model1_finish_no_open(struct Model1_file *Model1_file, struct Model1_dentry *Model1_dentry);

/* fs/ioctl.c */

extern int Model1_ioctl_preallocate(struct Model1_file *Model1_filp, void *Model1_argp);

/* fs/dcache.c */
extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_vfs_caches_init_early(void);
extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_vfs_caches_init(void);

extern struct Model1_kmem_cache *Model1_names_cachep;





extern int Model1_register_blkdev(unsigned int, const char *);
extern void Model1_unregister_blkdev(unsigned int, const char *);
extern struct Model1_block_device *Model1_bdget(Model1_dev_t);
extern struct Model1_block_device *Model1_bdgrab(struct Model1_block_device *Model1_bdev);
extern void Model1_bd_set_size(struct Model1_block_device *, Model1_loff_t Model1_size);
extern void Model1_bd_forget(struct Model1_inode *Model1_inode);
extern void Model1_bdput(struct Model1_block_device *);
extern void Model1_invalidate_bdev(struct Model1_block_device *);
extern void Model1_iterate_bdevs(void (*)(struct Model1_block_device *, void *), void *);
extern int Model1_sync_blockdev(struct Model1_block_device *Model1_bdev);
extern void Model1_kill_bdev(struct Model1_block_device *);
extern struct Model1_super_block *Model1_freeze_bdev(struct Model1_block_device *);
extern void Model1_emergency_thaw_all(void);
extern int Model1_thaw_bdev(struct Model1_block_device *Model1_bdev, struct Model1_super_block *Model1_sb);
extern int Model1_fsync_bdev(struct Model1_block_device *);

extern struct Model1_super_block *Model1_blockdev_superblock;

static inline __attribute__((no_instrument_function)) bool Model1_sb_is_blkdev_sb(struct Model1_super_block *Model1_sb)
{
 return Model1_sb == Model1_blockdev_superblock;
}
extern int Model1_sync_filesystem(struct Model1_super_block *);
extern const struct Model1_file_operations Model1_def_blk_fops;
extern const struct Model1_file_operations Model1_def_chr_fops;

extern int Model1_ioctl_by_bdev(struct Model1_block_device *, unsigned, unsigned long);
extern int Model1_blkdev_ioctl(struct Model1_block_device *, Model1_fmode_t, unsigned, unsigned long);
extern long Model1_compat_blkdev_ioctl(struct Model1_file *, unsigned, unsigned long);
extern int Model1_blkdev_get(struct Model1_block_device *Model1_bdev, Model1_fmode_t Model1_mode, void *Model1_holder);
extern struct Model1_block_device *Model1_blkdev_get_by_path(const char *Model1_path, Model1_fmode_t Model1_mode,
            void *Model1_holder);
extern struct Model1_block_device *Model1_blkdev_get_by_dev(Model1_dev_t Model1_dev, Model1_fmode_t Model1_mode,
           void *Model1_holder);
extern void Model1_blkdev_put(struct Model1_block_device *Model1_bdev, Model1_fmode_t Model1_mode);
extern int Model1___blkdev_reread_part(struct Model1_block_device *Model1_bdev);
extern int Model1_blkdev_reread_part(struct Model1_block_device *Model1_bdev);


extern int Model1_bd_link_disk_holder(struct Model1_block_device *Model1_bdev, struct Model1_gendisk *Model1_disk);
extern void Model1_bd_unlink_disk_holder(struct Model1_block_device *Model1_bdev,
      struct Model1_gendisk *Model1_disk);
/* fs/char_dev.c */

/* Marks the bottom of the first segment of free char majors */

extern int Model1_alloc_chrdev_region(Model1_dev_t *, unsigned, unsigned, const char *);
extern int Model1_register_chrdev_region(Model1_dev_t, unsigned, const char *);
extern int Model1___register_chrdev(unsigned int Model1_major, unsigned int Model1_baseminor,
        unsigned int Model1_count, const char *Model1_name,
        const struct Model1_file_operations *Model1_fops);
extern void Model1___unregister_chrdev(unsigned int Model1_major, unsigned int Model1_baseminor,
    unsigned int Model1_count, const char *Model1_name);
extern void Model1_unregister_chrdev_region(Model1_dev_t, unsigned);
extern void Model1_chrdev_show(struct Model1_seq_file *,Model1_off_t);

static inline __attribute__((no_instrument_function)) int Model1_register_chrdev(unsigned int Model1_major, const char *Model1_name,
      const struct Model1_file_operations *Model1_fops)
{
 return Model1___register_chrdev(Model1_major, 0, 256, Model1_name, Model1_fops);
}

static inline __attribute__((no_instrument_function)) void Model1_unregister_chrdev(unsigned int Model1_major, const char *Model1_name)
{
 Model1___unregister_chrdev(Model1_major, 0, 256, Model1_name);
}

/* fs/block_dev.c */





extern const char *Model1___bdevname(Model1_dev_t, char *Model1_buffer);
extern const char *Model1_bdevname(struct Model1_block_device *Model1_bdev, char *Model1_buffer);
extern struct Model1_block_device *Model1_lookup_bdev(const char *);
extern void Model1_blkdev_show(struct Model1_seq_file *,Model1_off_t);





extern void Model1_init_special_inode(struct Model1_inode *, Model1_umode_t, Model1_dev_t);

/* Invalid inode operations -- fs/bad_inode.c */
extern void Model1_make_bad_inode(struct Model1_inode *);
extern bool Model1_is_bad_inode(struct Model1_inode *);


static inline __attribute__((no_instrument_function)) bool Model1_op_is_write(unsigned int Model1_op)
{
 return Model1_op == Model1_REQ_OP_READ ? false : true;
}

/*
 * return data direction, READ or WRITE
 */
static inline __attribute__((no_instrument_function)) int Model1_bio_data_dir(struct Model1_bio *Model1_bio)
{
 return Model1_op_is_write(((Model1_bio)->Model1_bi_opf >> (8 * sizeof(unsigned int) - 3))) ? Model1_REQ_OP_WRITE : Model1_REQ_OP_READ;
}

extern void Model1_check_disk_size_change(struct Model1_gendisk *Model1_disk,
       struct Model1_block_device *Model1_bdev);
extern int Model1_revalidate_disk(struct Model1_gendisk *);
extern int Model1_check_disk_change(struct Model1_block_device *);
extern int Model1___invalidate_device(struct Model1_block_device *, bool);
extern int Model1_invalidate_partition(struct Model1_gendisk *, int);

unsigned long Model1_invalidate_mapping_pages(struct Model1_address_space *Model1_mapping,
     unsigned long Model1_start, unsigned long Model1_end);

static inline __attribute__((no_instrument_function)) void Model1_invalidate_remote_inode(struct Model1_inode *Model1_inode)
{
 if ((((Model1_inode->Model1_i_mode) & 00170000) == 0100000) || (((Model1_inode->Model1_i_mode) & 00170000) == 0040000) ||
     (((Model1_inode->Model1_i_mode) & 00170000) == 0120000))
  Model1_invalidate_mapping_pages(Model1_inode->Model1_i_mapping, 0, -1);
}
extern int Model1_invalidate_inode_pages2(struct Model1_address_space *Model1_mapping);
extern int Model1_invalidate_inode_pages2_range(struct Model1_address_space *Model1_mapping,
      unsigned long Model1_start, unsigned long Model1_end);
extern int Model1_write_inode_now(struct Model1_inode *, int);
extern int Model1_filemap_fdatawrite(struct Model1_address_space *);
extern int Model1_filemap_flush(struct Model1_address_space *);
extern int Model1_filemap_fdatawait(struct Model1_address_space *);
extern void Model1_filemap_fdatawait_keep_errors(struct Model1_address_space *);
extern int Model1_filemap_fdatawait_range(struct Model1_address_space *, Model1_loff_t Model1_lstart,
       Model1_loff_t Model1_lend);
extern int Model1_filemap_write_and_wait(struct Model1_address_space *Model1_mapping);
extern int Model1_filemap_write_and_wait_range(struct Model1_address_space *Model1_mapping,
            Model1_loff_t Model1_lstart, Model1_loff_t Model1_lend);
extern int Model1___filemap_fdatawrite_range(struct Model1_address_space *Model1_mapping,
    Model1_loff_t Model1_start, Model1_loff_t Model1_end, int Model1_sync_mode);
extern int Model1_filemap_fdatawrite_range(struct Model1_address_space *Model1_mapping,
    Model1_loff_t Model1_start, Model1_loff_t Model1_end);
extern int Model1_filemap_check_errors(struct Model1_address_space *Model1_mapping);

extern int Model1_vfs_fsync_range(struct Model1_file *Model1_file, Model1_loff_t Model1_start, Model1_loff_t Model1_end,
      int Model1_datasync);
extern int Model1_vfs_fsync(struct Model1_file *Model1_file, int Model1_datasync);

/*
 * Sync the bytes written if this was a synchronous write.  Expect ki_pos
 * to already be updated for the write, and will return either the amount
 * of bytes passed in, or an error if syncing the file failed.
 */
static inline __attribute__((no_instrument_function)) Model1_ssize_t Model1_generic_write_sync(struct Model1_kiocb *Model1_iocb, Model1_ssize_t Model1_count)
{
 if (Model1_iocb->Model1_ki_flags & (1 << 4)) {
  int Model1_ret = Model1_vfs_fsync_range(Model1_iocb->Model1_ki_filp,
    Model1_iocb->Model1_ki_pos - Model1_count, Model1_iocb->Model1_ki_pos - 1,
    (Model1_iocb->Model1_ki_flags & (1 << 5)) ? 0 : 1);
  if (Model1_ret)
   return Model1_ret;
 }

 return Model1_count;
}

extern void Model1_emergency_sync(void);
extern void Model1_emergency_remount(void);

extern Model1_sector_t Model1_bmap(struct Model1_inode *, Model1_sector_t);

extern int Model1_notify_change(struct Model1_dentry *, struct Model1_iattr *, struct Model1_inode **);
extern int Model1_inode_permission(struct Model1_inode *, int);
extern int Model1___inode_permission(struct Model1_inode *, int);
extern int Model1_generic_permission(struct Model1_inode *, int);
extern int Model1___check_sticky(struct Model1_inode *Model1_dir, struct Model1_inode *Model1_inode);

static inline __attribute__((no_instrument_function)) bool Model1_execute_ok(struct Model1_inode *Model1_inode)
{
 return (Model1_inode->Model1_i_mode & (00100|00010|00001)) || (((Model1_inode->Model1_i_mode) & 00170000) == 0040000);
}

static inline __attribute__((no_instrument_function)) void Model1_file_start_write(struct Model1_file *Model1_file)
{
 if (!(((Model1_file_inode(Model1_file)->Model1_i_mode) & 00170000) == 0100000))
  return;
 Model1___sb_start_write(Model1_file_inode(Model1_file)->Model1_i_sb, Model1_SB_FREEZE_WRITE, true);
}

static inline __attribute__((no_instrument_function)) bool Model1_file_start_write_trylock(struct Model1_file *Model1_file)
{
 if (!(((Model1_file_inode(Model1_file)->Model1_i_mode) & 00170000) == 0100000))
  return true;
 return Model1___sb_start_write(Model1_file_inode(Model1_file)->Model1_i_sb, Model1_SB_FREEZE_WRITE, false);
}

static inline __attribute__((no_instrument_function)) void Model1_file_end_write(struct Model1_file *Model1_file)
{
 if (!(((Model1_file_inode(Model1_file)->Model1_i_mode) & 00170000) == 0100000))
  return;
 Model1___sb_end_write(Model1_file_inode(Model1_file)->Model1_i_sb, Model1_SB_FREEZE_WRITE);
}

/*
 * get_write_access() gets write permission for a file.
 * put_write_access() releases this write permission.
 * This is used for regular files.
 * We cannot support write (and maybe mmap read-write shared) accesses and
 * MAP_DENYWRITE mmappings simultaneously. The i_writecount field of an inode
 * can have the following values:
 * 0: no writers, no VM_DENYWRITE mappings
 * < 0: (-i_writecount) vm_area_structs with VM_DENYWRITE set exist
 * > 0: (i_writecount) users are writing to the file.
 *
 * Normally we operate on that counter with atomic_{inc,dec} and it's safe
 * except for the cases where we don't hold i_writecount yet. Then we need to
 * use {get,deny}_write_access() - these functions check the sign and refuse
 * to do the change if sign is wrong.
 */
static inline __attribute__((no_instrument_function)) int Model1_get_write_access(struct Model1_inode *Model1_inode)
{
 return Model1_atomic_inc_unless_negative(&Model1_inode->Model1_i_writecount) ? 0 : -26;
}
static inline __attribute__((no_instrument_function)) int Model1_deny_write_access(struct Model1_file *Model1_file)
{
 struct Model1_inode *Model1_inode = Model1_file_inode(Model1_file);
 return Model1_atomic_dec_unless_positive(&Model1_inode->Model1_i_writecount) ? 0 : -26;
}
static inline __attribute__((no_instrument_function)) void Model1_put_write_access(struct Model1_inode * Model1_inode)
{
 Model1_atomic_dec(&Model1_inode->Model1_i_writecount);
}
static inline __attribute__((no_instrument_function)) void Model1_allow_write_access(struct Model1_file *Model1_file)
{
 if (Model1_file)
  Model1_atomic_inc(&Model1_file_inode(Model1_file)->Model1_i_writecount);
}
static inline __attribute__((no_instrument_function)) bool Model1_inode_is_open_for_write(const struct Model1_inode *Model1_inode)
{
 return Model1_atomic_read(&Model1_inode->Model1_i_writecount) > 0;
}
static inline __attribute__((no_instrument_function)) void Model1_i_readcount_dec(struct Model1_inode *Model1_inode)
{
 return;
}
static inline __attribute__((no_instrument_function)) void Model1_i_readcount_inc(struct Model1_inode *Model1_inode)
{
 return;
}

extern int Model1_do_pipe_flags(int *, int);
enum Model1_kernel_read_file_id {
 Model1_READING_UNKNOWN, Model1_READING_FIRMWARE, Model1_READING_FIRMWARE_PREALLOC_BUFFER, Model1_READING_MODULE, Model1_READING_KEXEC_IMAGE, Model1_READING_KEXEC_INITRAMFS, Model1_READING_POLICY, Model1_READING_MAX_ID,
};

static const char * const Model1_kernel_read_file_str[] = {
 "unknown", "firmware", "firmware", "kernel-module", "kexec-image", "kexec-initramfs", "security-policy", "",
};

static inline __attribute__((no_instrument_function)) const char *Model1_kernel_read_file_id_str(enum Model1_kernel_read_file_id Model1_id)
{
 if (Model1_id < 0 || Model1_id >= Model1_READING_MAX_ID)
  return Model1_kernel_read_file_str[Model1_READING_UNKNOWN];

 return Model1_kernel_read_file_str[Model1_id];
}

extern int Model1_kernel_read(struct Model1_file *, Model1_loff_t, char *, unsigned long);
extern int Model1_kernel_read_file(struct Model1_file *, void **, Model1_loff_t *, Model1_loff_t,
       enum Model1_kernel_read_file_id);
extern int Model1_kernel_read_file_from_path(char *, void **, Model1_loff_t *, Model1_loff_t,
          enum Model1_kernel_read_file_id);
extern int Model1_kernel_read_file_from_fd(int, void **, Model1_loff_t *, Model1_loff_t,
        enum Model1_kernel_read_file_id);
extern Model1_ssize_t Model1_kernel_write(struct Model1_file *, const char *, Model1_size_t, Model1_loff_t);
extern Model1_ssize_t Model1___kernel_write(struct Model1_file *, const char *, Model1_size_t, Model1_loff_t *);
extern struct Model1_file * Model1_open_exec(const char *);

/* fs/dcache.c -- generic fs support functions */
extern bool Model1_is_subdir(struct Model1_dentry *, struct Model1_dentry *);
extern bool Model1_path_is_under(struct Model1_path *, struct Model1_path *);

extern char *Model1_file_path(struct Model1_file *, char *, int);



/* needed for stackable file system support */
extern Model1_loff_t Model1_default_llseek(struct Model1_file *Model1_file, Model1_loff_t Model1_offset, int Model1_whence);

extern Model1_loff_t Model1_vfs_llseek(struct Model1_file *Model1_file, Model1_loff_t Model1_offset, int Model1_whence);

extern int Model1_inode_init_always(struct Model1_super_block *, struct Model1_inode *);
extern void Model1_inode_init_once(struct Model1_inode *);
extern void Model1_address_space_init_once(struct Model1_address_space *Model1_mapping);
extern struct Model1_inode * Model1_igrab(struct Model1_inode *);
extern Model1_ino_t Model1_iunique(struct Model1_super_block *, Model1_ino_t);
extern int Model1_inode_needs_sync(struct Model1_inode *Model1_inode);
extern int Model1_generic_delete_inode(struct Model1_inode *Model1_inode);
static inline __attribute__((no_instrument_function)) int Model1_generic_drop_inode(struct Model1_inode *Model1_inode)
{
 return !Model1_inode->Model1_i_nlink || Model1_inode_unhashed(Model1_inode);
}

extern struct Model1_inode *Model1_ilookup5_nowait(struct Model1_super_block *Model1_sb,
  unsigned long Model1_hashval, int (*Model1_test)(struct Model1_inode *, void *),
  void *Model1_data);
extern struct Model1_inode *Model1_ilookup5(struct Model1_super_block *Model1_sb, unsigned long Model1_hashval,
  int (*Model1_test)(struct Model1_inode *, void *), void *Model1_data);
extern struct Model1_inode *Model1_ilookup(struct Model1_super_block *Model1_sb, unsigned long Model1_ino);

extern struct Model1_inode * Model1_iget5_locked(struct Model1_super_block *, unsigned long, int (*Model1_test)(struct Model1_inode *, void *), int (*Model1_set)(struct Model1_inode *, void *), void *);
extern struct Model1_inode * Model1_iget_locked(struct Model1_super_block *, unsigned long);
extern struct Model1_inode *Model1_find_inode_nowait(struct Model1_super_block *,
           unsigned long,
           int (*Model1_match)(struct Model1_inode *,
          unsigned long, void *),
           void *Model1_data);
extern int Model1_insert_inode_locked4(struct Model1_inode *, unsigned long, int (*Model1_test)(struct Model1_inode *, void *), void *);
extern int Model1_insert_inode_locked(struct Model1_inode *);



static inline __attribute__((no_instrument_function)) void Model1_lockdep_annotate_inode_mutex_key(struct Model1_inode *Model1_inode) { };

extern void Model1_unlock_new_inode(struct Model1_inode *);
extern unsigned int Model1_get_next_ino(void);

extern void Model1___iget(struct Model1_inode * Model1_inode);
extern void Model1_iget_failed(struct Model1_inode *);
extern void Model1_clear_inode(struct Model1_inode *);
extern void Model1___destroy_inode(struct Model1_inode *);
extern struct Model1_inode *Model1_new_inode_pseudo(struct Model1_super_block *Model1_sb);
extern struct Model1_inode *Model1_new_inode(struct Model1_super_block *Model1_sb);
extern void Model1_free_inode_nonrcu(struct Model1_inode *Model1_inode);
extern int Model1_should_remove_suid(struct Model1_dentry *);
extern int Model1_file_remove_privs(struct Model1_file *);

extern void Model1___insert_inode_hash(struct Model1_inode *, unsigned long Model1_hashval);
static inline __attribute__((no_instrument_function)) void Model1_insert_inode_hash(struct Model1_inode *Model1_inode)
{
 Model1___insert_inode_hash(Model1_inode, Model1_inode->Model1_i_ino);
}

extern void Model1___remove_inode_hash(struct Model1_inode *);
static inline __attribute__((no_instrument_function)) void Model1_remove_inode_hash(struct Model1_inode *Model1_inode)
{
 if (!Model1_inode_unhashed(Model1_inode) && !Model1_hlist_fake(&Model1_inode->Model1_i_hash))
  Model1___remove_inode_hash(Model1_inode);
}

extern void Model1_inode_sb_list_add(struct Model1_inode *Model1_inode);


extern Model1_blk_qc_t Model1_submit_bio(struct Model1_bio *);
extern int Model1_bdev_read_only(struct Model1_block_device *);

extern int Model1_set_blocksize(struct Model1_block_device *, int);
extern int Model1_sb_set_blocksize(struct Model1_super_block *, int);
extern int Model1_sb_min_blocksize(struct Model1_super_block *, int);

extern int Model1_generic_file_mmap(struct Model1_file *, struct Model1_vm_area_struct *);
extern int Model1_generic_file_readonly_mmap(struct Model1_file *, struct Model1_vm_area_struct *);
extern Model1_ssize_t Model1_generic_write_checks(struct Model1_kiocb *, struct Model1_iov_iter *);
extern Model1_ssize_t Model1_generic_file_read_iter(struct Model1_kiocb *, struct Model1_iov_iter *);
extern Model1_ssize_t Model1___generic_file_write_iter(struct Model1_kiocb *, struct Model1_iov_iter *);
extern Model1_ssize_t Model1_generic_file_write_iter(struct Model1_kiocb *, struct Model1_iov_iter *);
extern Model1_ssize_t Model1_generic_file_direct_write(struct Model1_kiocb *, struct Model1_iov_iter *);
extern Model1_ssize_t Model1_generic_perform_write(struct Model1_file *, struct Model1_iov_iter *, Model1_loff_t);

Model1_ssize_t Model1_vfs_iter_read(struct Model1_file *Model1_file, struct Model1_iov_iter *Model1_iter, Model1_loff_t *Model1_ppos);
Model1_ssize_t Model1_vfs_iter_write(struct Model1_file *Model1_file, struct Model1_iov_iter *Model1_iter, Model1_loff_t *Model1_ppos);

/* fs/block_dev.c */
extern Model1_ssize_t Model1_blkdev_read_iter(struct Model1_kiocb *Model1_iocb, struct Model1_iov_iter *Model1_to);
extern Model1_ssize_t Model1_blkdev_write_iter(struct Model1_kiocb *Model1_iocb, struct Model1_iov_iter *Model1_from);
extern int Model1_blkdev_fsync(struct Model1_file *Model1_filp, Model1_loff_t Model1_start, Model1_loff_t Model1_end,
   int Model1_datasync);
extern void Model1_block_sync_page(struct Model1_page *Model1_page);

/* fs/splice.c */
extern Model1_ssize_t Model1_generic_file_splice_read(struct Model1_file *, Model1_loff_t *,
  struct Model1_pipe_inode_info *, Model1_size_t, unsigned int);
extern Model1_ssize_t Model1_default_file_splice_read(struct Model1_file *, Model1_loff_t *,
  struct Model1_pipe_inode_info *, Model1_size_t, unsigned int);
extern Model1_ssize_t Model1_iter_file_splice_write(struct Model1_pipe_inode_info *,
  struct Model1_file *, Model1_loff_t *, Model1_size_t, unsigned int);
extern Model1_ssize_t Model1_generic_splice_sendpage(struct Model1_pipe_inode_info *Model1_pipe,
  struct Model1_file *Model1_out, Model1_loff_t *, Model1_size_t Model1_len, unsigned int Model1_flags);
extern long Model1_do_splice_direct(struct Model1_file *Model1_in, Model1_loff_t *Model1_ppos, struct Model1_file *Model1_out,
  Model1_loff_t *Model1_opos, Model1_size_t Model1_len, unsigned int Model1_flags);


extern void
Model1_file_ra_state_init(struct Model1_file_ra_state *Model1_ra, struct Model1_address_space *Model1_mapping);
extern Model1_loff_t Model1_noop_llseek(struct Model1_file *Model1_file, Model1_loff_t Model1_offset, int Model1_whence);
extern Model1_loff_t Model1_no_llseek(struct Model1_file *Model1_file, Model1_loff_t Model1_offset, int Model1_whence);
extern Model1_loff_t Model1_vfs_setpos(struct Model1_file *Model1_file, Model1_loff_t Model1_offset, Model1_loff_t Model1_maxsize);
extern Model1_loff_t Model1_generic_file_llseek(struct Model1_file *Model1_file, Model1_loff_t Model1_offset, int Model1_whence);
extern Model1_loff_t Model1_generic_file_llseek_size(struct Model1_file *Model1_file, Model1_loff_t Model1_offset,
  int Model1_whence, Model1_loff_t Model1_maxsize, Model1_loff_t Model1_eof);
extern Model1_loff_t Model1_fixed_size_llseek(struct Model1_file *Model1_file, Model1_loff_t Model1_offset,
  int Model1_whence, Model1_loff_t Model1_size);
extern Model1_loff_t Model1_no_seek_end_llseek_size(struct Model1_file *, Model1_loff_t, int, Model1_loff_t);
extern Model1_loff_t Model1_no_seek_end_llseek(struct Model1_file *, Model1_loff_t, int);
extern int Model1_generic_file_open(struct Model1_inode * Model1_inode, struct Model1_file * Model1_filp);
extern int Model1_nonseekable_open(struct Model1_inode * Model1_inode, struct Model1_file * Model1_filp);


typedef void (Model1_dio_submit_t)(struct Model1_bio *Model1_bio, struct Model1_inode *Model1_inode,
       Model1_loff_t Model1_file_offset);

enum {
 /* need locking between buffered and direct access */
 Model1_DIO_LOCKING = 0x01,

 /* filesystem does not support filling holes */
 Model1_DIO_SKIP_HOLES = 0x02,

 /* filesystem can handle aio writes beyond i_size */
 Model1_DIO_ASYNC_EXTEND = 0x04,

 /* inode/fs/bdev does not need truncate protection */
 Model1_DIO_SKIP_DIO_COUNT = 0x08,
};

void Model1_dio_end_io(struct Model1_bio *Model1_bio, int error);

Model1_ssize_t Model1___blockdev_direct_IO(struct Model1_kiocb *Model1_iocb, struct Model1_inode *Model1_inode,
        struct Model1_block_device *Model1_bdev, struct Model1_iov_iter *Model1_iter,
        Model1_get_block_t Model1_get_block,
        Model1_dio_iodone_t Model1_end_io, Model1_dio_submit_t Model1_submit_io,
        int Model1_flags);

static inline __attribute__((no_instrument_function)) Model1_ssize_t Model1_blockdev_direct_IO(struct Model1_kiocb *Model1_iocb,
      struct Model1_inode *Model1_inode,
      struct Model1_iov_iter *Model1_iter,
      Model1_get_block_t Model1_get_block)
{
 return Model1___blockdev_direct_IO(Model1_iocb, Model1_inode, Model1_inode->Model1_i_sb->Model1_s_bdev, Model1_iter,
   Model1_get_block, ((void *)0), ((void *)0), Model1_DIO_LOCKING | Model1_DIO_SKIP_HOLES);
}


void Model1_inode_dio_wait(struct Model1_inode *Model1_inode);

/*
 * inode_dio_begin - signal start of a direct I/O requests
 * @inode: inode the direct I/O happens on
 *
 * This is called once we've finished processing a direct I/O request,
 * and is used to wake up callers waiting for direct I/O to be quiesced.
 */
static inline __attribute__((no_instrument_function)) void Model1_inode_dio_begin(struct Model1_inode *Model1_inode)
{
 Model1_atomic_inc(&Model1_inode->Model1_i_dio_count);
}

/*
 * inode_dio_end - signal finish of a direct I/O requests
 * @inode: inode the direct I/O happens on
 *
 * This is called once we've finished processing a direct I/O request,
 * and is used to wake up callers waiting for direct I/O to be quiesced.
 */
static inline __attribute__((no_instrument_function)) void Model1_inode_dio_end(struct Model1_inode *Model1_inode)
{
 if (Model1_atomic_dec_and_test(&Model1_inode->Model1_i_dio_count))
  Model1_wake_up_bit(&Model1_inode->Model1_i_state, 9);
}

extern void Model1_inode_set_flags(struct Model1_inode *Model1_inode, unsigned int Model1_flags,
       unsigned int Model1_mask);

extern const struct Model1_file_operations Model1_generic_ro_fops;



extern int Model1_readlink_copy(char *, int, const char *);
extern int Model1_page_readlink(struct Model1_dentry *, char *, int);
extern const char *Model1_page_get_link(struct Model1_dentry *, struct Model1_inode *,
     struct Model1_delayed_call *);
extern void Model1_page_put_link(void *);
extern int Model1___page_symlink(struct Model1_inode *Model1_inode, const char *Model1_symname, int Model1_len,
  int Model1_nofs);
extern int Model1_page_symlink(struct Model1_inode *Model1_inode, const char *Model1_symname, int Model1_len);
extern const struct Model1_inode_operations Model1_page_symlink_inode_operations;
extern void Model1_kfree_link(void *);
extern int Model1_generic_readlink(struct Model1_dentry *, char *, int);
extern void Model1_generic_fillattr(struct Model1_inode *, struct Model1_kstat *);
int Model1_vfs_getattr_nosec(struct Model1_path *Model1_path, struct Model1_kstat *Model1_stat);
extern int Model1_vfs_getattr(struct Model1_path *, struct Model1_kstat *);
void Model1___inode_add_bytes(struct Model1_inode *Model1_inode, Model1_loff_t Model1_bytes);
void Model1_inode_add_bytes(struct Model1_inode *Model1_inode, Model1_loff_t Model1_bytes);
void Model1___inode_sub_bytes(struct Model1_inode *Model1_inode, Model1_loff_t Model1_bytes);
void Model1_inode_sub_bytes(struct Model1_inode *Model1_inode, Model1_loff_t Model1_bytes);
Model1_loff_t Model1_inode_get_bytes(struct Model1_inode *Model1_inode);
void Model1_inode_set_bytes(struct Model1_inode *Model1_inode, Model1_loff_t Model1_bytes);
const char *Model1_simple_get_link(struct Model1_dentry *, struct Model1_inode *,
       struct Model1_delayed_call *);
extern const struct Model1_inode_operations Model1_simple_symlink_inode_operations;

extern int Model1_iterate_dir(struct Model1_file *, struct Model1_dir_context *);

extern int Model1_vfs_stat(const char *, struct Model1_kstat *);
extern int Model1_vfs_lstat(const char *, struct Model1_kstat *);
extern int Model1_vfs_fstat(unsigned int, struct Model1_kstat *);
extern int Model1_vfs_fstatat(int , const char *, struct Model1_kstat *, int);

extern int Model1___generic_block_fiemap(struct Model1_inode *Model1_inode,
      struct Model1_fiemap_extent_info *Model1_fieinfo,
      Model1_loff_t Model1_start, Model1_loff_t Model1_len,
      Model1_get_block_t *Model1_get_block);
extern int Model1_generic_block_fiemap(struct Model1_inode *Model1_inode,
    struct Model1_fiemap_extent_info *Model1_fieinfo, Model1_u64 Model1_start,
    Model1_u64 Model1_len, Model1_get_block_t *Model1_get_block);

extern void Model1_get_filesystem(struct Model1_file_system_type *Model1_fs);
extern void Model1_put_filesystem(struct Model1_file_system_type *Model1_fs);
extern struct Model1_file_system_type *Model1_get_fs_type(const char *Model1_name);
extern struct Model1_super_block *Model1_get_super(struct Model1_block_device *);
extern struct Model1_super_block *Model1_get_super_thawed(struct Model1_block_device *);
extern struct Model1_super_block *Model1_get_active_super(struct Model1_block_device *Model1_bdev);
extern void Model1_drop_super(struct Model1_super_block *Model1_sb);
extern void Model1_iterate_supers(void (*)(struct Model1_super_block *, void *), void *);
extern void Model1_iterate_supers_type(struct Model1_file_system_type *,
           void (*)(struct Model1_super_block *, void *), void *);

extern int Model1_dcache_dir_open(struct Model1_inode *, struct Model1_file *);
extern int Model1_dcache_dir_close(struct Model1_inode *, struct Model1_file *);
extern Model1_loff_t Model1_dcache_dir_lseek(struct Model1_file *, Model1_loff_t, int);
extern int Model1_dcache_readdir(struct Model1_file *, struct Model1_dir_context *);
extern int Model1_simple_setattr(struct Model1_dentry *, struct Model1_iattr *);
extern int Model1_simple_getattr(struct Model1_vfsmount *, struct Model1_dentry *, struct Model1_kstat *);
extern int Model1_simple_statfs(struct Model1_dentry *, struct Model1_kstatfs *);
extern int Model1_simple_open(struct Model1_inode *Model1_inode, struct Model1_file *Model1_file);
extern int Model1_simple_link(struct Model1_dentry *, struct Model1_inode *, struct Model1_dentry *);
extern int Model1_simple_unlink(struct Model1_inode *, struct Model1_dentry *);
extern int Model1_simple_rmdir(struct Model1_inode *, struct Model1_dentry *);
extern int Model1_simple_rename(struct Model1_inode *, struct Model1_dentry *, struct Model1_inode *, struct Model1_dentry *);
extern int Model1_noop_fsync(struct Model1_file *, Model1_loff_t, Model1_loff_t, int);
extern int Model1_simple_empty(struct Model1_dentry *);
extern int Model1_simple_readpage(struct Model1_file *Model1_file, struct Model1_page *Model1_page);
extern int Model1_simple_write_begin(struct Model1_file *Model1_file, struct Model1_address_space *Model1_mapping,
   Model1_loff_t Model1_pos, unsigned Model1_len, unsigned Model1_flags,
   struct Model1_page **Model1_pagep, void **Model1_fsdata);
extern int Model1_simple_write_end(struct Model1_file *Model1_file, struct Model1_address_space *Model1_mapping,
   Model1_loff_t Model1_pos, unsigned Model1_len, unsigned Model1_copied,
   struct Model1_page *Model1_page, void *Model1_fsdata);
extern int Model1_always_delete_dentry(const struct Model1_dentry *);
extern struct Model1_inode *Model1_alloc_anon_inode(struct Model1_super_block *);
extern int Model1_simple_nosetlease(struct Model1_file *, long, struct Model1_file_lock **, void **);
extern const struct Model1_dentry_operations Model1_simple_dentry_operations;

extern struct Model1_dentry *Model1_simple_lookup(struct Model1_inode *, struct Model1_dentry *, unsigned int Model1_flags);
extern Model1_ssize_t Model1_generic_read_dir(struct Model1_file *, char *, Model1_size_t, Model1_loff_t *);
extern const struct Model1_file_operations Model1_simple_dir_operations;
extern const struct Model1_inode_operations Model1_simple_dir_inode_operations;
extern void Model1_make_empty_dir_inode(struct Model1_inode *Model1_inode);
extern bool Model1_is_empty_dir_inode(struct Model1_inode *Model1_inode);
struct Model1_tree_descr { char *Model1_name; const struct Model1_file_operations *Model1_ops; int Model1_mode; };
struct Model1_dentry *Model1_d_alloc_name(struct Model1_dentry *, const char *);
extern int Model1_simple_fill_super(struct Model1_super_block *, unsigned long, struct Model1_tree_descr *);
extern int Model1_simple_pin_fs(struct Model1_file_system_type *, struct Model1_vfsmount **Model1_mount, int *Model1_count);
extern void Model1_simple_release_fs(struct Model1_vfsmount **Model1_mount, int *Model1_count);

extern Model1_ssize_t Model1_simple_read_from_buffer(void *Model1_to, Model1_size_t Model1_count,
   Model1_loff_t *Model1_ppos, const void *Model1_from, Model1_size_t Model1_available);
extern Model1_ssize_t Model1_simple_write_to_buffer(void *Model1_to, Model1_size_t Model1_available, Model1_loff_t *Model1_ppos,
  const void *Model1_from, Model1_size_t Model1_count);

extern int Model1___generic_file_fsync(struct Model1_file *, Model1_loff_t, Model1_loff_t, int);
extern int Model1_generic_file_fsync(struct Model1_file *, Model1_loff_t, Model1_loff_t, int);

extern int Model1_generic_check_addressable(unsigned, Model1_u64);


extern int Model1_buffer_migrate_page(struct Model1_address_space *,
    struct Model1_page *, struct Model1_page *,
    enum Model1_migrate_mode);




extern int Model1_inode_change_ok(const struct Model1_inode *, struct Model1_iattr *);
extern int Model1_inode_newsize_ok(const struct Model1_inode *, Model1_loff_t Model1_offset);
extern void Model1_setattr_copy(struct Model1_inode *Model1_inode, const struct Model1_iattr *Model1_attr);

extern int Model1_file_update_time(struct Model1_file *Model1_file);

extern int Model1_generic_show_options(struct Model1_seq_file *Model1_m, struct Model1_dentry *Model1_root);
extern void Model1_save_mount_options(struct Model1_super_block *Model1_sb, char *Model1_options);
extern void Model1_replace_mount_options(struct Model1_super_block *Model1_sb, char *Model1_options);

static inline __attribute__((no_instrument_function)) bool Model1_io_is_direct(struct Model1_file *Model1_filp)
{
 return (Model1_filp->Model1_f_flags & 00040000) || ((Model1_filp->Model1_f_mapping->Model1_host)->Model1_i_flags & 0);
}

static inline __attribute__((no_instrument_function)) int Model1_iocb_flags(struct Model1_file *Model1_file)
{
 int Model1_res = 0;
 if (Model1_file->Model1_f_flags & 00002000)
  Model1_res |= (1 << 1);
 if (Model1_io_is_direct(Model1_file))
  Model1_res |= (1 << 2);
 if ((Model1_file->Model1_f_flags & 00010000) || (((Model1_file->Model1_f_mapping->Model1_host)->Model1_i_sb->Model1_s_flags & (16)) || ((Model1_file->Model1_f_mapping->Model1_host)->Model1_i_flags & 1)))
  Model1_res |= (1 << 4);
 if (Model1_file->Model1_f_flags & 04000000)
  Model1_res |= (1 << 5);
 return Model1_res;
}

static inline __attribute__((no_instrument_function)) Model1_ino_t Model1_parent_ino(struct Model1_dentry *Model1_dentry)
{
 Model1_ino_t Model1_res;

 /*
	 * Don't strictly need d_lock here? If the parent ino could change
	 * then surely we'd have a deeper race in the caller?
	 */
 Model1_spin_lock(&Model1_dentry->Model1_d_lockref.Model1_lock);
 Model1_res = Model1_dentry->Model1_d_parent->Model1_d_inode->Model1_i_ino;
 Model1_spin_unlock(&Model1_dentry->Model1_d_lockref.Model1_lock);
 return Model1_res;
}

/* Transaction based IO helpers */

/*
 * An argresp is stored in an allocated page and holds the
 * size of the argument or response, along with its content
 */
struct Model1_simple_transaction_argresp {
 Model1_ssize_t Model1_size;
 char Model1_data[0];
};



char *Model1_simple_transaction_get(struct Model1_file *Model1_file, const char *Model1_buf,
    Model1_size_t Model1_size);
Model1_ssize_t Model1_simple_transaction_read(struct Model1_file *Model1_file, char *Model1_buf,
    Model1_size_t Model1_size, Model1_loff_t *Model1_pos);
int Model1_simple_transaction_release(struct Model1_inode *Model1_inode, struct Model1_file *Model1_file);

void Model1_simple_transaction_set(struct Model1_file *Model1_file, Model1_size_t Model1_n);

/*
 * simple attribute files
 *
 * These attributes behave similar to those in sysfs:
 *
 * Writing to an attribute immediately sets a value, an open file can be
 * written to multiple times.
 *
 * Reading from an attribute creates a buffer from the value that might get
 * read with multiple read calls. When the attribute has been read
 * completely, no further read calls are possible until the file is opened
 * again.
 *
 * All attributes contain a text representation of a numeric value
 * that are accessed with the get() and set() functions.
 */
static inline __attribute__((no_instrument_function)) __attribute__((format(printf, 1, 2)))
void Model1___simple_attr_check_format(const char *Model1_fmt, ...)
{
 /* don't do anything, just let the compiler check the arguments; */
}

int Model1_simple_attr_open(struct Model1_inode *Model1_inode, struct Model1_file *Model1_file,
       int (*Model1_get)(void *, Model1_u64 *), int (*Model1_set)(void *, Model1_u64),
       const char *Model1_fmt);
int Model1_simple_attr_release(struct Model1_inode *Model1_inode, struct Model1_file *Model1_file);
Model1_ssize_t Model1_simple_attr_read(struct Model1_file *Model1_file, char *Model1_buf,
    Model1_size_t Model1_len, Model1_loff_t *Model1_ppos);
Model1_ssize_t Model1_simple_attr_write(struct Model1_file *Model1_file, const char *Model1_buf,
     Model1_size_t Model1_len, Model1_loff_t *Model1_ppos);

struct Model1_ctl_table;
int Model1_proc_nr_files(struct Model1_ctl_table *Model1_table, int Model1_write,
    void *Model1_buffer, Model1_size_t *Model1_lenp, Model1_loff_t *Model1_ppos);
int Model1_proc_nr_dentry(struct Model1_ctl_table *Model1_table, int Model1_write,
    void *Model1_buffer, Model1_size_t *Model1_lenp, Model1_loff_t *Model1_ppos);
int Model1_proc_nr_inodes(struct Model1_ctl_table *Model1_table, int Model1_write,
     void *Model1_buffer, Model1_size_t *Model1_lenp, Model1_loff_t *Model1_ppos);
int __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_get_filesystem_list(char *Model1_buf);
static inline __attribute__((no_instrument_function)) bool Model1_is_sxid(Model1_umode_t Model1_mode)
{
 return (Model1_mode & 0004000) || ((Model1_mode & 0002000) && (Model1_mode & 00010));
}

static inline __attribute__((no_instrument_function)) int Model1_check_sticky(struct Model1_inode *Model1_dir, struct Model1_inode *Model1_inode)
{
 if (!(Model1_dir->Model1_i_mode & 0001000))
  return 0;

 return Model1___check_sticky(Model1_dir, Model1_inode);
}

static inline __attribute__((no_instrument_function)) void Model1_inode_has_no_xattr(struct Model1_inode *Model1_inode)
{
 if (!Model1_is_sxid(Model1_inode->Model1_i_mode) && (Model1_inode->Model1_i_sb->Model1_s_flags & (1<<28)))
  Model1_inode->Model1_i_flags |= 4096;
}

static inline __attribute__((no_instrument_function)) bool Model1_is_root_inode(struct Model1_inode *Model1_inode)
{
 return Model1_inode == Model1_inode->Model1_i_sb->Model1_s_root->Model1_d_inode;
}

static inline __attribute__((no_instrument_function)) bool Model1_dir_emit(struct Model1_dir_context *Model1_ctx,
       const char *Model1_name, int Model1_namelen,
       Model1_u64 Model1_ino, unsigned Model1_type)
{
 return Model1_ctx->Model1_actor(Model1_ctx, Model1_name, Model1_namelen, Model1_ctx->Model1_pos, Model1_ino, Model1_type) == 0;
}
static inline __attribute__((no_instrument_function)) bool Model1_dir_emit_dot(struct Model1_file *Model1_file, struct Model1_dir_context *Model1_ctx)
{
 return Model1_ctx->Model1_actor(Model1_ctx, ".", 1, Model1_ctx->Model1_pos,
     Model1_file->Model1_f_path.Model1_dentry->Model1_d_inode->Model1_i_ino, 4) == 0;
}
static inline __attribute__((no_instrument_function)) bool Model1_dir_emit_dotdot(struct Model1_file *Model1_file, struct Model1_dir_context *Model1_ctx)
{
 return Model1_ctx->Model1_actor(Model1_ctx, "..", 2, Model1_ctx->Model1_pos,
     Model1_parent_ino(Model1_file->Model1_f_path.Model1_dentry), 4) == 0;
}
static inline __attribute__((no_instrument_function)) bool Model1_dir_emit_dots(struct Model1_file *Model1_file, struct Model1_dir_context *Model1_ctx)
{
 if (Model1_ctx->Model1_pos == 0) {
  if (!Model1_dir_emit_dot(Model1_file, Model1_ctx))
   return false;
  Model1_ctx->Model1_pos = 1;
 }
 if (Model1_ctx->Model1_pos == 1) {
  if (!Model1_dir_emit_dotdot(Model1_file, Model1_ctx))
   return false;
  Model1_ctx->Model1_pos = 2;
 }
 return true;
}
static inline __attribute__((no_instrument_function)) bool Model1_dir_relax(struct Model1_inode *Model1_inode)
{
 Model1_inode_unlock(Model1_inode);
 Model1_inode_lock(Model1_inode);
 return !((Model1_inode)->Model1_i_flags & 16);
}

static inline __attribute__((no_instrument_function)) bool Model1_dir_relax_shared(struct Model1_inode *Model1_inode)
{
 Model1_inode_unlock_shared(Model1_inode);
 Model1_inode_lock_shared(Model1_inode);
 return !((Model1_inode)->Model1_i_flags & 16);
}

extern bool Model1_path_noexec(const struct Model1_path *Model1_path);
extern void Model1_inode_nohighmem(struct Model1_inode *Model1_inode);
/* include/linux/aio_abi.h
 *
 * Copyright 2000,2001,2002 Red Hat.
 *
 * Written by Benjamin LaHaise <bcrl@kvack.org>
 *
 * Distribute under the terms of the GPLv2 (see ../../COPYING) or under 
 * the following terms.
 *
 * Permission to use, copy, modify, and distribute this software and its
 * documentation is hereby granted, provided that the above copyright
 * notice appears in all copies.  This software is provided without any
 * warranty, express or implied.  Red Hat makes no representations about
 * the suitability of this software for any purpose.
 *
 * IN NO EVENT SHALL RED HAT BE LIABLE TO ANY PARTY FOR DIRECT, INDIRECT,
 * SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OF
 * THIS SOFTWARE AND ITS DOCUMENTATION, EVEN IF RED HAT HAS BEEN ADVISED
 * OF THE POSSIBILITY OF SUCH DAMAGE.
 *
 * RED HAT DISCLAIMS ANY WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE.  THE SOFTWARE PROVIDED HEREUNDER IS ON AN "AS IS" BASIS, AND
 * RED HAT HAS NO OBLIGATION TO PROVIDE MAINTENANCE, SUPPORT, UPDATES,
 * ENHANCEMENTS, OR MODIFICATIONS.
 */






typedef Model1___kernel_ulong_t Model1_aio_context_t;

enum {
 Model1_IOCB_CMD_PREAD = 0,
 Model1_IOCB_CMD_PWRITE = 1,
 Model1_IOCB_CMD_FSYNC = 2,
 Model1_IOCB_CMD_FDSYNC = 3,
 /* These two are experimental.
	 * IOCB_CMD_PREADX = 4,
	 * IOCB_CMD_POLL = 5,
	 */
 Model1_IOCB_CMD_NOOP = 6,
 Model1_IOCB_CMD_PREADV = 7,
 Model1_IOCB_CMD_PWRITEV = 8,
};

/*
 * Valid flags for the "aio_flags" member of the "struct iocb".
 *
 * IOCB_FLAG_RESFD - Set if the "aio_resfd" member of the "struct iocb"
 *                   is valid.
 */


/* read() from /dev/aio returns these structures. */
struct Model1_io_event {
 __u64 Model1_data; /* the data field from the iocb */
 __u64 Model1_obj; /* what iocb this event came from */
 Model1___s64 Model1_res; /* result code for this event */
 Model1___s64 Model1_res2; /* secondary result */
};
/*
 * we always use a 64bit off_t when communicating
 * with userland.  its up to libraries to do the
 * proper padding and aio_error abstraction
 */

struct Model1_iocb {
 /* these are internal to the kernel/libc. */
 __u64 Model1_aio_data; /* data to be returned in event's data */
 __u32 Model1_aio_key, Model1_aio_reserved1;
    /* the kernel sets aio_key to the req # */

 /* common fields */
 Model1___u16 Model1_aio_lio_opcode; /* see IOCB_CMD_ above */
 Model1___s16 Model1_aio_reqprio;
 __u32 Model1_aio_fildes;

 __u64 Model1_aio_buf;
 __u64 Model1_aio_nbytes;
 Model1___s64 Model1_aio_offset;

 /* extra parameters */
 __u64 Model1_aio_reserved2; /* TODO: use this for a (struct sigevent *) */

 /* flags for the "struct iocb" */
 __u32 Model1_aio_flags;

 /*
	 * if the IOCB_FLAG_RESFD flag of "aio_flags" is set, this is an
	 * eventfd to signal AIO readiness to
	 */
 __u32 Model1_aio_resfd;
}; /* 64 bytes */





/*
 * Architecture specific compatibility types
 */







/* IA32 compatible user structures for ptrace.
 * These should be used for 32bit coredumps too. */

struct Model1_user_i387_ia32_struct {
 Model1_u32 Model1_cwd;
 Model1_u32 Model1_swd;
 Model1_u32 Model1_twd;
 Model1_u32 Model1_fip;
 Model1_u32 Model1_fcs;
 Model1_u32 Model1_foo;
 Model1_u32 Model1_fos;
 Model1_u32 Model1_st_space[20]; /* 8*10 bytes for each FP-reg = 80 bytes */
};

/* FSAVE frame with extensions */
struct Model1_user32_fxsr_struct {
 unsigned short Model1_cwd;
 unsigned short Model1_swd;
 unsigned short Model1_twd; /* not compatible to 64bit twd */
 unsigned short Model1_fop;
 int Model1_fip;
 int Model1_fcs;
 int Model1_foo;
 int Model1_fos;
 int Model1_mxcsr;
 int Model1_reserved;
 int Model1_st_space[32]; /* 8*16 bytes for each FP-reg = 128 bytes */
 int Model1_xmm_space[32]; /* 8*16 bytes for each XMM-reg = 128 bytes */
 int Model1_padding[56];
};

struct Model1_user_regs_struct32 {
 __u32 Model1_ebx, Model1_ecx, Model1_edx, Model1_esi, Model1_edi, Model1_ebp, Model1_eax;
 unsigned short Model1_ds, Model1___ds, Model1_es, Model1___es;
 unsigned short Model1_fs, Model1___fs, Model1_gs, Model1___gs;
 __u32 Model1_orig_eax, Model1_eip;
 unsigned short Model1_cs, Model1___cs;
 __u32 Model1_eflags, Model1_esp;
 unsigned short Model1_ss, Model1___ss;
};

struct Model1_user32 {
  struct Model1_user_regs_struct32 Model1_regs; /* Where the registers are actually stored */
  int Model1_u_fpvalid; /* True if math co-processor being used. */
    /* for this mess. Not yet used. */
  struct Model1_user_i387_ia32_struct Model1_i387; /* Math Co-processor registers. */
/* The rest of this junk is to help gdb figure out what goes where */
  __u32 Model1_u_tsize; /* Text segment size (pages). */
  __u32 Model1_u_dsize; /* Data segment size (pages). */
  __u32 Model1_u_ssize; /* Stack segment size (pages). */
  __u32 Model1_start_code; /* Starting virtual address of text. */
  __u32 Model1_start_stack; /* Starting virtual address of stack area.
				   This is actually the bottom of the stack,
				   the top of the stack is always found in the
				   esp register.  */
  __u32 Model1_signal; /* Signal that caused the core dump. */
  int Model1_reserved; /* No __u32er used */
  __u32 Model1_u_ar0; /* Used by gdb to help find the values for */
    /* the registers. */
  __u32 Model1_u_fpstate; /* Math Co-processor pointer. */
  __u32 Model1_magic; /* To uniquely identify a core file */
  char Model1_u_comm[32]; /* User command that was responsible */
  int Model1_u_debugreg[8];
};





typedef Model1_u32 Model1_compat_size_t;
typedef Model1_s32 Model1_compat_ssize_t;
typedef Model1_s32 Model1_compat_time_t;
typedef Model1_s32 Model1_compat_clock_t;
typedef Model1_s32 Model1_compat_pid_t;
typedef Model1_u16 Model1___compat_uid_t;
typedef Model1_u16 Model1___compat_gid_t;
typedef Model1_u32 Model1___compat_uid32_t;
typedef Model1_u32 Model1___compat_gid32_t;
typedef Model1_u16 Model1_compat_mode_t;
typedef Model1_u32 Model1_compat_ino_t;
typedef Model1_u16 Model1_compat_dev_t;
typedef Model1_s32 Model1_compat_off_t;
typedef Model1_s64 Model1_compat_loff_t;
typedef Model1_u16 Model1_compat_nlink_t;
typedef Model1_u16 Model1_compat_ipc_pid_t;
typedef Model1_s32 Model1_compat_daddr_t;
typedef Model1_u32 Model1_compat_caddr_t;
typedef Model1___kernel_fsid_t Model1_compat_fsid_t;
typedef Model1_s32 Model1_compat_timer_t;
typedef Model1_s32 Model1_compat_key_t;

typedef Model1_s32 Model1_compat_int_t;
typedef Model1_s32 Model1_compat_long_t;
typedef Model1_s64 __attribute__((aligned(4))) Model1_compat_s64;
typedef Model1_u32 Model1_compat_uint_t;
typedef Model1_u32 Model1_compat_ulong_t;
typedef Model1_u32 Model1_compat_u32;
typedef Model1_u64 __attribute__((aligned(4))) Model1_compat_u64;
typedef Model1_u32 Model1_compat_uptr_t;

struct Model1_compat_timespec {
 Model1_compat_time_t Model1_tv_sec;
 Model1_s32 Model1_tv_nsec;
};

struct Model1_compat_timeval {
 Model1_compat_time_t Model1_tv_sec;
 Model1_s32 Model1_tv_usec;
};

struct Model1_compat_stat {
 Model1_compat_dev_t Model1_st_dev;
 Model1_u16 Model1___pad1;
 Model1_compat_ino_t Model1_st_ino;
 Model1_compat_mode_t Model1_st_mode;
 Model1_compat_nlink_t Model1_st_nlink;
 Model1___compat_uid_t Model1_st_uid;
 Model1___compat_gid_t Model1_st_gid;
 Model1_compat_dev_t Model1_st_rdev;
 Model1_u16 Model1___pad2;
 Model1_u32 Model1_st_size;
 Model1_u32 Model1_st_blksize;
 Model1_u32 Model1_st_blocks;
 Model1_u32 Model1_st_atime;
 Model1_u32 Model1_st_atime_nsec;
 Model1_u32 Model1_st_mtime;
 Model1_u32 Model1_st_mtime_nsec;
 Model1_u32 Model1_st_ctime;
 Model1_u32 Model1_st_ctime_nsec;
 Model1_u32 Model1___unused4;
 Model1_u32 Model1___unused5;
};

struct Model1_compat_flock {
 short Model1_l_type;
 short Model1_l_whence;
 Model1_compat_off_t Model1_l_start;
 Model1_compat_off_t Model1_l_len;
 Model1_compat_pid_t Model1_l_pid;
};





/*
 * IA32 uses 4 byte alignment for 64 bit quantities,
 * so we need to pack this structure.
 */
struct Model1_compat_flock64 {
 short Model1_l_type;
 short Model1_l_whence;
 Model1_compat_loff_t Model1_l_start;
 Model1_compat_loff_t Model1_l_len;
 Model1_compat_pid_t Model1_l_pid;
} __attribute__((packed));

struct Model1_compat_statfs {
 int Model1_f_type;
 int Model1_f_bsize;
 int Model1_f_blocks;
 int Model1_f_bfree;
 int Model1_f_bavail;
 int Model1_f_files;
 int Model1_f_ffree;
 Model1_compat_fsid_t Model1_f_fsid;
 int Model1_f_namelen; /* SunOS ignores this field. */
 int Model1_f_frsize;
 int Model1_f_flags;
 int Model1_f_spare[4];
};




typedef Model1_u32 Model1_compat_old_sigset_t; /* at least 32 bits */




typedef Model1_u32 Model1_compat_sigset_word;

typedef union Model1_compat_sigval {
 Model1_compat_int_t Model1_sival_int;
 Model1_compat_uptr_t Model1_sival_ptr;
} Model1_compat_sigval_t;

typedef struct Model1_compat_siginfo {
 int Model1_si_signo;
 int Model1_si_errno;
 int Model1_si_code;

 union {
  int Model1__pad[128/sizeof(int) - 3];

  /* kill() */
  struct {
   unsigned int Model1__pid; /* sender's pid */
   unsigned int Model1__uid; /* sender's uid */
  } Model1__kill;

  /* POSIX.1b timers */
  struct {
   Model1_compat_timer_t Model1__tid; /* timer id */
   int Model1__overrun; /* overrun count */
   Model1_compat_sigval_t Model1__sigval; /* same as below */
   int Model1__sys_private; /* not to be passed to user */
   int Model1__overrun_incr; /* amount to add to overrun */
  } Model1__timer;

  /* POSIX.1b signals */
  struct {
   unsigned int Model1__pid; /* sender's pid */
   unsigned int Model1__uid; /* sender's uid */
   Model1_compat_sigval_t Model1__sigval;
  } Model1__rt;

  /* SIGCHLD */
  struct {
   unsigned int Model1__pid; /* which child */
   unsigned int Model1__uid; /* sender's uid */
   int Model1__status; /* exit code */
   Model1_compat_clock_t Model1__utime;
   Model1_compat_clock_t Model1__stime;
  } Model1__sigchld;

  /* SIGCHLD (x32 version) */
  struct {
   unsigned int Model1__pid; /* which child */
   unsigned int Model1__uid; /* sender's uid */
   int Model1__status; /* exit code */
   Model1_compat_s64 Model1__utime;
   Model1_compat_s64 Model1__stime;
  } Model1__sigchld_x32;

  /* SIGILL, SIGFPE, SIGSEGV, SIGBUS */
  struct {
   unsigned int Model1__addr; /* faulting insn/memory ref. */
   short int Model1__addr_lsb; /* Valid LSB of the reported address. */
   union {
    /* used when si_code=SEGV_BNDERR */
    struct {
     Model1_compat_uptr_t Model1__lower;
     Model1_compat_uptr_t Model1__upper;
    } Model1__addr_bnd;
    /* used when si_code=SEGV_PKUERR */
    Model1_compat_u32 Model1__pkey;
   };
  } Model1__sigfault;

  /* SIGPOLL */
  struct {
   int Model1__band; /* POLL_IN, POLL_OUT, POLL_MSG */
   int Model1__fd;
  } Model1__sigpoll;

  struct {
   unsigned int Model1__call_addr; /* calling insn */
   int Model1__syscall; /* triggering system call number */
   unsigned int Model1__arch; /* AUDIT_ARCH_* of syscall */
  } Model1__sigsys;
 } Model1__sifields;
} Model1_compat_siginfo_t;




struct Model1_compat_ipc64_perm {
 Model1_compat_key_t Model1_key;
 Model1___compat_uid32_t Model1_uid;
 Model1___compat_gid32_t Model1_gid;
 Model1___compat_uid32_t Model1_cuid;
 Model1___compat_gid32_t Model1_cgid;
 unsigned short Model1_mode;
 unsigned short Model1___pad1;
 unsigned short Model1_seq;
 unsigned short Model1___pad2;
 Model1_compat_ulong_t Model1_unused1;
 Model1_compat_ulong_t Model1_unused2;
};

struct Model1_compat_semid64_ds {
 struct Model1_compat_ipc64_perm Model1_sem_perm;
 Model1_compat_time_t Model1_sem_otime;
 Model1_compat_ulong_t Model1___unused1;
 Model1_compat_time_t Model1_sem_ctime;
 Model1_compat_ulong_t Model1___unused2;
 Model1_compat_ulong_t Model1_sem_nsems;
 Model1_compat_ulong_t Model1___unused3;
 Model1_compat_ulong_t Model1___unused4;
};

struct Model1_compat_msqid64_ds {
 struct Model1_compat_ipc64_perm Model1_msg_perm;
 Model1_compat_time_t Model1_msg_stime;
 Model1_compat_ulong_t Model1___unused1;
 Model1_compat_time_t Model1_msg_rtime;
 Model1_compat_ulong_t Model1___unused2;
 Model1_compat_time_t Model1_msg_ctime;
 Model1_compat_ulong_t Model1___unused3;
 Model1_compat_ulong_t Model1_msg_cbytes;
 Model1_compat_ulong_t Model1_msg_qnum;
 Model1_compat_ulong_t Model1_msg_qbytes;
 Model1_compat_pid_t Model1_msg_lspid;
 Model1_compat_pid_t Model1_msg_lrpid;
 Model1_compat_ulong_t Model1___unused4;
 Model1_compat_ulong_t Model1___unused5;
};

struct Model1_compat_shmid64_ds {
 struct Model1_compat_ipc64_perm Model1_shm_perm;
 Model1_compat_size_t Model1_shm_segsz;
 Model1_compat_time_t Model1_shm_atime;
 Model1_compat_ulong_t Model1___unused1;
 Model1_compat_time_t Model1_shm_dtime;
 Model1_compat_ulong_t Model1___unused2;
 Model1_compat_time_t Model1_shm_ctime;
 Model1_compat_ulong_t Model1___unused3;
 Model1_compat_pid_t Model1_shm_cpid;
 Model1_compat_pid_t Model1_shm_lpid;
 Model1_compat_ulong_t Model1_shm_nattch;
 Model1_compat_ulong_t Model1___unused4;
 Model1_compat_ulong_t Model1___unused5;
};

/*
 * The type of struct elf_prstatus.pr_reg in compatible core dumps.
 */
typedef struct Model1_user_regs_struct32 Model1_compat_elf_gregset_t;


/*
 * A pointer passed in from user mode. This should not
 * be used for syscall parameters, just declare them
 * as pointers because the syscall entry code will have
 * appropriately converted them already.
 */

static inline __attribute__((no_instrument_function)) void *Model1_compat_ptr(Model1_compat_uptr_t Model1_uptr)
{
 return (void *)(unsigned long)Model1_uptr;
}

static inline __attribute__((no_instrument_function)) Model1_compat_uptr_t Model1_ptr_to_compat(void *Model1_uptr)
{
 return (Model1_u32)(unsigned long)Model1_uptr;
}

static inline __attribute__((no_instrument_function)) void *Model1_arch_compat_alloc_user_space(long Model1_len)
{
 Model1_compat_uptr_t Model1_sp;

 if (Model1_test_ti_thread_flag(Model1_current_thread_info(), 17)) {
  Model1_sp = ((struct Model1_pt_regs *)(Model1_get_current())->thread.Model1_sp0 - 1)->Model1_sp;
 } else {
  /* -128 for the x32 ABI redzone */
  Model1_sp = ((struct Model1_pt_regs *)(Model1_get_current())->thread.Model1_sp0 - 1)->Model1_sp - 128;
 }

 return (void *)((Model1_sp - Model1_len) & ~((__typeof__(Model1_sp - Model1_len))((16)-1)));
}

static inline __attribute__((no_instrument_function)) bool Model1_in_x32_syscall(void)
{




 return false;
}

static inline __attribute__((no_instrument_function)) bool Model1_in_compat_syscall(void)
{
 return Model1_in_ia32_syscall() || Model1_in_x32_syscall();
}
typedef struct Model1_compat_sigaltstack {
 Model1_compat_uptr_t Model1_ss_sp;
 int Model1_ss_flags;
 Model1_compat_size_t Model1_ss_size;
} Model1_compat_stack_t;





typedef Model1___compat_uid32_t Model1_compat_uid_t;
typedef Model1___compat_gid32_t Model1_compat_gid_t;

typedef Model1_compat_ulong_t Model1_compat_aio_context_t;

struct Model1_compat_sel_arg_struct;
struct Model1_rusage;

struct Model1_compat_itimerspec {
 struct Model1_compat_timespec Model1_it_interval;
 struct Model1_compat_timespec Model1_it_value;
};

struct Model1_compat_utimbuf {
 Model1_compat_time_t Model1_actime;
 Model1_compat_time_t Model1_modtime;
};

struct Model1_compat_itimerval {
 struct Model1_compat_timeval Model1_it_interval;
 struct Model1_compat_timeval Model1_it_value;
};

struct Model1_compat_tms {
 Model1_compat_clock_t Model1_tms_utime;
 Model1_compat_clock_t Model1_tms_stime;
 Model1_compat_clock_t Model1_tms_cutime;
 Model1_compat_clock_t Model1_tms_cstime;
};

struct Model1_compat_timex {
 Model1_compat_uint_t Model1_modes;
 Model1_compat_long_t Model1_offset;
 Model1_compat_long_t Model1_freq;
 Model1_compat_long_t Model1_maxerror;
 Model1_compat_long_t Model1_esterror;
 Model1_compat_int_t Model1_status;
 Model1_compat_long_t Model1_constant;
 Model1_compat_long_t Model1_precision;
 Model1_compat_long_t Model1_tolerance;
 struct Model1_compat_timeval Model1_time;
 Model1_compat_long_t Model1_tick;
 Model1_compat_long_t Model1_ppsfreq;
 Model1_compat_long_t Model1_jitter;
 Model1_compat_int_t Model1_shift;
 Model1_compat_long_t Model1_stabil;
 Model1_compat_long_t Model1_jitcnt;
 Model1_compat_long_t Model1_calcnt;
 Model1_compat_long_t Model1_errcnt;
 Model1_compat_long_t Model1_stbcnt;
 Model1_compat_int_t Model1_tai;

 Model1_compat_int_t:32; Model1_compat_int_t:32; Model1_compat_int_t:32; Model1_compat_int_t:32;
 Model1_compat_int_t:32; Model1_compat_int_t:32; Model1_compat_int_t:32; Model1_compat_int_t:32;
 Model1_compat_int_t:32; Model1_compat_int_t:32; Model1_compat_int_t:32;
};



typedef struct {
 Model1_compat_sigset_word Model1_sig[(64 / 32)];
} Model1_compat_sigset_t;

struct Model1_compat_sigaction {

 Model1_compat_uptr_t Model1_sa_handler;
 Model1_compat_ulong_t Model1_sa_flags;





 Model1_compat_uptr_t Model1_sa_restorer;

 Model1_compat_sigset_t Model1_sa_mask __attribute__((packed));
};

/*
 * These functions operate on 32- or 64-bit specs depending on
 * COMPAT_USE_64BIT_TIME, hence the void user pointer arguments.
 */
extern int Model1_compat_get_timespec(struct Model1_timespec *, const void *);
extern int Model1_compat_put_timespec(const struct Model1_timespec *, void *);
extern int Model1_compat_get_timeval(struct Model1_timeval *, const void *);
extern int Model1_compat_put_timeval(const struct Model1_timeval *, void *);

/*
 * This function convert a timespec if necessary and returns a *user
 * space* pointer.  If no conversion is necessary, it returns the
 * initial pointer.  NULL is a legitimate argument and will always
 * output NULL.
 */
extern int Model1_compat_convert_timespec(struct Model1_timespec **,
       const void *);

struct Model1_compat_iovec {
 Model1_compat_uptr_t Model1_iov_base;
 Model1_compat_size_t Model1_iov_len;
};

struct Model1_compat_rlimit {
 Model1_compat_ulong_t Model1_rlim_cur;
 Model1_compat_ulong_t Model1_rlim_max;
};

struct Model1_compat_rusage {
 struct Model1_compat_timeval Model1_ru_utime;
 struct Model1_compat_timeval Model1_ru_stime;
 Model1_compat_long_t Model1_ru_maxrss;
 Model1_compat_long_t Model1_ru_ixrss;
 Model1_compat_long_t Model1_ru_idrss;
 Model1_compat_long_t Model1_ru_isrss;
 Model1_compat_long_t Model1_ru_minflt;
 Model1_compat_long_t Model1_ru_majflt;
 Model1_compat_long_t Model1_ru_nswap;
 Model1_compat_long_t Model1_ru_inblock;
 Model1_compat_long_t Model1_ru_oublock;
 Model1_compat_long_t Model1_ru_msgsnd;
 Model1_compat_long_t Model1_ru_msgrcv;
 Model1_compat_long_t Model1_ru_nsignals;
 Model1_compat_long_t Model1_ru_nvcsw;
 Model1_compat_long_t Model1_ru_nivcsw;
};

extern int Model1_put_compat_rusage(const struct Model1_rusage *,
        struct Model1_compat_rusage *);

struct Model1_compat_siginfo;

extern long Model1_compat_sys_waitid(int, Model1_compat_pid_t,
  struct Model1_compat_siginfo *, int,
  struct Model1_compat_rusage *);

struct Model1_compat_dirent {
 Model1_u32 Model1_d_ino;
 Model1_compat_off_t Model1_d_off;
 Model1_u16 Model1_d_reclen;
 char Model1_d_name[256];
};

struct Model1_compat_ustat {
 Model1_compat_daddr_t Model1_f_tfree;
 Model1_compat_ino_t Model1_f_tinode;
 char Model1_f_fname[6];
 char Model1_f_fpack[6];
};



typedef struct Model1_compat_sigevent {
 Model1_compat_sigval_t Model1_sigev_value;
 Model1_compat_int_t Model1_sigev_signo;
 Model1_compat_int_t Model1_sigev_notify;
 union {
  Model1_compat_int_t Model1__pad[((64/sizeof(int)) - 3)];
  Model1_compat_int_t Model1__tid;

  struct {
   Model1_compat_uptr_t Model1__function;
   Model1_compat_uptr_t Model1__attribute;
  } Model1__sigev_thread;
 } Model1__sigev_un;
} Model1_compat_sigevent_t;

struct Model1_compat_ifmap {
 Model1_compat_ulong_t Model1_mem_start;
 Model1_compat_ulong_t Model1_mem_end;
 unsigned short Model1_base_addr;
 unsigned char Model1_irq;
 unsigned char Model1_dma;
 unsigned char Model1_port;
};

struct Model1_compat_if_settings {
 unsigned int Model1_type; /* Type of physical device or protocol */
 unsigned int Model1_size; /* Size of the data allocated by the caller */
 Model1_compat_uptr_t Model1_ifs_ifsu; /* union of pointers */
};

struct Model1_compat_ifreq {
 union {
  char Model1_ifrn_name[16]; /* if name, e.g. "en0" */
 } Model1_ifr_ifrn;
 union {
  struct Model1_sockaddr Model1_ifru_addr;
  struct Model1_sockaddr Model1_ifru_dstaddr;
  struct Model1_sockaddr Model1_ifru_broadaddr;
  struct Model1_sockaddr Model1_ifru_netmask;
  struct Model1_sockaddr Model1_ifru_hwaddr;
  short Model1_ifru_flags;
  Model1_compat_int_t Model1_ifru_ivalue;
  Model1_compat_int_t Model1_ifru_mtu;
  struct Model1_compat_ifmap Model1_ifru_map;
  char Model1_ifru_slave[16]; /* Just fits the size */
  char Model1_ifru_newname[16];
  Model1_compat_caddr_t Model1_ifru_data;
  struct Model1_compat_if_settings Model1_ifru_settings;
 } Model1_ifr_ifru;
};

struct Model1_compat_ifconf {
 Model1_compat_int_t Model1_ifc_len; /* size of buffer */
 Model1_compat_caddr_t Model1_ifcbuf;
};

struct Model1_compat_robust_list {
 Model1_compat_uptr_t Model1_next;
};

struct Model1_compat_robust_list_head {
 struct Model1_compat_robust_list Model1_list;
 Model1_compat_long_t Model1_futex_offset;
 Model1_compat_uptr_t Model1_list_op_pending;
};


struct Model1_compat_old_sigaction {
 Model1_compat_uptr_t Model1_sa_handler;
 Model1_compat_old_sigset_t Model1_sa_mask;
 Model1_compat_ulong_t Model1_sa_flags;
 Model1_compat_uptr_t Model1_sa_restorer;
};


struct Model1_compat_statfs;
struct Model1_compat_statfs64;
struct Model1_compat_old_linux_dirent;
struct Model1_compat_linux_dirent;
struct Model1_linux_dirent64;
struct Model1_compat_msghdr;
struct Model1_compat_mmsghdr;
struct Model1_compat_sysinfo;
struct Model1_compat_sysctl_args;
struct Model1_compat_kexec_segment;
struct Model1_compat_mq_attr;
struct Model1_compat_msgbuf;

extern void Model1_compat_exit_robust_list(struct Model1_task_struct *Model1_curr);

           long
Model1_compat_sys_set_robust_list(struct Model1_compat_robust_list_head *Model1_head,
      Model1_compat_size_t Model1_len);
           long
Model1_compat_sys_get_robust_list(int Model1_pid, Model1_compat_uptr_t *Model1_head_ptr,
      Model1_compat_size_t *Model1_len_ptr);

           long Model1_compat_sys_ipc(Model1_u32, int, int, Model1_u32, Model1_compat_uptr_t, Model1_u32);
           long Model1_compat_sys_shmat(int Model1_shmid, Model1_compat_uptr_t Model1_shmaddr, int Model1_shmflg);
           long Model1_compat_sys_semctl(int Model1_semid, int Model1_semnum, int Model1_cmd, int Model1_arg);
           long Model1_compat_sys_msgsnd(int Model1_msqid, Model1_compat_uptr_t Model1_msgp,
  Model1_compat_ssize_t Model1_msgsz, int Model1_msgflg);
           long Model1_compat_sys_msgrcv(int Model1_msqid, Model1_compat_uptr_t Model1_msgp,
  Model1_compat_ssize_t Model1_msgsz, Model1_compat_long_t Model1_msgtyp, int Model1_msgflg);
long Model1_compat_sys_msgctl(int Model1_first, int Model1_second, void *Model1_uptr);
long Model1_compat_sys_shmctl(int Model1_first, int Model1_second, void *Model1_uptr);
long Model1_compat_sys_semtimedop(int Model1_semid, struct Model1_sembuf *Model1_tsems,
  unsigned Model1_nsems, const struct Model1_compat_timespec *Model1_timeout);
           long Model1_compat_sys_keyctl(Model1_u32 Model1_option,
         Model1_u32 Model1_arg2, Model1_u32 Model1_arg3, Model1_u32 Model1_arg4, Model1_u32 Model1_arg5);
           long Model1_compat_sys_ustat(unsigned Model1_dev, struct Model1_compat_ustat *Model1_u32);

           Model1_ssize_t Model1_compat_sys_readv(Model1_compat_ulong_t Model1_fd,
  const struct Model1_compat_iovec *Model1_vec, Model1_compat_ulong_t Model1_vlen);
           Model1_ssize_t Model1_compat_sys_writev(Model1_compat_ulong_t Model1_fd,
  const struct Model1_compat_iovec *Model1_vec, Model1_compat_ulong_t Model1_vlen);
           Model1_ssize_t Model1_compat_sys_preadv(Model1_compat_ulong_t Model1_fd,
  const struct Model1_compat_iovec *Model1_vec,
  Model1_compat_ulong_t Model1_vlen, Model1_u32 Model1_pos_low, Model1_u32 Model1_pos_high);
           Model1_ssize_t Model1_compat_sys_pwritev(Model1_compat_ulong_t Model1_fd,
  const struct Model1_compat_iovec *Model1_vec,
  Model1_compat_ulong_t Model1_vlen, Model1_u32 Model1_pos_low, Model1_u32 Model1_pos_high);
           Model1_ssize_t Model1_compat_sys_preadv2(Model1_compat_ulong_t Model1_fd,
  const struct Model1_compat_iovec *Model1_vec,
  Model1_compat_ulong_t Model1_vlen, Model1_u32 Model1_pos_low, Model1_u32 Model1_pos_high, int Model1_flags);
           Model1_ssize_t Model1_compat_sys_pwritev2(Model1_compat_ulong_t Model1_fd,
  const struct Model1_compat_iovec *Model1_vec,
  Model1_compat_ulong_t Model1_vlen, Model1_u32 Model1_pos_low, Model1_u32 Model1_pos_high, int Model1_flags);


           long Model1_compat_sys_preadv64(unsigned long Model1_fd,
  const struct Model1_compat_iovec *Model1_vec,
  unsigned long Model1_vlen, Model1_loff_t Model1_pos);



           long Model1_compat_sys_pwritev64(unsigned long Model1_fd,
  const struct Model1_compat_iovec *Model1_vec,
  unsigned long Model1_vlen, Model1_loff_t Model1_pos);


           long Model1_compat_sys_lseek(unsigned int, Model1_compat_off_t, unsigned int);

           long Model1_compat_sys_execve(const char *Model1_filename, const Model1_compat_uptr_t *Model1_argv,
       const Model1_compat_uptr_t *Model1_envp);
           long Model1_compat_sys_execveat(int Model1_dfd, const char *Model1_filename,
       const Model1_compat_uptr_t *Model1_argv,
       const Model1_compat_uptr_t *Model1_envp, int Model1_flags);

           long Model1_compat_sys_select(int Model1_n, Model1_compat_ulong_t *Model1_inp,
  Model1_compat_ulong_t *Model1_outp, Model1_compat_ulong_t *Model1_exp,
  struct Model1_compat_timeval *Model1_tvp);

           long Model1_compat_sys_old_select(struct Model1_compat_sel_arg_struct *Model1_arg);

           long Model1_compat_sys_wait4(Model1_compat_pid_t Model1_pid,
     Model1_compat_uint_t *Model1_stat_addr, int Model1_options,
     struct Model1_compat_rusage *Model1_ru);






long Model1_compat_get_bitmap(unsigned long *Model1_mask, const Model1_compat_ulong_t *Model1_umask,
         unsigned long Model1_bitmap_size);
long Model1_compat_put_bitmap(Model1_compat_ulong_t *Model1_umask, unsigned long *Model1_mask,
         unsigned long Model1_bitmap_size);
int Model1_copy_siginfo_from_user32(Model1_siginfo_t *Model1_to, struct Model1_compat_siginfo *Model1_from);
int Model1_copy_siginfo_to_user32(struct Model1_compat_siginfo *Model1_to, const Model1_siginfo_t *Model1_from);
int Model1_get_compat_sigevent(struct Model1_sigevent *Model1_event,
  const struct Model1_compat_sigevent *Model1_u_event);
long Model1_compat_sys_rt_tgsigqueueinfo(Model1_compat_pid_t Model1_tgid, Model1_compat_pid_t Model1_pid, int Model1_sig,
      struct Model1_compat_siginfo *Model1_uinfo);

           long Model1_compat_sys_sigaction(int Model1_sig,
                                   const struct Model1_compat_old_sigaction *Model1_act,
                                   struct Model1_compat_old_sigaction *Model1_oact);


static inline __attribute__((no_instrument_function)) int Model1_compat_timeval_compare(struct Model1_compat_timeval *Model1_lhs,
     struct Model1_compat_timeval *Model1_rhs)
{
 if (Model1_lhs->Model1_tv_sec < Model1_rhs->Model1_tv_sec)
  return -1;
 if (Model1_lhs->Model1_tv_sec > Model1_rhs->Model1_tv_sec)
  return 1;
 return Model1_lhs->Model1_tv_usec - Model1_rhs->Model1_tv_usec;
}

static inline __attribute__((no_instrument_function)) int Model1_compat_timespec_compare(struct Model1_compat_timespec *Model1_lhs,
     struct Model1_compat_timespec *Model1_rhs)
{
 if (Model1_lhs->Model1_tv_sec < Model1_rhs->Model1_tv_sec)
  return -1;
 if (Model1_lhs->Model1_tv_sec > Model1_rhs->Model1_tv_sec)
  return 1;
 return Model1_lhs->Model1_tv_nsec - Model1_rhs->Model1_tv_nsec;
}

extern int Model1_get_compat_itimerspec(struct Model1_itimerspec *Model1_dst,
     const struct Model1_compat_itimerspec *Model1_src);
extern int Model1_put_compat_itimerspec(struct Model1_compat_itimerspec *Model1_dst,
     const struct Model1_itimerspec *Model1_src);

           long Model1_compat_sys_gettimeofday(struct Model1_compat_timeval *Model1_tv,
  struct Model1_timezone *Model1_tz);
           long Model1_compat_sys_settimeofday(struct Model1_compat_timeval *Model1_tv,
  struct Model1_timezone *Model1_tz);

           long Model1_compat_sys_adjtimex(struct Model1_compat_timex *Model1_utp);

extern __attribute__((format(printf, 1, 2))) int Model1_compat_printk(const char *Model1_fmt, ...);
extern void Model1_sigset_from_compat(Model1_sigset_t *Model1_set, const Model1_compat_sigset_t *Model1_compat);
extern void Model1_sigset_to_compat(Model1_compat_sigset_t *Model1_compat, const Model1_sigset_t *Model1_set);

           long Model1_compat_sys_migrate_pages(Model1_compat_pid_t Model1_pid,
  Model1_compat_ulong_t Model1_maxnode, const Model1_compat_ulong_t *Model1_old_nodes,
  const Model1_compat_ulong_t *Model1_new_nodes);

extern int Model1_compat_ptrace_request(struct Model1_task_struct *Model1_child,
     Model1_compat_long_t Model1_request,
     Model1_compat_ulong_t Model1_addr, Model1_compat_ulong_t Model1_data);

extern long Model1_compat_arch_ptrace(struct Model1_task_struct *Model1_child, Model1_compat_long_t Model1_request,
          Model1_compat_ulong_t Model1_addr, Model1_compat_ulong_t Model1_data);
           long Model1_compat_sys_ptrace(Model1_compat_long_t Model1_request, Model1_compat_long_t Model1_pid,
      Model1_compat_long_t Model1_addr, Model1_compat_long_t Model1_data);

           long Model1_compat_sys_lookup_dcookie(Model1_u32, Model1_u32, char *, Model1_compat_size_t);
/*
 * epoll (fs/eventpoll.c) compat bits follow ...
 */
struct Model1_epoll_event; /* fortunately, this one is fixed-layout */
           long Model1_compat_sys_epoll_pwait(int Model1_epfd,
   struct Model1_epoll_event *Model1_events,
   int Model1_maxevents, int Model1_timeout,
   const Model1_compat_sigset_t *Model1_sigmask,
   Model1_compat_size_t Model1_sigsetsize);

           long Model1_compat_sys_utime(const char *Model1_filename,
     struct Model1_compat_utimbuf *Model1_t);
           long Model1_compat_sys_utimensat(unsigned int Model1_dfd,
         const char *Model1_filename,
         struct Model1_compat_timespec *Model1_t,
         int Model1_flags);

           long Model1_compat_sys_time(Model1_compat_time_t *Model1_tloc);
           long Model1_compat_sys_stime(Model1_compat_time_t *Model1_tptr);
           long Model1_compat_sys_signalfd(int Model1_ufd,
        const Model1_compat_sigset_t *Model1_sigmask,
        Model1_compat_size_t Model1_sigsetsize);
           long Model1_compat_sys_timerfd_settime(int Model1_ufd, int Model1_flags,
       const struct Model1_compat_itimerspec *Model1_utmr,
       struct Model1_compat_itimerspec *Model1_otmr);
           long Model1_compat_sys_timerfd_gettime(int Model1_ufd,
       struct Model1_compat_itimerspec *Model1_otmr);

           long Model1_compat_sys_move_pages(Model1_pid_t Model1_pid, Model1_compat_ulong_t Model1_nr_pages,
          __u32 *Model1_pages,
          const int *Model1_nodes,
          int *Model1_status,
          int Model1_flags);
           long Model1_compat_sys_futimesat(unsigned int Model1_dfd,
         const char *Model1_filename,
         struct Model1_compat_timeval *Model1_t);
           long Model1_compat_sys_utimes(const char *Model1_filename,
      struct Model1_compat_timeval *Model1_t);
           long Model1_compat_sys_newstat(const char *Model1_filename,
       struct Model1_compat_stat *Model1_statbuf);
           long Model1_compat_sys_newlstat(const char *Model1_filename,
        struct Model1_compat_stat *Model1_statbuf);
           long Model1_compat_sys_newfstatat(unsigned int Model1_dfd,
          const char *Model1_filename,
          struct Model1_compat_stat *Model1_statbuf,
          int Model1_flag);
           long Model1_compat_sys_newfstat(unsigned int Model1_fd,
        struct Model1_compat_stat *Model1_statbuf);
           long Model1_compat_sys_statfs(const char *Model1_pathname,
      struct Model1_compat_statfs *Model1_buf);
           long Model1_compat_sys_fstatfs(unsigned int Model1_fd,
       struct Model1_compat_statfs *Model1_buf);
           long Model1_compat_sys_statfs64(const char *Model1_pathname,
        Model1_compat_size_t Model1_sz,
        struct Model1_compat_statfs64 *Model1_buf);
           long Model1_compat_sys_fstatfs64(unsigned int Model1_fd, Model1_compat_size_t Model1_sz,
         struct Model1_compat_statfs64 *Model1_buf);
           long Model1_compat_sys_fcntl64(unsigned int Model1_fd, unsigned int Model1_cmd,
       Model1_compat_ulong_t Model1_arg);
           long Model1_compat_sys_fcntl(unsigned int Model1_fd, unsigned int Model1_cmd,
     Model1_compat_ulong_t Model1_arg);
           long Model1_compat_sys_io_setup(unsigned Model1_nr_reqs, Model1_u32 *Model1_ctx32p);
           long Model1_compat_sys_io_getevents(Model1_compat_aio_context_t Model1_ctx_id,
     Model1_compat_long_t Model1_min_nr,
     Model1_compat_long_t Model1_nr,
     struct Model1_io_event *Model1_events,
     struct Model1_compat_timespec *Model1_timeout);
           long Model1_compat_sys_io_submit(Model1_compat_aio_context_t Model1_ctx_id, int Model1_nr,
         Model1_u32 *Model1_iocb);
           long Model1_compat_sys_mount(const char *Model1_dev_name,
     const char *Model1_dir_name,
     const char *Model1_type, Model1_compat_ulong_t Model1_flags,
     const void *Model1_data);
           long Model1_compat_sys_old_readdir(unsigned int Model1_fd,
           struct Model1_compat_old_linux_dirent *,
           unsigned int Model1_count);
           long Model1_compat_sys_getdents(unsigned int Model1_fd,
        struct Model1_compat_linux_dirent *Model1_dirent,
        unsigned int Model1_count);

           long Model1_compat_sys_getdents64(unsigned int Model1_fd,
          struct Model1_linux_dirent64 *Model1_dirent,
          unsigned int Model1_count);

           long Model1_compat_sys_vmsplice(int Model1_fd, const struct Model1_compat_iovec *,
        unsigned int Model1_nr_segs, unsigned int Model1_flags);
           long Model1_compat_sys_open(const char *Model1_filename, int Model1_flags,
    Model1_umode_t Model1_mode);
           long Model1_compat_sys_openat(int Model1_dfd, const char *Model1_filename,
      int Model1_flags, Model1_umode_t Model1_mode);
           long Model1_compat_sys_open_by_handle_at(int Model1_mountdirfd,
          struct Model1_file_handle *Model1_handle,
          int Model1_flags);
           long Model1_compat_sys_truncate(const char *, Model1_compat_off_t);
           long Model1_compat_sys_ftruncate(unsigned int, Model1_compat_ulong_t);
           long Model1_compat_sys_pselect6(int Model1_n, Model1_compat_ulong_t *Model1_inp,
        Model1_compat_ulong_t *Model1_outp,
        Model1_compat_ulong_t *Model1_exp,
        struct Model1_compat_timespec *Model1_tsp,
        void *Model1_sig);
           long Model1_compat_sys_ppoll(struct Model1_pollfd *Model1_ufds,
     unsigned int Model1_nfds,
     struct Model1_compat_timespec *Model1_tsp,
     const Model1_compat_sigset_t *Model1_sigmask,
     Model1_compat_size_t Model1_sigsetsize);
           long Model1_compat_sys_signalfd4(int Model1_ufd,
         const Model1_compat_sigset_t *Model1_sigmask,
         Model1_compat_size_t Model1_sigsetsize, int Model1_flags);
           long Model1_compat_sys_get_mempolicy(int *Model1_policy,
      Model1_compat_ulong_t *Model1_nmask,
      Model1_compat_ulong_t Model1_maxnode,
      Model1_compat_ulong_t Model1_addr,
      Model1_compat_ulong_t Model1_flags);
           long Model1_compat_sys_set_mempolicy(int Model1_mode, Model1_compat_ulong_t *Model1_nmask,
      Model1_compat_ulong_t Model1_maxnode);
           long Model1_compat_sys_mbind(Model1_compat_ulong_t Model1_start, Model1_compat_ulong_t Model1_len,
     Model1_compat_ulong_t Model1_mode,
     Model1_compat_ulong_t *Model1_nmask,
     Model1_compat_ulong_t Model1_maxnode, Model1_compat_ulong_t Model1_flags);

           long Model1_compat_sys_setsockopt(int Model1_fd, int Model1_level, int Model1_optname,
          char *Model1_optval, unsigned int Model1_optlen);
           long Model1_compat_sys_sendmsg(int Model1_fd, struct Model1_compat_msghdr *Model1_msg,
       unsigned Model1_flags);
           long Model1_compat_sys_sendmmsg(int Model1_fd, struct Model1_compat_mmsghdr *Model1_mmsg,
        unsigned Model1_vlen, unsigned int Model1_flags);
           long Model1_compat_sys_recvmsg(int Model1_fd, struct Model1_compat_msghdr *Model1_msg,
       unsigned int Model1_flags);
           long Model1_compat_sys_recv(int Model1_fd, void *Model1_buf, Model1_compat_size_t Model1_len,
    unsigned Model1_flags);
           long Model1_compat_sys_recvfrom(int Model1_fd, void *Model1_buf, Model1_compat_size_t Model1_len,
       unsigned Model1_flags, struct Model1_sockaddr *Model1_addr,
       int *Model1_addrlen);
           long Model1_compat_sys_recvmmsg(int Model1_fd, struct Model1_compat_mmsghdr *Model1_mmsg,
        unsigned Model1_vlen, unsigned int Model1_flags,
        struct Model1_compat_timespec *Model1_timeout);
           long Model1_compat_sys_nanosleep(struct Model1_compat_timespec *Model1_rqtp,
         struct Model1_compat_timespec *Model1_rmtp);
           long Model1_compat_sys_getitimer(int Model1_which,
         struct Model1_compat_itimerval *Model1_it);
           long Model1_compat_sys_setitimer(int Model1_which,
         struct Model1_compat_itimerval *Model1_in,
         struct Model1_compat_itimerval *Model1_out);
           long Model1_compat_sys_times(struct Model1_compat_tms *Model1_tbuf);
           long Model1_compat_sys_setrlimit(unsigned int Model1_resource,
         struct Model1_compat_rlimit *Model1_rlim);
           long Model1_compat_sys_getrlimit(unsigned int Model1_resource,
         struct Model1_compat_rlimit *Model1_rlim);
           long Model1_compat_sys_getrusage(int Model1_who, struct Model1_compat_rusage *Model1_ru);
           long Model1_compat_sys_sched_setaffinity(Model1_compat_pid_t Model1_pid,
         unsigned int Model1_len,
         Model1_compat_ulong_t *Model1_user_mask_ptr);
           long Model1_compat_sys_sched_getaffinity(Model1_compat_pid_t Model1_pid,
         unsigned int Model1_len,
         Model1_compat_ulong_t *Model1_user_mask_ptr);
           long Model1_compat_sys_timer_create(Model1_clockid_t Model1_which_clock,
   struct Model1_compat_sigevent *Model1_timer_event_spec,
   Model1_timer_t *Model1_created_timer_id);
           long Model1_compat_sys_timer_settime(Model1_timer_t Model1_timer_id, int Model1_flags,
      struct Model1_compat_itimerspec *Model1_new,
      struct Model1_compat_itimerspec *old);
           long Model1_compat_sys_timer_gettime(Model1_timer_t Model1_timer_id,
     struct Model1_compat_itimerspec *Model1_setting);
           long Model1_compat_sys_clock_settime(Model1_clockid_t Model1_which_clock,
      struct Model1_compat_timespec *Model1_tp);
           long Model1_compat_sys_clock_gettime(Model1_clockid_t Model1_which_clock,
      struct Model1_compat_timespec *Model1_tp);
           long Model1_compat_sys_clock_adjtime(Model1_clockid_t Model1_which_clock,
      struct Model1_compat_timex *Model1_tp);
           long Model1_compat_sys_clock_getres(Model1_clockid_t Model1_which_clock,
     struct Model1_compat_timespec *Model1_tp);
           long Model1_compat_sys_clock_nanosleep(Model1_clockid_t Model1_which_clock, int Model1_flags,
        struct Model1_compat_timespec *Model1_rqtp,
        struct Model1_compat_timespec *Model1_rmtp);
           long Model1_compat_sys_rt_sigtimedwait(Model1_compat_sigset_t *Model1_uthese,
  struct Model1_compat_siginfo *Model1_uinfo,
  struct Model1_compat_timespec *Model1_uts, Model1_compat_size_t Model1_sigsetsize);
           long Model1_compat_sys_rt_sigsuspend(Model1_compat_sigset_t *Model1_unewset,
      Model1_compat_size_t Model1_sigsetsize);
           long Model1_compat_sys_rt_sigprocmask(int Model1_how, Model1_compat_sigset_t *Model1_set,
       Model1_compat_sigset_t *Model1_oset,
       Model1_compat_size_t Model1_sigsetsize);
           long Model1_compat_sys_rt_sigpending(Model1_compat_sigset_t *Model1_uset,
      Model1_compat_size_t Model1_sigsetsize);

           long Model1_compat_sys_rt_sigaction(int,
     const struct Model1_compat_sigaction *,
     struct Model1_compat_sigaction *,
     Model1_compat_size_t);

           long Model1_compat_sys_rt_sigqueueinfo(Model1_compat_pid_t Model1_pid, int Model1_sig,
    struct Model1_compat_siginfo *Model1_uinfo);
           long Model1_compat_sys_sysinfo(struct Model1_compat_sysinfo *Model1_info);
           long Model1_compat_sys_ioctl(unsigned int Model1_fd, unsigned int Model1_cmd,
     Model1_compat_ulong_t Model1_arg);
           long Model1_compat_sys_futex(Model1_u32 *Model1_uaddr, int Model1_op, Model1_u32 Model1_val,
  struct Model1_compat_timespec *Model1_utime, Model1_u32 *Model1_uaddr2,
  Model1_u32 Model1_val3);
           long Model1_compat_sys_getsockopt(int Model1_fd, int Model1_level, int Model1_optname,
          char *Model1_optval, int *Model1_optlen);
           long Model1_compat_sys_kexec_load(Model1_compat_ulong_t Model1_entry,
          Model1_compat_ulong_t Model1_nr_segments,
          struct Model1_compat_kexec_segment *,
          Model1_compat_ulong_t Model1_flags);
           long Model1_compat_sys_mq_getsetattr(Model1_mqd_t Model1_mqdes,
   const struct Model1_compat_mq_attr *Model1_u_mqstat,
   struct Model1_compat_mq_attr *Model1_u_omqstat);
           long Model1_compat_sys_mq_notify(Model1_mqd_t Model1_mqdes,
   const struct Model1_compat_sigevent *Model1_u_notification);
           long Model1_compat_sys_mq_open(const char *Model1_u_name,
   int Model1_oflag, Model1_compat_mode_t Model1_mode,
   struct Model1_compat_mq_attr *Model1_u_attr);
           long Model1_compat_sys_mq_timedsend(Model1_mqd_t Model1_mqdes,
   const char *Model1_u_msg_ptr,
   Model1_compat_size_t Model1_msg_len, unsigned int Model1_msg_prio,
   const struct Model1_compat_timespec *Model1_u_abs_timeout);
           Model1_ssize_t Model1_compat_sys_mq_timedreceive(Model1_mqd_t Model1_mqdes,
   char *Model1_u_msg_ptr,
   Model1_compat_size_t Model1_msg_len, unsigned int *Model1_u_msg_prio,
   const struct Model1_compat_timespec *Model1_u_abs_timeout);
           long Model1_compat_sys_socketcall(int Model1_call, Model1_u32 *Model1_args);
           long Model1_compat_sys_sysctl(struct Model1_compat_sysctl_args *Model1_args);

extern Model1_ssize_t Model1_compat_rw_copy_check_uvector(int Model1_type,
  const struct Model1_compat_iovec *Model1_uvector,
  unsigned long Model1_nr_segs,
  unsigned long Model1_fast_segs, struct Model1_iovec *Model1_fast_pointer,
  struct Model1_iovec **Model1_ret_pointer);

extern void *Model1_compat_alloc_user_space(unsigned long Model1_len);

           Model1_ssize_t Model1_compat_sys_process_vm_readv(Model1_compat_pid_t Model1_pid,
  const struct Model1_compat_iovec *Model1_lvec,
  Model1_compat_ulong_t Model1_liovcnt, const struct Model1_compat_iovec *Model1_rvec,
  Model1_compat_ulong_t Model1_riovcnt, Model1_compat_ulong_t Model1_flags);
           Model1_ssize_t Model1_compat_sys_process_vm_writev(Model1_compat_pid_t Model1_pid,
  const struct Model1_compat_iovec *Model1_lvec,
  Model1_compat_ulong_t Model1_liovcnt, const struct Model1_compat_iovec *Model1_rvec,
  Model1_compat_ulong_t Model1_riovcnt, Model1_compat_ulong_t Model1_flags);

           long Model1_compat_sys_sendfile(int Model1_out_fd, int Model1_in_fd,
        Model1_compat_off_t *Model1_offset, Model1_compat_size_t Model1_count);
           long Model1_compat_sys_sendfile64(int Model1_out_fd, int Model1_in_fd,
        Model1_compat_loff_t *Model1_offset, Model1_compat_size_t Model1_count);
           long Model1_compat_sys_sigaltstack(const Model1_compat_stack_t *Model1_uss_ptr,
           Model1_compat_stack_t *Model1_uoss_ptr);


           long Model1_compat_sys_sigpending(Model1_compat_old_sigset_t *Model1_set);



           long Model1_compat_sys_sigprocmask(int Model1_how, Model1_compat_old_sigset_t *Model1_nset,
           Model1_compat_old_sigset_t *Model1_oset);


int Model1_compat_restore_altstack(const Model1_compat_stack_t *Model1_uss);
int Model1___compat_save_altstack(Model1_compat_stack_t *, unsigned long);
           long Model1_compat_sys_sched_rr_get_interval(Model1_compat_pid_t Model1_pid,
       struct Model1_compat_timespec *Model1_interval);

           long Model1_compat_sys_fanotify_mark(int, unsigned int, __u32, __u32,
         int, const char *);

/*
 * For most but not all architectures, "am I in a compat syscall?" and
 * "am I a compat task?" are the same question.  For architectures on which
 * they aren't the same question, arch code can override in_compat_syscall.
 */
/*
 * ethtool.h: Defines for Linux ethtool.
 *
 * Copyright (C) 1998 David S. Miller (davem@redhat.com)
 * Copyright 2001 Jeff Garzik <jgarzik@pobox.com>
 * Portions Copyright 2001 Sun Microsystems (thockin@sun.com)
 * Portions Copyright 2002 Intel (eli.kupermann@intel.com,
 *                                christopher.leech@intel.com,
 *                                scott.feldman@intel.com)
 * Portions Copyright (C) Sun Microsystems 2008
 */







/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Global definitions for the Ethernet IEEE 802.3 interface.
 *
 * Version:	@(#)if_ether.h	1.0.1a	02/08/94
 *
 * Author:	Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *		Donald Becker, <becker@super.org>
 *		Alan Cox, <alan@lxorguk.ukuu.org.uk>
 *		Steve Whitehouse, <gw7rrm@eeshack3.swan.ac.uk>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */




/*
 *	Definitions for the 'struct sk_buff' memory handlers.
 *
 *	Authors:
 *		Alan Cox, <gw4pts@gw4pts.ampr.org>
 *		Florian La Roche, <rzsfl@rz.uni-sb.de>
 *
 *	This program is free software; you can redistribute it and/or
 *	modify it under the terms of the GNU General Public License
 *	as published by the Free Software Foundation; either version
 *	2 of the License, or (at your option) any later version.
 */






static inline __attribute__((no_instrument_function)) void
Model1_kmemcheck_alloc_shadow(struct Model1_page *Model1_page, int Model1_order, Model1_gfp_t Model1_flags, int Model1_node)
{
}

static inline __attribute__((no_instrument_function)) void
Model1_kmemcheck_free_shadow(struct Model1_page *Model1_page, int Model1_order)
{
}

static inline __attribute__((no_instrument_function)) void
Model1_kmemcheck_slab_alloc(struct Model1_kmem_cache *Model1_s, Model1_gfp_t Model1_gfpflags, void *Model1_object,
       Model1_size_t Model1_size)
{
}

static inline __attribute__((no_instrument_function)) void Model1_kmemcheck_slab_free(struct Model1_kmem_cache *Model1_s, void *Model1_object,
           Model1_size_t Model1_size)
{
}

static inline __attribute__((no_instrument_function)) void Model1_kmemcheck_pagealloc_alloc(struct Model1_page *Model1_p,
 unsigned int Model1_order, Model1_gfp_t Model1_gfpflags)
{
}

static inline __attribute__((no_instrument_function)) bool Model1_kmemcheck_page_is_tracked(struct Model1_page *Model1_p)
{
 return false;
}

static inline __attribute__((no_instrument_function)) void Model1_kmemcheck_mark_unallocated(void *Model1_address, unsigned int Model1_n)
{
}

static inline __attribute__((no_instrument_function)) void Model1_kmemcheck_mark_uninitialized(void *Model1_address, unsigned int Model1_n)
{
}

static inline __attribute__((no_instrument_function)) void Model1_kmemcheck_mark_initialized(void *Model1_address, unsigned int Model1_n)
{
}

static inline __attribute__((no_instrument_function)) void Model1_kmemcheck_mark_freed(void *Model1_address, unsigned int Model1_n)
{
}

static inline __attribute__((no_instrument_function)) void Model1_kmemcheck_mark_unallocated_pages(struct Model1_page *Model1_p,
          unsigned int Model1_n)
{
}

static inline __attribute__((no_instrument_function)) void Model1_kmemcheck_mark_uninitialized_pages(struct Model1_page *Model1_p,
            unsigned int Model1_n)
{
}

static inline __attribute__((no_instrument_function)) void Model1_kmemcheck_mark_initialized_pages(struct Model1_page *Model1_p,
          unsigned int Model1_n)
{
}

static inline __attribute__((no_instrument_function)) bool Model1_kmemcheck_is_obj_initialized(unsigned long Model1_addr, Model1_size_t Model1_size)
{
 return true;
}
/*
 * NET		An implementation of the SOCKET network access protocol.
 *		This is the master header file for the Linux NET layer,
 *		or, in plain English: the networking handling part of the
 *		kernel.
 *
 * Version:	@(#)net.h	1.0.3	05/25/93
 *
 * Authors:	Orest Zborowski, <obz@Kodak.COM>
 *		Ross Biro
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */





/*
 * include/linux/random.h
 *
 * Include file for the random number generator.
 */











bool Model1___do_once_start(bool *Model1_done, unsigned long *Model1_flags);
void Model1___do_once_done(bool *Model1_done, struct Model1_static_key *Model1_once_key,
      unsigned long *Model1_flags);

/* Call a function exactly once. The idea of DO_ONCE() is to perform
 * a function call such as initialization of random seeds, etc, only
 * once, where DO_ONCE() can live in the fast-path. After @func has
 * been called with the passed arguments, the static key will patch
 * out the condition into a nop. DO_ONCE() guarantees type safety of
 * arguments!
 *
 * Not that the following is not equivalent ...
 *
 *   DO_ONCE(func, arg);
 *   DO_ONCE(func, arg);
 *
 * ... to this version:
 *
 *   void foo(void)
 *   {
 *     DO_ONCE(func, arg);
 *   }
 *
 *   foo();
 *   foo();
 *
 * In case the one-time invocation could be triggered from multiple
 * places, then a common helper function must be defined, so that only
 * a single static key will be placed there!
 */

/*
 * include/linux/random.h
 *
 * Include file for the random number generator.
 */










/*
 * There isn't anything here anymore, but the file must not be empty or patch
 * will delete it.
 */


extern int Model1_nr_irqs;
extern struct Model1_irq_desc *Model1_irq_to_desc(unsigned int Model1_irq);
unsigned int Model1_irq_get_next_irq(unsigned int Model1_offset);

/* ioctl()'s for the random number generator */

/* Get the entropy count. */


/* Add to (or subtract from) the entropy count.  (Superuser only.) */


/* Get the contents of the entropy pool.  (Superuser only.) */


/* 
 * Write bytes into the entropy pool and add to the entropy count.
 * (Superuser only.)
 */


/* Clear entropy count to 0.  (Superuser only.) */


/* Clear the entropy pool and associated counters.  (Superuser only.) */


struct Model1_rand_pool_info {
 int Model1_entropy_count;
 int Model1_buf_size;
 __u32 Model1_buf[0];
};

/*
 * Flags for getrandom(2)
 *
 * GRND_NONBLOCK	Don't block and return EAGAIN instead
 * GRND_RANDOM		Use the /dev/random pool instead of /dev/urandom
 */

struct Model1_random_ready_callback {
 struct Model1_list_head Model1_list;
 void (*func)(struct Model1_random_ready_callback *Model1_rdy);
 struct Model1_module *Model1_owner;
};

extern void Model1_add_device_randomness(const void *, unsigned int);
extern void Model1_add_input_randomness(unsigned int Model1_type, unsigned int Model1_code,
     unsigned int Model1_value);
extern void Model1_add_interrupt_randomness(int Model1_irq, int Model1_irq_flags);

extern void Model1_get_random_bytes(void *Model1_buf, int Model1_nbytes);
extern int Model1_add_random_ready_callback(struct Model1_random_ready_callback *Model1_rdy);
extern void Model1_del_random_ready_callback(struct Model1_random_ready_callback *Model1_rdy);
extern void Model1_get_random_bytes_arch(void *Model1_buf, int Model1_nbytes);
extern int Model1_random_int_secret_init(void);


extern const struct Model1_file_operations Model1_random_fops, Model1_urandom_fops;


unsigned int Model1_get_random_int(void);
unsigned long Model1_get_random_long(void);
unsigned long Model1_randomize_range(unsigned long Model1_start, unsigned long Model1_end, unsigned long Model1_len);

Model1_u32 Model1_prandom_u32(void);
void Model1_prandom_bytes(void *Model1_buf, Model1_size_t Model1_nbytes);
void Model1_prandom_seed(Model1_u32 Model1_seed);
void Model1_prandom_reseed_late(void);

struct Model1_rnd_state {
 __u32 Model1_s1, Model1_s2, Model1_s3, Model1_s4;
};

Model1_u32 Model1_prandom_u32_state(struct Model1_rnd_state *Model1_state);
void Model1_prandom_bytes_state(struct Model1_rnd_state *Model1_state, void *Model1_buf, Model1_size_t Model1_nbytes);
void Model1_prandom_seed_full_state(struct Model1_rnd_state *Model1_pcpu_state);




/**
 * prandom_u32_max - returns a pseudo-random number in interval [0, ep_ro)
 * @ep_ro: right open interval endpoint
 *
 * Returns a pseudo-random number that is in interval [0, ep_ro). Note
 * that the result depends on PRNG being well distributed in [0, ~0U]
 * u32 space. Here we use maximally equidistributed combined Tausworthe
 * generator, that is, prandom_u32(). This is useful when requesting a
 * random index of an array containing ep_ro elements, for example.
 *
 * Returns: pseudo-random number in interval [0, ep_ro)
 */
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_prandom_u32_max(Model1_u32 Model1_ep_ro)
{
 return (Model1_u32)(((Model1_u64) Model1_prandom_u32() * Model1_ep_ro) >> 32);
}

/*
 * Handle minimum values for seeds
 */
static inline __attribute__((no_instrument_function)) Model1_u32 Model1___seed(Model1_u32 Model1_x, Model1_u32 Model1_m)
{
 return (Model1_x < Model1_m) ? Model1_x + Model1_m : Model1_x;
}

/**
 * prandom_seed_state - set seed for prandom_u32_state().
 * @state: pointer to state structure to receive the seed.
 * @seed: arbitrary 64-bit value to use as a seed.
 */
static inline __attribute__((no_instrument_function)) void Model1_prandom_seed_state(struct Model1_rnd_state *Model1_state, Model1_u64 Model1_seed)
{
 Model1_u32 Model1_i = (Model1_seed >> 32) ^ (Model1_seed << 10) ^ Model1_seed;

 Model1_state->Model1_s1 = Model1___seed(Model1_i, 2U);
 Model1_state->Model1_s2 = Model1___seed(Model1_i, 8U);
 Model1_state->Model1_s3 = Model1___seed(Model1_i, 16U);
 Model1_state->Model1_s4 = Model1___seed(Model1_i, 128U);
}



/*
 * This file is part of the Linux kernel.
 *
 * Copyright (c) 2011-2014, Intel Corporation
 * Authors: Fenghua Yu <fenghua.yu@intel.com>,
 *          H. Peter Anvin <hpa@linux.intel.com>
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms and conditions of the GNU General Public License,
 * version 2, as published by the Free Software Foundation.
 *
 * This program is distributed in the hope it will be useful, but WITHOUT
 * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
 * more details.
 *
 * You should have received a copy of the GNU General Public License along with
 * this program; if not, write to the Free Software Foundation, Inc.,
 * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
 *
 */
/* Unconditional execution of RDRAND and RDSEED */

static inline __attribute__((no_instrument_function)) bool Model1_rdrand_long(unsigned long *Model1_v)
{
 bool Model1_ok;
 unsigned int Model1_retry = 10;
 do {
  asm volatile(".byte 0x48,0x0f,0xc7,0xf0" "\n\t"
        "\n\tset" "c" " %[_cc_" "c" "]\n"
        : [_cc_c] "=qm" (Model1_ok), "=a" (*Model1_v));
  if (Model1_ok)
   return true;
 } while (--Model1_retry);
 return false;
}

static inline __attribute__((no_instrument_function)) bool Model1_rdrand_int(unsigned int *Model1_v)
{
 bool Model1_ok;
 unsigned int Model1_retry = 10;
 do {
  asm volatile(".byte 0x0f,0xc7,0xf0" "\n\t"
        "\n\tset" "c" " %[_cc_" "c" "]\n"
        : [_cc_c] "=qm" (Model1_ok), "=a" (*Model1_v));
  if (Model1_ok)
   return true;
 } while (--Model1_retry);
 return false;
}

static inline __attribute__((no_instrument_function)) bool Model1_rdseed_long(unsigned long *Model1_v)
{
 bool Model1_ok;
 asm volatile(".byte 0x48,0x0f,0xc7,0xf8" "\n\t"
       "\n\tset" "c" " %[_cc_" "c" "]\n"
       : [_cc_c] "=qm" (Model1_ok), "=a" (*Model1_v));
 return Model1_ok;
}

static inline __attribute__((no_instrument_function)) bool Model1_rdseed_int(unsigned int *Model1_v)
{
 bool Model1_ok;
 asm volatile(".byte 0x0f,0xc7,0xf8" "\n\t"
       "\n\tset" "c" " %[_cc_" "c" "]\n"
       : [_cc_c] "=qm" (Model1_ok), "=a" (*Model1_v));
 return Model1_ok;
}

/* Conditional execution based on CPU type */



/*
 * These are the generic interfaces; they must not be declared if the
 * stubs in <linux/random.h> are to be invoked,
 * i.e. CONFIG_ARCH_RANDOM is not defined.
 */


static inline __attribute__((no_instrument_function)) bool Model1_arch_get_random_long(unsigned long *Model1_v)
{
 return (__builtin_constant_p(( 4*32+30)) && ( (((( 4*32+30))>>5)==(0) && (1UL<<((( 4*32+30))&31) & ((1<<(( 0*32+ 0) & 31))|(1<<(( 0*32+ 3)) & 31)|(1<<(( 0*32+ 5) & 31))|(1<<(( 0*32+ 6) & 31))| (1<<(( 0*32+ 8) & 31))|(1<<(( 0*32+13)) & 31)|(1<<(( 0*32+24) & 31))|(1<<(( 0*32+15) & 31))| (1<<(( 0*32+25) & 31))|(1<<(( 0*32+26) & 31))) )) || (((( 4*32+30))>>5)==(1) && (1UL<<((( 4*32+30))&31) & ((1<<(( 1*32+29) & 31))|0) )) || (((( 4*32+30))>>5)==(2) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(3) && (1UL<<((( 4*32+30))&31) & ((1<<(( 3*32+20) & 31))) )) || (((( 4*32+30))>>5)==(4) && (1UL<<((( 4*32+30))&31) & (0) )) || (((( 4*32+30))>>5)==(5) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(6) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(7) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(8) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(9) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(10) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(11) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(12) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(13) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(14) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(15) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(16) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(17) && (1UL<<((( 4*32+30))&31) & 0 )) || (sizeof(struct { int:-!!(18 != 18); })) || (sizeof(struct { int:-!!(18 != 18); }))) ? 1 : (__builtin_constant_p((( 4*32+30))) ? Model1_constant_test_bit((( 4*32+30)), ((unsigned long *)((&Model1_boot_cpu_data)->Model1_x86_capability))) : Model1_variable_test_bit((( 4*32+30)), ((unsigned long *)((&Model1_boot_cpu_data)->Model1_x86_capability))))) ? Model1_rdrand_long(Model1_v) : false;
}

static inline __attribute__((no_instrument_function)) bool Model1_arch_get_random_int(unsigned int *Model1_v)
{
 return (__builtin_constant_p(( 4*32+30)) && ( (((( 4*32+30))>>5)==(0) && (1UL<<((( 4*32+30))&31) & ((1<<(( 0*32+ 0) & 31))|(1<<(( 0*32+ 3)) & 31)|(1<<(( 0*32+ 5) & 31))|(1<<(( 0*32+ 6) & 31))| (1<<(( 0*32+ 8) & 31))|(1<<(( 0*32+13)) & 31)|(1<<(( 0*32+24) & 31))|(1<<(( 0*32+15) & 31))| (1<<(( 0*32+25) & 31))|(1<<(( 0*32+26) & 31))) )) || (((( 4*32+30))>>5)==(1) && (1UL<<((( 4*32+30))&31) & ((1<<(( 1*32+29) & 31))|0) )) || (((( 4*32+30))>>5)==(2) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(3) && (1UL<<((( 4*32+30))&31) & ((1<<(( 3*32+20) & 31))) )) || (((( 4*32+30))>>5)==(4) && (1UL<<((( 4*32+30))&31) & (0) )) || (((( 4*32+30))>>5)==(5) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(6) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(7) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(8) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(9) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(10) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(11) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(12) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(13) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(14) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(15) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(16) && (1UL<<((( 4*32+30))&31) & 0 )) || (((( 4*32+30))>>5)==(17) && (1UL<<((( 4*32+30))&31) & 0 )) || (sizeof(struct { int:-!!(18 != 18); })) || (sizeof(struct { int:-!!(18 != 18); }))) ? 1 : (__builtin_constant_p((( 4*32+30))) ? Model1_constant_test_bit((( 4*32+30)), ((unsigned long *)((&Model1_boot_cpu_data)->Model1_x86_capability))) : Model1_variable_test_bit((( 4*32+30)), ((unsigned long *)((&Model1_boot_cpu_data)->Model1_x86_capability))))) ? Model1_rdrand_int(Model1_v) : false;
}

static inline __attribute__((no_instrument_function)) bool Model1_arch_get_random_seed_long(unsigned long *Model1_v)
{
 return (__builtin_constant_p(( 9*32+18)) && ( (((( 9*32+18))>>5)==(0) && (1UL<<((( 9*32+18))&31) & ((1<<(( 0*32+ 0) & 31))|(1<<(( 0*32+ 3)) & 31)|(1<<(( 0*32+ 5) & 31))|(1<<(( 0*32+ 6) & 31))| (1<<(( 0*32+ 8) & 31))|(1<<(( 0*32+13)) & 31)|(1<<(( 0*32+24) & 31))|(1<<(( 0*32+15) & 31))| (1<<(( 0*32+25) & 31))|(1<<(( 0*32+26) & 31))) )) || (((( 9*32+18))>>5)==(1) && (1UL<<((( 9*32+18))&31) & ((1<<(( 1*32+29) & 31))|0) )) || (((( 9*32+18))>>5)==(2) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(3) && (1UL<<((( 9*32+18))&31) & ((1<<(( 3*32+20) & 31))) )) || (((( 9*32+18))>>5)==(4) && (1UL<<((( 9*32+18))&31) & (0) )) || (((( 9*32+18))>>5)==(5) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(6) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(7) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(8) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(9) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(10) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(11) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(12) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(13) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(14) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(15) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(16) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(17) && (1UL<<((( 9*32+18))&31) & 0 )) || (sizeof(struct { int:-!!(18 != 18); })) || (sizeof(struct { int:-!!(18 != 18); }))) ? 1 : (__builtin_constant_p((( 9*32+18))) ? Model1_constant_test_bit((( 9*32+18)), ((unsigned long *)((&Model1_boot_cpu_data)->Model1_x86_capability))) : Model1_variable_test_bit((( 9*32+18)), ((unsigned long *)((&Model1_boot_cpu_data)->Model1_x86_capability))))) ? Model1_rdseed_long(Model1_v) : false;
}

static inline __attribute__((no_instrument_function)) bool Model1_arch_get_random_seed_int(unsigned int *Model1_v)
{
 return (__builtin_constant_p(( 9*32+18)) && ( (((( 9*32+18))>>5)==(0) && (1UL<<((( 9*32+18))&31) & ((1<<(( 0*32+ 0) & 31))|(1<<(( 0*32+ 3)) & 31)|(1<<(( 0*32+ 5) & 31))|(1<<(( 0*32+ 6) & 31))| (1<<(( 0*32+ 8) & 31))|(1<<(( 0*32+13)) & 31)|(1<<(( 0*32+24) & 31))|(1<<(( 0*32+15) & 31))| (1<<(( 0*32+25) & 31))|(1<<(( 0*32+26) & 31))) )) || (((( 9*32+18))>>5)==(1) && (1UL<<((( 9*32+18))&31) & ((1<<(( 1*32+29) & 31))|0) )) || (((( 9*32+18))>>5)==(2) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(3) && (1UL<<((( 9*32+18))&31) & ((1<<(( 3*32+20) & 31))) )) || (((( 9*32+18))>>5)==(4) && (1UL<<((( 9*32+18))&31) & (0) )) || (((( 9*32+18))>>5)==(5) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(6) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(7) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(8) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(9) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(10) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(11) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(12) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(13) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(14) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(15) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(16) && (1UL<<((( 9*32+18))&31) & 0 )) || (((( 9*32+18))>>5)==(17) && (1UL<<((( 9*32+18))&31) & 0 )) || (sizeof(struct { int:-!!(18 != 18); })) || (sizeof(struct { int:-!!(18 != 18); }))) ? 1 : (__builtin_constant_p((( 9*32+18))) ? Model1_constant_test_bit((( 9*32+18)), ((unsigned long *)((&Model1_boot_cpu_data)->Model1_x86_capability))) : Model1_variable_test_bit((( 9*32+18)), ((unsigned long *)((&Model1_boot_cpu_data)->Model1_x86_capability))))) ? Model1_rdseed_int(Model1_v) : false;
}

extern void Model1_x86_init_rdrand(struct Model1_cpuinfo_x86 *Model1_c);
/* Pseudo random number generator from numerical recipes. */
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_next_pseudo_random32(Model1_u32 Model1_seed)
{
 return Model1_seed * 1664525 + 1013904223;
}






/*
 * NET		An implementation of the SOCKET network access protocol.
 *		This is the master header file for the Linux NET layer,
 *		or, in plain English: the networking handling part of the
 *		kernel.
 *
 * Version:	@(#)net.h	1.0.3	05/25/93
 *
 * Authors:	Orest Zborowski, <obz@Kodak.COM>
 *		Ross Biro
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */





typedef enum {
 Model1_SS_FREE = 0, /* not allocated		*/
 Model1_SS_UNCONNECTED, /* unconnected to any socket	*/
 Model1_SS_CONNECTING, /* in process of connecting	*/
 Model1_SS_CONNECTED, /* connected to socket		*/
 Model1_SS_DISCONNECTING /* in process of disconnecting	*/
} Model1_socket_state;

struct Model1_poll_table_struct;
struct Model1_pipe_inode_info;
struct Model1_inode;
struct Model1_file;
struct Model1_net;

/* Historically, SOCKWQ_ASYNC_NOSPACE & SOCKWQ_ASYNC_WAITDATA were located
 * in sock->flags, but moved into sk->sk_wq->flags to be RCU protected.
 * Eventually all flags will be in sk->sk_wq_flags.
 */







/**
 * enum sock_type - Socket types
 * @SOCK_STREAM: stream (connection) socket
 * @SOCK_DGRAM: datagram (conn.less) socket
 * @SOCK_RAW: raw socket
 * @SOCK_RDM: reliably-delivered message
 * @SOCK_SEQPACKET: sequential packet socket
 * @SOCK_DCCP: Datagram Congestion Control Protocol socket
 * @SOCK_PACKET: linux specific way of getting packets at the dev level.
 *		  For writing rarp and other similar things on the user level.
 *
 * When adding some new socket type please
 * grep ARCH_HAS_SOCKET_TYPE include/asm-* /socket.h, at least MIPS
 * overrides this enum for binary compat reasons.
 */
enum Model1_sock_type {
 Model1_SOCK_STREAM = 1,
 Model1_SOCK_DGRAM = 2,
 Model1_SOCK_RAW = 3,
 Model1_SOCK_RDM = 4,
 Model1_SOCK_SEQPACKET = 5,
 Model1_SOCK_DCCP = 6,
 Model1_SOCK_PACKET = 10,
};


/* Mask which covers at least up to SOCK_MASK-1.  The
 * remaining bits are used as flags. */


/* Flags for socket, socketpair, accept4 */







enum Model1_sock_shutdown_cmd {
 Model1_SHUT_RD,
 Model1_SHUT_WR,
 Model1_SHUT_RDWR,
};

struct Model1_socket_wq {
 /* Note: wait MUST be first field of socket_wq */
 Model1_wait_queue_head_t Model1_wait;
 struct Model1_fasync_struct *Model1_fasync_list;
 unsigned long Model1_flags; /* %SOCKWQ_ASYNC_NOSPACE, etc */
 struct Model1_callback_head Model1_rcu;
} __attribute__((__aligned__((1 << (6)))));

/**
 *  struct socket - general BSD socket
 *  @state: socket state (%SS_CONNECTED, etc)
 *  @type: socket type (%SOCK_STREAM, etc)
 *  @flags: socket flags (%SOCK_NOSPACE, etc)
 *  @ops: protocol specific socket operations
 *  @file: File back pointer for gc
 *  @sk: internal networking protocol agnostic socket representation
 *  @wq: wait queue for several uses
 */
struct Model1_socket {
 Model1_socket_state Model1_state;

                               ;
 short Model1_type;
                             ;

 unsigned long Model1_flags;

 struct Model1_socket_wq *Model1_wq;

 struct Model1_file *Model1_file;
 struct Model1_sock *Model1_sk;
 const struct Model1_proto_ops *Model1_ops;
};

struct Model1_vm_area_struct;
struct Model1_page;
struct Model1_sockaddr;
struct Model1_msghdr;
struct Model1_module;

struct Model1_proto_ops {
 int Model1_family;
 struct Model1_module *Model1_owner;
 int (*Model1_release) (struct Model1_socket *Model1_sock);
 int (*Model1_bind) (struct Model1_socket *Model1_sock,
          struct Model1_sockaddr *Model1_myaddr,
          int Model1_sockaddr_len);
 int (*Model1_connect) (struct Model1_socket *Model1_sock,
          struct Model1_sockaddr *Model1_vaddr,
          int Model1_sockaddr_len, int Model1_flags);
 int (*Model1_socketpair)(struct Model1_socket *Model1_sock1,
          struct Model1_socket *Model1_sock2);
 int (*Model1_accept) (struct Model1_socket *Model1_sock,
          struct Model1_socket *Model1_newsock, int Model1_flags);
 int (*Model1_getname) (struct Model1_socket *Model1_sock,
          struct Model1_sockaddr *Model1_addr,
          int *Model1_sockaddr_len, int Model1_peer);
 unsigned int (*Model1_poll) (struct Model1_file *Model1_file, struct Model1_socket *Model1_sock,
          struct Model1_poll_table_struct *Model1_wait);
 int (*Model1_ioctl) (struct Model1_socket *Model1_sock, unsigned int Model1_cmd,
          unsigned long Model1_arg);

 int (*Model1_compat_ioctl) (struct Model1_socket *Model1_sock, unsigned int Model1_cmd,
          unsigned long Model1_arg);

 int (*Model1_listen) (struct Model1_socket *Model1_sock, int Model1_len);
 int (*Model1_shutdown) (struct Model1_socket *Model1_sock, int Model1_flags);
 int (*Model1_setsockopt)(struct Model1_socket *Model1_sock, int Model1_level,
          int Model1_optname, char *Model1_optval, unsigned int Model1_optlen);
 int (*Model1_getsockopt)(struct Model1_socket *Model1_sock, int Model1_level,
          int Model1_optname, char *Model1_optval, int *Model1_optlen);

 int (*Model1_compat_setsockopt)(struct Model1_socket *Model1_sock, int Model1_level,
          int Model1_optname, char *Model1_optval, unsigned int Model1_optlen);
 int (*Model1_compat_getsockopt)(struct Model1_socket *Model1_sock, int Model1_level,
          int Model1_optname, char *Model1_optval, int *Model1_optlen);

 int (*Model1_sendmsg) (struct Model1_socket *Model1_sock, struct Model1_msghdr *Model1_m,
          Model1_size_t Model1_total_len);
 /* Notes for implementing recvmsg:
	 * ===============================
	 * msg->msg_namelen should get updated by the recvmsg handlers
	 * iff msg_name != NULL. It is by default 0 to prevent
	 * returning uninitialized memory to user space.  The recvfrom
	 * handlers can assume that msg.msg_name is either NULL or has
	 * a minimum size of sizeof(struct sockaddr_storage).
	 */
 int (*Model1_recvmsg) (struct Model1_socket *Model1_sock, struct Model1_msghdr *Model1_m,
          Model1_size_t Model1_total_len, int Model1_flags);
 int (*Model1_mmap) (struct Model1_file *Model1_file, struct Model1_socket *Model1_sock,
          struct Model1_vm_area_struct * Model1_vma);
 Model1_ssize_t (*Model1_sendpage) (struct Model1_socket *Model1_sock, struct Model1_page *Model1_page,
          int Model1_offset, Model1_size_t Model1_size, int Model1_flags);
 Model1_ssize_t (*Model1_splice_read)(struct Model1_socket *Model1_sock, Model1_loff_t *Model1_ppos,
           struct Model1_pipe_inode_info *Model1_pipe, Model1_size_t Model1_len, unsigned int Model1_flags);
 int (*Model1_set_peek_off)(struct Model1_sock *Model1_sk, int Model1_val);
 int (*Model1_peek_len)(struct Model1_socket *Model1_sock);
};




struct Model1_net_proto_family {
 int Model1_family;
 int (*Model1_create)(struct Model1_net *Model1_net, struct Model1_socket *Model1_sock,
      int Model1_protocol, int Model1_kern);
 struct Model1_module *Model1_owner;
};

struct Model1_iovec;
struct Model1_kvec;

enum {
 Model1_SOCK_WAKE_IO,
 Model1_SOCK_WAKE_WAITD,
 Model1_SOCK_WAKE_SPACE,
 Model1_SOCK_WAKE_URG,
};

int Model1_sock_wake_async(struct Model1_socket_wq *Model1_sk_wq, int Model1_how, int Model1_band);
int Model1_sock_register(const struct Model1_net_proto_family *Model1_fam);
void Model1_sock_unregister(int Model1_family);
int Model1___sock_create(struct Model1_net *Model1_net, int Model1_family, int Model1_type, int Model1_proto,
    struct Model1_socket **Model1_res, int Model1_kern);
int Model1_sock_create(int Model1_family, int Model1_type, int Model1_proto, struct Model1_socket **Model1_res);
int Model1_sock_create_kern(struct Model1_net *Model1_net, int Model1_family, int Model1_type, int Model1_proto, struct Model1_socket **Model1_res);
int Model1_sock_create_lite(int Model1_family, int Model1_type, int Model1_proto, struct Model1_socket **Model1_res);
struct Model1_socket *Model1_sock_alloc(void);
void Model1_sock_release(struct Model1_socket *Model1_sock);
int Model1_sock_sendmsg(struct Model1_socket *Model1_sock, struct Model1_msghdr *Model1_msg);
int Model1_sock_recvmsg(struct Model1_socket *Model1_sock, struct Model1_msghdr *Model1_msg, int Model1_flags);
struct Model1_file *Model1_sock_alloc_file(struct Model1_socket *Model1_sock, int Model1_flags, const char *Model1_dname);
struct Model1_socket *Model1_sockfd_lookup(int Model1_fd, int *err);
struct Model1_socket *Model1_sock_from_file(struct Model1_file *Model1_file, int *err);

int Model1_net_ratelimit(void);
int Model1_kernel_sendmsg(struct Model1_socket *Model1_sock, struct Model1_msghdr *Model1_msg, struct Model1_kvec *Model1_vec,
     Model1_size_t Model1_num, Model1_size_t Model1_len);
int Model1_kernel_recvmsg(struct Model1_socket *Model1_sock, struct Model1_msghdr *Model1_msg, struct Model1_kvec *Model1_vec,
     Model1_size_t Model1_num, Model1_size_t Model1_len, int Model1_flags);

int Model1_kernel_bind(struct Model1_socket *Model1_sock, struct Model1_sockaddr *Model1_addr, int Model1_addrlen);
int Model1_kernel_listen(struct Model1_socket *Model1_sock, int Model1_backlog);
int Model1_kernel_accept(struct Model1_socket *Model1_sock, struct Model1_socket **Model1_newsock, int Model1_flags);
int Model1_kernel_connect(struct Model1_socket *Model1_sock, struct Model1_sockaddr *Model1_addr, int Model1_addrlen,
     int Model1_flags);
int Model1_kernel_getsockname(struct Model1_socket *Model1_sock, struct Model1_sockaddr *Model1_addr,
         int *Model1_addrlen);
int Model1_kernel_getpeername(struct Model1_socket *Model1_sock, struct Model1_sockaddr *Model1_addr,
         int *Model1_addrlen);
int Model1_kernel_getsockopt(struct Model1_socket *Model1_sock, int Model1_level, int Model1_optname, char *Model1_optval,
        int *Model1_optlen);
int Model1_kernel_setsockopt(struct Model1_socket *Model1_sock, int Model1_level, int Model1_optname, char *Model1_optval,
        unsigned int Model1_optlen);
int Model1_kernel_sendpage(struct Model1_socket *Model1_sock, struct Model1_page *Model1_page, int Model1_offset,
      Model1_size_t Model1_size, int Model1_flags);
int Model1_kernel_sock_ioctl(struct Model1_socket *Model1_sock, int Model1_cmd, unsigned long Model1_arg);
int Model1_kernel_sock_shutdown(struct Model1_socket *Model1_sock, enum Model1_sock_shutdown_cmd Model1_how);
struct Model1_module;

struct Model1_ts_config;




/**
 * struct ts_state - search state
 * @offset: offset for next match
 * @cb: control buffer, for persistent variables of get_next_block()
 */
struct Model1_ts_state
{
 unsigned int Model1_offset;
 char Model1_cb[40];
};

/**
 * struct ts_ops - search module operations
 * @name: name of search algorithm
 * @init: initialization function to prepare a search
 * @find: find the next occurrence of the pattern
 * @destroy: destroy algorithm specific parts of a search configuration
 * @get_pattern: return head of pattern
 * @get_pattern_len: return length of pattern
 * @owner: module reference to algorithm
 */
struct Model1_ts_ops
{
 const char *Model1_name;
 struct Model1_ts_config * (*Model1_init)(const void *, unsigned int, Model1_gfp_t, int);
 unsigned int (*Model1_find)(struct Model1_ts_config *,
     struct Model1_ts_state *);
 void (*Model1_destroy)(struct Model1_ts_config *);
 void * (*Model1_get_pattern)(struct Model1_ts_config *);
 unsigned int (*Model1_get_pattern_len)(struct Model1_ts_config *);
 struct Model1_module *Model1_owner;
 struct Model1_list_head Model1_list;
};

/**
 * struct ts_config - search configuration
 * @ops: operations of chosen algorithm
 * @flags: flags
 * @get_next_block: callback to fetch the next block to search in
 * @finish: callback to finalize a search
 */
struct Model1_ts_config
{
 struct Model1_ts_ops *Model1_ops;
 int Model1_flags;

 /**
	 * get_next_block - fetch next block of data
	 * @consumed: number of bytes consumed by the caller
	 * @dst: destination buffer
	 * @conf: search configuration
	 * @state: search state
	 *
	 * Called repeatedly until 0 is returned. Must assign the
	 * head of the next block of data to &*dst and return the length
	 * of the block or 0 if at the end. consumed == 0 indicates
	 * a new search. May store/read persistent values in state->cb.
	 */
 unsigned int (*Model1_get_next_block)(unsigned int Model1_consumed,
        const Model1_u8 **Model1_dst,
        struct Model1_ts_config *Model1_conf,
        struct Model1_ts_state *Model1_state);

 /**
	 * finish - finalize/clean a series of get_next_block() calls
	 * @conf: search configuration
	 * @state: search state
	 *
	 * Called after the last use of get_next_block(), may be used
	 * to cleanup any leftovers.
	 */
 void (*Model1_finish)(struct Model1_ts_config *Model1_conf,
       struct Model1_ts_state *Model1_state);
};

/**
 * textsearch_next - continue searching for a pattern
 * @conf: search configuration
 * @state: search state
 *
 * Continues a search looking for more occurrences of the pattern.
 * textsearch_find() must be called to find the first occurrence
 * in order to reset the state.
 *
 * Returns the position of the next occurrence of the pattern or
 * UINT_MAX if not match was found.
 */
static inline __attribute__((no_instrument_function)) unsigned int Model1_textsearch_next(struct Model1_ts_config *Model1_conf,
        struct Model1_ts_state *Model1_state)
{
 unsigned int Model1_ret = Model1_conf->Model1_ops->Model1_find(Model1_conf, Model1_state);

 if (Model1_conf->Model1_finish)
  Model1_conf->Model1_finish(Model1_conf, Model1_state);

 return Model1_ret;
}

/**
 * textsearch_find - start searching for a pattern
 * @conf: search configuration
 * @state: search state
 *
 * Returns the position of first occurrence of the pattern or
 * UINT_MAX if no match was found.
 */
static inline __attribute__((no_instrument_function)) unsigned int Model1_textsearch_find(struct Model1_ts_config *Model1_conf,
        struct Model1_ts_state *Model1_state)
{
 Model1_state->Model1_offset = 0;
 return Model1_textsearch_next(Model1_conf, Model1_state);
}

/**
 * textsearch_get_pattern - return head of the pattern
 * @conf: search configuration
 */
static inline __attribute__((no_instrument_function)) void *Model1_textsearch_get_pattern(struct Model1_ts_config *Model1_conf)
{
 return Model1_conf->Model1_ops->Model1_get_pattern(Model1_conf);
}

/**
 * textsearch_get_pattern_len - return length of the pattern
 * @conf: search configuration
 */
static inline __attribute__((no_instrument_function)) unsigned int Model1_textsearch_get_pattern_len(struct Model1_ts_config *Model1_conf)
{
 return Model1_conf->Model1_ops->Model1_get_pattern_len(Model1_conf);
}

extern int Model1_textsearch_register(struct Model1_ts_ops *);
extern int Model1_textsearch_unregister(struct Model1_ts_ops *);
extern struct Model1_ts_config *Model1_textsearch_prepare(const char *, const void *,
         unsigned int, Model1_gfp_t, int);
extern void Model1_textsearch_destroy(struct Model1_ts_config *Model1_conf);
extern unsigned int Model1_textsearch_find_continuous(struct Model1_ts_config *,
            struct Model1_ts_state *,
            const void *, unsigned int);





static inline __attribute__((no_instrument_function)) struct Model1_ts_config *Model1_alloc_ts_config(Model1_size_t Model1_payload,
      Model1_gfp_t Model1_gfp_mask)
{
 struct Model1_ts_config *Model1_conf;

 Model1_conf = Model1_kzalloc((((sizeof(*Model1_conf)) + 8 -1) & ~(8 -1)) + Model1_payload, Model1_gfp_mask);
 if (Model1_conf == ((void *)0))
  return Model1_ERR_PTR(-12);

 return Model1_conf;
}

static inline __attribute__((no_instrument_function)) void *Model1_ts_config_priv(struct Model1_ts_config *Model1_conf)
{
 return ((Model1_u8 *) Model1_conf + (((sizeof(struct Model1_ts_config)) + 8 -1) & ~(8 -1)));
}
/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Checksumming functions for IP, TCP, UDP and so on
 *
 * Authors:	Jorge Cwik, <jorge@laser.satlink.net>
 *		Arnt Gulbrandsen, <agulbra@nvg.unit.no>
 *		Borrows very liberally from tcp.c and ip.c, see those
 *		files for more names.
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */










/*
 * User space memory access functions
 */










static inline __attribute__((no_instrument_function)) void Model1_kasan_check_read(const void *Model1_p, unsigned int Model1_size) { }
static inline __attribute__((no_instrument_function)) void Model1_kasan_check_write(const void *Model1_p, unsigned int Model1_size) { }




/*
 * Supervisor Mode Access Prevention support
 *
 * Copyright (C) 2012 Intel Corporation
 * Author: H. Peter Anvin <hpa@linux.intel.com>
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public License
 * as published by the Free Software Foundation; version 2
 * of the License.
 */
/* "Raw" instruction opcodes */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_clac(void)
{
 /* Note: a barrier is implicit in alternative() */
 asm volatile ("661:\n\t" "" "\n662:\n" ".skip -(((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")) > 0) * " "((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")),0x90\n" "663" ":\n" ".pushsection .altinstructions,\"a\"\n" " .long 661b - .\n" " .long " "664""1""f - .\n" " .word " "( 9*32+20)" "\n" " .byte " "663""b-661b" "\n" " .byte " "665""1""f-""664""1""f" "\n" " .byte " "663""b-662b" "\n" ".popsection\n" ".pushsection .altinstr_replacement, \"ax\"\n" "664""1"":\n\t" ".byte 0x0f,0x01,0xca" "\n" "665""1" ":\n\t" ".popsection" : : : "memory");
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_stac(void)
{
 /* Note: a barrier is implicit in alternative() */
 asm volatile ("661:\n\t" "" "\n662:\n" ".skip -(((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")) > 0) * " "((" "665""1""f-""664""1""f" ")-(" "662b-661b" ")),0x90\n" "663" ":\n" ".pushsection .altinstructions,\"a\"\n" " .long 661b - .\n" " .long " "664""1""f - .\n" " .word " "( 9*32+20)" "\n" " .byte " "663""b-661b" "\n" " .byte " "665""1""f-""664""1""f" "\n" " .byte " "663""b-662b" "\n" ".popsection\n" ".pushsection .altinstr_replacement, \"ax\"\n" "664""1"":\n\t" ".byte 0x0f,0x01,0xcb" "\n" "665""1" ":\n\t" ".popsection" : : : "memory");
}

/* These macros can be used in asm() statements */




/*
 * The fs value determines whether argument validity checking should be
 * performed or not.  If get_fs() == USER_DS, checking is performed, with
 * get_fs() == KERNEL_DS, checking is bypassed.
 *
 * For historical reasons, these macros are grossly misnamed.
 */
/*
 * Test whether a block of memory is a valid user space address.
 * Returns 0 if the range is valid, nonzero otherwise.
 */
static inline __attribute__((no_instrument_function)) bool Model1___chk_range_not_ok(unsigned long Model1_addr, unsigned long Model1_size, unsigned long Model1_limit)
{
 /*
	 * If we have used "sizeof()" for the size,
	 * we know it won't overflow the limit (but
	 * it might overflow the 'addr', so it's
	 * important to subtract the size from the
	 * limit, not add it to the address).
	 */
 if (__builtin_constant_p(Model1_size))
  return __builtin_expect(!!(Model1_addr > Model1_limit - Model1_size), 0);

 /* Arbitrary sizes? Be careful about overflow */
 Model1_addr += Model1_size;
 if (__builtin_expect(!!(Model1_addr < Model1_size), 0))
  return true;
 return __builtin_expect(!!(Model1_addr > Model1_limit), 0);
}







/**
 * access_ok: - Checks if a user space pointer is valid
 * @type: Type of access: %VERIFY_READ or %VERIFY_WRITE.  Note that
 *        %VERIFY_WRITE is a superset of %VERIFY_READ - if it is safe
 *        to write to a block, it is always safe to read from it.
 * @addr: User space pointer to start of block to check
 * @size: Size of block to check
 *
 * Context: User context only. This function may sleep if pagefaults are
 *          enabled.
 *
 * Checks if a pointer to a block of memory in user space is valid.
 *
 * Returns true (nonzero) if the memory block may be valid, false (zero)
 * if it is definitely invalid.
 *
 * Note that, depending on architecture, this function probably just
 * checks that the pointer is in the user space range - after calling
 * this function, memory access functions may still return -EFAULT.
 */



/*
 * The exception table consists of triples of addresses relative to the
 * exception table entry itself. The first address is of an instruction
 * that is allowed to fault, the second is the target at which the program
 * should continue. The third is a handler function to deal with the fault
 * caused by the instruction in the first field.
 *
 * All the routines below use bits of fixup code that are out of line
 * with the main instruction path.  This means when everything is well,
 * we don't even have to jump over them.  Further, they do not intrude
 * on our cache or tlb entries.
 */

struct Model1_exception_table_entry {
 int Model1_insn, Model1_fixup, Model1_handler;
};
extern int Model1_fixup_exception(struct Model1_pt_regs *Model1_regs, int Model1_trapnr);
extern bool Model1_ex_has_fault_handler(unsigned long Model1_ip);
extern void Model1_early_fixup_exception(struct Model1_pt_regs *Model1_regs, int Model1_trapnr);

/*
 * These are the main single-value transfer routines.  They automatically
 * use the right size if we just have the right pointer type.
 *
 * This gets kind of ugly. We want to return _two_ values in "get_user()"
 * and yet we don't want to do any pointers, because that is too much
 * of a performance impact. Thus we have a few rather ugly macros here,
 * and hide all the ugliness from the user.
 *
 * The "__xxx" versions of the user access functions are versions that
 * do not verify the address space, that must have been done previously
 * with a separate "access_ok()" call (this is used when we do multiple
 * accesses to the same area of user memory).
 */

extern int Model1___get_user_1(void);
extern int Model1___get_user_2(void);
extern int Model1___get_user_4(void);
extern int Model1___get_user_8(void);
extern int Model1___get_user_bad(void);




/*
 * This is a type: either unsigned long, if the argument fits into
 * that type, or otherwise unsigned long long.
 */



/**
 * get_user: - Get a simple variable from user space.
 * @x:   Variable to store result.
 * @ptr: Source address, in user space.
 *
 * Context: User context only. This function may sleep if pagefaults are
 *          enabled.
 *
 * This macro copies a single simple variable from user space to kernel
 * space.  It supports simple types like char and int, but not larger
 * data types like structures or arrays.
 *
 * @ptr must have pointer-to-simple-variable type, and the result of
 * dereferencing @ptr must be assignable to @x without a cast.
 *
 * Returns zero on success, or -EFAULT on error.
 * On error, the variable @x is set to zero.
 */
/*
 * Careful: we have to cast the result to the type of the pointer
 * for sign reasons.
 *
 * The use of _ASM_DX as the register specifier is a bit of a
 * simplification, as gcc only cares about it as the starting point
 * and not size: for a 64-bit value it will use %ecx:%edx on 32 bits
 * (%ecx being the next register in gcc's x86 register sequence), and
 * %rdx on 64 bits.
 *
 * Clang/LLVM cares about the size of the register, but still wants
 * the base register for something that ends up being a pair.
 */
extern void Model1___put_user_bad(void);

/*
 * Strange magic calling convention: pointer in %ecx,
 * value in %eax(:%edx), return value in %eax. clobbers %rbx
 */
extern void Model1___put_user_1(void);
extern void Model1___put_user_2(void);
extern void Model1___put_user_4(void);
extern void Model1___put_user_8(void);

/**
 * put_user: - Write a simple value into user space.
 * @x:   Value to copy to user space.
 * @ptr: Destination address, in user space.
 *
 * Context: User context only. This function may sleep if pagefaults are
 *          enabled.
 *
 * This macro copies a single simple value from kernel space to user
 * space.  It supports simple types like char and int, but not larger
 * data types like structures or arrays.
 *
 * @ptr must have pointer-to-simple-variable type, and @x must be assignable
 * to the result of dereferencing @ptr.
 *
 * Returns zero on success, or -EFAULT on error.
 */
/*
 * This doesn't do __uaccess_begin/end - the exception handling
 * around it must do that.
 */
/*
 * This doesn't do __uaccess_begin/end - the exception handling
 * around it must do that.
 */
/* FIXME: this hack is definitely wrong -AK */
struct Model1___large_struct { unsigned long Model1_buf[100]; };


/*
 * Tell gcc we read from memory instead of writing: this is because
 * we do not write to any memory gcc knows about, so there are no
 * aliasing issues.
 */
/*
 * uaccess_try and catch
 */
/**
 * __get_user: - Get a simple variable from user space, with less checking.
 * @x:   Variable to store result.
 * @ptr: Source address, in user space.
 *
 * Context: User context only. This function may sleep if pagefaults are
 *          enabled.
 *
 * This macro copies a single simple variable from user space to kernel
 * space.  It supports simple types like char and int, but not larger
 * data types like structures or arrays.
 *
 * @ptr must have pointer-to-simple-variable type, and the result of
 * dereferencing @ptr must be assignable to @x without a cast.
 *
 * Caller must check the pointer with access_ok() before calling this
 * function.
 *
 * Returns zero on success, or -EFAULT on error.
 * On error, the variable @x is set to zero.
 */




/**
 * __put_user: - Write a simple value into user space, with less checking.
 * @x:   Value to copy to user space.
 * @ptr: Destination address, in user space.
 *
 * Context: User context only. This function may sleep if pagefaults are
 *          enabled.
 *
 * This macro copies a single simple value from kernel space to user
 * space.  It supports simple types like char and int, but not larger
 * data types like structures or arrays.
 *
 * @ptr must have pointer-to-simple-variable type, and @x must be assignable
 * to the result of dereferencing @ptr.
 *
 * Caller must check the pointer with access_ok() before calling this
 * function.
 *
 * Returns zero on success, or -EFAULT on error.
 */







/*
 * {get|put}_user_try and catch
 *
 * get_user_try {
 *	get_user_ex(...);
 * } get_user_catch(err)
 */
extern unsigned long
Model1_copy_from_user_nmi(void *Model1_to, const void *Model1_from, unsigned long Model1_n);
extern __attribute__((warn_unused_result)) long
Model1_strncpy_from_user(char *Model1_dst, const char *Model1_src, long Model1_count);

extern __attribute__((warn_unused_result)) long Model1_strlen_user(const char *Model1_str);
extern __attribute__((warn_unused_result)) long Model1_strnlen_user(const char *Model1_str, long Model1_n);

unsigned long __attribute__((warn_unused_result)) Model1_clear_user(void *Model1_mem, unsigned long Model1_len);
unsigned long __attribute__((warn_unused_result)) Model1___clear_user(void *Model1_mem, unsigned long Model1_len);

extern void Model1___cmpxchg_wrong_size(void)
                                                     ;
/*
 * movsl can be slow when source and dest are not both 8-byte aligned
 */



/*
 * User space memory access functions
 */
/*
 * Copy To/From Userspace
 */

/* Handles exceptions in both to and from, but doesn't do access_ok */
__attribute__((warn_unused_result)) unsigned long
Model1_copy_user_enhanced_fast_string(void *Model1_to, const void *Model1_from, unsigned Model1_len);
__attribute__((warn_unused_result)) unsigned long
Model1_copy_user_generic_string(void *Model1_to, const void *Model1_from, unsigned Model1_len);
__attribute__((warn_unused_result)) unsigned long
Model1_copy_user_generic_unrolled(void *Model1_to, const void *Model1_from, unsigned Model1_len);

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) __attribute__((warn_unused_result)) unsigned long
Model1_copy_user_generic(void *Model1_to, const void *Model1_from, unsigned Model1_len)
{
 unsigned Model1_ret;

 /*
	 * If CPU has ERMS feature, use copy_user_enhanced_fast_string.
	 * Otherwise, if CPU has rep_good feature, use copy_user_generic_string.
	 * Otherwise, use copy_user_generic_unrolled.
	 */
 asm volatile ("661:\n\t" "call %P[old]" "\n662:\n" ".skip -((" "((" "665""1""f-""664""1""f" ") ^ (((" "665""1""f-""664""1""f" ") ^ (" "665""2""f-""664""2""f" ")) & -(-((" "665""1""f-""664""1""f" ") - (" "665""2""f-""664""2""f" ")))))" " - (" "662b-661b" ")) > 0) * " "(" "((" "665""1""f-""664""1""f" ") ^ (((" "665""1""f-""664""1""f" ") ^ (" "665""2""f-""664""2""f" ")) & -(-((" "665""1""f-""664""1""f" ") - (" "665""2""f-""664""2""f" ")))))" " - (" "662b-661b" ")), 0x90\n" "663" ":\n" ".pushsection .altinstructions,\"a\"\n" " .long 661b - .\n" " .long " "664""1""f - .\n" " .word " "( 3*32+16)" "\n" " .byte " "663""b-661b" "\n" " .byte " "665""1""f-""664""1""f" "\n" " .byte " "663""b-662b" "\n" " .long 661b - .\n" " .long " "664""2""f - .\n" " .word " "( 9*32+ 9)" "\n" " .byte " "663""b-661b" "\n" " .byte " "665""2""f-""664""2""f" "\n" " .byte " "663""b-662b" "\n" ".popsection\n" ".pushsection .altinstr_replacement, \"ax\"\n" "664""1"":\n\t" "call %P[new1]" "\n" "665""1" ":\n\t" "664""2"":\n\t" "call %P[new2]" "\n" "665""2" ":\n\t" ".popsection" : "=a" (Model1_ret), "=D" (Model1_to), "=S" (Model1_from), "=d" (Model1_len) : [old] "i" (Model1_copy_user_generic_unrolled), [new1] "i" (Model1_copy_user_generic_string), [new2] "i" (Model1_copy_user_enhanced_fast_string), "1" (Model1_to), "2" (Model1_from), "3" (Model1_len) : "memory", "rcx", "r8", "r9", "r10", "r11");
 return Model1_ret;
}

__attribute__((warn_unused_result)) unsigned long
Model1_copy_in_user(void *Model1_to, const void *Model1_from, unsigned Model1_len);

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) __attribute__((warn_unused_result))
int Model1___copy_from_user_nocheck(void *Model1_dst, const void *Model1_src, unsigned Model1_size)
{
 int Model1_ret = 0;

 Model1_check_object_size(Model1_dst, Model1_size, false);
 if (!__builtin_constant_p(Model1_size))
  return Model1_copy_user_generic(Model1_dst, ( void *)Model1_src, Model1_size);
 switch (Model1_size) {
 case 1:
  Model1_stac();
  asm volatile("\n" "1:	mov""b"" %2,%""b""1\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	xor""b"" %""b""1,%""b""1\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r" (Model1_ret), "=q"(*(Model1_u8 *)Model1_dst) : "m" ((*(struct Model1___large_struct *)((Model1_u8 *)Model1_src))), "i" (1), "0" (Model1_ret));

  Model1_clac();
  return Model1_ret;
 case 2:
  Model1_stac();
  asm volatile("\n" "1:	mov""w"" %2,%""w""1\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	xor""w"" %""w""1,%""w""1\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r" (Model1_ret), "=r"(*(Model1_u16 *)Model1_dst) : "m" ((*(struct Model1___large_struct *)((Model1_u16 *)Model1_src))), "i" (2), "0" (Model1_ret));

  Model1_clac();
  return Model1_ret;
 case 4:
  Model1_stac();
  asm volatile("\n" "1:	mov""l"" %2,%""k""1\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	xor""l"" %""k""1,%""k""1\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r" (Model1_ret), "=r"(*(Model1_u32 *)Model1_dst) : "m" ((*(struct Model1___large_struct *)((Model1_u32 *)Model1_src))), "i" (4), "0" (Model1_ret));

  Model1_clac();
  return Model1_ret;
 case 8:
  Model1_stac();
  asm volatile("\n" "1:	mov""q"" %2,%""""1\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	xor""q"" %""""1,%""""1\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r" (Model1_ret), "=r"(*(Model1_u64 *)Model1_dst) : "m" ((*(struct Model1___large_struct *)((Model1_u64 *)Model1_src))), "i" (8), "0" (Model1_ret));

  Model1_clac();
  return Model1_ret;
 case 10:
  Model1_stac();
  asm volatile("\n" "1:	mov""q"" %2,%""""1\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	xor""q"" %""""1,%""""1\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r" (Model1_ret), "=r"(*(Model1_u64 *)Model1_dst) : "m" ((*(struct Model1___large_struct *)((Model1_u64 *)Model1_src))), "i" (10), "0" (Model1_ret));

  if (__builtin_expect(!!(!Model1_ret), 1))
   asm volatile("\n" "1:	mov""w"" %2,%""w""1\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	xor""w"" %""w""1,%""w""1\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r" (Model1_ret), "=r"(*(Model1_u16 *)(8 + (char *)Model1_dst)) : "m" ((*(struct Model1___large_struct *)((Model1_u16 *)(8 + (char *)Model1_src)))), "i" (2), "0" (Model1_ret));


  Model1_clac();
  return Model1_ret;
 case 16:
  Model1_stac();
  asm volatile("\n" "1:	mov""q"" %2,%""""1\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	xor""q"" %""""1,%""""1\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r" (Model1_ret), "=r"(*(Model1_u64 *)Model1_dst) : "m" ((*(struct Model1___large_struct *)((Model1_u64 *)Model1_src))), "i" (16), "0" (Model1_ret));

  if (__builtin_expect(!!(!Model1_ret), 1))
   asm volatile("\n" "1:	mov""q"" %2,%""""1\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	xor""q"" %""""1,%""""1\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r" (Model1_ret), "=r"(*(Model1_u64 *)(8 + (char *)Model1_dst)) : "m" ((*(struct Model1___large_struct *)((Model1_u64 *)(8 + (char *)Model1_src)))), "i" (8), "0" (Model1_ret));


  Model1_clac();
  return Model1_ret;
 default:
  return Model1_copy_user_generic(Model1_dst, ( void *)Model1_src, Model1_size);
 }
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) __attribute__((warn_unused_result))
int Model1___copy_from_user(void *Model1_dst, const void *Model1_src, unsigned Model1_size)
{
 Model1_might_fault();
 Model1_kasan_check_write(Model1_dst, Model1_size);
 return Model1___copy_from_user_nocheck(Model1_dst, Model1_src, Model1_size);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) __attribute__((warn_unused_result))
int Model1___copy_to_user_nocheck(void *Model1_dst, const void *Model1_src, unsigned Model1_size)
{
 int Model1_ret = 0;

 Model1_check_object_size(Model1_src, Model1_size, true);
 if (!__builtin_constant_p(Model1_size))
  return Model1_copy_user_generic(( void *)Model1_dst, Model1_src, Model1_size);
 switch (Model1_size) {
 case 1:
  Model1_stac();
  asm volatile("\n" "1:	mov""b"" %""b""1,%2\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r"(Model1_ret) : "iq"(*(Model1_u8 *)Model1_src), "m" ((*(struct Model1___large_struct *)((Model1_u8 *)Model1_dst))), "i" (1), "0" (Model1_ret));

  Model1_clac();
  return Model1_ret;
 case 2:
  Model1_stac();
  asm volatile("\n" "1:	mov""w"" %""w""1,%2\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r"(Model1_ret) : "ir"(*(Model1_u16 *)Model1_src), "m" ((*(struct Model1___large_struct *)((Model1_u16 *)Model1_dst))), "i" (2), "0" (Model1_ret));

  Model1_clac();
  return Model1_ret;
 case 4:
  Model1_stac();
  asm volatile("\n" "1:	mov""l"" %""k""1,%2\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r"(Model1_ret) : "ir"(*(Model1_u32 *)Model1_src), "m" ((*(struct Model1___large_struct *)((Model1_u32 *)Model1_dst))), "i" (4), "0" (Model1_ret));

  Model1_clac();
  return Model1_ret;
 case 8:
  Model1_stac();
  asm volatile("\n" "1:	mov""q"" %""""1,%2\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r"(Model1_ret) : "er"(*(Model1_u64 *)Model1_src), "m" ((*(struct Model1___large_struct *)((Model1_u64 *)Model1_dst))), "i" (8), "0" (Model1_ret));

  Model1_clac();
  return Model1_ret;
 case 10:
  Model1_stac();
  asm volatile("\n" "1:	mov""q"" %""""1,%2\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r"(Model1_ret) : "er"(*(Model1_u64 *)Model1_src), "m" ((*(struct Model1___large_struct *)((Model1_u64 *)Model1_dst))), "i" (10), "0" (Model1_ret));

  if (__builtin_expect(!!(!Model1_ret), 1)) {
   asm("":::"memory");
   asm volatile("\n" "1:	mov""w"" %""w""1,%2\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r"(Model1_ret) : "ir"(4[(Model1_u16 *)Model1_src]), "m" ((*(struct Model1___large_struct *)(4 + (Model1_u16 *)Model1_dst))), "i" (2), "0" (Model1_ret));

  }
  Model1_clac();
  return Model1_ret;
 case 16:
  Model1_stac();
  asm volatile("\n" "1:	mov""q"" %""""1,%2\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r"(Model1_ret) : "er"(*(Model1_u64 *)Model1_src), "m" ((*(struct Model1___large_struct *)((Model1_u64 *)Model1_dst))), "i" (16), "0" (Model1_ret));

  if (__builtin_expect(!!(!Model1_ret), 1)) {
   asm("":::"memory");
   asm volatile("\n" "1:	mov""q"" %""""1,%2\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r"(Model1_ret) : "er"(1[(Model1_u64 *)Model1_src]), "m" ((*(struct Model1___large_struct *)(1 + (Model1_u64 *)Model1_dst))), "i" (8), "0" (Model1_ret));

  }
  Model1_clac();
  return Model1_ret;
 default:
  return Model1_copy_user_generic(( void *)Model1_dst, Model1_src, Model1_size);
 }
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) __attribute__((warn_unused_result))
int Model1___copy_to_user(void *Model1_dst, const void *Model1_src, unsigned Model1_size)
{
 Model1_might_fault();
 Model1_kasan_check_read(Model1_src, Model1_size);
 return Model1___copy_to_user_nocheck(Model1_dst, Model1_src, Model1_size);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) __attribute__((warn_unused_result))
int Model1___copy_in_user(void *Model1_dst, const void *Model1_src, unsigned Model1_size)
{
 int Model1_ret = 0;

 Model1_might_fault();
 if (!__builtin_constant_p(Model1_size))
  return Model1_copy_user_generic(( void *)Model1_dst,
      ( void *)Model1_src, Model1_size);
 switch (Model1_size) {
 case 1: {
  Model1_u8 Model1_tmp;
  Model1_stac();
  asm volatile("\n" "1:	mov""b"" %2,%""b""1\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	xor""b"" %""b""1,%""b""1\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r" (Model1_ret), "=q"(Model1_tmp) : "m" ((*(struct Model1___large_struct *)((Model1_u8 *)Model1_src))), "i" (1), "0" (Model1_ret));

  if (__builtin_expect(!!(!Model1_ret), 1))
   asm volatile("\n" "1:	mov""b"" %""b""1,%2\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r"(Model1_ret) : "iq"(Model1_tmp), "m" ((*(struct Model1___large_struct *)((Model1_u8 *)Model1_dst))), "i" (1), "0" (Model1_ret));

  Model1_clac();
  return Model1_ret;
 }
 case 2: {
  Model1_u16 Model1_tmp;
  Model1_stac();
  asm volatile("\n" "1:	mov""w"" %2,%""w""1\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	xor""w"" %""w""1,%""w""1\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r" (Model1_ret), "=r"(Model1_tmp) : "m" ((*(struct Model1___large_struct *)((Model1_u16 *)Model1_src))), "i" (2), "0" (Model1_ret));

  if (__builtin_expect(!!(!Model1_ret), 1))
   asm volatile("\n" "1:	mov""w"" %""w""1,%2\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r"(Model1_ret) : "ir"(Model1_tmp), "m" ((*(struct Model1___large_struct *)((Model1_u16 *)Model1_dst))), "i" (2), "0" (Model1_ret));

  Model1_clac();
  return Model1_ret;
 }

 case 4: {
  Model1_u32 Model1_tmp;
  Model1_stac();
  asm volatile("\n" "1:	mov""l"" %2,%""k""1\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	xor""l"" %""k""1,%""k""1\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r" (Model1_ret), "=r"(Model1_tmp) : "m" ((*(struct Model1___large_struct *)((Model1_u32 *)Model1_src))), "i" (4), "0" (Model1_ret));

  if (__builtin_expect(!!(!Model1_ret), 1))
   asm volatile("\n" "1:	mov""l"" %""k""1,%2\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r"(Model1_ret) : "ir"(Model1_tmp), "m" ((*(struct Model1___large_struct *)((Model1_u32 *)Model1_dst))), "i" (4), "0" (Model1_ret));

  Model1_clac();
  return Model1_ret;
 }
 case 8: {
  Model1_u64 Model1_tmp;
  Model1_stac();
  asm volatile("\n" "1:	mov""q"" %2,%""""1\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	xor""q"" %""""1,%""""1\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r" (Model1_ret), "=r"(Model1_tmp) : "m" ((*(struct Model1___large_struct *)((Model1_u64 *)Model1_src))), "i" (8), "0" (Model1_ret));

  if (__builtin_expect(!!(!Model1_ret), 1))
   asm volatile("\n" "1:	mov""q"" %""""1,%2\n" "2:\n" ".section .fixup,\"ax\"\n" "3:	mov %3,%0\n" "	jmp 2b\n" ".previous\n" " .pushsection \"__ex_table\",\"a\"\n" " .balign 4\n" " .long (" "1b" ") - .\n" " .long (" "3b" ") - .\n" " .long (" "ex_handler_default" ") - .\n" " .popsection\n" : "=r"(Model1_ret) : "er"(Model1_tmp), "m" ((*(struct Model1___large_struct *)((Model1_u64 *)Model1_dst))), "i" (8), "0" (Model1_ret));

  Model1_clac();
  return Model1_ret;
 }
 default:
  return Model1_copy_user_generic(( void *)Model1_dst,
      ( void *)Model1_src, Model1_size);
 }
}

static __attribute__((warn_unused_result)) inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int
Model1___copy_from_user_inatomic(void *Model1_dst, const void *Model1_src, unsigned Model1_size)
{
 Model1_kasan_check_write(Model1_dst, Model1_size);
 return Model1___copy_from_user_nocheck(Model1_dst, Model1_src, Model1_size);
}

static __attribute__((warn_unused_result)) inline __attribute__((no_instrument_function)) __attribute__((always_inline)) int
Model1___copy_to_user_inatomic(void *Model1_dst, const void *Model1_src, unsigned Model1_size)
{
 Model1_kasan_check_read(Model1_src, Model1_size);
 return Model1___copy_to_user_nocheck(Model1_dst, Model1_src, Model1_size);
}

extern long Model1___copy_user_nocache(void *Model1_dst, const void *Model1_src,
    unsigned Model1_size, int Model1_zerorest);

static inline __attribute__((no_instrument_function)) int
Model1___copy_from_user_nocache(void *Model1_dst, const void *Model1_src, unsigned Model1_size)
{
 Model1_might_fault();
 Model1_kasan_check_write(Model1_dst, Model1_size);
 return Model1___copy_user_nocache(Model1_dst, Model1_src, Model1_size, 1);
}

static inline __attribute__((no_instrument_function)) int
Model1___copy_from_user_inatomic_nocache(void *Model1_dst, const void *Model1_src,
      unsigned Model1_size)
{
 Model1_kasan_check_write(Model1_dst, Model1_size);
 return Model1___copy_user_nocache(Model1_dst, Model1_src, Model1_size, 0);
}

unsigned long
Model1_copy_user_handle_tail(char *Model1_to, char *Model1_from, unsigned Model1_len);


unsigned long __attribute__((warn_unused_result)) Model1__copy_from_user(void *Model1_to, const void *Model1_from,
        unsigned Model1_n);
unsigned long __attribute__((warn_unused_result)) Model1__copy_to_user(void *Model1_to, const void *Model1_from,
      unsigned Model1_n);

extern void
Model1___bad_copy_user(void);

static inline __attribute__((no_instrument_function)) void Model1_copy_user_overflow(int Model1_size, unsigned long Model1_count)
{
 ({ int Model1___ret_warn_on = !!(1); if (__builtin_expect(!!(Model1___ret_warn_on), 0)) Model1_warn_slowpath_fmt("./arch/x86/include/asm/uaccess.h", 709, "Buffer overflow detected (%d < %lu)!\n", Model1_size, Model1_count); __builtin_expect(!!(Model1___ret_warn_on), 0); });
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) unsigned long __attribute__((warn_unused_result))
Model1_copy_from_user(void *Model1_to, const void *Model1_from, unsigned long Model1_n)
{
 int Model1_sz = __builtin_object_size(Model1_to, 0);

 Model1_might_fault();

 Model1_kasan_check_write(Model1_to, Model1_n);

 if (__builtin_expect(!!(Model1_sz < 0 || Model1_sz >= Model1_n), 1)) {
  Model1_check_object_size(Model1_to, Model1_n, false);
  Model1_n = Model1__copy_from_user(Model1_to, Model1_from, Model1_n);
 } else if (!__builtin_constant_p(Model1_n))
  Model1_copy_user_overflow(Model1_sz, Model1_n);
 else
  Model1___bad_copy_user();

 return Model1_n;
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) unsigned long __attribute__((warn_unused_result))
Model1_copy_to_user(void *Model1_to, const void *Model1_from, unsigned long Model1_n)
{
 int Model1_sz = __builtin_object_size(Model1_from, 0);

 Model1_kasan_check_read(Model1_from, Model1_n);

 Model1_might_fault();

 if (__builtin_expect(!!(Model1_sz < 0 || Model1_sz >= Model1_n), 1)) {
  Model1_check_object_size(Model1_from, Model1_n, true);
  Model1_n = Model1__copy_to_user(Model1_to, Model1_from, Model1_n);
 } else if (!__builtin_constant_p(Model1_n))
  Model1_copy_user_overflow(Model1_sz, Model1_n);
 else
  Model1___bad_copy_user();

 return Model1_n;
}

/*
 * We rely on the nested NMI work to allow atomic faults from the NMI path; the
 * nested NMI paths are careful to preserve CR2.
 *
 * Caller must use pagefault_enable/disable, or run in interrupt context,
 * and also do a uaccess_ok() check
 */


/*
 * The "unsafe" user accesses aren't really "unsafe", but the naming
 * is a big fat warning: you have to not only do the access_ok()
 * checking before using them, but you have to surround them with the
 * user_access_begin/end() pair.
 */






/*
 * Checksums for x86-64
 * Copyright 2002 by Andi Kleen, SuSE Labs
 * with some code from asm-x86/checksum.h
 */





/**
 * csum_fold - Fold and invert a 32bit checksum.
 * sum: 32bit unfolded sum
 *
 * Fold a 32bit running checksum to 16bit and invert it. This is usually
 * the last step before putting a checksum into a packet.
 * Make sure not to mix with 64bit checksums.
 */
static inline __attribute__((no_instrument_function)) Model1___sum16 Model1_csum_fold(Model1___wsum Model1_sum)
{
#if CY_ABSTRACT1
    //We assume no csum is used
    return 0;
#else
 asm("  addl %1,%0\n"
     "  adcl $0xffff,%0"
     : "=r" (Model1_sum)
     : "r" (( Model1_u32)Model1_sum << 16),
       "0" (( Model1_u32)Model1_sum & 0xffff0000));
 return ( Model1___sum16)(~( Model1_u32)Model1_sum >> 16);
#endif
}

/*
 *	This is a version of ip_compute_csum() optimized for IP headers,
 *	which always checksum on 4 octet boundaries.
 *
 *	By Jorge Cwik <jorge@laser.satlink.net>, adapted for linux by
 *	Arnt Gulbrandsen.
 */

/**
 * ip_fast_csum - Compute the IPv4 header checksum efficiently.
 * iph: ipv4 header
 * ihl: length of header / 4
 */
static inline __attribute__((no_instrument_function)) Model1___sum16 Model1_ip_fast_csum(const void *Model1_iph, unsigned int Model1_ihl)
{
#if CY_ABSTRACT1
    //Not sure if it will be called, but we assume no csum is used
    return 0;
#else
 unsigned int Model1_sum;

 asm("  movl (%1), %0\n"
     "  subl $4, %2\n"
     "  jbe 2f\n"
     "  addl 4(%1), %0\n"
     "  adcl 8(%1), %0\n"
     "  adcl 12(%1), %0\n"
     "1: adcl 16(%1), %0\n"
     "  lea 4(%1), %1\n"
     "  decl %2\n"
     "  jne	1b\n"
     "  adcl $0, %0\n"
     "  movl %0, %2\n"
     "  shrl $16, %0\n"
     "  addw %w2, %w0\n"
     "  adcl $0, %0\n"
     "  notl %0\n"
     "2:"
 /* Since the input registers which are loaded with iph and ihl
	   are modified, we must also specify them as outputs, or gcc
	   will assume they contain their original values. */
     : "=r" (Model1_sum), "=r" (Model1_iph), "=r" (Model1_ihl)
     : "1" (Model1_iph), "2" (Model1_ihl)
     : "memory");
 return ( Model1___sum16)Model1_sum;
#endif
}

/**
 * csum_tcpup_nofold - Compute an IPv4 pseudo header checksum.
 * @saddr: source address
 * @daddr: destination address
 * @len: length of packet
 * @proto: ip protocol of packet
 * @sum: initial sum to be added in (32bit unfolded)
 *
 * Returns the pseudo header checksum the input data. Result is
 * 32bit unfolded.
 */
static inline __attribute__((no_instrument_function)) Model1___wsum
Model1_csum_tcpudp_nofold(Model1___be32 Model1_saddr, Model1___be32 Model1_daddr, __u32 Model1_len,
     __u8 Model1_proto, Model1___wsum Model1_sum)
{
#if CY_ABSTRACT1
    //We assume no csum is used
    return 0;
#else
 asm("  addl %1, %0\n"
     "  adcl %2, %0\n"
     "  adcl %3, %0\n"
     "  adcl $0, %0\n"
     : "=r" (Model1_sum)
     : "g" (Model1_daddr), "g" (Model1_saddr),
       "g" ((Model1_len + Model1_proto)<<8), "0" (Model1_sum));
 return Model1_sum;
#endif
}


/**
 * csum_tcpup_magic - Compute an IPv4 pseudo header checksum.
 * @saddr: source address
 * @daddr: destination address
 * @len: length of packet
 * @proto: ip protocol of packet
 * @sum: initial sum to be added in (32bit unfolded)
 *
 * Returns the 16bit pseudo header checksum the input data already
 * complemented and ready to be filled in.
 */
static inline __attribute__((no_instrument_function)) Model1___sum16 Model1_csum_tcpudp_magic(Model1___be32 Model1_saddr, Model1___be32 Model1_daddr,
     __u32 Model1_len, __u8 Model1_proto,
     Model1___wsum Model1_sum)
{
 return Model1_csum_fold(Model1_csum_tcpudp_nofold(Model1_saddr, Model1_daddr, Model1_len, Model1_proto, Model1_sum));
}

/**
 * csum_partial - Compute an internet checksum.
 * @buff: buffer to be checksummed
 * @len: length of buffer.
 * @sum: initial sum to be added in (32bit unfolded)
 *
 * Returns the 32bit unfolded internet checksum of the buffer.
 * Before filling it in it needs to be csum_fold()'ed.
 * buff should be aligned to a 64bit boundary if possible.
 */
extern Model1___wsum Model1_csum_partial(const void *Model1_buff, int Model1_len, Model1___wsum Model1_sum);





/* Do not call this directly. Use the wrappers below */
extern Model1___wsum Model1_csum_partial_copy_generic(const void *Model1_src, const void *Model1_dst,
     int Model1_len, Model1___wsum Model1_sum,
     int *Model1_src_err_ptr, int *Model1_dst_err_ptr);


extern Model1___wsum Model1_csum_partial_copy_from_user(const void *Model1_src, void *Model1_dst,
       int Model1_len, Model1___wsum Model1_isum, int *Model1_errp);
extern Model1___wsum Model1_csum_partial_copy_to_user(const void *Model1_src, void *Model1_dst,
     int Model1_len, Model1___wsum Model1_isum, int *Model1_errp);
extern Model1___wsum Model1_csum_partial_copy_nocheck(const void *Model1_src, void *Model1_dst,
     int Model1_len, Model1___wsum Model1_sum);

/* Old names. To be removed. */



/**
 * ip_compute_csum - Compute an 16bit IP checksum.
 * @buff: buffer address.
 * @len: length of buffer.
 *
 * Returns the 16bit folded/inverted checksum of the passed buffer.
 * Ready to fill in.
 */
extern Model1___sum16 Model1_ip_compute_csum(const void *Model1_buff, int Model1_len);

/**
 * csum_ipv6_magic - Compute checksum of an IPv6 pseudo header.
 * @saddr: source address
 * @daddr: destination address
 * @len: length of packet
 * @proto: protocol of packet
 * @sum: initial sum (32bit unfolded) to be added in
 *
 * Computes an IPv6 pseudo header checksum. This sum is added the checksum
 * into UDP/TCP packets and contains some link layer information.
 * Returns the unfolded 32bit checksum.
 */

struct Model1_in6_addr;


extern Model1___sum16
Model1_csum_ipv6_magic(const struct Model1_in6_addr *Model1_saddr, const struct Model1_in6_addr *Model1_daddr,
  __u32 Model1_len, __u8 Model1_proto, Model1___wsum Model1_sum);

static inline __attribute__((no_instrument_function)) unsigned Model1_add32_with_carry(unsigned Model1_a, unsigned Model1_b)
{
 asm("addl %2,%0\n\t"
     "adcl $0,%0"
     : "=r" (Model1_a)
     : "0" (Model1_a), "rm" (Model1_b));
 return Model1_a;
}


static inline __attribute__((no_instrument_function)) Model1___wsum Model1_csum_add(Model1___wsum Model1_csum, Model1___wsum Model1_addend)
{
 return ( Model1___wsum)Model1_add32_with_carry(( unsigned)Model1_csum,
      ( unsigned)Model1_addend);
}
static inline __attribute__((no_instrument_function)) Model1___wsum Model1_csum_sub(Model1___wsum Model1_csum, Model1___wsum Model1_addend)
{
 return Model1_csum_add(Model1_csum, ~Model1_addend);
}

static inline __attribute__((no_instrument_function)) Model1___sum16 Model1_csum16_add(Model1___sum16 Model1_csum, Model1___be16 Model1_addend)
{
 Model1_u16 Model1_res = ( Model1_u16)Model1_csum;

 Model1_res += ( Model1_u16)Model1_addend;
 return ( Model1___sum16)(Model1_res + (Model1_res < ( Model1_u16)Model1_addend));
}

static inline __attribute__((no_instrument_function)) Model1___sum16 Model1_csum16_sub(Model1___sum16 Model1_csum, Model1___be16 Model1_addend)
{
 return Model1_csum16_add(Model1_csum, ~Model1_addend);
}

static inline __attribute__((no_instrument_function)) Model1___wsum
Model1_csum_block_add(Model1___wsum Model1_csum, Model1___wsum Model1_csum2, int Model1_offset)
{
 Model1_u32 Model1_sum = ( Model1_u32)Model1_csum2;

 /* rotate sum to align it with a 16b boundary */
 if (Model1_offset & 1)
  Model1_sum = Model1_ror32(Model1_sum, 8);

 return Model1_csum_add(Model1_csum, ( Model1___wsum)Model1_sum);
}

static inline __attribute__((no_instrument_function)) Model1___wsum
Model1_csum_block_add_ext(Model1___wsum Model1_csum, Model1___wsum Model1_csum2, int Model1_offset, int Model1_len)
{
 return Model1_csum_block_add(Model1_csum, Model1_csum2, Model1_offset);
}

static inline __attribute__((no_instrument_function)) Model1___wsum
Model1_csum_block_sub(Model1___wsum Model1_csum, Model1___wsum Model1_csum2, int Model1_offset)
{
 return Model1_csum_block_add(Model1_csum, ~Model1_csum2, Model1_offset);
}

static inline __attribute__((no_instrument_function)) Model1___wsum Model1_csum_unfold(Model1___sum16 Model1_n)
{
 return ( Model1___wsum)Model1_n;
}

static inline __attribute__((no_instrument_function)) Model1___wsum Model1_csum_partial_ext(const void *Model1_buff, int Model1_len, Model1___wsum Model1_sum)
{
 return Model1_csum_partial(Model1_buff, Model1_len, Model1_sum);
}



static inline __attribute__((no_instrument_function)) void Model1_csum_replace_by_diff(Model1___sum16 *Model1_sum, Model1___wsum Model1_diff)
{
 *Model1_sum = Model1_csum_fold(Model1_csum_add(Model1_diff, ~Model1_csum_unfold(*Model1_sum)));
}

static inline __attribute__((no_instrument_function)) void Model1_csum_replace4(Model1___sum16 *Model1_sum, Model1___be32 Model1_from, Model1___be32 Model1_to)
{
 Model1___wsum Model1_tmp = Model1_csum_sub(~Model1_csum_unfold(*Model1_sum), ( Model1___wsum)Model1_from);

 *Model1_sum = Model1_csum_fold(Model1_csum_add(Model1_tmp, ( Model1___wsum)Model1_to));
}

/* Implements RFC 1624 (Incremental Internet Checksum)
 * 3. Discussion states :
 *     HC' = ~(~HC + ~m + m')
 *  m : old value of a 16bit field
 *  m' : new value of a 16bit field
 */
static inline __attribute__((no_instrument_function)) void Model1_csum_replace2(Model1___sum16 *Model1_sum, Model1___be16 old, Model1___be16 Model1_new)
{
 *Model1_sum = ~Model1_csum16_add(Model1_csum16_sub(~(*Model1_sum), old), Model1_new);
}

struct Model1_sk_buff;
void Model1_inet_proto_csum_replace4(Model1___sum16 *Model1_sum, struct Model1_sk_buff *Model1_skb,
         Model1___be32 Model1_from, Model1___be32 Model1_to, bool Model1_pseudohdr);
void Model1_inet_proto_csum_replace16(Model1___sum16 *Model1_sum, struct Model1_sk_buff *Model1_skb,
          const Model1___be32 *Model1_from, const Model1___be32 *Model1_to,
          bool Model1_pseudohdr);
void Model1_inet_proto_csum_replace_by_diff(Model1___sum16 *Model1_sum, struct Model1_sk_buff *Model1_skb,
         Model1___wsum Model1_diff, bool Model1_pseudohdr);

static inline __attribute__((no_instrument_function)) void Model1_inet_proto_csum_replace2(Model1___sum16 *Model1_sum, struct Model1_sk_buff *Model1_skb,
         Model1___be16 Model1_from, Model1___be16 Model1_to,
         bool Model1_pseudohdr)
{
 Model1_inet_proto_csum_replace4(Model1_sum, Model1_skb, ( Model1___be32)Model1_from,
     ( Model1___be32)Model1_to, Model1_pseudohdr);
}

static inline __attribute__((no_instrument_function)) Model1___wsum Model1_remcsum_adjust(void *Model1_ptr, Model1___wsum Model1_csum,
        int Model1_start, int Model1_offset)
{
 Model1___sum16 *Model1_psum = (Model1___sum16 *)(Model1_ptr + Model1_offset);
 Model1___wsum Model1_delta;

 /* Subtract out checksum up to start */
 Model1_csum = Model1_csum_sub(Model1_csum, Model1_csum_partial(Model1_ptr, Model1_start, 0));

 /* Set derived checksum in packet */
 Model1_delta = Model1_csum_sub(( Model1___wsum)Model1_csum_fold(Model1_csum),
    ( Model1___wsum)*Model1_psum);
 *Model1_psum = Model1_csum_fold(Model1_csum);

 return Model1_delta;
}

static inline __attribute__((no_instrument_function)) void Model1_remcsum_unadjust(Model1___sum16 *Model1_psum, Model1___wsum Model1_delta)
{
 *Model1_psum = Model1_csum_fold(Model1_csum_sub(Model1_delta, *Model1_psum));
}





/*
 * include/linux/sizes.h
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2 as
 * published by the Free Software Foundation.
 */



/*
 * Copyright (C) 2008 Advanced Micro Devices, Inc.
 *
 * Author: Joerg Roedel <joerg.roedel@amd.com>
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License version 2 as published
 * by the Free Software Foundation.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
 */






struct Model1_device;
struct Model1_scatterlist;
struct Model1_bus_type;
static inline __attribute__((no_instrument_function)) void Model1_dma_debug_add_bus(struct Model1_bus_type *Model1_bus)
{
}

static inline __attribute__((no_instrument_function)) void Model1_dma_debug_init(Model1_u32 Model1_num_entries)
{
}

static inline __attribute__((no_instrument_function)) int Model1_dma_debug_resize_entries(Model1_u32 Model1_num_entries)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model1_debug_dma_map_page(struct Model1_device *Model1_dev, struct Model1_page *Model1_page,
          Model1_size_t Model1_offset, Model1_size_t Model1_size,
          int Model1_direction, Model1_dma_addr_t Model1_dma_addr,
          bool Model1_map_single)
{
}

static inline __attribute__((no_instrument_function)) void Model1_debug_dma_mapping_error(struct Model1_device *Model1_dev,
       Model1_dma_addr_t Model1_dma_addr)
{
}

static inline __attribute__((no_instrument_function)) void Model1_debug_dma_unmap_page(struct Model1_device *Model1_dev, Model1_dma_addr_t Model1_addr,
     Model1_size_t Model1_size, int Model1_direction,
     bool Model1_map_single)
{
}

static inline __attribute__((no_instrument_function)) void Model1_debug_dma_map_sg(struct Model1_device *Model1_dev, struct Model1_scatterlist *Model1_sg,
        int Model1_nents, int Model1_mapped_ents, int Model1_direction)
{
}

static inline __attribute__((no_instrument_function)) void Model1_debug_dma_unmap_sg(struct Model1_device *Model1_dev,
          struct Model1_scatterlist *Model1_sglist,
          int Model1_nelems, int Model1_dir)
{
}

static inline __attribute__((no_instrument_function)) void Model1_debug_dma_alloc_coherent(struct Model1_device *Model1_dev, Model1_size_t Model1_size,
         Model1_dma_addr_t Model1_dma_addr, void *Model1_virt)
{
}

static inline __attribute__((no_instrument_function)) void Model1_debug_dma_free_coherent(struct Model1_device *Model1_dev, Model1_size_t Model1_size,
        void *Model1_virt, Model1_dma_addr_t Model1_addr)
{
}

static inline __attribute__((no_instrument_function)) void Model1_debug_dma_sync_single_for_cpu(struct Model1_device *Model1_dev,
       Model1_dma_addr_t Model1_dma_handle,
       Model1_size_t Model1_size, int Model1_direction)
{
}

static inline __attribute__((no_instrument_function)) void Model1_debug_dma_sync_single_for_device(struct Model1_device *Model1_dev,
          Model1_dma_addr_t Model1_dma_handle,
          Model1_size_t Model1_size, int Model1_direction)
{
}

static inline __attribute__((no_instrument_function)) void Model1_debug_dma_sync_single_range_for_cpu(struct Model1_device *Model1_dev,
             Model1_dma_addr_t Model1_dma_handle,
             unsigned long Model1_offset,
             Model1_size_t Model1_size,
             int Model1_direction)
{
}

static inline __attribute__((no_instrument_function)) void Model1_debug_dma_sync_single_range_for_device(struct Model1_device *Model1_dev,
         Model1_dma_addr_t Model1_dma_handle,
         unsigned long Model1_offset,
         Model1_size_t Model1_size,
         int Model1_direction)
{
}

static inline __attribute__((no_instrument_function)) void Model1_debug_dma_sync_sg_for_cpu(struct Model1_device *Model1_dev,
          struct Model1_scatterlist *Model1_sg,
          int Model1_nelems, int Model1_direction)
{
}

static inline __attribute__((no_instrument_function)) void Model1_debug_dma_sync_sg_for_device(struct Model1_device *Model1_dev,
      struct Model1_scatterlist *Model1_sg,
      int Model1_nelems, int Model1_direction)
{
}

static inline __attribute__((no_instrument_function)) void Model1_debug_dma_dump_mappings(struct Model1_device *Model1_dev)
{
}

static inline __attribute__((no_instrument_function)) void Model1_debug_dma_assert_idle(struct Model1_page *Model1_page)
{
}


/*
 * These definitions mirror those in pci.h, so they can be used
 * interchangeably with their PCI_ counterparts.
 */
enum Model1_dma_data_direction {
 Model1_DMA_BIDIRECTIONAL = 0,
 Model1_DMA_TO_DEVICE = 1,
 Model1_DMA_FROM_DEVICE = 2,
 Model1_DMA_NONE = 3,
};




/**
 * List of possible attributes associated with a DMA mapping. The semantics
 * of each attribute should be defined in Documentation/DMA-attributes.txt.
 *
 * DMA_ATTR_WRITE_BARRIER: DMA to a memory region with this attribute
 * forces all pending DMA writes to complete.
 */

/*
 * DMA_ATTR_WEAK_ORDERING: Specifies that reads and writes to the mapping
 * may be weakly ordered, that is that reads and writes may pass each other.
 */

/*
 * DMA_ATTR_WRITE_COMBINE: Specifies that writes to the mapping may be
 * buffered to improve performance.
 */

/*
 * DMA_ATTR_NON_CONSISTENT: Lets the platform to choose to return either
 * consistent or non-consistent memory as it sees fit.
 */

/*
 * DMA_ATTR_NO_KERNEL_MAPPING: Lets the platform to avoid creating a kernel
 * virtual mapping for the allocated buffer.
 */

/*
 * DMA_ATTR_SKIP_CPU_SYNC: Allows platform code to skip synchronization of
 * the CPU cache for the given buffer assuming that it has been already
 * transferred to 'device' domain.
 */

/*
 * DMA_ATTR_FORCE_CONTIGUOUS: Forces contiguous allocation of the buffer
 * in physical memory.
 */

/*
 * DMA_ATTR_ALLOC_SINGLE_PAGES: This is a hint to the DMA-mapping subsystem
 * that it's probably not worth the time to try to allocate memory to in a way
 * that gives better TLB efficiency.
 */


/*
 * A dma_addr_t can hold any valid DMA or bus address for the platform.
 * It can be given to a device to use as a DMA source or target.  A CPU cannot
 * reference a dma_addr_t directly because there may be translation between
 * its physical address space and the bus address space.
 */
struct Model1_dma_map_ops {
 void* (*Model1_alloc)(struct Model1_device *Model1_dev, Model1_size_t Model1_size,
    Model1_dma_addr_t *Model1_dma_handle, Model1_gfp_t Model1_gfp,
    unsigned long Model1_attrs);
 void (*Model1_free)(struct Model1_device *Model1_dev, Model1_size_t Model1_size,
         void *Model1_vaddr, Model1_dma_addr_t Model1_dma_handle,
         unsigned long Model1_attrs);
 int (*Model1_mmap)(struct Model1_device *, struct Model1_vm_area_struct *,
     void *, Model1_dma_addr_t, Model1_size_t,
     unsigned long Model1_attrs);

 int (*Model1_get_sgtable)(struct Model1_device *Model1_dev, struct Model1_sg_table *Model1_sgt, void *,
      Model1_dma_addr_t, Model1_size_t, unsigned long Model1_attrs);

 Model1_dma_addr_t (*Model1_map_page)(struct Model1_device *Model1_dev, struct Model1_page *Model1_page,
          unsigned long Model1_offset, Model1_size_t Model1_size,
          enum Model1_dma_data_direction Model1_dir,
          unsigned long Model1_attrs);
 void (*Model1_unmap_page)(struct Model1_device *Model1_dev, Model1_dma_addr_t Model1_dma_handle,
      Model1_size_t Model1_size, enum Model1_dma_data_direction Model1_dir,
      unsigned long Model1_attrs);
 /*
	 * map_sg returns 0 on error and a value > 0 on success.
	 * It should never return a value < 0.
	 */
 int (*Model1_map_sg)(struct Model1_device *Model1_dev, struct Model1_scatterlist *Model1_sg,
        int Model1_nents, enum Model1_dma_data_direction Model1_dir,
        unsigned long Model1_attrs);
 void (*Model1_unmap_sg)(struct Model1_device *Model1_dev,
    struct Model1_scatterlist *Model1_sg, int Model1_nents,
    enum Model1_dma_data_direction Model1_dir,
    unsigned long Model1_attrs);
 void (*Model1_sync_single_for_cpu)(struct Model1_device *Model1_dev,
        Model1_dma_addr_t Model1_dma_handle, Model1_size_t Model1_size,
        enum Model1_dma_data_direction Model1_dir);
 void (*Model1_sync_single_for_device)(struct Model1_device *Model1_dev,
           Model1_dma_addr_t Model1_dma_handle, Model1_size_t Model1_size,
           enum Model1_dma_data_direction Model1_dir);
 void (*Model1_sync_sg_for_cpu)(struct Model1_device *Model1_dev,
    struct Model1_scatterlist *Model1_sg, int Model1_nents,
    enum Model1_dma_data_direction Model1_dir);
 void (*Model1_sync_sg_for_device)(struct Model1_device *Model1_dev,
       struct Model1_scatterlist *Model1_sg, int Model1_nents,
       enum Model1_dma_data_direction Model1_dir);
 int (*Model1_mapping_error)(struct Model1_device *Model1_dev, Model1_dma_addr_t Model1_dma_addr);
 int (*Model1_dma_supported)(struct Model1_device *Model1_dev, Model1_u64 Model1_mask);
 int (*Model1_set_dma_mask)(struct Model1_device *Model1_dev, Model1_u64 Model1_mask);



 int Model1_is_phys;
};

extern struct Model1_dma_map_ops Model1_dma_noop_ops;





static inline __attribute__((no_instrument_function)) int Model1_valid_dma_direction(int Model1_dma_direction)
{
 return ((Model1_dma_direction == Model1_DMA_BIDIRECTIONAL) ||
  (Model1_dma_direction == Model1_DMA_TO_DEVICE) ||
  (Model1_dma_direction == Model1_DMA_FROM_DEVICE));
}

static inline __attribute__((no_instrument_function)) int Model1_is_device_dma_capable(struct Model1_device *Model1_dev)
{
 return Model1_dev->Model1_dma_mask != ((void *)0) && *Model1_dev->Model1_dma_mask != 0x0ULL;
}



/*
 * IOMMU interface. See Documentation/DMA-API-HOWTO.txt and
 * Documentation/DMA-API.txt for documentation.
 */
















struct Model1_device;
struct Model1_page;
struct Model1_scatterlist;

extern int Model1_swiotlb_force;

/*
 * Maximum allowable number of contiguous slabs to map,
 * must be a power of 2.  What is the appropriate value ?
 * The complexity of {map,unmap}_single is linearly dependent on this value.
 */


/*
 * log of the size of each IO TLB slab.  The number of slabs is command line
 * controllable.
 */


extern void Model1_swiotlb_init(int Model1_verbose);
int Model1_swiotlb_init_with_tbl(char *Model1_tlb, unsigned long Model1_nslabs, int Model1_verbose);
extern unsigned long Model1_swiotlb_nr_tbl(void);
unsigned long Model1_swiotlb_size_or_default(void);
extern int Model1_swiotlb_late_init_with_tbl(char *Model1_tlb, unsigned long Model1_nslabs);

/*
 * Enumeration for sync targets
 */
enum Model1_dma_sync_target {
 Model1_SYNC_FOR_CPU = 0,
 Model1_SYNC_FOR_DEVICE = 1,
};

/* define the last possible byte of physical address space as a mapping error */


extern Model1_phys_addr_t Model1_swiotlb_tbl_map_single(struct Model1_device *Model1_hwdev,
       Model1_dma_addr_t Model1_tbl_dma_addr,
       Model1_phys_addr_t Model1_phys, Model1_size_t Model1_size,
       enum Model1_dma_data_direction Model1_dir);

extern void Model1_swiotlb_tbl_unmap_single(struct Model1_device *Model1_hwdev,
         Model1_phys_addr_t Model1_tlb_addr,
         Model1_size_t Model1_size, enum Model1_dma_data_direction Model1_dir);

extern void Model1_swiotlb_tbl_sync_single(struct Model1_device *Model1_hwdev,
        Model1_phys_addr_t Model1_tlb_addr,
        Model1_size_t Model1_size, enum Model1_dma_data_direction Model1_dir,
        enum Model1_dma_sync_target Model1_target);

/* Accessory functions. */
extern void
*Model1_swiotlb_alloc_coherent(struct Model1_device *Model1_hwdev, Model1_size_t Model1_size,
   Model1_dma_addr_t *Model1_dma_handle, Model1_gfp_t Model1_flags);

extern void
Model1_swiotlb_free_coherent(struct Model1_device *Model1_hwdev, Model1_size_t Model1_size,
        void *Model1_vaddr, Model1_dma_addr_t Model1_dma_handle);

extern Model1_dma_addr_t Model1_swiotlb_map_page(struct Model1_device *Model1_dev, struct Model1_page *Model1_page,
       unsigned long Model1_offset, Model1_size_t Model1_size,
       enum Model1_dma_data_direction Model1_dir,
       unsigned long Model1_attrs);
extern void Model1_swiotlb_unmap_page(struct Model1_device *Model1_hwdev, Model1_dma_addr_t Model1_dev_addr,
          Model1_size_t Model1_size, enum Model1_dma_data_direction Model1_dir,
          unsigned long Model1_attrs);

extern int
Model1_swiotlb_map_sg(struct Model1_device *Model1_hwdev, struct Model1_scatterlist *Model1_sg, int Model1_nents,
        enum Model1_dma_data_direction Model1_dir);

extern void
Model1_swiotlb_unmap_sg(struct Model1_device *Model1_hwdev, struct Model1_scatterlist *Model1_sg, int Model1_nents,
   enum Model1_dma_data_direction Model1_dir);

extern int
Model1_swiotlb_map_sg_attrs(struct Model1_device *Model1_hwdev, struct Model1_scatterlist *Model1_sgl, int Model1_nelems,
       enum Model1_dma_data_direction Model1_dir,
       unsigned long Model1_attrs);

extern void
Model1_swiotlb_unmap_sg_attrs(struct Model1_device *Model1_hwdev, struct Model1_scatterlist *Model1_sgl,
         int Model1_nelems, enum Model1_dma_data_direction Model1_dir,
         unsigned long Model1_attrs);

extern void
Model1_swiotlb_sync_single_for_cpu(struct Model1_device *Model1_hwdev, Model1_dma_addr_t Model1_dev_addr,
       Model1_size_t Model1_size, enum Model1_dma_data_direction Model1_dir);

extern void
Model1_swiotlb_sync_sg_for_cpu(struct Model1_device *Model1_hwdev, struct Model1_scatterlist *Model1_sg,
   int Model1_nelems, enum Model1_dma_data_direction Model1_dir);

extern void
Model1_swiotlb_sync_single_for_device(struct Model1_device *Model1_hwdev, Model1_dma_addr_t Model1_dev_addr,
          Model1_size_t Model1_size, enum Model1_dma_data_direction Model1_dir);

extern void
Model1_swiotlb_sync_sg_for_device(struct Model1_device *Model1_hwdev, struct Model1_scatterlist *Model1_sg,
      int Model1_nelems, enum Model1_dma_data_direction Model1_dir);

extern int
Model1_swiotlb_dma_mapping_error(struct Model1_device *Model1_hwdev, Model1_dma_addr_t Model1_dma_addr);

extern int
Model1_swiotlb_dma_supported(struct Model1_device *Model1_hwdev, Model1_u64 Model1_mask);


extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_swiotlb_free(void);




extern void Model1_swiotlb_print_info(void);
extern int Model1_is_swiotlb_buffer(Model1_phys_addr_t Model1_paddr);


extern int Model1_swiotlb;
extern int __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_pci_swiotlb_detect_override(void);
extern int __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_pci_swiotlb_detect_4gb(void);
extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_pci_swiotlb_init(void);
extern void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_pci_swiotlb_late_init(void);
static inline __attribute__((no_instrument_function)) void Model1_dma_mark_clean(void *Model1_addr, Model1_size_t Model1_size) {}

extern void *Model1_x86_swiotlb_alloc_coherent(struct Model1_device *Model1_hwdev, Model1_size_t Model1_size,
     Model1_dma_addr_t *Model1_dma_handle, Model1_gfp_t Model1_flags,
     unsigned long Model1_attrs);
extern void Model1_x86_swiotlb_free_coherent(struct Model1_device *Model1_dev, Model1_size_t Model1_size,
     void *Model1_vaddr, Model1_dma_addr_t Model1_dma_addr,
     unsigned long Model1_attrs);



/*
 * Contiguous Memory Allocator for DMA mapping framework
 * Copyright (c) 2010-2011 by Samsung Electronics.
 * Written by:
 *	Marek Szyprowski <m.szyprowski@samsung.com>
 *	Michal Nazarewicz <mina86@mina86.com>
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public License as
 * published by the Free Software Foundation; either version 2 of the
 * License or (at your optional) any later version of the license.
 */

/*
 * Contiguous Memory Allocator
 *
 *   The Contiguous Memory Allocator (CMA) makes it possible to
 *   allocate big contiguous chunks of memory after the system has
 *   booted.
 *
 * Why is it needed?
 *
 *   Various devices on embedded systems have no scatter-getter and/or
 *   IO map support and require contiguous blocks of memory to
 *   operate.  They include devices such as cameras, hardware video
 *   coders, etc.
 *
 *   Such devices often require big memory buffers (a full HD frame
 *   is, for instance, more then 2 mega pixels large, i.e. more than 6
 *   MB of memory), which makes mechanisms such as kmalloc() or
 *   alloc_page() ineffective.
 *
 *   At the same time, a solution where a big memory region is
 *   reserved for a device is suboptimal since often more memory is
 *   reserved then strictly required and, moreover, the memory is
 *   inaccessible to page system even if device drivers don't use it.
 *
 *   CMA tries to solve this issue by operating on memory regions
 *   where only movable pages can be allocated from.  This way, kernel
 *   can use the memory for pagecache and when device driver requests
 *   it, allocated pages can be migrated.
 *
 * Driver usage
 *
 *   CMA should not be used by the device drivers directly. It is
 *   only a helper framework for dma-mapping subsystem.
 *
 *   For more information, see kernel-docs in drivers/base/dma-contiguous.c
 */





struct Model1_cma;
struct Model1_page;
static inline __attribute__((no_instrument_function)) struct Model1_cma *Model1_dev_get_cma_area(struct Model1_device *Model1_dev)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) void Model1_dev_set_cma_area(struct Model1_device *Model1_dev, struct Model1_cma *Model1_cma) { }

static inline __attribute__((no_instrument_function)) void Model1_dma_contiguous_set_default(struct Model1_cma *Model1_cma) { }

static inline __attribute__((no_instrument_function)) void Model1_dma_contiguous_reserve(Model1_phys_addr_t Model1_limit) { }

static inline __attribute__((no_instrument_function)) int Model1_dma_contiguous_reserve_area(Model1_phys_addr_t Model1_size, Model1_phys_addr_t Model1_base,
           Model1_phys_addr_t Model1_limit, struct Model1_cma **Model1_res_cma,
           bool Model1_fixed)
{
 return -38;
}

static inline __attribute__((no_instrument_function))
int Model1_dma_declare_contiguous(struct Model1_device *Model1_dev, Model1_phys_addr_t Model1_size,
      Model1_phys_addr_t Model1_base, Model1_phys_addr_t Model1_limit)
{
 return -38;
}

static inline __attribute__((no_instrument_function))
struct Model1_page *Model1_dma_alloc_from_contiguous(struct Model1_device *Model1_dev, Model1_size_t Model1_count,
           unsigned int Model1_order)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function))
bool Model1_dma_release_from_contiguous(struct Model1_device *Model1_dev, struct Model1_page *Model1_pages,
     int Model1_count)
{
 return false;
}
extern int Model1_iommu_merge;
extern struct Model1_device Model1_x86_dma_fallback_dev;
extern int Model1_panic_on_overflow;

extern struct Model1_dma_map_ops *Model1_dma_ops;

static inline __attribute__((no_instrument_function)) struct Model1_dma_map_ops *Model1_get_dma_ops(struct Model1_device *Model1_dev)
{



 if (__builtin_expect(!!(!Model1_dev), 0) || !Model1_dev->Model1_archdata.Model1_dma_ops)
  return Model1_dma_ops;
 else
  return Model1_dev->Model1_archdata.Model1_dma_ops;

}

bool Model1_arch_dma_alloc_attrs(struct Model1_device **Model1_dev, Model1_gfp_t *Model1_gfp);



extern int Model1_dma_supported(struct Model1_device *Model1_hwdev, Model1_u64 Model1_mask);

extern void *Model1_dma_generic_alloc_coherent(struct Model1_device *Model1_dev, Model1_size_t Model1_size,
     Model1_dma_addr_t *Model1_dma_addr, Model1_gfp_t Model1_flag,
     unsigned long Model1_attrs);

extern void Model1_dma_generic_free_coherent(struct Model1_device *Model1_dev, Model1_size_t Model1_size,
          void *Model1_vaddr, Model1_dma_addr_t Model1_dma_addr,
          unsigned long Model1_attrs);







static inline __attribute__((no_instrument_function)) bool Model1_dma_capable(struct Model1_device *Model1_dev, Model1_dma_addr_t Model1_addr, Model1_size_t Model1_size)
{
 if (!Model1_dev->Model1_dma_mask)
  return 0;

 return Model1_addr + Model1_size - 1 <= *Model1_dev->Model1_dma_mask;
}

static inline __attribute__((no_instrument_function)) Model1_dma_addr_t Model1_phys_to_dma(struct Model1_device *Model1_dev, Model1_phys_addr_t Model1_paddr)
{
 return Model1_paddr;
}

static inline __attribute__((no_instrument_function)) Model1_phys_addr_t Model1_dma_to_phys(struct Model1_device *Model1_dev, Model1_dma_addr_t Model1_daddr)
{
 return Model1_daddr;
}


static inline __attribute__((no_instrument_function)) void
Model1_dma_cache_sync(struct Model1_device *Model1_dev, void *Model1_vaddr, Model1_size_t Model1_size,
 enum Model1_dma_data_direction Model1_dir)
{
 Model1_flush_write_buffers();
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_dma_alloc_coherent_mask(struct Model1_device *Model1_dev,
          Model1_gfp_t Model1_gfp)
{
 unsigned long Model1_dma_mask = 0;

 Model1_dma_mask = Model1_dev->Model1_coherent_dma_mask;
 if (!Model1_dma_mask)
  Model1_dma_mask = (Model1_gfp & (( Model1_gfp_t)0x01u)) ? (((24) == 64) ? ~0ULL : ((1ULL<<(24))-1)) : (((32) == 64) ? ~0ULL : ((1ULL<<(32))-1));

 return Model1_dma_mask;
}

static inline __attribute__((no_instrument_function)) Model1_gfp_t Model1_dma_alloc_coherent_gfp_flags(struct Model1_device *Model1_dev, Model1_gfp_t Model1_gfp)
{
 unsigned long Model1_dma_mask = Model1_dma_alloc_coherent_mask(Model1_dev, Model1_gfp);

 if (Model1_dma_mask <= (((24) == 64) ? ~0ULL : ((1ULL<<(24))-1)))
  Model1_gfp |= (( Model1_gfp_t)0x01u);

 if (Model1_dma_mask <= (((32) == 64) ? ~0ULL : ((1ULL<<(32))-1)) && !(Model1_gfp & (( Model1_gfp_t)0x01u)))
  Model1_gfp |= (( Model1_gfp_t)0x04u);

       return Model1_gfp;
}
static inline __attribute__((no_instrument_function)) Model1_dma_addr_t Model1_dma_map_single_attrs(struct Model1_device *Model1_dev, void *Model1_ptr,
           Model1_size_t Model1_size,
           enum Model1_dma_data_direction Model1_dir,
           unsigned long Model1_attrs)
{
 struct Model1_dma_map_ops *Model1_ops = Model1_get_dma_ops(Model1_dev);
 Model1_dma_addr_t Model1_addr;

 Model1_kmemcheck_mark_initialized(Model1_ptr, Model1_size);
 do { if (__builtin_expect(!!(!Model1_valid_dma_direction(Model1_dir)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (178), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);
 Model1_addr = Model1_ops->Model1_map_page(Model1_dev, (((struct Model1_page *)(0xffffea0000000000UL)) + (Model1___phys_addr_nodebug((unsigned long)(Model1_ptr)) >> 12)),
        ((unsigned long)(Model1_ptr) & ~(~(((1UL) << 12)-1))), Model1_size,
        Model1_dir, Model1_attrs);
 Model1_debug_dma_map_page(Model1_dev, (((struct Model1_page *)(0xffffea0000000000UL)) + (Model1___phys_addr_nodebug((unsigned long)(Model1_ptr)) >> 12)),
      ((unsigned long)(Model1_ptr) & ~(~(((1UL) << 12)-1))), Model1_size,
      Model1_dir, Model1_addr, true);
 return Model1_addr;
}

static inline __attribute__((no_instrument_function)) void Model1_dma_unmap_single_attrs(struct Model1_device *Model1_dev, Model1_dma_addr_t Model1_addr,
       Model1_size_t Model1_size,
       enum Model1_dma_data_direction Model1_dir,
       unsigned long Model1_attrs)
{
 struct Model1_dma_map_ops *Model1_ops = Model1_get_dma_ops(Model1_dev);

 do { if (__builtin_expect(!!(!Model1_valid_dma_direction(Model1_dir)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (195), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);
 if (Model1_ops->Model1_unmap_page)
  Model1_ops->Model1_unmap_page(Model1_dev, Model1_addr, Model1_size, Model1_dir, Model1_attrs);
 Model1_debug_dma_unmap_page(Model1_dev, Model1_addr, Model1_size, Model1_dir, true);
}

/*
 * dma_maps_sg_attrs returns 0 on error and > 0 on success.
 * It should never return a value < 0.
 */
static inline __attribute__((no_instrument_function)) int Model1_dma_map_sg_attrs(struct Model1_device *Model1_dev, struct Model1_scatterlist *Model1_sg,
       int Model1_nents, enum Model1_dma_data_direction Model1_dir,
       unsigned long Model1_attrs)
{
 struct Model1_dma_map_ops *Model1_ops = Model1_get_dma_ops(Model1_dev);
 int Model1_i, Model1_ents;
 struct Model1_scatterlist *Model1_s;

 for (Model1_i = 0, Model1_s = (Model1_sg); Model1_i < (Model1_nents); Model1_i++, Model1_s = Model1_sg_next(Model1_s))
  Model1_kmemcheck_mark_initialized(Model1_sg_virt(Model1_s), Model1_s->Model1_length);
 do { if (__builtin_expect(!!(!Model1_valid_dma_direction(Model1_dir)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (215), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);
 Model1_ents = Model1_ops->Model1_map_sg(Model1_dev, Model1_sg, Model1_nents, Model1_dir, Model1_attrs);
 do { if (__builtin_expect(!!(Model1_ents < 0), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (217), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);
 Model1_debug_dma_map_sg(Model1_dev, Model1_sg, Model1_nents, Model1_ents, Model1_dir);

 return Model1_ents;
}

static inline __attribute__((no_instrument_function)) void Model1_dma_unmap_sg_attrs(struct Model1_device *Model1_dev, struct Model1_scatterlist *Model1_sg,
          int Model1_nents, enum Model1_dma_data_direction Model1_dir,
          unsigned long Model1_attrs)
{
 struct Model1_dma_map_ops *Model1_ops = Model1_get_dma_ops(Model1_dev);

 do { if (__builtin_expect(!!(!Model1_valid_dma_direction(Model1_dir)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (229), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);
 Model1_debug_dma_unmap_sg(Model1_dev, Model1_sg, Model1_nents, Model1_dir);
 if (Model1_ops->Model1_unmap_sg)
  Model1_ops->Model1_unmap_sg(Model1_dev, Model1_sg, Model1_nents, Model1_dir, Model1_attrs);
}

static inline __attribute__((no_instrument_function)) Model1_dma_addr_t Model1_dma_map_page(struct Model1_device *Model1_dev, struct Model1_page *Model1_page,
          Model1_size_t Model1_offset, Model1_size_t Model1_size,
          enum Model1_dma_data_direction Model1_dir)
{
 struct Model1_dma_map_ops *Model1_ops = Model1_get_dma_ops(Model1_dev);
 Model1_dma_addr_t Model1_addr;

 Model1_kmemcheck_mark_initialized(Model1_lowmem_page_address(Model1_page) + Model1_offset, Model1_size);
 do { if (__builtin_expect(!!(!Model1_valid_dma_direction(Model1_dir)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (243), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);
 Model1_addr = Model1_ops->Model1_map_page(Model1_dev, Model1_page, Model1_offset, Model1_size, Model1_dir, 0);
 Model1_debug_dma_map_page(Model1_dev, Model1_page, Model1_offset, Model1_size, Model1_dir, Model1_addr, false);

 return Model1_addr;
}

static inline __attribute__((no_instrument_function)) void Model1_dma_unmap_page(struct Model1_device *Model1_dev, Model1_dma_addr_t Model1_addr,
      Model1_size_t Model1_size, enum Model1_dma_data_direction Model1_dir)
{
 struct Model1_dma_map_ops *Model1_ops = Model1_get_dma_ops(Model1_dev);

 do { if (__builtin_expect(!!(!Model1_valid_dma_direction(Model1_dir)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (255), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);
 if (Model1_ops->Model1_unmap_page)
  Model1_ops->Model1_unmap_page(Model1_dev, Model1_addr, Model1_size, Model1_dir, 0);
 Model1_debug_dma_unmap_page(Model1_dev, Model1_addr, Model1_size, Model1_dir, false);
}

static inline __attribute__((no_instrument_function)) void Model1_dma_sync_single_for_cpu(struct Model1_device *Model1_dev, Model1_dma_addr_t Model1_addr,
        Model1_size_t Model1_size,
        enum Model1_dma_data_direction Model1_dir)
{
 struct Model1_dma_map_ops *Model1_ops = Model1_get_dma_ops(Model1_dev);

 do { if (__builtin_expect(!!(!Model1_valid_dma_direction(Model1_dir)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (267), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);
 if (Model1_ops->Model1_sync_single_for_cpu)
  Model1_ops->Model1_sync_single_for_cpu(Model1_dev, Model1_addr, Model1_size, Model1_dir);
 Model1_debug_dma_sync_single_for_cpu(Model1_dev, Model1_addr, Model1_size, Model1_dir);
}

static inline __attribute__((no_instrument_function)) void Model1_dma_sync_single_for_device(struct Model1_device *Model1_dev,
           Model1_dma_addr_t Model1_addr, Model1_size_t Model1_size,
           enum Model1_dma_data_direction Model1_dir)
{
 struct Model1_dma_map_ops *Model1_ops = Model1_get_dma_ops(Model1_dev);

 do { if (__builtin_expect(!!(!Model1_valid_dma_direction(Model1_dir)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (279), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);
 if (Model1_ops->Model1_sync_single_for_device)
  Model1_ops->Model1_sync_single_for_device(Model1_dev, Model1_addr, Model1_size, Model1_dir);
 Model1_debug_dma_sync_single_for_device(Model1_dev, Model1_addr, Model1_size, Model1_dir);
}

static inline __attribute__((no_instrument_function)) void Model1_dma_sync_single_range_for_cpu(struct Model1_device *Model1_dev,
       Model1_dma_addr_t Model1_addr,
       unsigned long Model1_offset,
       Model1_size_t Model1_size,
       enum Model1_dma_data_direction Model1_dir)
{
 const struct Model1_dma_map_ops *Model1_ops = Model1_get_dma_ops(Model1_dev);

 do { if (__builtin_expect(!!(!Model1_valid_dma_direction(Model1_dir)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (293), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);
 if (Model1_ops->Model1_sync_single_for_cpu)
  Model1_ops->Model1_sync_single_for_cpu(Model1_dev, Model1_addr + Model1_offset, Model1_size, Model1_dir);
 Model1_debug_dma_sync_single_range_for_cpu(Model1_dev, Model1_addr, Model1_offset, Model1_size, Model1_dir);
}

static inline __attribute__((no_instrument_function)) void Model1_dma_sync_single_range_for_device(struct Model1_device *Model1_dev,
          Model1_dma_addr_t Model1_addr,
          unsigned long Model1_offset,
          Model1_size_t Model1_size,
          enum Model1_dma_data_direction Model1_dir)
{
 const struct Model1_dma_map_ops *Model1_ops = Model1_get_dma_ops(Model1_dev);

 do { if (__builtin_expect(!!(!Model1_valid_dma_direction(Model1_dir)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (307), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);
 if (Model1_ops->Model1_sync_single_for_device)
  Model1_ops->Model1_sync_single_for_device(Model1_dev, Model1_addr + Model1_offset, Model1_size, Model1_dir);
 Model1_debug_dma_sync_single_range_for_device(Model1_dev, Model1_addr, Model1_offset, Model1_size, Model1_dir);
}

static inline __attribute__((no_instrument_function)) void
Model1_dma_sync_sg_for_cpu(struct Model1_device *Model1_dev, struct Model1_scatterlist *Model1_sg,
      int Model1_nelems, enum Model1_dma_data_direction Model1_dir)
{
 struct Model1_dma_map_ops *Model1_ops = Model1_get_dma_ops(Model1_dev);

 do { if (__builtin_expect(!!(!Model1_valid_dma_direction(Model1_dir)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (319), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);
 if (Model1_ops->Model1_sync_sg_for_cpu)
  Model1_ops->Model1_sync_sg_for_cpu(Model1_dev, Model1_sg, Model1_nelems, Model1_dir);
 Model1_debug_dma_sync_sg_for_cpu(Model1_dev, Model1_sg, Model1_nelems, Model1_dir);
}

static inline __attribute__((no_instrument_function)) void
Model1_dma_sync_sg_for_device(struct Model1_device *Model1_dev, struct Model1_scatterlist *Model1_sg,
         int Model1_nelems, enum Model1_dma_data_direction Model1_dir)
{
 struct Model1_dma_map_ops *Model1_ops = Model1_get_dma_ops(Model1_dev);

 do { if (__builtin_expect(!!(!Model1_valid_dma_direction(Model1_dir)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (331), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);
 if (Model1_ops->Model1_sync_sg_for_device)
  Model1_ops->Model1_sync_sg_for_device(Model1_dev, Model1_sg, Model1_nelems, Model1_dir);
 Model1_debug_dma_sync_sg_for_device(Model1_dev, Model1_sg, Model1_nelems, Model1_dir);

}






extern int Model1_dma_common_mmap(struct Model1_device *Model1_dev, struct Model1_vm_area_struct *Model1_vma,
      void *Model1_cpu_addr, Model1_dma_addr_t Model1_dma_addr, Model1_size_t Model1_size);

void *Model1_dma_common_contiguous_remap(struct Model1_page *Model1_page, Model1_size_t Model1_size,
   unsigned long Model1_vm_flags,
   Model1_pgprot_t Model1_prot, const void *Model1_caller);

void *Model1_dma_common_pages_remap(struct Model1_page **Model1_pages, Model1_size_t Model1_size,
   unsigned long Model1_vm_flags, Model1_pgprot_t Model1_prot,
   const void *Model1_caller);
void Model1_dma_common_free_remap(void *Model1_cpu_addr, Model1_size_t Model1_size, unsigned long Model1_vm_flags);

/**
 * dma_mmap_attrs - map a coherent DMA allocation into user space
 * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
 * @vma: vm_area_struct describing requested user mapping
 * @cpu_addr: kernel CPU-view address returned from dma_alloc_attrs
 * @handle: device-view address returned from dma_alloc_attrs
 * @size: size of memory originally requested in dma_alloc_attrs
 * @attrs: attributes of mapping properties requested in dma_alloc_attrs
 *
 * Map a coherent DMA buffer previously allocated by dma_alloc_attrs
 * into user space.  The coherent DMA buffer must not be freed by the
 * driver until the user space mapping has been released.
 */
static inline __attribute__((no_instrument_function)) int
Model1_dma_mmap_attrs(struct Model1_device *Model1_dev, struct Model1_vm_area_struct *Model1_vma, void *Model1_cpu_addr,
        Model1_dma_addr_t Model1_dma_addr, Model1_size_t Model1_size, unsigned long Model1_attrs)
{
 struct Model1_dma_map_ops *Model1_ops = Model1_get_dma_ops(Model1_dev);
 do { if (__builtin_expect(!!(!Model1_ops), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (373), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);
 if (Model1_ops->Model1_mmap)
  return Model1_ops->Model1_mmap(Model1_dev, Model1_vma, Model1_cpu_addr, Model1_dma_addr, Model1_size, Model1_attrs);
 return Model1_dma_common_mmap(Model1_dev, Model1_vma, Model1_cpu_addr, Model1_dma_addr, Model1_size);
}



int
Model1_dma_common_get_sgtable(struct Model1_device *Model1_dev, struct Model1_sg_table *Model1_sgt,
         void *Model1_cpu_addr, Model1_dma_addr_t Model1_dma_addr, Model1_size_t Model1_size);

static inline __attribute__((no_instrument_function)) int
Model1_dma_get_sgtable_attrs(struct Model1_device *Model1_dev, struct Model1_sg_table *Model1_sgt, void *Model1_cpu_addr,
        Model1_dma_addr_t Model1_dma_addr, Model1_size_t Model1_size,
        unsigned long Model1_attrs)
{
 struct Model1_dma_map_ops *Model1_ops = Model1_get_dma_ops(Model1_dev);
 do { if (__builtin_expect(!!(!Model1_ops), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (391), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);
 if (Model1_ops->Model1_get_sgtable)
  return Model1_ops->Model1_get_sgtable(Model1_dev, Model1_sgt, Model1_cpu_addr, Model1_dma_addr, Model1_size,
     Model1_attrs);
 return Model1_dma_common_get_sgtable(Model1_dev, Model1_sgt, Model1_cpu_addr, Model1_dma_addr, Model1_size);
}







static inline __attribute__((no_instrument_function)) void *Model1_dma_alloc_attrs(struct Model1_device *Model1_dev, Model1_size_t Model1_size,
           Model1_dma_addr_t *Model1_dma_handle, Model1_gfp_t Model1_flag,
           unsigned long Model1_attrs)
{
 struct Model1_dma_map_ops *Model1_ops = Model1_get_dma_ops(Model1_dev);
 void *Model1_cpu_addr;

 do { if (__builtin_expect(!!(!Model1_ops), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (411), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);

 if ((0))
  return Model1_cpu_addr;

 if (!Model1_arch_dma_alloc_attrs(&Model1_dev, &Model1_flag))
  return ((void *)0);
 if (!Model1_ops->Model1_alloc)
  return ((void *)0);

 Model1_cpu_addr = Model1_ops->Model1_alloc(Model1_dev, Model1_size, Model1_dma_handle, Model1_flag, Model1_attrs);
 Model1_debug_dma_alloc_coherent(Model1_dev, Model1_size, *Model1_dma_handle, Model1_cpu_addr);
 return Model1_cpu_addr;
}

static inline __attribute__((no_instrument_function)) void Model1_dma_free_attrs(struct Model1_device *Model1_dev, Model1_size_t Model1_size,
         void *Model1_cpu_addr, Model1_dma_addr_t Model1_dma_handle,
         unsigned long Model1_attrs)
{
 struct Model1_dma_map_ops *Model1_ops = Model1_get_dma_ops(Model1_dev);

 do { if (__builtin_expect(!!(!Model1_ops), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/dma-mapping.h"), "i" (432), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);
 ({ int Model1___ret_warn_on = !!(({ unsigned long Model1__flags; do { ({ unsigned long Model1___dummy; typeof(Model1__flags) Model1___dummy2; (void)(&Model1___dummy == &Model1___dummy2); 1; }); Model1__flags = Model1_arch_local_save_flags(); } while (0); ({ ({ unsigned long Model1___dummy; typeof(Model1__flags) Model1___dummy2; (void)(&Model1___dummy == &Model1___dummy2); 1; }); Model1_arch_irqs_disabled_flags(Model1__flags); }); })); if (__builtin_expect(!!(Model1___ret_warn_on), 0)) Model1_warn_slowpath_null("./include/linux/dma-mapping.h", 433); __builtin_expect(!!(Model1___ret_warn_on), 0); });

 if ((0))
  return;

 if (!Model1_ops->Model1_free || !Model1_cpu_addr)
  return;

 Model1_debug_dma_free_coherent(Model1_dev, Model1_size, Model1_cpu_addr, Model1_dma_handle);
 Model1_ops->Model1_free(Model1_dev, Model1_size, Model1_cpu_addr, Model1_dma_handle, Model1_attrs);
}

static inline __attribute__((no_instrument_function)) void *Model1_dma_alloc_coherent(struct Model1_device *Model1_dev, Model1_size_t Model1_size,
  Model1_dma_addr_t *Model1_dma_handle, Model1_gfp_t Model1_flag)
{
 return Model1_dma_alloc_attrs(Model1_dev, Model1_size, Model1_dma_handle, Model1_flag, 0);
}

static inline __attribute__((no_instrument_function)) void Model1_dma_free_coherent(struct Model1_device *Model1_dev, Model1_size_t Model1_size,
  void *Model1_cpu_addr, Model1_dma_addr_t Model1_dma_handle)
{
 return Model1_dma_free_attrs(Model1_dev, Model1_size, Model1_cpu_addr, Model1_dma_handle, 0);
}

static inline __attribute__((no_instrument_function)) void *Model1_dma_alloc_noncoherent(struct Model1_device *Model1_dev, Model1_size_t Model1_size,
  Model1_dma_addr_t *Model1_dma_handle, Model1_gfp_t Model1_gfp)
{
 return Model1_dma_alloc_attrs(Model1_dev, Model1_size, Model1_dma_handle, Model1_gfp,
          (1UL << 3));
}

static inline __attribute__((no_instrument_function)) void Model1_dma_free_noncoherent(struct Model1_device *Model1_dev, Model1_size_t Model1_size,
  void *Model1_cpu_addr, Model1_dma_addr_t Model1_dma_handle)
{
 Model1_dma_free_attrs(Model1_dev, Model1_size, Model1_cpu_addr, Model1_dma_handle,
         (1UL << 3));
}

static inline __attribute__((no_instrument_function)) int Model1_dma_mapping_error(struct Model1_device *Model1_dev, Model1_dma_addr_t Model1_dma_addr)
{
 Model1_debug_dma_mapping_error(Model1_dev, Model1_dma_addr);

 if (Model1_get_dma_ops(Model1_dev)->Model1_mapping_error)
  return Model1_get_dma_ops(Model1_dev)->Model1_mapping_error(Model1_dev, Model1_dma_addr);


 return Model1_dma_addr == 0;



}
static inline __attribute__((no_instrument_function)) int Model1_dma_set_mask(struct Model1_device *Model1_dev, Model1_u64 Model1_mask)
{
 struct Model1_dma_map_ops *Model1_ops = Model1_get_dma_ops(Model1_dev);

 if (Model1_ops->Model1_set_dma_mask)
  return Model1_ops->Model1_set_dma_mask(Model1_dev, Model1_mask);

 if (!Model1_dev->Model1_dma_mask || !Model1_dma_supported(Model1_dev, Model1_mask))
  return -5;
 *Model1_dev->Model1_dma_mask = Model1_mask;
 return 0;
}


static inline __attribute__((no_instrument_function)) Model1_u64 Model1_dma_get_mask(struct Model1_device *Model1_dev)
{
 if (Model1_dev && Model1_dev->Model1_dma_mask && *Model1_dev->Model1_dma_mask)
  return *Model1_dev->Model1_dma_mask;
 return (((32) == 64) ? ~0ULL : ((1ULL<<(32))-1));
}




static inline __attribute__((no_instrument_function)) int Model1_dma_set_coherent_mask(struct Model1_device *Model1_dev, Model1_u64 Model1_mask)
{
 if (!Model1_dma_supported(Model1_dev, Model1_mask))
  return -5;
 Model1_dev->Model1_coherent_dma_mask = Model1_mask;
 return 0;
}


/*
 * Set both the DMA mask and the coherent DMA mask to the same thing.
 * Note that we don't check the return value from dma_set_coherent_mask()
 * as the DMA API guarantees that the coherent DMA mask can be set to
 * the same or smaller than the streaming DMA mask.
 */
static inline __attribute__((no_instrument_function)) int Model1_dma_set_mask_and_coherent(struct Model1_device *Model1_dev, Model1_u64 Model1_mask)
{
 int Model1_rc = Model1_dma_set_mask(Model1_dev, Model1_mask);
 if (Model1_rc == 0)
  Model1_dma_set_coherent_mask(Model1_dev, Model1_mask);
 return Model1_rc;
}

/*
 * Similar to the above, except it deals with the case where the device
 * does not have dev->dma_mask appropriately setup.
 */
static inline __attribute__((no_instrument_function)) int Model1_dma_coerce_mask_and_coherent(struct Model1_device *Model1_dev, Model1_u64 Model1_mask)
{
 Model1_dev->Model1_dma_mask = &Model1_dev->Model1_coherent_dma_mask;
 return Model1_dma_set_mask_and_coherent(Model1_dev, Model1_mask);
}

extern Model1_u64 Model1_dma_get_required_mask(struct Model1_device *Model1_dev);


static inline __attribute__((no_instrument_function)) void Model1_arch_setup_dma_ops(struct Model1_device *Model1_dev, Model1_u64 Model1_dma_base,
          Model1_u64 Model1_size, const struct Model1_iommu_ops *Model1_iommu,
          bool Model1_coherent) { }



static inline __attribute__((no_instrument_function)) void Model1_arch_teardown_dma_ops(struct Model1_device *Model1_dev) { }


static inline __attribute__((no_instrument_function)) unsigned int Model1_dma_get_max_seg_size(struct Model1_device *Model1_dev)
{
 if (Model1_dev->Model1_dma_parms && Model1_dev->Model1_dma_parms->Model1_max_segment_size)
  return Model1_dev->Model1_dma_parms->Model1_max_segment_size;
 return 0x00010000;
}

static inline __attribute__((no_instrument_function)) unsigned int Model1_dma_set_max_seg_size(struct Model1_device *Model1_dev,
      unsigned int Model1_size)
{
 if (Model1_dev->Model1_dma_parms) {
  Model1_dev->Model1_dma_parms->Model1_max_segment_size = Model1_size;
  return 0;
 }
 return -5;
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_dma_get_seg_boundary(struct Model1_device *Model1_dev)
{
 if (Model1_dev->Model1_dma_parms && Model1_dev->Model1_dma_parms->Model1_segment_boundary_mask)
  return Model1_dev->Model1_dma_parms->Model1_segment_boundary_mask;
 return (((32) == 64) ? ~0ULL : ((1ULL<<(32))-1));
}

static inline __attribute__((no_instrument_function)) int Model1_dma_set_seg_boundary(struct Model1_device *Model1_dev, unsigned long Model1_mask)
{
 if (Model1_dev->Model1_dma_parms) {
  Model1_dev->Model1_dma_parms->Model1_segment_boundary_mask = Model1_mask;
  return 0;
 }
 return -5;
}


static inline __attribute__((no_instrument_function)) unsigned long Model1_dma_max_pfn(struct Model1_device *Model1_dev)
{
 return *Model1_dev->Model1_dma_mask >> 12;
}


static inline __attribute__((no_instrument_function)) void *Model1_dma_zalloc_coherent(struct Model1_device *Model1_dev, Model1_size_t Model1_size,
     Model1_dma_addr_t *Model1_dma_handle, Model1_gfp_t Model1_flag)
{
 void *Model1_ret = Model1_dma_alloc_coherent(Model1_dev, Model1_size, Model1_dma_handle,
           Model1_flag | (( Model1_gfp_t)0x8000u));
 return Model1_ret;
}


static inline __attribute__((no_instrument_function)) int Model1_dma_get_cache_alignment(void)
{



 return 1;
}


/* flags for the coherent memory api */
static inline __attribute__((no_instrument_function)) int
Model1_dma_declare_coherent_memory(struct Model1_device *Model1_dev, Model1_phys_addr_t Model1_phys_addr,
       Model1_dma_addr_t Model1_device_addr, Model1_size_t Model1_size, int Model1_flags)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) void
Model1_dma_release_declared_memory(struct Model1_device *Model1_dev)
{
}

static inline __attribute__((no_instrument_function)) void *
Model1_dma_mark_declared_memory_occupied(struct Model1_device *Model1_dev,
      Model1_dma_addr_t Model1_device_addr, Model1_size_t Model1_size)
{
 return Model1_ERR_PTR(-16);
}


/*
 * Managed DMA API
 */
extern void *Model1_dmam_alloc_coherent(struct Model1_device *Model1_dev, Model1_size_t Model1_size,
     Model1_dma_addr_t *Model1_dma_handle, Model1_gfp_t Model1_gfp);
extern void Model1_dmam_free_coherent(struct Model1_device *Model1_dev, Model1_size_t Model1_size, void *Model1_vaddr,
          Model1_dma_addr_t Model1_dma_handle);
extern void *Model1_dmam_alloc_noncoherent(struct Model1_device *Model1_dev, Model1_size_t Model1_size,
        Model1_dma_addr_t *Model1_dma_handle, Model1_gfp_t Model1_gfp);
extern void Model1_dmam_free_noncoherent(struct Model1_device *Model1_dev, Model1_size_t Model1_size, void *Model1_vaddr,
      Model1_dma_addr_t Model1_dma_handle);







static inline __attribute__((no_instrument_function)) int Model1_dmam_declare_coherent_memory(struct Model1_device *Model1_dev,
    Model1_phys_addr_t Model1_phys_addr, Model1_dma_addr_t Model1_device_addr,
    Model1_size_t Model1_size, Model1_gfp_t Model1_gfp)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model1_dmam_release_declared_memory(struct Model1_device *Model1_dev)
{
}


static inline __attribute__((no_instrument_function)) void *Model1_dma_alloc_wc(struct Model1_device *Model1_dev, Model1_size_t Model1_size,
     Model1_dma_addr_t *Model1_dma_addr, Model1_gfp_t Model1_gfp)
{
 return Model1_dma_alloc_attrs(Model1_dev, Model1_size, Model1_dma_addr, Model1_gfp,
          (1UL << 2));
}




static inline __attribute__((no_instrument_function)) void Model1_dma_free_wc(struct Model1_device *Model1_dev, Model1_size_t Model1_size,
          void *Model1_cpu_addr, Model1_dma_addr_t Model1_dma_addr)
{
 return Model1_dma_free_attrs(Model1_dev, Model1_size, Model1_cpu_addr, Model1_dma_addr,
         (1UL << 2));
}




static inline __attribute__((no_instrument_function)) int Model1_dma_mmap_wc(struct Model1_device *Model1_dev,
         struct Model1_vm_area_struct *Model1_vma,
         void *Model1_cpu_addr, Model1_dma_addr_t Model1_dma_addr,
         Model1_size_t Model1_size)
{
 return Model1_dma_mmap_attrs(Model1_dev, Model1_vma, Model1_cpu_addr, Model1_dma_addr, Model1_size,
         (1UL << 2));
}
/*
 * Network device features.
 *
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public License
 * as published by the Free Software Foundation; either version
 * 2 of the License, or (at your option) any later version.
 */





typedef Model1_u64 Model1_netdev_features_t;

enum {
 Model1_NETIF_F_SG_BIT, /* Scatter/gather IO. */
 Model1_NETIF_F_IP_CSUM_BIT, /* Can checksum TCP/UDP over IPv4. */
 Model1___UNUSED_NETIF_F_1,
 Model1_NETIF_F_HW_CSUM_BIT, /* Can checksum all the packets. */
 Model1_NETIF_F_IPV6_CSUM_BIT, /* Can checksum TCP/UDP over IPV6 */
 Model1_NETIF_F_HIGHDMA_BIT, /* Can DMA to high memory. */
 Model1_NETIF_F_FRAGLIST_BIT, /* Scatter/gather IO. */
 Model1_NETIF_F_HW_VLAN_CTAG_TX_BIT, /* Transmit VLAN CTAG HW acceleration */
 Model1_NETIF_F_HW_VLAN_CTAG_RX_BIT, /* Receive VLAN CTAG HW acceleration */
 Model1_NETIF_F_HW_VLAN_CTAG_FILTER_BIT,/* Receive filtering on VLAN CTAGs */
 Model1_NETIF_F_VLAN_CHALLENGED_BIT, /* Device cannot handle VLAN packets */
 Model1_NETIF_F_GSO_BIT, /* Enable software GSO. */
 Model1_NETIF_F_LLTX_BIT, /* LockLess TX - deprecated. Please */
     /* do not use LLTX in new drivers */
 Model1_NETIF_F_NETNS_LOCAL_BIT, /* Does not change network namespaces */
 Model1_NETIF_F_GRO_BIT, /* Generic receive offload */
 Model1_NETIF_F_LRO_BIT, /* large receive offload */

 /**/Model1_NETIF_F_GSO_SHIFT, /* keep the order of SKB_GSO_* bits */
 Model1_NETIF_F_TSO_BIT /* ... TCPv4 segmentation */
  = Model1_NETIF_F_GSO_SHIFT,
 Model1_NETIF_F_UFO_BIT, /* ... UDPv4 fragmentation */
 Model1_NETIF_F_GSO_ROBUST_BIT, /* ... ->SKB_GSO_DODGY */
 Model1_NETIF_F_TSO_ECN_BIT, /* ... TCP ECN support */
 Model1_NETIF_F_TSO_MANGLEID_BIT, /* ... IPV4 ID mangling allowed */
 Model1_NETIF_F_TSO6_BIT, /* ... TCPv6 segmentation */
 Model1_NETIF_F_FSO_BIT, /* ... FCoE segmentation */
 Model1_NETIF_F_GSO_GRE_BIT, /* ... GRE with TSO */
 Model1_NETIF_F_GSO_GRE_CSUM_BIT, /* ... GRE with csum with TSO */
 Model1_NETIF_F_GSO_IPXIP4_BIT, /* ... IP4 or IP6 over IP4 with TSO */
 Model1_NETIF_F_GSO_IPXIP6_BIT, /* ... IP4 or IP6 over IP6 with TSO */
 Model1_NETIF_F_GSO_UDP_TUNNEL_BIT, /* ... UDP TUNNEL with TSO */
 Model1_NETIF_F_GSO_UDP_TUNNEL_CSUM_BIT,/* ... UDP TUNNEL with TSO & CSUM */
 Model1_NETIF_F_GSO_PARTIAL_BIT, /* ... Only segment inner-most L4
					 *     in hardware and all other
					 *     headers in software.
					 */
 Model1_NETIF_F_GSO_TUNNEL_REMCSUM_BIT, /* ... TUNNEL with TSO & REMCSUM */
 Model1_NETIF_F_GSO_SCTP_BIT, /* ... SCTP fragmentation */
 /**/Model1_NETIF_F_GSO_LAST = /* last bit, see GSO_MASK */
  Model1_NETIF_F_GSO_SCTP_BIT,

 Model1_NETIF_F_FCOE_CRC_BIT, /* FCoE CRC32 */
 Model1_NETIF_F_SCTP_CRC_BIT, /* SCTP checksum offload */
 Model1_NETIF_F_FCOE_MTU_BIT, /* Supports max FCoE MTU, 2158 bytes*/
 Model1_NETIF_F_NTUPLE_BIT, /* N-tuple filters supported */
 Model1_NETIF_F_RXHASH_BIT, /* Receive hashing offload */
 Model1_NETIF_F_RXCSUM_BIT, /* Receive checksumming offload */
 Model1_NETIF_F_NOCACHE_COPY_BIT, /* Use no-cache copyfromuser */
 Model1_NETIF_F_LOOPBACK_BIT, /* Enable loopback */
 Model1_NETIF_F_RXFCS_BIT, /* Append FCS to skb pkt data */
 Model1_NETIF_F_RXALL_BIT, /* Receive errored frames too */
 Model1_NETIF_F_HW_VLAN_STAG_TX_BIT, /* Transmit VLAN STAG HW acceleration */
 Model1_NETIF_F_HW_VLAN_STAG_RX_BIT, /* Receive VLAN STAG HW acceleration */
 Model1_NETIF_F_HW_VLAN_STAG_FILTER_BIT,/* Receive filtering on VLAN STAGs */
 Model1_NETIF_F_HW_L2FW_DOFFLOAD_BIT, /* Allow L2 Forwarding in Hardware */
 Model1_NETIF_F_BUSY_POLL_BIT, /* Busy poll */

 Model1_NETIF_F_HW_TC_BIT, /* Offload TC infrastructure */

 /*
	 * Add your fresh new feature above and remember to update
	 * netdev_features_strings[] in net/core/ethtool.c and maybe
	 * some feature mask #defines below. Please also describe it
	 * in Documentation/networking/netdev-features.txt.
	 */

 /**/Model1_NETDEV_FEATURE_COUNT
};

/* copy'n'paste compression ;) */
/* Features valid for ethtool to change */
/* = all defined minus driver/device-class-related */



/* remember that ((t)1 << t_BITS) is undefined in C99 */




/* Segmentation offload feature mask */



/* List of IP checksum features. Note that NETIF_F_ HW_CSUM should not be
 * set in features when NETIF_F_IP_CSUM or NETIF_F_IPV6_CSUM are set--
 * this would be contradictory
 */
/* List of features with software fallbacks. */



/*
 * If one device supports one of these features, then enable them
 * for all in netdev_increment_features.
 */




/*
 * If one device doesn't support one of these features, then disable it
 * for all in netdev_increment_features.
 */


/*
 * If upper/master device has these features disabled, they must be disabled
 * on all lower/slave devices as well.
 */


/* changeable features with no special hardware requirements */





/*
 *	Types and definitions for AF_INET6 
 *	Linux INET6 implementation 
 *
 *	Authors:
 *	Pedro Roque		<roque@di.fc.ul.pt>	
 *
 *	Sources:
 *	IPv6 Program Interfaces for BSD Systems
 *      <draft-ietf-ipngwg-bsd-api-05.txt>
 *
 *	Advanced Sockets API for IPv6
 *	<draft-stevens-advanced-api-00.txt>
 *
 *	This program is free software; you can redistribute it and/or
 *      modify it under the terms of the GNU General Public License
 *      as published by the Free Software Foundation; either version
 *      2 of the License, or (at your option) any later version.
 */




/*
 *	Types and definitions for AF_INET6 
 *	Linux INET6 implementation 
 *
 *	Authors:
 *	Pedro Roque		<roque@di.fc.ul.pt>	
 *
 *	Sources:
 *	IPv6 Program Interfaces for BSD Systems
 *      <draft-ietf-ipngwg-bsd-api-05.txt>
 *
 *	Advanced Sockets API for IPv6
 *	<draft-stevens-advanced-api-00.txt>
 *
 *	This program is free software; you can redistribute it and/or
 *      modify it under the terms of the GNU General Public License
 *      as published by the Free Software Foundation; either version
 *      2 of the License, or (at your option) any later version.
 */







/*
 *	IPv6 address structure
 */


struct Model1_in6_addr {
 union {
  __u8 Model1_u6_addr8[16];

  Model1___be16 Model1_u6_addr16[8];
  Model1___be32 Model1_u6_addr32[4];

 } Model1_in6_u;





};



struct Model1_sockaddr_in6 {
 unsigned short int Model1_sin6_family; /* AF_INET6 */
 Model1___be16 Model1_sin6_port; /* Transport layer port # */
 Model1___be32 Model1_sin6_flowinfo; /* IPv6 flow information */
 struct Model1_in6_addr Model1_sin6_addr; /* IPv6 address */
 __u32 Model1_sin6_scope_id; /* scope id (new in RFC2553) */
};



struct Model1_ipv6_mreq {
 /* IPv6 multicast address of group */
 struct Model1_in6_addr Model1_ipv6mr_multiaddr;

 /* local IPv6 address of interface */
 int Model1_ipv6mr_ifindex;
};




struct Model1_in6_flowlabel_req {
 struct Model1_in6_addr Model1_flr_dst;
 Model1___be32 Model1_flr_label;
 __u8 Model1_flr_action;
 __u8 Model1_flr_share;
 Model1___u16 Model1_flr_flags;
 Model1___u16 Model1_flr_expires;
 Model1___u16 Model1_flr_linger;
 __u32 Model1___flr_pad;
 /* Options in format of IPV6_PKTOPTIONS */
};
/*
 *	Bitmask constant declarations to help applications select out the 
 *	flow label and priority fields.
 *
 *	Note that this are in host byte order while the flowinfo field of
 *	sockaddr_in6 is in network byte order.
 */




/* These definitions are obsolete */
/*
 *	IPV6 extension headers
 */
/*
 *	IPv6 TLV options.
 */







/*
 *	IPV6 socket options
 */
/* IPV6_MTU_DISCOVER values */




/* same as IPV6_PMTUDISC_PROBE, provided for symetry with IPv4
 * also see comments on IP_PMTUDISC_INTERFACE
 */

/* weaker version of IPV6_PMTUDISC_INTERFACE, which allows packets to
 * get fragmented if they exceed the interface mtu
 */


/* Flowlabel */
/*
 * Multicast:
 * Following socket options are shared between IPv4 and IPv6.
 *
 * MCAST_JOIN_GROUP		42
 * MCAST_BLOCK_SOURCE		43
 * MCAST_UNBLOCK_SOURCE		44
 * MCAST_LEAVE_GROUP		45
 * MCAST_JOIN_SOURCE_GROUP	46
 * MCAST_LEAVE_SOURCE_GROUP	47
 * MCAST_MSFILTER		48
 */

/*
 * Advanced API (RFC3542) (1)
 *
 * Note: IPV6_RECVRTHDRDSTOPTS does not exist. see net/ipv6/datagram.c.
 */
/*
 * Netfilter (1)
 *
 * Following socket options are used in ip6_tables;
 * see include/linux/netfilter_ipv6/ip6_tables.h.
 *
 * IP6T_SO_SET_REPLACE / IP6T_SO_GET_INFO		64
 * IP6T_SO_SET_ADD_COUNTERS / IP6T_SO_GET_ENTRIES	65
 */

/*
 * Advanced API (RFC3542) (2)
 */



/*
 * Netfilter (2)
 *
 * Following socket options are used in ip6_tables;
 * see include/linux/netfilter_ipv6/ip6_tables.h.
 *
 * IP6T_SO_GET_REVISION_MATCH	68
 * IP6T_SO_GET_REVISION_TARGET	69
 * IP6T_SO_ORIGINAL_DST		80
 */


/* RFC5014: Source address selection */
/* RFC5082: Generalized Ttl Security Mechanism */







/*
 * Multicast Routing:
 * see include/uapi/linux/mroute6.h.
 *
 * MRT6_BASE			200
 * ...
 * MRT6_MAX
 */

/* IPv6 Wildcard Address (::) and Loopback Address (::1) defined in RFC2553
 * NOTE: Be aware the IN6ADDR_* constants and in6addr_* externals are defined
 * in network byte order, not in host byte order as are the IPv4 equivalents
 */
extern const struct Model1_in6_addr Model1_in6addr_any;

extern const struct Model1_in6_addr Model1_in6addr_loopback;

extern const struct Model1_in6_addr Model1_in6addr_linklocal_allnodes;


extern const struct Model1_in6_addr Model1_in6addr_linklocal_allrouters;


extern const struct Model1_in6_addr Model1_in6addr_interfacelocal_allnodes;


extern const struct Model1_in6_addr Model1_in6addr_interfacelocal_allrouters;


extern const struct Model1_in6_addr Model1_in6addr_sitelocal_allrouters;
/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Global definitions for the Ethernet IEEE 802.3 interface.
 *
 * Version:	@(#)if_ether.h	1.0.1a	02/08/94
 *
 * Author:	Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *		Donald Becker, <becker@super.org>
 *		Alan Cox, <alan@lxorguk.ukuu.org.uk>
 *		Steve Whitehouse, <gw7rrm@eeshack3.swan.ac.uk>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */






/*
 *	IEEE 802.3 Ethernet magic constants.  The frame sizes omit the preamble
 *	and FCS/CRC (frame check sequence).
 */
/*
 *	These are the defined Ethernet Protocol ID's.
 */
/*
 *	Non DIX types. Won't clash for 1500 types.
 */
/*
 *	This is an Ethernet frame header.
 */

struct Model1_ethhdr {
 unsigned char Model1_h_dest[6]; /* destination eth addr	*/
 unsigned char Model1_h_source[6]; /* source ether addr	*/
 Model1___be16 Model1_h_proto; /* packet type ID field	*/
} __attribute__((packed));

/**
 * struct flow_dissector_key_control:
 * @thoff: Transport header offset
 */
struct Model1_flow_dissector_key_control {
 Model1_u16 Model1_thoff;
 Model1_u16 Model1_addr_type;
 Model1_u32 Model1_flags;
};





/**
 * struct flow_dissector_key_basic:
 * @thoff: Transport header offset
 * @n_proto: Network header protocol (eg. IPv4/IPv6)
 * @ip_proto: Transport header protocol (eg. TCP/UDP)
 */
struct Model1_flow_dissector_key_basic {
 Model1___be16 Model1_n_proto;
 Model1_u8 Model1_ip_proto;
 Model1_u8 Model1_padding;
};

struct Model1_flow_dissector_key_tags {
 Model1_u32 Model1_vlan_id:12,
  Model1_flow_label:20;
};

struct Model1_flow_dissector_key_keyid {
 Model1___be32 Model1_keyid;
};

/**
 * struct flow_dissector_key_ipv4_addrs:
 * @src: source ip address
 * @dst: destination ip address
 */
struct Model1_flow_dissector_key_ipv4_addrs {
 /* (src,dst) must be grouped, in the same way than in IP header */
 Model1___be32 Model1_src;
 Model1___be32 Model1_dst;
};

/**
 * struct flow_dissector_key_ipv6_addrs:
 * @src: source ip address
 * @dst: destination ip address
 */
struct Model1_flow_dissector_key_ipv6_addrs {
 /* (src,dst) must be grouped, in the same way than in IP header */
 struct Model1_in6_addr Model1_src;
 struct Model1_in6_addr Model1_dst;
};

/**
 * struct flow_dissector_key_tipc_addrs:
 * @srcnode: source node address
 */
struct Model1_flow_dissector_key_tipc_addrs {
 Model1___be32 Model1_srcnode;
};

/**
 * struct flow_dissector_key_addrs:
 * @v4addrs: IPv4 addresses
 * @v6addrs: IPv6 addresses
 */
struct Model1_flow_dissector_key_addrs {
 union {
  struct Model1_flow_dissector_key_ipv4_addrs Model1_v4addrs;
  struct Model1_flow_dissector_key_ipv6_addrs Model1_v6addrs;
  struct Model1_flow_dissector_key_tipc_addrs Model1_tipcaddrs;
 };
};

/**
 * flow_dissector_key_tp_ports:
 *	@ports: port numbers of Transport header
 *		src: source port number
 *		dst: destination port number
 */
struct Model1_flow_dissector_key_ports {
 union {
  Model1___be32 Model1_ports;
  struct {
   Model1___be16 Model1_src;
   Model1___be16 Model1_dst;
  };
 };
};


/**
 * struct flow_dissector_key_eth_addrs:
 * @src: source Ethernet address
 * @dst: destination Ethernet address
 */
struct Model1_flow_dissector_key_eth_addrs {
 /* (dst,src) must be grouped, in the same way than in ETH header */
 unsigned char Model1_dst[6];
 unsigned char Model1_src[6];
};

enum Model1_flow_dissector_key_id {
 Model1_FLOW_DISSECTOR_KEY_CONTROL, /* struct flow_dissector_key_control */
 Model1_FLOW_DISSECTOR_KEY_BASIC, /* struct flow_dissector_key_basic */
 Model1_FLOW_DISSECTOR_KEY_IPV4_ADDRS, /* struct flow_dissector_key_ipv4_addrs */
 Model1_FLOW_DISSECTOR_KEY_IPV6_ADDRS, /* struct flow_dissector_key_ipv6_addrs */
 Model1_FLOW_DISSECTOR_KEY_PORTS, /* struct flow_dissector_key_ports */
 Model1_FLOW_DISSECTOR_KEY_ETH_ADDRS, /* struct flow_dissector_key_eth_addrs */
 Model1_FLOW_DISSECTOR_KEY_TIPC_ADDRS, /* struct flow_dissector_key_tipc_addrs */
 Model1_FLOW_DISSECTOR_KEY_VLANID, /* struct flow_dissector_key_flow_tags */
 Model1_FLOW_DISSECTOR_KEY_FLOW_LABEL, /* struct flow_dissector_key_flow_tags */
 Model1_FLOW_DISSECTOR_KEY_GRE_KEYID, /* struct flow_dissector_key_keyid */
 Model1_FLOW_DISSECTOR_KEY_MPLS_ENTROPY, /* struct flow_dissector_key_keyid */

 Model1_FLOW_DISSECTOR_KEY_MAX,
};






struct Model1_flow_dissector_key {
 enum Model1_flow_dissector_key_id Model1_key_id;
 Model1_size_t Model1_offset; /* offset of struct flow_dissector_key_*
			  in target the struct */
};

struct Model1_flow_dissector {
 unsigned int Model1_used_keys; /* each bit repesents presence of one key id */
 unsigned short int Model1_offset[Model1_FLOW_DISSECTOR_KEY_MAX];
};

struct Model1_flow_keys {
 struct Model1_flow_dissector_key_control Model1_control;

 struct Model1_flow_dissector_key_basic Model1_basic;
 struct Model1_flow_dissector_key_tags Model1_tags;
 struct Model1_flow_dissector_key_keyid Model1_keyid;
 struct Model1_flow_dissector_key_ports Model1_ports;
 struct Model1_flow_dissector_key_addrs Model1_addrs;
};




Model1___be32 Model1_flow_get_u32_src(const struct Model1_flow_keys *Model1_flow);
Model1___be32 Model1_flow_get_u32_dst(const struct Model1_flow_keys *Model1_flow);

extern struct Model1_flow_dissector Model1_flow_keys_dissector;
extern struct Model1_flow_dissector Model1_flow_keys_buf_dissector;

/* struct flow_keys_digest:
 *
 * This structure is used to hold a digest of the full flow keys. This is a
 * larger "hash" of a flow to allow definitively matching specific flows where
 * the 32 bit skb->hash is not large enough. The size is limited to 16 bytes so
 * that it can by used in CB of skb (see sch_choke for an example).
 */

struct Model1_flow_keys_digest {
 Model1_u8 Model1_data[16];
};

void Model1_make_flow_keys_digest(struct Model1_flow_keys_digest *Model1_digest,
      const struct Model1_flow_keys *Model1_flow);

static inline __attribute__((no_instrument_function)) bool Model1_flow_keys_have_l4(struct Model1_flow_keys *Model1_keys)
{
 return (Model1_keys->Model1_ports.Model1_ports || Model1_keys->Model1_tags.Model1_flow_label);
}

Model1_u32 Model1_flow_hash_from_keys(struct Model1_flow_keys *Model1_keys);

static inline __attribute__((no_instrument_function)) bool Model1_dissector_uses_key(const struct Model1_flow_dissector *Model1_flow_dissector,
          enum Model1_flow_dissector_key_id Model1_key_id)
{
 return Model1_flow_dissector->Model1_used_keys & (1 << Model1_key_id);
}

static inline __attribute__((no_instrument_function)) void *Model1_skb_flow_dissector_target(struct Model1_flow_dissector *Model1_flow_dissector,
           enum Model1_flow_dissector_key_id Model1_key_id,
           void *Model1_target_container)
{
 return ((char *)Model1_target_container) + Model1_flow_dissector->Model1_offset[Model1_key_id];
}
/*
 * Function declerations and data structures related to the splice
 * implementation.
 *
 * Copyright (C) 2007 Jens Axboe <jens.axboe@oracle.com>
 *
 */




/**
 *	struct pipe_buffer - a linux kernel pipe buffer
 *	@page: the page containing the data for the pipe buffer
 *	@offset: offset of data inside the @page
 *	@len: length of data inside the @page
 *	@ops: operations associated with this buffer. See @pipe_buf_operations.
 *	@flags: pipe buffer flags. See above.
 *	@private: private data owned by the ops.
 **/
struct Model1_pipe_buffer {
 struct Model1_page *Model1_page;
 unsigned int Model1_offset, Model1_len;
 const struct Model1_pipe_buf_operations *Model1_ops;
 unsigned int Model1_flags;
 unsigned long Model1_private;
};

/**
 *	struct pipe_inode_info - a linux kernel pipe
 *	@mutex: mutex protecting the whole thing
 *	@wait: reader/writer wait point in case of empty/full pipe
 *	@nrbufs: the number of non-empty pipe buffers in this pipe
 *	@buffers: total number of buffers (should be a power of 2)
 *	@curbuf: the current pipe buffer entry
 *	@tmp_page: cached released page
 *	@readers: number of current readers of this pipe
 *	@writers: number of current writers of this pipe
 *	@files: number of struct file referring this pipe (protected by ->i_lock)
 *	@waiting_writers: number of writers blocked waiting for room
 *	@r_counter: reader counter
 *	@w_counter: writer counter
 *	@fasync_readers: reader side fasync
 *	@fasync_writers: writer side fasync
 *	@bufs: the circular array of pipe buffers
 *	@user: the user who created this pipe
 **/
struct Model1_pipe_inode_info {
 struct Model1_mutex Model1_mutex;
 Model1_wait_queue_head_t Model1_wait;
 unsigned int Model1_nrbufs, Model1_curbuf, Model1_buffers;
 unsigned int Model1_readers;
 unsigned int Model1_writers;
 unsigned int Model1_files;
 unsigned int Model1_waiting_writers;
 unsigned int Model1_r_counter;
 unsigned int Model1_w_counter;
 struct Model1_page *Model1_tmp_page;
 struct Model1_fasync_struct *Model1_fasync_readers;
 struct Model1_fasync_struct *Model1_fasync_writers;
 struct Model1_pipe_buffer *Model1_bufs;
 struct Model1_user_struct *Model1_user;
};

/*
 * Note on the nesting of these functions:
 *
 * ->confirm()
 *	->steal()
 *	...
 *	->map()
 *	...
 *	->unmap()
 *
 * That is, ->map() must be called on a confirmed buffer,
 * same goes for ->steal(). See below for the meaning of each
 * operation. Also see kerneldoc in fs/pipe.c for the pipe
 * and generic variants of these hooks.
 */
struct Model1_pipe_buf_operations {
 /*
	 * This is set to 1, if the generic pipe read/write may coalesce
	 * data into an existing buffer. If this is set to 0, a new pipe
	 * page segment is always used for new data.
	 */
 int Model1_can_merge;

 /*
	 * ->confirm() verifies that the data in the pipe buffer is there
	 * and that the contents are good. If the pages in the pipe belong
	 * to a file system, we may need to wait for IO completion in this
	 * hook. Returns 0 for good, or a negative error value in case of
	 * error.
	 */
 int (*Model1_confirm)(struct Model1_pipe_inode_info *, struct Model1_pipe_buffer *);

 /*
	 * When the contents of this pipe buffer has been completely
	 * consumed by a reader, ->release() is called.
	 */
 void (*Model1_release)(struct Model1_pipe_inode_info *, struct Model1_pipe_buffer *);

 /*
	 * Attempt to take ownership of the pipe buffer and its contents.
	 * ->steal() returns 0 for success, in which case the contents
	 * of the pipe (the buf->page) is locked and now completely owned
	 * by the caller. The page may then be transferred to a different
	 * mapping, the most often used case is insertion into different
	 * file address space cache.
	 */
 int (*Model1_steal)(struct Model1_pipe_inode_info *, struct Model1_pipe_buffer *);

 /*
	 * Get a reference to the pipe buffer.
	 */
 void (*Model1_get)(struct Model1_pipe_inode_info *, struct Model1_pipe_buffer *);
};

/* Differs from PIPE_BUF in that PIPE_SIZE is the length of the actual
   memory allocation, whereas PIPE_BUF makes atomicity guarantees.  */


/* Pipe lock and unlock operations */
void Model1_pipe_lock(struct Model1_pipe_inode_info *);
void Model1_pipe_unlock(struct Model1_pipe_inode_info *);
void Model1_pipe_double_lock(struct Model1_pipe_inode_info *, struct Model1_pipe_inode_info *);

extern unsigned int Model1_pipe_max_size, Model1_pipe_min_size;
extern unsigned long Model1_pipe_user_pages_hard;
extern unsigned long Model1_pipe_user_pages_soft;
int Model1_pipe_proc_fn(struct Model1_ctl_table *, int, void *, Model1_size_t *, Model1_loff_t *);


/* Drop the inode semaphore and wait for a pipe event, atomically */
void Model1_pipe_wait(struct Model1_pipe_inode_info *Model1_pipe);

struct Model1_pipe_inode_info *Model1_alloc_pipe_info(void);
void Model1_free_pipe_info(struct Model1_pipe_inode_info *);

/* Generic pipe buffer ops functions */
void Model1_generic_pipe_buf_get(struct Model1_pipe_inode_info *, struct Model1_pipe_buffer *);
int Model1_generic_pipe_buf_confirm(struct Model1_pipe_inode_info *, struct Model1_pipe_buffer *);
int Model1_generic_pipe_buf_steal(struct Model1_pipe_inode_info *, struct Model1_pipe_buffer *);
void Model1_generic_pipe_buf_release(struct Model1_pipe_inode_info *, struct Model1_pipe_buffer *);

extern const struct Model1_pipe_buf_operations Model1_nosteal_pipe_buf_ops;

/* for F_SETPIPE_SZ and F_GETPIPE_SZ */
long Model1_pipe_fcntl(struct Model1_file *, unsigned int, unsigned long Model1_arg);
struct Model1_pipe_inode_info *Model1_get_pipe_info(struct Model1_file *Model1_file);

int Model1_create_pipe_files(struct Model1_file **, int);

/*
 * Flags passed in from splice/tee/vmsplice
 */


     /* we may still block on the fd we splice */
     /* from/to, of course */



/*
 * Passed to the actors
 */
struct Model1_splice_desc {
 Model1_size_t Model1_total_len; /* remaining length */
 unsigned int Model1_len; /* current length */
 unsigned int Model1_flags; /* splice flags */
 /*
	 * actor() private data
	 */
 union {
  void *Model1_userptr; /* memory to write to */
  struct Model1_file *Model1_file; /* file to read/write */
  void *Model1_data; /* cookie */
 } Model1_u;
 Model1_loff_t Model1_pos; /* file position */
 Model1_loff_t *Model1_opos; /* sendfile: output position */
 Model1_size_t Model1_num_spliced; /* number of bytes already spliced */
 bool Model1_need_wakeup; /* need to wake up writer */
};

struct Model1_partial_page {
 unsigned int Model1_offset;
 unsigned int Model1_len;
 unsigned long Model1_private;
};

/*
 * Passed to splice_to_pipe
 */
struct Model1_splice_pipe_desc {
 struct Model1_page **Model1_pages; /* page map */
 struct Model1_partial_page *Model1_partial; /* pages[] may not be contig */
 int Model1_nr_pages; /* number of populated pages in map */
 unsigned int Model1_nr_pages_max; /* pages[] & partial[] arrays size */
 unsigned int Model1_flags; /* splice flags */
 const struct Model1_pipe_buf_operations *Model1_ops;/* ops associated with output pipe */
 void (*Model1_spd_release)(struct Model1_splice_pipe_desc *, unsigned int);
};

typedef int (Model1_splice_actor)(struct Model1_pipe_inode_info *, struct Model1_pipe_buffer *,
      struct Model1_splice_desc *);
typedef int (Model1_splice_direct_actor)(struct Model1_pipe_inode_info *,
      struct Model1_splice_desc *);

extern Model1_ssize_t Model1_splice_from_pipe(struct Model1_pipe_inode_info *, struct Model1_file *,
    Model1_loff_t *, Model1_size_t, unsigned int,
    Model1_splice_actor *);
extern Model1_ssize_t Model1___splice_from_pipe(struct Model1_pipe_inode_info *,
      struct Model1_splice_desc *, Model1_splice_actor *);
extern Model1_ssize_t Model1_splice_to_pipe(struct Model1_pipe_inode_info *,
         struct Model1_splice_pipe_desc *);
extern Model1_ssize_t Model1_splice_direct_to_actor(struct Model1_file *, struct Model1_splice_desc *,
          Model1_splice_direct_actor *);

/*
 * for dynamic pipe sizing
 */
extern int Model1_splice_grow_spd(const struct Model1_pipe_inode_info *, struct Model1_splice_pipe_desc *);
extern void Model1_splice_shrink_spd(struct Model1_splice_pipe_desc *);
extern void Model1_spd_release_page(struct Model1_splice_pipe_desc *, unsigned int);

extern const struct Model1_pipe_buf_operations Model1_page_cache_pipe_buf_ops;






struct Model1_sockaddr_pkt {
 unsigned short Model1_spkt_family;
 unsigned char Model1_spkt_device[14];
 Model1___be16 Model1_spkt_protocol;
};

struct Model1_sockaddr_ll {
 unsigned short Model1_sll_family;
 Model1___be16 Model1_sll_protocol;
 int Model1_sll_ifindex;
 unsigned short Model1_sll_hatype;
 unsigned char Model1_sll_pkttype;
 unsigned char Model1_sll_halen;
 unsigned char Model1_sll_addr[8];
};

/* Packet types */
/* Unused, PACKET_FASTROUTE and PACKET_LOOPBACK are invisible to user space */


/* Packet socket options */




/* Value 4 is still used by obsolete turbo-packet. */
struct Model1_tpacket_stats {
 unsigned int Model1_tp_packets;
 unsigned int Model1_tp_drops;
};

struct Model1_tpacket_stats_v3 {
 unsigned int Model1_tp_packets;
 unsigned int Model1_tp_drops;
 unsigned int Model1_tp_freeze_q_cnt;
};

struct Model1_tpacket_rollover_stats {
 __u64 __attribute__((aligned(8))) Model1_tp_all;
 __u64 __attribute__((aligned(8))) Model1_tp_huge;
 __u64 __attribute__((aligned(8))) Model1_tp_failed;
};

union Model1_tpacket_stats_u {
 struct Model1_tpacket_stats Model1_stats1;
 struct Model1_tpacket_stats_v3 Model1_stats3;
};

struct Model1_tpacket_auxdata {
 __u32 Model1_tp_status;
 __u32 Model1_tp_len;
 __u32 Model1_tp_snaplen;
 Model1___u16 Model1_tp_mac;
 Model1___u16 Model1_tp_net;
 Model1___u16 Model1_tp_vlan_tci;
 Model1___u16 Model1_tp_vlan_tpid;
};

/* Rx ring - header status */
/* Tx ring - header status */





/* Rx and Tx ring - header status */




/* Rx ring - feature request bits */


struct Model1_tpacket_hdr {
 unsigned long Model1_tp_status;
 unsigned int Model1_tp_len;
 unsigned int Model1_tp_snaplen;
 unsigned short Model1_tp_mac;
 unsigned short Model1_tp_net;
 unsigned int Model1_tp_sec;
 unsigned int Model1_tp_usec;
};





struct Model1_tpacket2_hdr {
 __u32 Model1_tp_status;
 __u32 Model1_tp_len;
 __u32 Model1_tp_snaplen;
 Model1___u16 Model1_tp_mac;
 Model1___u16 Model1_tp_net;
 __u32 Model1_tp_sec;
 __u32 Model1_tp_nsec;
 Model1___u16 Model1_tp_vlan_tci;
 Model1___u16 Model1_tp_vlan_tpid;
 __u8 Model1_tp_padding[4];
};

struct Model1_tpacket_hdr_variant1 {
 __u32 Model1_tp_rxhash;
 __u32 Model1_tp_vlan_tci;
 Model1___u16 Model1_tp_vlan_tpid;
 Model1___u16 Model1_tp_padding;
};

struct Model1_tpacket3_hdr {
 __u32 Model1_tp_next_offset;
 __u32 Model1_tp_sec;
 __u32 Model1_tp_nsec;
 __u32 Model1_tp_snaplen;
 __u32 Model1_tp_len;
 __u32 Model1_tp_status;
 Model1___u16 Model1_tp_mac;
 Model1___u16 Model1_tp_net;
 /* pkt_hdr variants */
 union {
  struct Model1_tpacket_hdr_variant1 Model1_hv1;
 };
 __u8 Model1_tp_padding[8];
};

struct Model1_tpacket_bd_ts {
 unsigned int Model1_ts_sec;
 union {
  unsigned int Model1_ts_usec;
  unsigned int Model1_ts_nsec;
 };
};

struct Model1_tpacket_hdr_v1 {
 __u32 Model1_block_status;
 __u32 Model1_num_pkts;
 __u32 Model1_offset_to_first_pkt;

 /* Number of valid bytes (including padding)
	 * blk_len <= tp_block_size
	 */
 __u32 Model1_blk_len;

 /*
	 * Quite a few uses of sequence number:
	 * 1. Make sure cache flush etc worked.
	 *    Well, one can argue - why not use the increasing ts below?
	 *    But look at 2. below first.
	 * 2. When you pass around blocks to other user space decoders,
	 *    you can see which blk[s] is[are] outstanding etc.
	 * 3. Validate kernel code.
	 */
 __u64 __attribute__((aligned(8))) Model1_seq_num;

 /*
	 * ts_last_pkt:
	 *
	 * Case 1.	Block has 'N'(N >=1) packets and TMO'd(timed out)
	 *		ts_last_pkt == 'time-stamp of last packet' and NOT the
	 *		time when the timer fired and the block was closed.
	 *		By providing the ts of the last packet we can absolutely
	 *		guarantee that time-stamp wise, the first packet in the
	 *		next block will never precede the last packet of the
	 *		previous block.
	 * Case 2.	Block has zero packets and TMO'd
	 *		ts_last_pkt = time when the timer fired and the block
	 *		was closed.
	 * Case 3.	Block has 'N' packets and NO TMO.
	 *		ts_last_pkt = time-stamp of the last pkt in the block.
	 *
	 * ts_first_pkt:
	 *		Is always the time-stamp when the block was opened.
	 *		Case a)	ZERO packets
	 *			No packets to deal with but atleast you know the
	 *			time-interval of this block.
	 *		Case b) Non-zero packets
	 *			Use the ts of the first packet in the block.
	 *
	 */
 struct Model1_tpacket_bd_ts Model1_ts_first_pkt, Model1_ts_last_pkt;
};

union Model1_tpacket_bd_header_u {
 struct Model1_tpacket_hdr_v1 Model1_bh1;
};

struct Model1_tpacket_block_desc {
 __u32 Model1_version;
 __u32 Model1_offset_to_priv;
 union Model1_tpacket_bd_header_u Model1_hdr;
};




enum Model1_tpacket_versions {
 Model1_TPACKET_V1,
 Model1_TPACKET_V2,
 Model1_TPACKET_V3
};

/*
   Frame structure:

   - Start. Frame must be aligned to TPACKET_ALIGNMENT=16
   - struct tpacket_hdr
   - pad to TPACKET_ALIGNMENT=16
   - struct sockaddr_ll
   - Gap, chosen so that packet data (Start+tp_net) alignes to TPACKET_ALIGNMENT=16
   - Start+tp_mac: [ Optional MAC header ]
   - Start+tp_net: Packet data, aligned to TPACKET_ALIGNMENT=16.
   - Pad to align to TPACKET_ALIGNMENT=16
 */

struct Model1_tpacket_req {
 unsigned int Model1_tp_block_size; /* Minimal size of contiguous block */
 unsigned int Model1_tp_block_nr; /* Number of blocks */
 unsigned int Model1_tp_frame_size; /* Size of frame */
 unsigned int Model1_tp_frame_nr; /* Total number of frames */
};

struct Model1_tpacket_req3 {
 unsigned int Model1_tp_block_size; /* Minimal size of contiguous block */
 unsigned int Model1_tp_block_nr; /* Number of blocks */
 unsigned int Model1_tp_frame_size; /* Size of frame */
 unsigned int Model1_tp_frame_nr; /* Total number of frames */
 unsigned int Model1_tp_retire_blk_tov; /* timeout in msecs */
 unsigned int Model1_tp_sizeof_priv; /* offset to private data area */
 unsigned int Model1_tp_feature_req_word;
};

union Model1_tpacket_req_u {
 struct Model1_tpacket_req Model1_req;
 struct Model1_tpacket_req3 Model1_req3;
};

struct Model1_packet_mreq {
 int Model1_mr_ifindex;
 unsigned short Model1_mr_type;
 unsigned short Model1_mr_alen;
 unsigned char Model1_mr_address[8];
};
/*
 *
 *	Generic internet FLOW.
 *
 */
/*
 * ifindex generation is per-net namespace, and loopback is
 * always the 1st device in ns (see net_dev_init), thus any
 * loopback device should get ifindex 1
 */



struct Model1_flowi_tunnel {
 Model1___be64 Model1_tun_id;
};

struct Model1_flowi_common {
 int Model1_flowic_oif;
 int Model1_flowic_iif;
 __u32 Model1_flowic_mark;
 __u8 Model1_flowic_tos;
 __u8 Model1_flowic_scope;
 __u8 Model1_flowic_proto;
 __u8 Model1_flowic_flags;




 __u32 Model1_flowic_secid;
 struct Model1_flowi_tunnel Model1_flowic_tun_key;
};

union Model1_flowi_uli {
 struct {
  Model1___be16 Model1_dport;
  Model1___be16 Model1_sport;
 } Model1_ports;

 struct {
  __u8 Model1_type;
  __u8 Model1_code;
 } Model1_icmpt;

 struct {
  Model1___le16 Model1_dport;
  Model1___le16 Model1_sport;
 } Model1_dnports;

 Model1___be32 Model1_spi;
 Model1___be32 Model1_gre_key;

 struct {
  __u8 Model1_type;
 } Model1_mht;
};

struct Model1_flowi4 {
 struct Model1_flowi_common Model1___fl_common;
 /* (saddr,daddr) must be grouped, same order as in IP header */
 Model1___be32 Model1_saddr;
 Model1___be32 Model1_daddr;

 union Model1_flowi_uli Model1_uli;







} __attribute__((__aligned__(64/8)));

static inline __attribute__((no_instrument_function)) void Model1_flowi4_init_output(struct Model1_flowi4 *Model1_fl4, int Model1_oif,
          __u32 Model1_mark, __u8 Model1_tos, __u8 Model1_scope,
          __u8 Model1_proto, __u8 Model1_flags,
          Model1___be32 Model1_daddr, Model1___be32 Model1_saddr,
          Model1___be16 Model1_dport, Model1___be16 Model1_sport)
{
 Model1_fl4->Model1___fl_common.Model1_flowic_oif = Model1_oif;
 Model1_fl4->Model1___fl_common.Model1_flowic_iif = 1;
 Model1_fl4->Model1___fl_common.Model1_flowic_mark = Model1_mark;
 Model1_fl4->Model1___fl_common.Model1_flowic_tos = Model1_tos;
 Model1_fl4->Model1___fl_common.Model1_flowic_scope = Model1_scope;
 Model1_fl4->Model1___fl_common.Model1_flowic_proto = Model1_proto;
 Model1_fl4->Model1___fl_common.Model1_flowic_flags = Model1_flags;
 Model1_fl4->Model1___fl_common.Model1_flowic_secid = 0;
 Model1_fl4->Model1___fl_common.Model1_flowic_tun_key.Model1_tun_id = 0;
 Model1_fl4->Model1_daddr = Model1_daddr;
 Model1_fl4->Model1_saddr = Model1_saddr;
 Model1_fl4->Model1_uli.Model1_ports.Model1_dport = Model1_dport;
 Model1_fl4->Model1_uli.Model1_ports.Model1_sport = Model1_sport;
}

/* Reset some input parameters after previous lookup */
static inline __attribute__((no_instrument_function)) void Model1_flowi4_update_output(struct Model1_flowi4 *Model1_fl4, int Model1_oif, __u8 Model1_tos,
     Model1___be32 Model1_daddr, Model1___be32 Model1_saddr)
{
 Model1_fl4->Model1___fl_common.Model1_flowic_oif = Model1_oif;
 Model1_fl4->Model1___fl_common.Model1_flowic_tos = Model1_tos;
 Model1_fl4->Model1_daddr = Model1_daddr;
 Model1_fl4->Model1_saddr = Model1_saddr;
}


struct Model1_flowi6 {
 struct Model1_flowi_common Model1___fl_common;
 struct Model1_in6_addr Model1_daddr;
 struct Model1_in6_addr Model1_saddr;
 /* Note: flowi6_tos is encoded in flowlabel, too. */
 Model1___be32 Model1_flowlabel;
 union Model1_flowi_uli Model1_uli;







} __attribute__((__aligned__(64/8)));

struct Model1_flowidn {
 struct Model1_flowi_common Model1___fl_common;






 Model1___le16 Model1_daddr;
 Model1___le16 Model1_saddr;
 union Model1_flowi_uli Model1_uli;


} __attribute__((__aligned__(64/8)));

struct Model1_flowi {
 union {
  struct Model1_flowi_common Model1___fl_common;
  struct Model1_flowi4 Model1_ip4;
  struct Model1_flowi6 Model1_ip6;
  struct Model1_flowidn Model1_dn;
 } Model1_u;
} __attribute__((__aligned__(64/8)));

static inline __attribute__((no_instrument_function)) struct Model1_flowi *Model1_flowi4_to_flowi(struct Model1_flowi4 *Model1_fl4)
{
 return ({ const typeof( ((struct Model1_flowi *)0)->Model1_u.Model1_ip4 ) *Model1___mptr = (Model1_fl4); (struct Model1_flowi *)( (char *)Model1___mptr - __builtin_offsetof(struct Model1_flowi, Model1_u.Model1_ip4) );});
}

static inline __attribute__((no_instrument_function)) struct Model1_flowi *Model1_flowi6_to_flowi(struct Model1_flowi6 *Model1_fl6)
{
 return ({ const typeof( ((struct Model1_flowi *)0)->Model1_u.Model1_ip6 ) *Model1___mptr = (Model1_fl6); (struct Model1_flowi *)( (char *)Model1___mptr - __builtin_offsetof(struct Model1_flowi, Model1_u.Model1_ip6) );});
}

static inline __attribute__((no_instrument_function)) struct Model1_flowi *Model1_flowidn_to_flowi(struct Model1_flowidn *Model1_fldn)
{
 return ({ const typeof( ((struct Model1_flowi *)0)->Model1_u.Model1_dn ) *Model1___mptr = (Model1_fldn); (struct Model1_flowi *)( (char *)Model1___mptr - __builtin_offsetof(struct Model1_flowi, Model1_u.Model1_dn) );});
}

typedef unsigned long Model1_flow_compare_t;

static inline __attribute__((no_instrument_function)) Model1_size_t Model1_flow_key_size(Model1_u16 Model1_family)
{
 switch (Model1_family) {
 case 2:
  do { bool Model1___cond = !(!(sizeof(struct Model1_flowi4) % sizeof(Model1_flow_compare_t))); extern void Model1___compiletime_assert_203(void) ; if (Model1___cond) Model1___compiletime_assert_203(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
  return sizeof(struct Model1_flowi4) / sizeof(Model1_flow_compare_t);
 case 10:
  do { bool Model1___cond = !(!(sizeof(struct Model1_flowi6) % sizeof(Model1_flow_compare_t))); extern void Model1___compiletime_assert_206(void) ; if (Model1___cond) Model1___compiletime_assert_206(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
  return sizeof(struct Model1_flowi6) / sizeof(Model1_flow_compare_t);
 case 12:
  do { bool Model1___cond = !(!(sizeof(struct Model1_flowidn) % sizeof(Model1_flow_compare_t))); extern void Model1___compiletime_assert_209(void) ; if (Model1___cond) Model1___compiletime_assert_209(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
  return sizeof(struct Model1_flowidn) / sizeof(Model1_flow_compare_t);
 }
 return 0;
}





struct Model1_net;
struct Model1_sock;
struct Model1_flow_cache_ops;

struct Model1_flow_cache_object {
 const struct Model1_flow_cache_ops *Model1_ops;
};

struct Model1_flow_cache_ops {
 struct Model1_flow_cache_object *(*Model1_get)(struct Model1_flow_cache_object *);
 int (*Model1_check)(struct Model1_flow_cache_object *);
 void (*Model1_delete)(struct Model1_flow_cache_object *);
};

typedef struct Model1_flow_cache_object *(*Model1_flow_resolve_t)(
  struct Model1_net *Model1_net, const struct Model1_flowi *Model1_key, Model1_u16 Model1_family,
  Model1_u8 Model1_dir, struct Model1_flow_cache_object *Model1_oldobj, void *Model1_ctx);

struct Model1_flow_cache_object *Model1_flow_cache_lookup(struct Model1_net *Model1_net,
         const struct Model1_flowi *Model1_key, Model1_u16 Model1_family,
         Model1_u8 Model1_dir, Model1_flow_resolve_t Model1_resolver,
         void *Model1_ctx);
int Model1_flow_cache_init(struct Model1_net *Model1_net);
void Model1_flow_cache_fini(struct Model1_net *Model1_net);

void Model1_flow_cache_flush(struct Model1_net *Model1_net);
void Model1_flow_cache_flush_deferred(struct Model1_net *Model1_net);
extern Model1_atomic_t Model1_flow_cache_genid;

__u32 Model1___get_hash_from_flowi6(const struct Model1_flowi6 *Model1_fl6, struct Model1_flow_keys *Model1_keys);

static inline __attribute__((no_instrument_function)) __u32 Model1_get_hash_from_flowi6(const struct Model1_flowi6 *Model1_fl6)
{
 struct Model1_flow_keys Model1_keys;

 return Model1___get_hash_from_flowi6(Model1_fl6, &Model1_keys);
}

__u32 Model1___get_hash_from_flowi4(const struct Model1_flowi4 *Model1_fl4, struct Model1_flow_keys *Model1_keys);

static inline __attribute__((no_instrument_function)) __u32 Model1_get_hash_from_flowi4(const struct Model1_flowi4 *Model1_fl4)
{
 struct Model1_flow_keys Model1_keys;

 return Model1___get_hash_from_flowi4(Model1_fl4, &Model1_keys);
}

/* The interface for checksum offload between the stack and networking drivers
 * is as follows...
 *
 * A. IP checksum related features
 *
 * Drivers advertise checksum offload capabilities in the features of a device.
 * From the stack's point of view these are capabilities offered by the driver,
 * a driver typically only advertises features that it is capable of offloading
 * to its device.
 *
 * The checksum related features are:
 *
 *	NETIF_F_HW_CSUM	- The driver (or its device) is able to compute one
 *			  IP (one's complement) checksum for any combination
 *			  of protocols or protocol layering. The checksum is
 *			  computed and set in a packet per the CHECKSUM_PARTIAL
 *			  interface (see below).
 *
 *	NETIF_F_IP_CSUM - Driver (device) is only able to checksum plain
 *			  TCP or UDP packets over IPv4. These are specifically
 *			  unencapsulated packets of the form IPv4|TCP or
 *			  IPv4|UDP where the Protocol field in the IPv4 header
 *			  is TCP or UDP. The IPv4 header may contain IP options
 *			  This feature cannot be set in features for a device
 *			  with NETIF_F_HW_CSUM also set. This feature is being
 *			  DEPRECATED (see below).
 *
 *	NETIF_F_IPV6_CSUM - Driver (device) is only able to checksum plain
 *			  TCP or UDP packets over IPv6. These are specifically
 *			  unencapsulated packets of the form IPv6|TCP or
 *			  IPv4|UDP where the Next Header field in the IPv6
 *			  header is either TCP or UDP. IPv6 extension headers
 *			  are not supported with this feature. This feature
 *			  cannot be set in features for a device with
 *			  NETIF_F_HW_CSUM also set. This feature is being
 *			  DEPRECATED (see below).
 *
 *	NETIF_F_RXCSUM - Driver (device) performs receive checksum offload.
 *			 This flag is used only used to disable the RX checksum
 *			 feature for a device. The stack will accept receive
 *			 checksum indication in packets received on a device
 *			 regardless of whether NETIF_F_RXCSUM is set.
 *
 * B. Checksumming of received packets by device. Indication of checksum
 *    verification is in set skb->ip_summed. Possible values are:
 *
 * CHECKSUM_NONE:
 *
 *   Device did not checksum this packet e.g. due to lack of capabilities.
 *   The packet contains full (though not verified) checksum in packet but
 *   not in skb->csum. Thus, skb->csum is undefined in this case.
 *
 * CHECKSUM_UNNECESSARY:
 *
 *   The hardware you're dealing with doesn't calculate the full checksum
 *   (as in CHECKSUM_COMPLETE), but it does parse headers and verify checksums
 *   for specific protocols. For such packets it will set CHECKSUM_UNNECESSARY
 *   if their checksums are okay. skb->csum is still undefined in this case
 *   though. A driver or device must never modify the checksum field in the
 *   packet even if checksum is verified.
 *
 *   CHECKSUM_UNNECESSARY is applicable to following protocols:
 *     TCP: IPv6 and IPv4.
 *     UDP: IPv4 and IPv6. A device may apply CHECKSUM_UNNECESSARY to a
 *       zero UDP checksum for either IPv4 or IPv6, the networking stack
 *       may perform further validation in this case.
 *     GRE: only if the checksum is present in the header.
 *     SCTP: indicates the CRC in SCTP header has been validated.
 *
 *   skb->csum_level indicates the number of consecutive checksums found in
 *   the packet minus one that have been verified as CHECKSUM_UNNECESSARY.
 *   For instance if a device receives an IPv6->UDP->GRE->IPv4->TCP packet
 *   and a device is able to verify the checksums for UDP (possibly zero),
 *   GRE (checksum flag is set), and TCP-- skb->csum_level would be set to
 *   two. If the device were only able to verify the UDP checksum and not
 *   GRE, either because it doesn't support GRE checksum of because GRE
 *   checksum is bad, skb->csum_level would be set to zero (TCP checksum is
 *   not considered in this case).
 *
 * CHECKSUM_COMPLETE:
 *
 *   This is the most generic way. The device supplied checksum of the _whole_
 *   packet as seen by netif_rx() and fills out in skb->csum. Meaning, the
 *   hardware doesn't need to parse L3/L4 headers to implement this.
 *
 *   Note: Even if device supports only some protocols, but is able to produce
 *   skb->csum, it MUST use CHECKSUM_COMPLETE, not CHECKSUM_UNNECESSARY.
 *
 * CHECKSUM_PARTIAL:
 *
 *   A checksum is set up to be offloaded to a device as described in the
 *   output description for CHECKSUM_PARTIAL. This may occur on a packet
 *   received directly from another Linux OS, e.g., a virtualized Linux kernel
 *   on the same host, or it may be set in the input path in GRO or remote
 *   checksum offload. For the purposes of checksum verification, the checksum
 *   referred to by skb->csum_start + skb->csum_offset and any preceding
 *   checksums in the packet are considered verified. Any checksums in the
 *   packet that are after the checksum being offloaded are not considered to
 *   be verified.
 *
 * C. Checksumming on transmit for non-GSO. The stack requests checksum offload
 *    in the skb->ip_summed for a packet. Values are:
 *
 * CHECKSUM_PARTIAL:
 *
 *   The driver is required to checksum the packet as seen by hard_start_xmit()
 *   from skb->csum_start up to the end, and to record/write the checksum at
 *   offset skb->csum_start + skb->csum_offset. A driver may verify that the
 *   csum_start and csum_offset values are valid values given the length and
 *   offset of the packet, however they should not attempt to validate that the
 *   checksum refers to a legitimate transport layer checksum-- it is the
 *   purview of the stack to validate that csum_start and csum_offset are set
 *   correctly.
 *
 *   When the stack requests checksum offload for a packet, the driver MUST
 *   ensure that the checksum is set correctly. A driver can either offload the
 *   checksum calculation to the device, or call skb_checksum_help (in the case
 *   that the device does not support offload for a particular checksum).
 *
 *   NETIF_F_IP_CSUM and NETIF_F_IPV6_CSUM are being deprecated in favor of
 *   NETIF_F_HW_CSUM. New devices should use NETIF_F_HW_CSUM to indicate
 *   checksum offload capability. If a	device has limited checksum capabilities
 *   (for instance can only perform NETIF_F_IP_CSUM or NETIF_F_IPV6_CSUM as
 *   described above) a helper function can be called to resolve
 *   CHECKSUM_PARTIAL. The helper functions are skb_csum_off_chk*. The helper
 *   function takes a spec argument that describes the protocol layer that is
 *   supported for checksum offload and can be called for each packet. If a
 *   packet does not match the specification for offload, skb_checksum_help
 *   is called to resolve the checksum.
 *
 * CHECKSUM_NONE:
 *
 *   The skb was already checksummed by the protocol, or a checksum is not
 *   required.
 *
 * CHECKSUM_UNNECESSARY:
 *
 *   This has the same meaning on as CHECKSUM_NONE for checksum offload on
 *   output.
 *
 * CHECKSUM_COMPLETE:
 *   Not used in checksum output. If a driver observes a packet with this value
 *   set in skbuff, if should treat as CHECKSUM_NONE being set.
 *
 * D. Non-IP checksum (CRC) offloads
 *
 *   NETIF_F_SCTP_CRC - This feature indicates that a device is capable of
 *     offloading the SCTP CRC in a packet. To perform this offload the stack
 *     will set ip_summed to CHECKSUM_PARTIAL and set csum_start and csum_offset
 *     accordingly. Note the there is no indication in the skbuff that the
 *     CHECKSUM_PARTIAL refers to an SCTP checksum, a driver that supports
 *     both IP checksum offload and SCTP CRC offload must verify which offload
 *     is configured for a packet presumably by inspecting packet headers.
 *
 *   NETIF_F_FCOE_CRC - This feature indicates that a device is capable of
 *     offloading the FCOE CRC in a packet. To perform this offload the stack
 *     will set ip_summed to CHECKSUM_PARTIAL and set csum_start and csum_offset
 *     accordingly. Note the there is no indication in the skbuff that the
 *     CHECKSUM_PARTIAL refers to an FCOE checksum, a driver that supports
 *     both IP checksum offload and FCOE CRC offload must verify which offload
 *     is configured for a packet presumably by inspecting packet headers.
 *
 * E. Checksumming on output with GSO.
 *
 * In the case of a GSO packet (skb_is_gso(skb) is true), checksum offload
 * is implied by the SKB_GSO_* flags in gso_type. Most obviously, if the
 * gso_type is SKB_GSO_TCPV4 or SKB_GSO_TCPV6, TCP checksum offload as
 * part of the GSO operation is implied. If a checksum is being offloaded
 * with GSO then ip_summed is CHECKSUM_PARTIAL, csum_start and csum_offset
 * are set to refer to the outermost checksum being offload (two offloaded
 * checksums are possible with UDP encapsulation).
 */

/* Don't change this without changing skb_csum_unnecessary! */





/* Maximum value in skb->csum_level */
/* return minimum truesize of one skb containing X bytes of data */




struct Model1_net_device;
struct Model1_scatterlist;
struct Model1_pipe_inode_info;
struct Model1_iov_iter;
struct Model1_napi_struct;


struct Model1_nf_conntrack {
 Model1_atomic_t Model1_use;
};
struct Model1_sk_buff_head {
 /* These two members must be first. */
 struct Model1_sk_buff *Model1_next;
 struct Model1_sk_buff *Model1_prev;

 __u32 Model1_qlen;
 Model1_spinlock_t Model1_lock;
};

struct Model1_sk_buff;

/* To allow 64K frame to be packed as single skb without frag_list we
 * require 64K/PAGE_SIZE pages plus 1 additional page to allow for
 * buffers which do not start on a page boundary.
 *
 * Since GRO uses frags we allocate at least 16 regardless of page
 * size.
 */





extern int Model1_sysctl_max_skb_frags;

/* Set skb_shinfo(skb)->gso_size to this in case you want skb_segment to
 * segment using its current segmentation instead.
 */


typedef struct Model1_skb_frag_struct Model1_skb_frag_t;

struct Model1_skb_frag_struct {
 struct {
  struct Model1_page *Model1_p;
 } Model1_page;

 __u32 Model1_page_offset;
 __u32 Model1_size;




};

static inline __attribute__((no_instrument_function)) unsigned int Model1_skb_frag_size(const Model1_skb_frag_t *Model1_frag)
{
 return Model1_frag->Model1_size;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_frag_size_set(Model1_skb_frag_t *Model1_frag, unsigned int Model1_size)
{
 Model1_frag->Model1_size = Model1_size;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_frag_size_add(Model1_skb_frag_t *Model1_frag, int Model1_delta)
{
 Model1_frag->Model1_size += Model1_delta;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_frag_size_sub(Model1_skb_frag_t *Model1_frag, int Model1_delta)
{
 Model1_frag->Model1_size -= Model1_delta;
}



/**
 * struct skb_shared_hwtstamps - hardware time stamps
 * @hwtstamp:	hardware time stamp transformed into duration
 *		since arbitrary point in time
 *
 * Software time stamps generated by ktime_get_real() are stored in
 * skb->tstamp.
 *
 * hwtstamps can only be compared against other hwtstamps from
 * the same device.
 *
 * This structure is attached to packets as part of the
 * &skb_shared_info. Use skb_hwtstamps() to get a pointer.
 */
struct Model1_skb_shared_hwtstamps {
 Model1_ktime_t Model1_hwtstamp;
};

/* Definitions for tx_flags in struct skb_shared_info */
enum {
 /* generate hardware time stamp */
 Model1_SKBTX_HW_TSTAMP = 1 << 0,

 /* generate software time stamp when queueing packet to NIC */
 Model1_SKBTX_SW_TSTAMP = 1 << 1,

 /* device driver is going to provide hardware time stamp */
 Model1_SKBTX_IN_PROGRESS = 1 << 2,

 /* device driver supports TX zero-copy buffers */
 Model1_SKBTX_DEV_ZEROCOPY = 1 << 3,

 /* generate wifi status information (where possible) */
 Model1_SKBTX_WIFI_STATUS = 1 << 4,

 /* This indicates at least one fragment might be overwritten
	 * (as in vmsplice(), sendfile() ...)
	 * If we need to compute a TX checksum, we'll need to copy
	 * all frags to avoid possible bad checksum
	 */
 Model1_SKBTX_SHARED_FRAG = 1 << 5,

 /* generate software time stamp when entering packet scheduling */
 Model1_SKBTX_SCHED_TSTAMP = 1 << 6,
};





/*
 * The callback notifies userspace to release buffers when skb DMA is done in
 * lower device, the skb last reference should be 0 when calling this.
 * The zerocopy_success argument is true if zero copy transmit occurred,
 * false on data copy or out of memory error caused by data copy attempt.
 * The ctx field is used to track device context.
 * The desc field is used to track userspace buffer index.
 */
struct Model1_ubuf_info {
 void (*Model1_callback)(struct Model1_ubuf_info *, bool Model1_zerocopy_success);
 void *Model1_ctx;
 unsigned long Model1_desc;
};

/* This data is invariant across clones and lives at
 * the end of the header data, ie. at skb->end.
 */
struct Model1_skb_shared_info {
 unsigned char Model1_nr_frags;
 __u8 Model1_tx_flags;
 unsigned short Model1_gso_size;
 /* Warning: this field is not always filled in (UFO)! */
 unsigned short Model1_gso_segs;
 unsigned short Model1_gso_type;
 struct Model1_sk_buff *Model1_frag_list;
 struct Model1_skb_shared_hwtstamps Model1_hwtstamps;
 Model1_u32 Model1_tskey;
 Model1___be32 Model1_ip6_frag_id;

 /*
	 * Warning : all fields before dataref are cleared in __alloc_skb()
	 */
 Model1_atomic_t Model1_dataref;

 /* Intermediate layers must ensure that destructor_arg
	 * remains valid until skb destructor */
 void * Model1_destructor_arg;

 /* must be last field, see pskb_expand_head() */
 Model1_skb_frag_t Model1_frags[(65536/((1UL) << 12) + 1)];
};

/* We divide dataref into two halves.  The higher 16 bits hold references
 * to the payload part of skb->data.  The lower 16 bits hold references to
 * the entire skb->data.  A clone of a headerless skb holds the length of
 * the header in skb->hdr_len.
 *
 * All users must obey the rule that the skb->data reference count must be
 * greater than or equal to the payload reference count.
 *
 * Holding a reference to the payload part means that the user does not
 * care about modifications to the header part of skb->data.
 */




enum {
 Model1_SKB_FCLONE_UNAVAILABLE, /* skb has no fclone (from head_cache) */
 Model1_SKB_FCLONE_ORIG, /* orig skb (from fclone_cache) */
 Model1_SKB_FCLONE_CLONE, /* companion fclone skb (from fclone_cache) */
};

enum {
 Model1_SKB_GSO_TCPV4 = 1 << 0,
 Model1_SKB_GSO_UDP = 1 << 1,

 /* This indicates the skb is from an untrusted source. */
 Model1_SKB_GSO_DODGY = 1 << 2,

 /* This indicates the tcp segment has CWR set. */
 Model1_SKB_GSO_TCP_ECN = 1 << 3,

 Model1_SKB_GSO_TCP_FIXEDID = 1 << 4,

 Model1_SKB_GSO_TCPV6 = 1 << 5,

 Model1_SKB_GSO_FCOE = 1 << 6,

 Model1_SKB_GSO_GRE = 1 << 7,

 Model1_SKB_GSO_GRE_CSUM = 1 << 8,

 Model1_SKB_GSO_IPXIP4 = 1 << 9,

 Model1_SKB_GSO_IPXIP6 = 1 << 10,

 Model1_SKB_GSO_UDP_TUNNEL = 1 << 11,

 Model1_SKB_GSO_UDP_TUNNEL_CSUM = 1 << 12,

 Model1_SKB_GSO_PARTIAL = 1 << 13,

 Model1_SKB_GSO_TUNNEL_REMCSUM = 1 << 14,

 Model1_SKB_GSO_SCTP = 1 << 15,
};






typedef unsigned int Model1_sk_buff_data_t;




/**
 * struct skb_mstamp - multi resolution time stamps
 * @stamp_us: timestamp in us resolution
 * @stamp_jiffies: timestamp in jiffies
 */
struct Model1_skb_mstamp {
 union {
  Model1_u64 Model1_v64;
  struct {
   Model1_u32 Model1_stamp_us;
   Model1_u32 Model1_stamp_jiffies;
  };
 };
};

/**
 * skb_mstamp_get - get current timestamp
 * @cl: place to store timestamps
 */
static inline __attribute__((no_instrument_function)) void Model1_skb_mstamp_get(struct Model1_skb_mstamp *Model1_cl)
{
 Model1_u64 Model1_val = Model1_local_clock();

 ({ Model1_uint32_t Model1___base = (1000L); Model1_uint32_t Model1___rem; Model1___rem = ((Model1_uint64_t)(Model1_val)) % Model1___base; (Model1_val) = ((Model1_uint64_t)(Model1_val)) / Model1___base; Model1___rem; });
 Model1_cl->Model1_stamp_us = (Model1_u32)Model1_val;
 Model1_cl->Model1_stamp_jiffies = (Model1_u32)Model1_jiffies;
}

/**
 * skb_mstamp_delta - compute the difference in usec between two skb_mstamp
 * @t1: pointer to newest sample
 * @t0: pointer to oldest sample
 */
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_skb_mstamp_us_delta(const struct Model1_skb_mstamp *Model1_t1,
          const struct Model1_skb_mstamp *Model1_t0)
{
 Model1_s32 Model1_delta_us = Model1_t1->Model1_stamp_us - Model1_t0->Model1_stamp_us;
 Model1_u32 Model1_delta_jiffies = Model1_t1->Model1_stamp_jiffies - Model1_t0->Model1_stamp_jiffies;

 /* If delta_us is negative, this might be because interval is too big,
	 * or local_clock() drift is too big : fallback using jiffies.
	 */
 if (Model1_delta_us <= 0 ||
     Model1_delta_jiffies >= (((int)(~0U>>1)) / (1000000L / 1000)))

  Model1_delta_us = Model1_jiffies_to_usecs(Model1_delta_jiffies);

 return Model1_delta_us;
}

static inline __attribute__((no_instrument_function)) bool Model1_skb_mstamp_after(const struct Model1_skb_mstamp *Model1_t1,
        const struct Model1_skb_mstamp *Model1_t0)
{
 Model1_s32 Model1_diff = Model1_t1->Model1_stamp_jiffies - Model1_t0->Model1_stamp_jiffies;

 if (!Model1_diff)
  Model1_diff = Model1_t1->Model1_stamp_us - Model1_t0->Model1_stamp_us;
 return Model1_diff > 0;
}

/** 
 *	struct sk_buff - socket buffer
 *	@next: Next buffer in list
 *	@prev: Previous buffer in list
 *	@tstamp: Time we arrived/left
 *	@rbnode: RB tree node, alternative to next/prev for netem/tcp
 *	@sk: Socket we are owned by
 *	@dev: Device we arrived on/are leaving by
 *	@cb: Control buffer. Free for use by every layer. Put private vars here
 *	@_skb_refdst: destination entry (with norefcount bit)
 *	@sp: the security path, used for xfrm
 *	@len: Length of actual data
 *	@data_len: Data length
 *	@mac_len: Length of link layer header
 *	@hdr_len: writable header length of cloned skb
 *	@csum: Checksum (must include start/offset pair)
 *	@csum_start: Offset from skb->head where checksumming should start
 *	@csum_offset: Offset from csum_start where checksum should be stored
 *	@priority: Packet queueing priority
 *	@ignore_df: allow local fragmentation
 *	@cloned: Head may be cloned (check refcnt to be sure)
 *	@ip_summed: Driver fed us an IP checksum
 *	@nohdr: Payload reference only, must not modify header
 *	@nfctinfo: Relationship of this skb to the connection
 *	@pkt_type: Packet class
 *	@fclone: skbuff clone status
 *	@ipvs_property: skbuff is owned by ipvs
 *	@peeked: this packet has been seen already, so stats have been
 *		done for it, don't do them again
 *	@nf_trace: netfilter packet trace flag
 *	@protocol: Packet protocol from driver
 *	@destructor: Destruct function
 *	@nfct: Associated connection, if any
 *	@nf_bridge: Saved data about a bridged frame - see br_netfilter.c
 *	@skb_iif: ifindex of device we arrived on
 *	@tc_index: Traffic control index
 *	@tc_verd: traffic control verdict
 *	@hash: the packet hash
 *	@queue_mapping: Queue mapping for multiqueue devices
 *	@xmit_more: More SKBs are pending for this queue
 *	@ndisc_nodetype: router type (from link layer)
 *	@ooo_okay: allow the mapping of a socket to a queue to be changed
 *	@l4_hash: indicate hash is a canonical 4-tuple hash over transport
 *		ports.
 *	@sw_hash: indicates hash was computed in software stack
 *	@wifi_acked_valid: wifi_acked was set
 *	@wifi_acked: whether frame was acked on wifi or not
 *	@no_fcs:  Request NIC to treat last 4 bytes as Ethernet FCS
  *	@napi_id: id of the NAPI struct this skb came from
 *	@secmark: security marking
 *	@offload_fwd_mark: fwding offload mark
 *	@mark: Generic packet mark
 *	@vlan_proto: vlan encapsulation protocol
 *	@vlan_tci: vlan tag control information
 *	@inner_protocol: Protocol (encapsulation)
 *	@inner_transport_header: Inner transport layer header (encapsulation)
 *	@inner_network_header: Network layer header (encapsulation)
 *	@inner_mac_header: Link layer header (encapsulation)
 *	@transport_header: Transport layer header
 *	@network_header: Network layer header
 *	@mac_header: Link layer header
 *	@tail: Tail pointer
 *	@end: End pointer
 *	@head: Head of buffer
 *	@data: Data head pointer
 *	@truesize: Buffer size
 *	@users: User count - see {datagram,tcp}.c
 */

struct Model1_sk_buff {
 union {
  struct {
   /* These two members must be first. */
   struct Model1_sk_buff *Model1_next;
   struct Model1_sk_buff *Model1_prev;

   union {
    Model1_ktime_t Model1_tstamp;
    struct Model1_skb_mstamp Model1_skb_mstamp;
   };
  };
  struct Model1_rb_node Model1_rbnode; /* used in netem & tcp stack */
 };
 struct Model1_sock *Model1_sk;
 struct Model1_net_device *Model1_dev;

 /*
	 * This is the control buffer. It is free to use for every
	 * layer. Please put your private variables there. If you
	 * want to keep them across layers you have to do a skb_clone()
	 * first. This is owned by whoever has the skb queued ATM.
	 */
 char Model1_cb[48] __attribute__((aligned(8)));

 unsigned long Model1__skb_refdst;
 void (*Model1_destructor)(struct Model1_sk_buff *Model1_skb);

 struct Model1_sec_path *Model1_sp;


 struct Model1_nf_conntrack *Model1_nfct;




 unsigned int Model1_len,
    Model1_data_len;
 Model1___u16 Model1_mac_len,
    Model1_hdr_len;

 /* Following fields are _not_ copied in __copy_skb_header()
	 * Note that queue_mapping is here mostly to fill a hole.
	 */
                                 ;
 Model1___u16 Model1_queue_mapping;
 __u8 Model1_cloned:1,
    Model1_nohdr:1,
    Model1_fclone:2,
    Model1_peeked:1,
    Model1_head_frag:1,
    Model1_xmit_more:1;
 /* one bit hole */
                               ;

 /* fields enclosed in headers_start/headers_end are copied
	 * using a single memcpy() in __copy_skb_header()
	 */
 /* private: */
 __u32 Model1_headers_start[0];
 /* public: */

/* if you move pkt_type around you also must adapt those constants */







 __u8 Model1___pkt_type_offset[0];
 __u8 Model1_pkt_type:3;
 __u8 Model1_pfmemalloc:1;
 __u8 Model1_ignore_df:1;
 __u8 Model1_nfctinfo:3;

 __u8 Model1_nf_trace:1;
 __u8 Model1_ip_summed:2;
 __u8 Model1_ooo_okay:1;
 __u8 Model1_l4_hash:1;
 __u8 Model1_sw_hash:1;
 __u8 Model1_wifi_acked_valid:1;
 __u8 Model1_wifi_acked:1;

 __u8 Model1_no_fcs:1;
 /* Indicates the inner headers are valid in the skbuff. */
 __u8 Model1_encapsulation:1;
 __u8 Model1_encap_hdr_csum:1;
 __u8 Model1_csum_valid:1;
 __u8 Model1_csum_complete_sw:1;
 __u8 Model1_csum_level:2;
 __u8 Model1_csum_bad:1;


 __u8 Model1_ndisc_nodetype:2;

 __u8 Model1_ipvs_property:1;
 __u8 Model1_inner_protocol_type:1;
 __u8 Model1_remcsum_offload:1;
 /* 3 or 5 bit hole */


 Model1___u16 Model1_tc_index; /* traffic control index */

 Model1___u16 Model1_tc_verd; /* traffic control verdict */



 union {
  Model1___wsum Model1_csum;
  struct {
   Model1___u16 Model1_csum_start;
   Model1___u16 Model1_csum_offset;
  };
 };
 __u32 Model1_priority;
 int Model1_skb_iif;
 __u32 Model1_hash;
 Model1___be16 Model1_vlan_proto;
 Model1___u16 Model1_vlan_tci;

 union {
  unsigned int Model1_napi_id;
  unsigned int Model1_sender_cpu;
 };

 union {

  __u32 Model1_secmark;




 };

 union {
  __u32 Model1_mark;
  __u32 Model1_reserved_tailroom;
 };

 union {
  Model1___be16 Model1_inner_protocol;
  __u8 Model1_inner_ipproto;
 };

 Model1___u16 Model1_inner_transport_header;
 Model1___u16 Model1_inner_network_header;
 Model1___u16 Model1_inner_mac_header;

 Model1___be16 Model1_protocol;
 Model1___u16 Model1_transport_header;
 Model1___u16 Model1_network_header;
 Model1___u16 Model1_mac_header;

 /* private: */
 __u32 Model1_headers_end[0];
 /* public: */

 /* These elements must be at the end, see alloc_skb() for details.  */
 Model1_sk_buff_data_t Model1_tail;
 Model1_sk_buff_data_t Model1_end;
 unsigned char *Model1_head,
    *Model1_data;
 unsigned int Model1_truesize;
 Model1_atomic_t Model1_users;
};


/*
 *	Handling routines are only of interest to the kernel
 */







/* Returns true if the skb was allocated from PFMEMALLOC reserves */
static inline __attribute__((no_instrument_function)) bool Model1_skb_pfmemalloc(const struct Model1_sk_buff *Model1_skb)
{
 return __builtin_expect(!!(Model1_skb->Model1_pfmemalloc), 0);
}

/*
 * skb might have a dst pointer attached, refcounted or not.
 * _skb_refdst low order bit is set if refcount was _not_ taken
 */



/**
 * skb_dst - returns skb dst_entry
 * @skb: buffer
 *
 * Returns skb dst_entry, regardless of reference taken or not.
 */
static inline __attribute__((no_instrument_function)) struct Model1_dst_entry *Model1_skb_dst(const struct Model1_sk_buff *Model1_skb)
{
 /* If refdst was not refcounted, check we still are in a 
	 * rcu_read_lock section
	 */
 ({ int Model1___ret_warn_on = !!((Model1_skb->Model1__skb_refdst & 1UL) && !Model1_rcu_read_lock_held() && !Model1_rcu_read_lock_bh_held()); if (__builtin_expect(!!(Model1___ret_warn_on), 0)) Model1_warn_slowpath_null("./include/linux/skbuff.h", 838); __builtin_expect(!!(Model1___ret_warn_on), 0); });


 return (struct Model1_dst_entry *)(Model1_skb->Model1__skb_refdst & ~(1UL));
}

/**
 * skb_dst_set - sets skb dst
 * @skb: buffer
 * @dst: dst entry
 *
 * Sets skb dst, assuming a reference was taken on dst and should
 * be released by skb_dst_drop()
 */
static inline __attribute__((no_instrument_function)) void Model1_skb_dst_set(struct Model1_sk_buff *Model1_skb, struct Model1_dst_entry *Model1_dst)
{
 Model1_skb->Model1__skb_refdst = (unsigned long)Model1_dst;
}

/**
 * skb_dst_set_noref - sets skb dst, hopefully, without taking reference
 * @skb: buffer
 * @dst: dst entry
 *
 * Sets skb dst, assuming a reference was not taken on dst.
 * If dst entry is cached, we do not take reference and dst_release
 * will be avoided by refdst_drop. If dst entry is not cached, we take
 * reference, so that last dst_release can destroy the dst immediately.
 */
static inline __attribute__((no_instrument_function)) void Model1_skb_dst_set_noref(struct Model1_sk_buff *Model1_skb, struct Model1_dst_entry *Model1_dst)
{
 ({ int Model1___ret_warn_on = !!(!Model1_rcu_read_lock_held() && !Model1_rcu_read_lock_bh_held()); if (__builtin_expect(!!(Model1___ret_warn_on), 0)) Model1_warn_slowpath_null("./include/linux/skbuff.h", 867); __builtin_expect(!!(Model1___ret_warn_on), 0); });
 Model1_skb->Model1__skb_refdst = (unsigned long)Model1_dst | 1UL;
}

/**
 * skb_dst_is_noref - Test if skb dst isn't refcounted
 * @skb: buffer
 */
static inline __attribute__((no_instrument_function)) bool Model1_skb_dst_is_noref(const struct Model1_sk_buff *Model1_skb)
{
 return (Model1_skb->Model1__skb_refdst & 1UL) && Model1_skb_dst(Model1_skb);
}

static inline __attribute__((no_instrument_function)) struct Model1_rtable *Model1_skb_rtable(const struct Model1_sk_buff *Model1_skb)
{
 return (struct Model1_rtable *)Model1_skb_dst(Model1_skb);
}

/* For mangling skb->pkt_type from user space side from applications
 * such as nft, tc, etc, we only allow a conservative subset of
 * possible pkt_types to be set.
*/
static inline __attribute__((no_instrument_function)) bool Model1_skb_pkt_type_ok(Model1_u32 Model1_ptype)
{
 return Model1_ptype <= 3;
}

void Model1_kfree_skb(struct Model1_sk_buff *Model1_skb);
void Model1_kfree_skb_list(struct Model1_sk_buff *Model1_segs);
void Model1_skb_tx_error(struct Model1_sk_buff *Model1_skb);
void Model1_consume_skb(struct Model1_sk_buff *Model1_skb);
void Model1___kfree_skb(struct Model1_sk_buff *Model1_skb);
extern struct Model1_kmem_cache *Model1_skbuff_head_cache;

void Model1_kfree_skb_partial(struct Model1_sk_buff *Model1_skb, bool Model1_head_stolen);
bool Model1_skb_try_coalesce(struct Model1_sk_buff *Model1_to, struct Model1_sk_buff *Model1_from,
        bool *Model1_fragstolen, int *Model1_delta_truesize);

struct Model1_sk_buff *Model1___alloc_skb(unsigned int Model1_size, Model1_gfp_t Model1_priority, int Model1_flags,
       int Model1_node);
struct Model1_sk_buff *Model1___build_skb(void *Model1_data, unsigned int Model1_frag_size);
struct Model1_sk_buff *Model1_build_skb(void *Model1_data, unsigned int Model1_frag_size);
static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1_alloc_skb(unsigned int Model1_size,
     Model1_gfp_t Model1_priority)
{
 return Model1___alloc_skb(Model1_size, Model1_priority, 0, (-1));
}

struct Model1_sk_buff *Model1_alloc_skb_with_frags(unsigned long Model1_header_len,
         unsigned long Model1_data_len,
         int Model1_max_page_order,
         int *Model1_errcode,
         Model1_gfp_t Model1_gfp_mask);

/* Layout of fast clones : [skb1][skb2][fclone_ref] */
struct Model1_sk_buff_fclones {
 struct Model1_sk_buff Model1_skb1;

 struct Model1_sk_buff Model1_skb2;

 Model1_atomic_t Model1_fclone_ref;
};

/**
 *	skb_fclone_busy - check if fclone is busy
 *	@skb: buffer
 *
 * Returns true if skb is a fast clone, and its clone is not freed.
 * Some drivers call skb_orphan() in their ndo_start_xmit(),
 * so we also check that this didnt happen.
 */
static inline __attribute__((no_instrument_function)) bool Model1_skb_fclone_busy(const struct Model1_sock *Model1_sk,
       const struct Model1_sk_buff *Model1_skb)
{
 const struct Model1_sk_buff_fclones *Model1_fclones;

 Model1_fclones = ({ const typeof( ((struct Model1_sk_buff_fclones *)0)->Model1_skb1 ) *Model1___mptr = (Model1_skb); (struct Model1_sk_buff_fclones *)( (char *)Model1___mptr - __builtin_offsetof(struct Model1_sk_buff_fclones, Model1_skb1) );});

 return Model1_skb->Model1_fclone == Model1_SKB_FCLONE_ORIG &&
        Model1_atomic_read(&Model1_fclones->Model1_fclone_ref) > 1 &&
        Model1_fclones->Model1_skb2.Model1_sk == Model1_sk;
}

static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1_alloc_skb_fclone(unsigned int Model1_size,
            Model1_gfp_t Model1_priority)
{
 return Model1___alloc_skb(Model1_size, Model1_priority, 0x01, (-1));
}

struct Model1_sk_buff *Model1___alloc_skb_head(Model1_gfp_t Model1_priority, int Model1_node);
static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1_alloc_skb_head(Model1_gfp_t Model1_priority)
{
 return Model1___alloc_skb_head(Model1_priority, -1);
}

struct Model1_sk_buff *Model1_skb_morph(struct Model1_sk_buff *Model1_dst, struct Model1_sk_buff *Model1_src);
int Model1_skb_copy_ubufs(struct Model1_sk_buff *Model1_skb, Model1_gfp_t Model1_gfp_mask);
struct Model1_sk_buff *Model1_skb_clone(struct Model1_sk_buff *Model1_skb, Model1_gfp_t Model1_priority);
struct Model1_sk_buff *Model1_skb_copy(const struct Model1_sk_buff *Model1_skb, Model1_gfp_t Model1_priority);
struct Model1_sk_buff *Model1___pskb_copy_fclone(struct Model1_sk_buff *Model1_skb, int Model1_headroom,
       Model1_gfp_t Model1_gfp_mask, bool Model1_fclone);
static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1___pskb_copy(struct Model1_sk_buff *Model1_skb, int Model1_headroom,
       Model1_gfp_t Model1_gfp_mask)
{
 return Model1___pskb_copy_fclone(Model1_skb, Model1_headroom, Model1_gfp_mask, false);
}

int Model1_pskb_expand_head(struct Model1_sk_buff *Model1_skb, int Model1_nhead, int Model1_ntail, Model1_gfp_t Model1_gfp_mask);
struct Model1_sk_buff *Model1_skb_realloc_headroom(struct Model1_sk_buff *Model1_skb,
         unsigned int Model1_headroom);
struct Model1_sk_buff *Model1_skb_copy_expand(const struct Model1_sk_buff *Model1_skb, int Model1_newheadroom,
    int Model1_newtailroom, Model1_gfp_t Model1_priority);
int Model1_skb_to_sgvec_nomark(struct Model1_sk_buff *Model1_skb, struct Model1_scatterlist *Model1_sg,
   int Model1_offset, int Model1_len);
int Model1_skb_to_sgvec(struct Model1_sk_buff *Model1_skb, struct Model1_scatterlist *Model1_sg, int Model1_offset,
   int Model1_len);
int Model1_skb_cow_data(struct Model1_sk_buff *Model1_skb, int Model1_tailbits, struct Model1_sk_buff **Model1_trailer);
int Model1_skb_pad(struct Model1_sk_buff *Model1_skb, int Model1_pad);


int Model1_skb_append_datato_frags(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb,
       int Model1_getfrag(void *Model1_from, char *Model1_to, int Model1_offset,
     int Model1_len, int Model1_odd, struct Model1_sk_buff *Model1_skb),
       void *Model1_from, int Model1_length);

int Model1_skb_append_pagefrags(struct Model1_sk_buff *Model1_skb, struct Model1_page *Model1_page,
    int Model1_offset, Model1_size_t Model1_size);

struct Model1_skb_seq_state {
 __u32 Model1_lower_offset;
 __u32 Model1_upper_offset;
 __u32 Model1_frag_idx;
 __u32 Model1_stepped_offset;
 struct Model1_sk_buff *Model1_root_skb;
 struct Model1_sk_buff *Model1_cur_skb;
 __u8 *Model1_frag_data;
};

void Model1_skb_prepare_seq_read(struct Model1_sk_buff *Model1_skb, unsigned int Model1_from,
     unsigned int Model1_to, struct Model1_skb_seq_state *Model1_st);
unsigned int Model1_skb_seq_read(unsigned int Model1_consumed, const Model1_u8 **Model1_data,
     struct Model1_skb_seq_state *Model1_st);
void Model1_skb_abort_seq_read(struct Model1_skb_seq_state *Model1_st);

unsigned int Model1_skb_find_text(struct Model1_sk_buff *Model1_skb, unsigned int Model1_from,
      unsigned int Model1_to, struct Model1_ts_config *Model1_config);

/*
 * Packet hash types specify the type of hash in skb_set_hash.
 *
 * Hash types refer to the protocol layer addresses which are used to
 * construct a packet's hash. The hashes are used to differentiate or identify
 * flows of the protocol layer for the hash type. Hash types are either
 * layer-2 (L2), layer-3 (L3), or layer-4 (L4).
 *
 * Properties of hashes:
 *
 * 1) Two packets in different flows have different hash values
 * 2) Two packets in the same flow should have the same hash value
 *
 * A hash at a higher layer is considered to be more specific. A driver should
 * set the most specific hash possible.
 *
 * A driver cannot indicate a more specific hash than the layer at which a hash
 * was computed. For instance an L3 hash cannot be set as an L4 hash.
 *
 * A driver may indicate a hash level which is less specific than the
 * actual layer the hash was computed on. For instance, a hash computed
 * at L4 may be considered an L3 hash. This should only be done if the
 * driver can't unambiguously determine that the HW computed the hash at
 * the higher layer. Note that the "should" in the second property above
 * permits this.
 */
enum Model1_pkt_hash_types {
 Model1_PKT_HASH_TYPE_NONE, /* Undefined type */
 Model1_PKT_HASH_TYPE_L2, /* Input: src_MAC, dest_MAC */
 Model1_PKT_HASH_TYPE_L3, /* Input: src_IP, dst_IP */
 Model1_PKT_HASH_TYPE_L4, /* Input: src_IP, dst_IP, src_port, dst_port */
};

static inline __attribute__((no_instrument_function)) void Model1_skb_clear_hash(struct Model1_sk_buff *Model1_skb)
{
 Model1_skb->Model1_hash = 0;
 Model1_skb->Model1_sw_hash = 0;
 Model1_skb->Model1_l4_hash = 0;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_clear_hash_if_not_l4(struct Model1_sk_buff *Model1_skb)
{
 if (!Model1_skb->Model1_l4_hash)
  Model1_skb_clear_hash(Model1_skb);
}

static inline __attribute__((no_instrument_function)) void
Model1___skb_set_hash(struct Model1_sk_buff *Model1_skb, __u32 Model1_hash, bool Model1_is_sw, bool Model1_is_l4)
{
 Model1_skb->Model1_l4_hash = Model1_is_l4;
 Model1_skb->Model1_sw_hash = Model1_is_sw;
 Model1_skb->Model1_hash = Model1_hash;
}

static inline __attribute__((no_instrument_function)) void
Model1_skb_set_hash(struct Model1_sk_buff *Model1_skb, __u32 Model1_hash, enum Model1_pkt_hash_types Model1_type)
{
 /* Used by drivers to set hash from HW */
 Model1___skb_set_hash(Model1_skb, Model1_hash, false, Model1_type == Model1_PKT_HASH_TYPE_L4);
}

static inline __attribute__((no_instrument_function)) void
Model1___skb_set_sw_hash(struct Model1_sk_buff *Model1_skb, __u32 Model1_hash, bool Model1_is_l4)
{
 Model1___skb_set_hash(Model1_skb, Model1_hash, true, Model1_is_l4);
}

void Model1___skb_get_hash(struct Model1_sk_buff *Model1_skb);
Model1_u32 Model1___skb_get_hash_symmetric(struct Model1_sk_buff *Model1_skb);
Model1_u32 Model1_skb_get_poff(const struct Model1_sk_buff *Model1_skb);
Model1_u32 Model1___skb_get_poff(const struct Model1_sk_buff *Model1_skb, void *Model1_data,
     const struct Model1_flow_keys *Model1_keys, int Model1_hlen);
Model1___be32 Model1___skb_flow_get_ports(const struct Model1_sk_buff *Model1_skb, int Model1_thoff, Model1_u8 Model1_ip_proto,
       void *Model1_data, int Model1_hlen_proto);

static inline __attribute__((no_instrument_function)) Model1___be32 Model1_skb_flow_get_ports(const struct Model1_sk_buff *Model1_skb,
     int Model1_thoff, Model1_u8 Model1_ip_proto)
{
 return Model1___skb_flow_get_ports(Model1_skb, Model1_thoff, Model1_ip_proto, ((void *)0), 0);
}

void Model1_skb_flow_dissector_init(struct Model1_flow_dissector *Model1_flow_dissector,
        const struct Model1_flow_dissector_key *Model1_key,
        unsigned int Model1_key_count);

bool Model1___skb_flow_dissect(const struct Model1_sk_buff *Model1_skb,
   struct Model1_flow_dissector *Model1_flow_dissector,
   void *Model1_target_container,
   void *Model1_data, Model1___be16 Model1_proto, int Model1_nhoff, int Model1_hlen,
   unsigned int Model1_flags);

static inline __attribute__((no_instrument_function)) bool Model1_skb_flow_dissect(const struct Model1_sk_buff *Model1_skb,
        struct Model1_flow_dissector *Model1_flow_dissector,
        void *Model1_target_container, unsigned int Model1_flags)
{
 return Model1___skb_flow_dissect(Model1_skb, Model1_flow_dissector, Model1_target_container,
      ((void *)0), 0, 0, 0, Model1_flags);
}

static inline __attribute__((no_instrument_function)) bool Model1_skb_flow_dissect_flow_keys(const struct Model1_sk_buff *Model1_skb,
           struct Model1_flow_keys *Model1_flow,
           unsigned int Model1_flags)
{
 memset(Model1_flow, 0, sizeof(*Model1_flow));
 return Model1___skb_flow_dissect(Model1_skb, &Model1_flow_keys_dissector, Model1_flow,
      ((void *)0), 0, 0, 0, Model1_flags);
}

static inline __attribute__((no_instrument_function)) bool Model1_skb_flow_dissect_flow_keys_buf(struct Model1_flow_keys *Model1_flow,
        void *Model1_data, Model1___be16 Model1_proto,
        int Model1_nhoff, int Model1_hlen,
        unsigned int Model1_flags)
{
 memset(Model1_flow, 0, sizeof(*Model1_flow));
 return Model1___skb_flow_dissect(((void *)0), &Model1_flow_keys_buf_dissector, Model1_flow,
      Model1_data, Model1_proto, Model1_nhoff, Model1_hlen, Model1_flags);
}

static inline __attribute__((no_instrument_function)) __u32 Model1_skb_get_hash(struct Model1_sk_buff *Model1_skb)
{
 if (!Model1_skb->Model1_l4_hash && !Model1_skb->Model1_sw_hash)
  Model1___skb_get_hash(Model1_skb);

 return Model1_skb->Model1_hash;
}

__u32 Model1___skb_get_hash_flowi6(struct Model1_sk_buff *Model1_skb, const struct Model1_flowi6 *Model1_fl6);

static inline __attribute__((no_instrument_function)) __u32 Model1_skb_get_hash_flowi6(struct Model1_sk_buff *Model1_skb, const struct Model1_flowi6 *Model1_fl6)
{
 if (!Model1_skb->Model1_l4_hash && !Model1_skb->Model1_sw_hash) {
  struct Model1_flow_keys Model1_keys;
  __u32 Model1_hash = Model1___get_hash_from_flowi6(Model1_fl6, &Model1_keys);

  Model1___skb_set_sw_hash(Model1_skb, Model1_hash, Model1_flow_keys_have_l4(&Model1_keys));
 }

 return Model1_skb->Model1_hash;
}

__u32 Model1___skb_get_hash_flowi4(struct Model1_sk_buff *Model1_skb, const struct Model1_flowi4 *Model1_fl);

static inline __attribute__((no_instrument_function)) __u32 Model1_skb_get_hash_flowi4(struct Model1_sk_buff *Model1_skb, const struct Model1_flowi4 *Model1_fl4)
{
 if (!Model1_skb->Model1_l4_hash && !Model1_skb->Model1_sw_hash) {
  struct Model1_flow_keys Model1_keys;
  __u32 Model1_hash = Model1___get_hash_from_flowi4(Model1_fl4, &Model1_keys);

  Model1___skb_set_sw_hash(Model1_skb, Model1_hash, Model1_flow_keys_have_l4(&Model1_keys));
 }

 return Model1_skb->Model1_hash;
}

__u32 Model1_skb_get_hash_perturb(const struct Model1_sk_buff *Model1_skb, Model1_u32 Model1_perturb);

static inline __attribute__((no_instrument_function)) __u32 Model1_skb_get_hash_raw(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb->Model1_hash;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_copy_hash(struct Model1_sk_buff *Model1_to, const struct Model1_sk_buff *Model1_from)
{
 Model1_to->Model1_hash = Model1_from->Model1_hash;
 Model1_to->Model1_sw_hash = Model1_from->Model1_sw_hash;
 Model1_to->Model1_l4_hash = Model1_from->Model1_l4_hash;
};


static inline __attribute__((no_instrument_function)) unsigned char *Model1_skb_end_pointer(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb->Model1_head + Model1_skb->Model1_end;
}

static inline __attribute__((no_instrument_function)) unsigned int Model1_skb_end_offset(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb->Model1_end;
}
/* Internal */


static inline __attribute__((no_instrument_function)) struct Model1_skb_shared_hwtstamps *Model1_skb_hwtstamps(struct Model1_sk_buff *Model1_skb)
{
 return &((struct Model1_skb_shared_info *)(Model1_skb_end_pointer(Model1_skb)))->Model1_hwtstamps;
}

/**
 *	skb_queue_empty - check if a queue is empty
 *	@list: queue head
 *
 *	Returns true if the queue is empty, false otherwise.
 */
static inline __attribute__((no_instrument_function)) int Model1_skb_queue_empty(const struct Model1_sk_buff_head *Model1_list)
{
 return Model1_list->Model1_next == (const struct Model1_sk_buff *) Model1_list;
}

/**
 *	skb_queue_is_last - check if skb is the last entry in the queue
 *	@list: queue head
 *	@skb: buffer
 *
 *	Returns true if @skb is the last buffer on the list.
 */
static inline __attribute__((no_instrument_function)) bool Model1_skb_queue_is_last(const struct Model1_sk_buff_head *Model1_list,
         const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb->Model1_next == (const struct Model1_sk_buff *) Model1_list;
}

/**
 *	skb_queue_is_first - check if skb is the first entry in the queue
 *	@list: queue head
 *	@skb: buffer
 *
 *	Returns true if @skb is the first buffer on the list.
 */
static inline __attribute__((no_instrument_function)) bool Model1_skb_queue_is_first(const struct Model1_sk_buff_head *Model1_list,
          const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb->Model1_prev == (const struct Model1_sk_buff *) Model1_list;
}

/**
 *	skb_queue_next - return the next packet in the queue
 *	@list: queue head
 *	@skb: current buffer
 *
 *	Return the next packet in @list after @skb.  It is only valid to
 *	call this if skb_queue_is_last() evaluates to false.
 */
static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1_skb_queue_next(const struct Model1_sk_buff_head *Model1_list,
          const struct Model1_sk_buff *Model1_skb)
{
 /* This BUG_ON may seem severe, but if we just return then we
	 * are going to dereference garbage.
	 */
 do { if (__builtin_expect(!!(Model1_skb_queue_is_last(Model1_list, Model1_skb)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/skbuff.h"), "i" (1263), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);
 return Model1_skb->Model1_next;
}

/**
 *	skb_queue_prev - return the prev packet in the queue
 *	@list: queue head
 *	@skb: current buffer
 *
 *	Return the prev packet in @list before @skb.  It is only valid to
 *	call this if skb_queue_is_first() evaluates to false.
 */
static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1_skb_queue_prev(const struct Model1_sk_buff_head *Model1_list,
          const struct Model1_sk_buff *Model1_skb)
{
 /* This BUG_ON may seem severe, but if we just return then we
	 * are going to dereference garbage.
	 */
 do { if (__builtin_expect(!!(Model1_skb_queue_is_first(Model1_list, Model1_skb)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/skbuff.h"), "i" (1281), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);
 return Model1_skb->Model1_prev;
}

/**
 *	skb_get - reference buffer
 *	@skb: buffer to reference
 *
 *	Makes another reference to a socket buffer and returns a pointer
 *	to the buffer.
 */
static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1_skb_get(struct Model1_sk_buff *Model1_skb)
{
 Model1_atomic_inc(&Model1_skb->Model1_users);
 return Model1_skb;
}

/*
 * If users == 1, we are the only owner and are can avoid redundant
 * atomic change.
 */

/**
 *	skb_cloned - is the buffer a clone
 *	@skb: buffer to check
 *
 *	Returns true if the buffer was generated with skb_clone() and is
 *	one of multiple shared copies of the buffer. Cloned buffers are
 *	shared data so must not be written to under normal circumstances.
 */
static inline __attribute__((no_instrument_function)) int Model1_skb_cloned(const struct Model1_sk_buff *Model1_skb)
{
#if CY_ABSTRACT4
    return false; //TODO: Model1_skb->Model1_cloned should always be false
#else
 return Model1_skb->Model1_cloned &&
        (Model1_atomic_read(&((struct Model1_skb_shared_info *)(Model1_skb_end_pointer(Model1_skb)))->Model1_dataref) & ((1 << 16) - 1)) != 1;
#endif
}

static inline __attribute__((no_instrument_function)) int Model1_skb_unclone(struct Model1_sk_buff *Model1_skb, Model1_gfp_t Model1_pri)
{
 do { if (Model1_gfpflags_allow_blocking(Model1_pri)) do { Model1__cond_resched(); } while (0); } while (0);

 if (Model1_skb_cloned(Model1_skb))
  return Model1_pskb_expand_head(Model1_skb, 0, 0, Model1_pri);

 return 0;
}

/**
 *	skb_header_cloned - is the header a clone
 *	@skb: buffer to check
 *
 *	Returns true if modifying the header part of the buffer requires
 *	the data to be copied.
 */
static inline __attribute__((no_instrument_function)) int Model1_skb_header_cloned(const struct Model1_sk_buff *Model1_skb)
{
 int Model1_dataref;

 if (!Model1_skb->Model1_cloned)
  return 0;

 Model1_dataref = Model1_atomic_read(&((struct Model1_skb_shared_info *)(Model1_skb_end_pointer(Model1_skb)))->Model1_dataref);
 Model1_dataref = (Model1_dataref & ((1 << 16) - 1)) - (Model1_dataref >> 16);
 return Model1_dataref != 1;
}

static inline __attribute__((no_instrument_function)) int Model1_skb_header_unclone(struct Model1_sk_buff *Model1_skb, Model1_gfp_t Model1_pri)
{
 do { if (Model1_gfpflags_allow_blocking(Model1_pri)) do { Model1__cond_resched(); } while (0); } while (0);

 if (Model1_skb_header_cloned(Model1_skb))
  return Model1_pskb_expand_head(Model1_skb, 0, 0, Model1_pri);

 return 0;
}

/**
 *	skb_header_release - release reference to header
 *	@skb: buffer to operate on
 *
 *	Drop a reference to the header part of the buffer.  This is done
 *	by acquiring a payload reference.  You must not read from the header
 *	part of skb->data after this.
 *	Note : Check if you can use __skb_header_release() instead.
 */
static inline __attribute__((no_instrument_function)) void Model1_skb_header_release(struct Model1_sk_buff *Model1_skb)
{
 do { if (__builtin_expect(!!(Model1_skb->Model1_nohdr), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/skbuff.h"), "i" (1367), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);
 Model1_skb->Model1_nohdr = 1;
 Model1_atomic_add(1 << 16, &((struct Model1_skb_shared_info *)(Model1_skb_end_pointer(Model1_skb)))->Model1_dataref);
}

/**
 *	__skb_header_release - release reference to header
 *	@skb: buffer to operate on
 *
 *	Variant of skb_header_release() assuming skb is private to caller.
 *	We can avoid one atomic operation.
 */
static inline __attribute__((no_instrument_function)) void Model1___skb_header_release(struct Model1_sk_buff *Model1_skb)
{
 Model1_skb->Model1_nohdr = 1;
 Model1_atomic_set(&((struct Model1_skb_shared_info *)(Model1_skb_end_pointer(Model1_skb)))->Model1_dataref, 1 + (1 << 16));
}


/**
 *	skb_shared - is the buffer shared
 *	@skb: buffer to check
 *
 *	Returns true if more than one person has a reference to this
 *	buffer.
 */
static inline __attribute__((no_instrument_function)) int Model1_skb_shared(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_atomic_read(&Model1_skb->Model1_users) != 1;
}

/**
 *	skb_share_check - check if buffer is shared and if so clone it
 *	@skb: buffer to check
 *	@pri: priority for memory allocation
 *
 *	If the buffer is shared the buffer is cloned and the old copy
 *	drops a reference. A new clone with a single reference is returned.
 *	If the buffer is not shared the original buffer is returned. When
 *	being called from interrupt status or with spinlocks held pri must
 *	be GFP_ATOMIC.
 *
 *	NULL is returned on a memory allocation failure.
 */
static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1_skb_share_check(struct Model1_sk_buff *Model1_skb, Model1_gfp_t Model1_pri)
{
 do { if (Model1_gfpflags_allow_blocking(Model1_pri)) do { Model1__cond_resched(); } while (0); } while (0);
 if (Model1_skb_shared(Model1_skb)) {
  struct Model1_sk_buff *Model1_nskb = Model1_skb_clone(Model1_skb, Model1_pri);

  if (__builtin_expect(!!(Model1_nskb), 1))
   Model1_consume_skb(Model1_skb);
  else
   Model1_kfree_skb(Model1_skb);
  Model1_skb = Model1_nskb;
 }
 return Model1_skb;
}

/*
 *	Copy shared buffers into a new sk_buff. We effectively do COW on
 *	packets to handle cases where we have a local reader and forward
 *	and a couple of other messy ones. The normal one is tcpdumping
 *	a packet thats being forwarded.
 */

/**
 *	skb_unshare - make a copy of a shared buffer
 *	@skb: buffer to check
 *	@pri: priority for memory allocation
 *
 *	If the socket buffer is a clone then this function creates a new
 *	copy of the data, drops a reference count on the old copy and returns
 *	the new copy with the reference count at 1. If the buffer is not a clone
 *	the original buffer is returned. When called with a spinlock held or
 *	from interrupt state @pri must be %GFP_ATOMIC
 *
 *	%NULL is returned on a memory allocation failure.
 */
static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1_skb_unshare(struct Model1_sk_buff *Model1_skb,
       Model1_gfp_t Model1_pri)
{
 do { if (Model1_gfpflags_allow_blocking(Model1_pri)) do { Model1__cond_resched(); } while (0); } while (0);
 if (Model1_skb_cloned(Model1_skb)) {
  struct Model1_sk_buff *Model1_nskb = Model1_skb_copy(Model1_skb, Model1_pri);

  /* Free our shared copy */
  if (__builtin_expect(!!(Model1_nskb), 1))
   Model1_consume_skb(Model1_skb);
  else
   Model1_kfree_skb(Model1_skb);
  Model1_skb = Model1_nskb;
 }
 return Model1_skb;
}

/**
 *	skb_peek - peek at the head of an &sk_buff_head
 *	@list_: list to peek at
 *
 *	Peek an &sk_buff. Unlike most other operations you _MUST_
 *	be careful with this one. A peek leaves the buffer on the
 *	list and someone else may run off with it. You must hold
 *	the appropriate locks or have a private queue to do this.
 *
 *	Returns %NULL for an empty list or a pointer to the head element.
 *	The reference count is not incremented and the reference is therefore
 *	volatile. Use with caution.
 */
static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1_skb_peek(const struct Model1_sk_buff_head *Model1_list_)
{
 struct Model1_sk_buff *Model1_skb = Model1_list_->Model1_next;

 if (Model1_skb == (struct Model1_sk_buff *)Model1_list_)
  Model1_skb = ((void *)0);
 return Model1_skb;
}

/**
 *	skb_peek_next - peek skb following the given one from a queue
 *	@skb: skb to start from
 *	@list_: list to peek at
 *
 *	Returns %NULL when the end of the list is met or a pointer to the
 *	next element. The reference count is not incremented and the
 *	reference is therefore volatile. Use with caution.
 */
static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1_skb_peek_next(struct Model1_sk_buff *Model1_skb,
  const struct Model1_sk_buff_head *Model1_list_)
{
 struct Model1_sk_buff *Model1_next = Model1_skb->Model1_next;

 if (Model1_next == (struct Model1_sk_buff *)Model1_list_)
  Model1_next = ((void *)0);
 return Model1_next;
}

/**
 *	skb_peek_tail - peek at the tail of an &sk_buff_head
 *	@list_: list to peek at
 *
 *	Peek an &sk_buff. Unlike most other operations you _MUST_
 *	be careful with this one. A peek leaves the buffer on the
 *	list and someone else may run off with it. You must hold
 *	the appropriate locks or have a private queue to do this.
 *
 *	Returns %NULL for an empty list or a pointer to the tail element.
 *	The reference count is not incremented and the reference is therefore
 *	volatile. Use with caution.
 */
static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1_skb_peek_tail(const struct Model1_sk_buff_head *Model1_list_)
{
 struct Model1_sk_buff *Model1_skb = Model1_list_->Model1_prev;

 if (Model1_skb == (struct Model1_sk_buff *)Model1_list_)
  Model1_skb = ((void *)0);
 return Model1_skb;

}

/**
 *	skb_queue_len	- get queue length
 *	@list_: list to measure
 *
 *	Return the length of an &sk_buff queue.
 */
static inline __attribute__((no_instrument_function)) __u32 Model1_skb_queue_len(const struct Model1_sk_buff_head *Model1_list_)
{
 return Model1_list_->Model1_qlen;
}

/**
 *	__skb_queue_head_init - initialize non-spinlock portions of sk_buff_head
 *	@list: queue to initialize
 *
 *	This initializes only the list and queue length aspects of
 *	an sk_buff_head object.  This allows to initialize the list
 *	aspects of an sk_buff_head without reinitializing things like
 *	the spinlock.  It can also be used for on-stack sk_buff_head
 *	objects where the spinlock is known to not be used.
 */
static inline __attribute__((no_instrument_function)) void Model1___skb_queue_head_init(struct Model1_sk_buff_head *Model1_list)
{
 Model1_list->Model1_prev = Model1_list->Model1_next = (struct Model1_sk_buff *)Model1_list;
 Model1_list->Model1_qlen = 0;
}

/*
 * This function creates a split out lock class for each invocation;
 * this is needed for now since a whole lot of users of the skb-queue
 * infrastructure in drivers have different locking usage (in hardirq)
 * than the networking core (in softirq only). In the long run either the
 * network layer or drivers should need annotation to consolidate the
 * main types of usage into 3 classes.
 */
static inline __attribute__((no_instrument_function)) void Model1_skb_queue_head_init(struct Model1_sk_buff_head *Model1_list)
{
 do { Model1_spinlock_check(&Model1_list->Model1_lock); do { *(&(&Model1_list->Model1_lock)->Model1_rlock) = (Model1_raw_spinlock_t) { .Model1_raw_lock = { { (0) } }, }; } while (0); } while (0);
 Model1___skb_queue_head_init(Model1_list);
}

static inline __attribute__((no_instrument_function)) void Model1_skb_queue_head_init_class(struct Model1_sk_buff_head *Model1_list,
  struct Model1_lock_class_key *Model1_class)
{
 Model1_skb_queue_head_init(Model1_list);
 do { (void)(Model1_class); } while (0);
}

/*
 *	Insert an sk_buff on a list.
 *
 *	The "__skb_xxxx()" functions are the non-atomic ones that
 *	can only be called with interrupts disabled.
 */
void Model1_skb_insert(struct Model1_sk_buff *old, struct Model1_sk_buff *Model1_newsk,
  struct Model1_sk_buff_head *Model1_list);
static inline __attribute__((no_instrument_function)) void Model1___skb_insert(struct Model1_sk_buff *Model1_newsk,
    struct Model1_sk_buff *Model1_prev, struct Model1_sk_buff *Model1_next,
    struct Model1_sk_buff_head *Model1_list)
{
 Model1_newsk->Model1_next = Model1_next;
 Model1_newsk->Model1_prev = Model1_prev;
 Model1_next->Model1_prev = Model1_prev->Model1_next = Model1_newsk;
 Model1_list->Model1_qlen++;
}

static inline __attribute__((no_instrument_function)) void Model1___skb_queue_splice(const struct Model1_sk_buff_head *Model1_list,
          struct Model1_sk_buff *Model1_prev,
          struct Model1_sk_buff *Model1_next)
{
 struct Model1_sk_buff *Model1_first = Model1_list->Model1_next;
 struct Model1_sk_buff *Model1_last = Model1_list->Model1_prev;

 Model1_first->Model1_prev = Model1_prev;
 Model1_prev->Model1_next = Model1_first;

 Model1_last->Model1_next = Model1_next;
 Model1_next->Model1_prev = Model1_last;
}

/**
 *	skb_queue_splice - join two skb lists, this is designed for stacks
 *	@list: the new list to add
 *	@head: the place to add it in the first list
 */
static inline __attribute__((no_instrument_function)) void Model1_skb_queue_splice(const struct Model1_sk_buff_head *Model1_list,
        struct Model1_sk_buff_head *Model1_head)
{
 if (!Model1_skb_queue_empty(Model1_list)) {
  Model1___skb_queue_splice(Model1_list, (struct Model1_sk_buff *) Model1_head, Model1_head->Model1_next);
  Model1_head->Model1_qlen += Model1_list->Model1_qlen;
 }
}

/**
 *	skb_queue_splice_init - join two skb lists and reinitialise the emptied list
 *	@list: the new list to add
 *	@head: the place to add it in the first list
 *
 *	The list at @list is reinitialised
 */
static inline __attribute__((no_instrument_function)) void Model1_skb_queue_splice_init(struct Model1_sk_buff_head *Model1_list,
      struct Model1_sk_buff_head *Model1_head)
{
 if (!Model1_skb_queue_empty(Model1_list)) {
  Model1___skb_queue_splice(Model1_list, (struct Model1_sk_buff *) Model1_head, Model1_head->Model1_next);
  Model1_head->Model1_qlen += Model1_list->Model1_qlen;
  Model1___skb_queue_head_init(Model1_list);
 }
}

/**
 *	skb_queue_splice_tail - join two skb lists, each list being a queue
 *	@list: the new list to add
 *	@head: the place to add it in the first list
 */
static inline __attribute__((no_instrument_function)) void Model1_skb_queue_splice_tail(const struct Model1_sk_buff_head *Model1_list,
      struct Model1_sk_buff_head *Model1_head)
{
 if (!Model1_skb_queue_empty(Model1_list)) {
  Model1___skb_queue_splice(Model1_list, Model1_head->Model1_prev, (struct Model1_sk_buff *) Model1_head);
  Model1_head->Model1_qlen += Model1_list->Model1_qlen;
 }
}

/**
 *	skb_queue_splice_tail_init - join two skb lists and reinitialise the emptied list
 *	@list: the new list to add
 *	@head: the place to add it in the first list
 *
 *	Each of the lists is a queue.
 *	The list at @list is reinitialised
 */
static inline __attribute__((no_instrument_function)) void Model1_skb_queue_splice_tail_init(struct Model1_sk_buff_head *Model1_list,
           struct Model1_sk_buff_head *Model1_head)
{
 if (!Model1_skb_queue_empty(Model1_list)) {
  Model1___skb_queue_splice(Model1_list, Model1_head->Model1_prev, (struct Model1_sk_buff *) Model1_head);
  Model1_head->Model1_qlen += Model1_list->Model1_qlen;
  Model1___skb_queue_head_init(Model1_list);
 }
}

/**
 *	__skb_queue_after - queue a buffer at the list head
 *	@list: list to use
 *	@prev: place after this buffer
 *	@newsk: buffer to queue
 *
 *	Queue a buffer int the middle of a list. This function takes no locks
 *	and you must therefore hold required locks before calling it.
 *
 *	A buffer cannot be placed on two lists at the same time.
 */
static inline __attribute__((no_instrument_function)) void Model1___skb_queue_after(struct Model1_sk_buff_head *Model1_list,
         struct Model1_sk_buff *Model1_prev,
         struct Model1_sk_buff *Model1_newsk)
{
 Model1___skb_insert(Model1_newsk, Model1_prev, Model1_prev->Model1_next, Model1_list);
}

void Model1_skb_append(struct Model1_sk_buff *old, struct Model1_sk_buff *Model1_newsk,
  struct Model1_sk_buff_head *Model1_list);

static inline __attribute__((no_instrument_function)) void Model1___skb_queue_before(struct Model1_sk_buff_head *Model1_list,
          struct Model1_sk_buff *Model1_next,
          struct Model1_sk_buff *Model1_newsk)
{
 Model1___skb_insert(Model1_newsk, Model1_next->Model1_prev, Model1_next, Model1_list);
}

/**
 *	__skb_queue_head - queue a buffer at the list head
 *	@list: list to use
 *	@newsk: buffer to queue
 *
 *	Queue a buffer at the start of a list. This function takes no locks
 *	and you must therefore hold required locks before calling it.
 *
 *	A buffer cannot be placed on two lists at the same time.
 */
void Model1_skb_queue_head(struct Model1_sk_buff_head *Model1_list, struct Model1_sk_buff *Model1_newsk);
static inline __attribute__((no_instrument_function)) void Model1___skb_queue_head(struct Model1_sk_buff_head *Model1_list,
        struct Model1_sk_buff *Model1_newsk)
{
 Model1___skb_queue_after(Model1_list, (struct Model1_sk_buff *)Model1_list, Model1_newsk);
}

/**
 *	__skb_queue_tail - queue a buffer at the list tail
 *	@list: list to use
 *	@newsk: buffer to queue
 *
 *	Queue a buffer at the end of a list. This function takes no locks
 *	and you must therefore hold required locks before calling it.
 *
 *	A buffer cannot be placed on two lists at the same time.
 */
void Model1_skb_queue_tail(struct Model1_sk_buff_head *Model1_list, struct Model1_sk_buff *Model1_newsk);
static inline __attribute__((no_instrument_function)) void Model1___skb_queue_tail(struct Model1_sk_buff_head *Model1_list,
       struct Model1_sk_buff *Model1_newsk)
{
 Model1___skb_queue_before(Model1_list, (struct Model1_sk_buff *)Model1_list, Model1_newsk);
}

/*
 * remove sk_buff from list. _Must_ be called atomically, and with
 * the list known..
 */
void Model1_skb_unlink(struct Model1_sk_buff *Model1_skb, struct Model1_sk_buff_head *Model1_list);
static inline __attribute__((no_instrument_function)) void Model1___skb_unlink(struct Model1_sk_buff *Model1_skb, struct Model1_sk_buff_head *Model1_list)
{
 struct Model1_sk_buff *Model1_next, *Model1_prev;

 Model1_list->Model1_qlen--;
 Model1_next = Model1_skb->Model1_next;
 Model1_prev = Model1_skb->Model1_prev;
 Model1_skb->Model1_next = Model1_skb->Model1_prev = ((void *)0);
 Model1_next->Model1_prev = Model1_prev;
 Model1_prev->Model1_next = Model1_next;
}

/**
 *	__skb_dequeue - remove from the head of the queue
 *	@list: list to dequeue from
 *
 *	Remove the head of the list. This function does not take any locks
 *	so must be used with appropriate locks held only. The head item is
 *	returned or %NULL if the list is empty.
 */
struct Model1_sk_buff *Model1_skb_dequeue(struct Model1_sk_buff_head *Model1_list);
static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1___skb_dequeue(struct Model1_sk_buff_head *Model1_list)
{
 struct Model1_sk_buff *Model1_skb = Model1_skb_peek(Model1_list);
 if (Model1_skb)
  Model1___skb_unlink(Model1_skb, Model1_list);
 return Model1_skb;
}

/**
 *	__skb_dequeue_tail - remove from the tail of the queue
 *	@list: list to dequeue from
 *
 *	Remove the tail of the list. This function does not take any locks
 *	so must be used with appropriate locks held only. The tail item is
 *	returned or %NULL if the list is empty.
 */
struct Model1_sk_buff *Model1_skb_dequeue_tail(struct Model1_sk_buff_head *Model1_list);
static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1___skb_dequeue_tail(struct Model1_sk_buff_head *Model1_list)
{
 struct Model1_sk_buff *Model1_skb = Model1_skb_peek_tail(Model1_list);
 if (Model1_skb)
  Model1___skb_unlink(Model1_skb, Model1_list);
 return Model1_skb;
}


static inline __attribute__((no_instrument_function)) bool Model1_skb_is_nonlinear(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb->Model1_data_len;
}

static inline __attribute__((no_instrument_function)) unsigned int Model1_skb_headlen(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb->Model1_len - Model1_skb->Model1_data_len;
}

static inline __attribute__((no_instrument_function)) int Model1_skb_pagelen(const struct Model1_sk_buff *Model1_skb)
{
 int Model1_i, Model1_len = 0;

 for (Model1_i = (int)((struct Model1_skb_shared_info *)(Model1_skb_end_pointer(Model1_skb)))->Model1_nr_frags - 1; Model1_i >= 0; Model1_i--)
  Model1_len += Model1_skb_frag_size(&((struct Model1_skb_shared_info *)(Model1_skb_end_pointer(Model1_skb)))->Model1_frags[Model1_i]);
 return Model1_len + Model1_skb_headlen(Model1_skb);
}

/**
 * __skb_fill_page_desc - initialise a paged fragment in an skb
 * @skb: buffer containing fragment to be initialised
 * @i: paged fragment index to initialise
 * @page: the page to use for this fragment
 * @off: the offset to the data with @page
 * @size: the length of the data
 *
 * Initialises the @i'th fragment of @skb to point to &size bytes at
 * offset @off within @page.
 *
 * Does not take any additional reference on the fragment.
 */
static inline __attribute__((no_instrument_function)) void Model1___skb_fill_page_desc(struct Model1_sk_buff *Model1_skb, int Model1_i,
     struct Model1_page *Model1_page, int Model1_off, int Model1_size)
{
 Model1_skb_frag_t *Model1_frag = &((struct Model1_skb_shared_info *)(Model1_skb_end_pointer(Model1_skb)))->Model1_frags[Model1_i];

 /*
	 * Propagate page pfmemalloc to the skb if we can. The problem is
	 * that not all callers have unique ownership of the page but rely
	 * on page_is_pfmemalloc doing the right thing(tm).
	 */
 Model1_frag->Model1_page.Model1_p = Model1_page;
 Model1_frag->Model1_page_offset = Model1_off;
 Model1_skb_frag_size_set(Model1_frag, Model1_size);

 Model1_page = Model1_compound_head(Model1_page);
 if (Model1_page_is_pfmemalloc(Model1_page))
  Model1_skb->Model1_pfmemalloc = true;
}

/**
 * skb_fill_page_desc - initialise a paged fragment in an skb
 * @skb: buffer containing fragment to be initialised
 * @i: paged fragment index to initialise
 * @page: the page to use for this fragment
 * @off: the offset to the data with @page
 * @size: the length of the data
 *
 * As per __skb_fill_page_desc() -- initialises the @i'th fragment of
 * @skb to point to @size bytes at offset @off within @page. In
 * addition updates @skb such that @i is the last fragment.
 *
 * Does not take any additional reference on the fragment.
 */
static inline __attribute__((no_instrument_function)) void Model1_skb_fill_page_desc(struct Model1_sk_buff *Model1_skb, int Model1_i,
          struct Model1_page *Model1_page, int Model1_off, int Model1_size)
{
 Model1___skb_fill_page_desc(Model1_skb, Model1_i, Model1_page, Model1_off, Model1_size);
 ((struct Model1_skb_shared_info *)(Model1_skb_end_pointer(Model1_skb)))->Model1_nr_frags = Model1_i + 1;
}

void Model1_skb_add_rx_frag(struct Model1_sk_buff *Model1_skb, int Model1_i, struct Model1_page *Model1_page, int Model1_off,
       int Model1_size, unsigned int Model1_truesize);

void Model1_skb_coalesce_rx_frag(struct Model1_sk_buff *Model1_skb, int Model1_i, int Model1_size,
     unsigned int Model1_truesize);






static inline __attribute__((no_instrument_function)) unsigned char *Model1_skb_tail_pointer(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb->Model1_head + Model1_skb->Model1_tail;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_reset_tail_pointer(struct Model1_sk_buff *Model1_skb)
{
 Model1_skb->Model1_tail = Model1_skb->Model1_data - Model1_skb->Model1_head;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_set_tail_pointer(struct Model1_sk_buff *Model1_skb, const int Model1_offset)
{
 Model1_skb_reset_tail_pointer(Model1_skb);
 Model1_skb->Model1_tail += Model1_offset;
}
/*
 *	Add data to an sk_buff
 */
unsigned char *Model1_pskb_put(struct Model1_sk_buff *Model1_skb, struct Model1_sk_buff *Model1_tail, int Model1_len);
unsigned char *Model1_skb_put(struct Model1_sk_buff *Model1_skb, unsigned int Model1_len);
static inline __attribute__((no_instrument_function)) unsigned char *Model1___skb_put(struct Model1_sk_buff *Model1_skb, unsigned int Model1_len)
{
 unsigned char *Model1_tmp = Model1_skb_tail_pointer(Model1_skb);
 do { if (__builtin_expect(!!(Model1_skb_is_nonlinear(Model1_skb)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/skbuff.h"), "i" (1909), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);
 Model1_skb->Model1_tail += Model1_len;
 Model1_skb->Model1_len += Model1_len;
 return Model1_tmp;
}

unsigned char *Model1_skb_push(struct Model1_sk_buff *Model1_skb, unsigned int Model1_len);
static inline __attribute__((no_instrument_function)) unsigned char *Model1___skb_push(struct Model1_sk_buff *Model1_skb, unsigned int Model1_len)
{
 Model1_skb->Model1_data -= Model1_len;
 Model1_skb->Model1_len += Model1_len;
 return Model1_skb->Model1_data;
}

unsigned char *Model1_skb_pull(struct Model1_sk_buff *Model1_skb, unsigned int Model1_len);
static inline __attribute__((no_instrument_function)) unsigned char *Model1___skb_pull(struct Model1_sk_buff *Model1_skb, unsigned int Model1_len)
{
 Model1_skb->Model1_len -= Model1_len;
 do { if (__builtin_expect(!!(Model1_skb->Model1_len < Model1_skb->Model1_data_len), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/skbuff.h"), "i" (1927), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);
 return Model1_skb->Model1_data += Model1_len;
}

static inline __attribute__((no_instrument_function)) unsigned char *Model1_skb_pull_inline(struct Model1_sk_buff *Model1_skb, unsigned int Model1_len)
{
 return __builtin_expect(!!(Model1_len > Model1_skb->Model1_len), 0) ? ((void *)0) : Model1___skb_pull(Model1_skb, Model1_len);
}

unsigned char *Model1___pskb_pull_tail(struct Model1_sk_buff *Model1_skb, int Model1_delta);

static inline __attribute__((no_instrument_function)) unsigned char *Model1___pskb_pull(struct Model1_sk_buff *Model1_skb, unsigned int Model1_len)
{
 if (Model1_len > Model1_skb_headlen(Model1_skb) &&
     !Model1___pskb_pull_tail(Model1_skb, Model1_len - Model1_skb_headlen(Model1_skb)))
  return ((void *)0);
 Model1_skb->Model1_len -= Model1_len;
 return Model1_skb->Model1_data += Model1_len;
}

static inline __attribute__((no_instrument_function)) unsigned char *Model1_pskb_pull(struct Model1_sk_buff *Model1_skb, unsigned int Model1_len)
{
 return __builtin_expect(!!(Model1_len > Model1_skb->Model1_len), 0) ? ((void *)0) : Model1___pskb_pull(Model1_skb, Model1_len);
}

static inline __attribute__((no_instrument_function)) int Model1_pskb_may_pull(struct Model1_sk_buff *Model1_skb, unsigned int Model1_len)
{
 if (__builtin_expect(!!(Model1_len <= Model1_skb_headlen(Model1_skb)), 1))
  return 1;
 if (__builtin_expect(!!(Model1_len > Model1_skb->Model1_len), 0))
  return 0;
 return Model1___pskb_pull_tail(Model1_skb, Model1_len - Model1_skb_headlen(Model1_skb)) != ((void *)0);
}

/**
 *	skb_headroom - bytes at buffer head
 *	@skb: buffer to check
 *
 *	Return the number of bytes of free space at the head of an &sk_buff.
 */
static inline __attribute__((no_instrument_function)) unsigned int Model1_skb_headroom(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb->Model1_data - Model1_skb->Model1_head;
}

/**
 *	skb_tailroom - bytes at buffer end
 *	@skb: buffer to check
 *
 *	Return the number of bytes of free space at the tail of an sk_buff
 */
static inline __attribute__((no_instrument_function)) int Model1_skb_tailroom(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb_is_nonlinear(Model1_skb) ? 0 : Model1_skb->Model1_end - Model1_skb->Model1_tail;
}

/**
 *	skb_availroom - bytes at buffer end
 *	@skb: buffer to check
 *
 *	Return the number of bytes of free space at the tail of an sk_buff
 *	allocated by sk_stream_alloc()
 */
static inline __attribute__((no_instrument_function)) int Model1_skb_availroom(const struct Model1_sk_buff *Model1_skb)
{
 if (Model1_skb_is_nonlinear(Model1_skb))
  return 0;

 return Model1_skb->Model1_end - Model1_skb->Model1_tail - Model1_skb->Model1_reserved_tailroom;
}

/**
 *	skb_reserve - adjust headroom
 *	@skb: buffer to alter
 *	@len: bytes to move
 *
 *	Increase the headroom of an empty &sk_buff by reducing the tail
 *	room. This is only allowed for an empty buffer.
 */
static inline __attribute__((no_instrument_function)) void Model1_skb_reserve(struct Model1_sk_buff *Model1_skb, int Model1_len)
{
 Model1_skb->Model1_data += Model1_len;
 Model1_skb->Model1_tail += Model1_len;
}

/**
 *	skb_tailroom_reserve - adjust reserved_tailroom
 *	@skb: buffer to alter
 *	@mtu: maximum amount of headlen permitted
 *	@needed_tailroom: minimum amount of reserved_tailroom
 *
 *	Set reserved_tailroom so that headlen can be as large as possible but
 *	not larger than mtu and tailroom cannot be smaller than
 *	needed_tailroom.
 *	The required headroom should already have been reserved before using
 *	this function.
 */
static inline __attribute__((no_instrument_function)) void Model1_skb_tailroom_reserve(struct Model1_sk_buff *Model1_skb, unsigned int Model1_mtu,
     unsigned int Model1_needed_tailroom)
{
 do { if (__builtin_expect(!!(Model1_skb_is_nonlinear(Model1_skb)), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/skbuff.h"), "i" (2027), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);
 if (Model1_mtu < Model1_skb_tailroom(Model1_skb) - Model1_needed_tailroom)
  /* use at most mtu */
  Model1_skb->Model1_reserved_tailroom = Model1_skb_tailroom(Model1_skb) - Model1_mtu;
 else
  /* use up to all available space */
  Model1_skb->Model1_reserved_tailroom = Model1_needed_tailroom;
}




static inline __attribute__((no_instrument_function)) void Model1_skb_set_inner_protocol(struct Model1_sk_buff *Model1_skb,
       Model1___be16 Model1_protocol)
{
 Model1_skb->Model1_inner_protocol = Model1_protocol;
 Model1_skb->Model1_inner_protocol_type = 0;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_set_inner_ipproto(struct Model1_sk_buff *Model1_skb,
      __u8 Model1_ipproto)
{
 Model1_skb->Model1_inner_ipproto = Model1_ipproto;
 Model1_skb->Model1_inner_protocol_type = 1;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_reset_inner_headers(struct Model1_sk_buff *Model1_skb)
{
 Model1_skb->Model1_inner_mac_header = Model1_skb->Model1_mac_header;
 Model1_skb->Model1_inner_network_header = Model1_skb->Model1_network_header;
 Model1_skb->Model1_inner_transport_header = Model1_skb->Model1_transport_header;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_reset_mac_len(struct Model1_sk_buff *Model1_skb)
{
 Model1_skb->Model1_mac_len = Model1_skb->Model1_network_header - Model1_skb->Model1_mac_header;
}

static inline __attribute__((no_instrument_function)) unsigned char *Model1_skb_inner_transport_header(const struct Model1_sk_buff
       *Model1_skb)
{
 return Model1_skb->Model1_head + Model1_skb->Model1_inner_transport_header;
}

static inline __attribute__((no_instrument_function)) int Model1_skb_inner_transport_offset(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb_inner_transport_header(Model1_skb) - Model1_skb->Model1_data;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_reset_inner_transport_header(struct Model1_sk_buff *Model1_skb)
{
 Model1_skb->Model1_inner_transport_header = Model1_skb->Model1_data - Model1_skb->Model1_head;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_set_inner_transport_header(struct Model1_sk_buff *Model1_skb,
         const int Model1_offset)
{
 Model1_skb_reset_inner_transport_header(Model1_skb);
 Model1_skb->Model1_inner_transport_header += Model1_offset;
}

static inline __attribute__((no_instrument_function)) unsigned char *Model1_skb_inner_network_header(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb->Model1_head + Model1_skb->Model1_inner_network_header;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_reset_inner_network_header(struct Model1_sk_buff *Model1_skb)
{
 Model1_skb->Model1_inner_network_header = Model1_skb->Model1_data - Model1_skb->Model1_head;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_set_inner_network_header(struct Model1_sk_buff *Model1_skb,
      const int Model1_offset)
{
 Model1_skb_reset_inner_network_header(Model1_skb);
 Model1_skb->Model1_inner_network_header += Model1_offset;
}

static inline __attribute__((no_instrument_function)) unsigned char *Model1_skb_inner_mac_header(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb->Model1_head + Model1_skb->Model1_inner_mac_header;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_reset_inner_mac_header(struct Model1_sk_buff *Model1_skb)
{
 Model1_skb->Model1_inner_mac_header = Model1_skb->Model1_data - Model1_skb->Model1_head;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_set_inner_mac_header(struct Model1_sk_buff *Model1_skb,
         const int Model1_offset)
{
 Model1_skb_reset_inner_mac_header(Model1_skb);
 Model1_skb->Model1_inner_mac_header += Model1_offset;
}
static inline __attribute__((no_instrument_function)) bool Model1_skb_transport_header_was_set(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb->Model1_transport_header != (typeof(Model1_skb->Model1_transport_header))~0U;
}

static inline __attribute__((no_instrument_function)) unsigned char *Model1_skb_transport_header(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb->Model1_head + Model1_skb->Model1_transport_header;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_reset_transport_header(struct Model1_sk_buff *Model1_skb)
{
 Model1_skb->Model1_transport_header = Model1_skb->Model1_data - Model1_skb->Model1_head;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_set_transport_header(struct Model1_sk_buff *Model1_skb,
         const int Model1_offset)
{
 Model1_skb_reset_transport_header(Model1_skb);
 Model1_skb->Model1_transport_header += Model1_offset;
}

static inline __attribute__((no_instrument_function)) unsigned char *Model1_skb_network_header(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb->Model1_head + Model1_skb->Model1_network_header;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_reset_network_header(struct Model1_sk_buff *Model1_skb)
{
 Model1_skb->Model1_network_header = Model1_skb->Model1_data - Model1_skb->Model1_head;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_set_network_header(struct Model1_sk_buff *Model1_skb, const int Model1_offset)
{
 Model1_skb_reset_network_header(Model1_skb);
 Model1_skb->Model1_network_header += Model1_offset;
}

static inline __attribute__((no_instrument_function)) unsigned char *Model1_skb_mac_header(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb->Model1_head + Model1_skb->Model1_mac_header;
}

static inline __attribute__((no_instrument_function)) int Model1_skb_mac_header_was_set(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb->Model1_mac_header != (typeof(Model1_skb->Model1_mac_header))~0U;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_reset_mac_header(struct Model1_sk_buff *Model1_skb)
{
 Model1_skb->Model1_mac_header = Model1_skb->Model1_data - Model1_skb->Model1_head;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_set_mac_header(struct Model1_sk_buff *Model1_skb, const int Model1_offset)
{
 Model1_skb_reset_mac_header(Model1_skb);
 Model1_skb->Model1_mac_header += Model1_offset;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_pop_mac_header(struct Model1_sk_buff *Model1_skb)
{
 Model1_skb->Model1_mac_header = Model1_skb->Model1_network_header;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_probe_transport_header(struct Model1_sk_buff *Model1_skb,
           const int Model1_offset_hint)
{
 struct Model1_flow_keys Model1_keys;

 if (Model1_skb_transport_header_was_set(Model1_skb))
  return;
 else if (Model1_skb_flow_dissect_flow_keys(Model1_skb, &Model1_keys, 0))
  Model1_skb_set_transport_header(Model1_skb, Model1_keys.Model1_control.Model1_thoff);
 else
  Model1_skb_set_transport_header(Model1_skb, Model1_offset_hint);
}

static inline __attribute__((no_instrument_function)) void Model1_skb_mac_header_rebuild(struct Model1_sk_buff *Model1_skb)
{
 if (Model1_skb_mac_header_was_set(Model1_skb)) {
  const unsigned char *Model1_old_mac = Model1_skb_mac_header(Model1_skb);

  Model1_skb_set_mac_header(Model1_skb, -Model1_skb->Model1_mac_len);
  Model1_memmove(Model1_skb_mac_header(Model1_skb), Model1_old_mac, Model1_skb->Model1_mac_len);
 }
}

static inline __attribute__((no_instrument_function)) int Model1_skb_checksum_start_offset(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb->Model1_csum_start - Model1_skb_headroom(Model1_skb);
}

static inline __attribute__((no_instrument_function)) unsigned char *Model1_skb_checksum_start(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb->Model1_head + Model1_skb->Model1_csum_start;
}

static inline __attribute__((no_instrument_function)) int Model1_skb_transport_offset(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb_transport_header(Model1_skb) - Model1_skb->Model1_data;
}

static inline __attribute__((no_instrument_function)) Model1_u32 Model1_skb_network_header_len(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb->Model1_transport_header - Model1_skb->Model1_network_header;
}

static inline __attribute__((no_instrument_function)) Model1_u32 Model1_skb_inner_network_header_len(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb->Model1_inner_transport_header - Model1_skb->Model1_inner_network_header;
}

static inline __attribute__((no_instrument_function)) int Model1_skb_network_offset(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb_network_header(Model1_skb) - Model1_skb->Model1_data;
}

static inline __attribute__((no_instrument_function)) int Model1_skb_inner_network_offset(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb_inner_network_header(Model1_skb) - Model1_skb->Model1_data;
}

static inline __attribute__((no_instrument_function)) int Model1_pskb_network_may_pull(struct Model1_sk_buff *Model1_skb, unsigned int Model1_len)
{
 return Model1_pskb_may_pull(Model1_skb, Model1_skb_network_offset(Model1_skb) + Model1_len);
}

/*
 * CPUs often take a performance hit when accessing unaligned memory
 * locations. The actual performance hit varies, it can be small if the
 * hardware handles it or large if we have to take an exception and fix it
 * in software.
 *
 * Since an ethernet header is 14 bytes network drivers often end up with
 * the IP header at an unaligned offset. The IP header can be aligned by
 * shifting the start of the packet by 2 bytes. Drivers should do this
 * with:
 *
 * skb_reserve(skb, NET_IP_ALIGN);
 *
 * The downside to this alignment of the IP header is that the DMA is now
 * unaligned. On some architectures the cost of an unaligned DMA is high
 * and this cost outweighs the gains made by aligning the IP header.
 *
 * Since this trade off varies between architectures, we allow NET_IP_ALIGN
 * to be overridden.
 */




/*
 * The networking layer reserves some headroom in skb data (via
 * dev_alloc_skb). This is used to avoid having to reallocate skb data when
 * the header has to grow. In the default case, if the header has to grow
 * 32 bytes or less we avoid the reallocation.
 *
 * Unfortunately this headroom changes the DMA alignment of the resulting
 * network packet. As for NET_IP_ALIGN, this unaligned DMA is expensive
 * on some architectures. An architecture can override this value,
 * perhaps setting it to a cacheline in size (since that will maintain
 * cacheline alignment of the DMA). It must be a power of 2.
 *
 * Various parts of the networking layer expect at least 32 bytes of
 * headroom, you should not reduce this.
 *
 * Using max(32, L1_CACHE_BYTES) makes sense (especially with RPS)
 * to reduce average number of cache lines per packet.
 * get_rps_cpus() for example only access one 64 bytes aligned block :
 * NET_IP_ALIGN(2) + ethernet_header(14) + IP_header(20/40) + ports(8)
 */




int Model1____pskb_trim(struct Model1_sk_buff *Model1_skb, unsigned int Model1_len);

static inline __attribute__((no_instrument_function)) void Model1___skb_trim(struct Model1_sk_buff *Model1_skb, unsigned int Model1_len)
{
 if (__builtin_expect(!!(Model1_skb_is_nonlinear(Model1_skb)), 0)) {
  ({ int Model1___ret_warn_on = !!(1); if (__builtin_expect(!!(Model1___ret_warn_on), 0)) Model1_warn_slowpath_null("./include/linux/skbuff.h", 2301); __builtin_expect(!!(Model1___ret_warn_on), 0); });
  return;
 }
 Model1_skb->Model1_len = Model1_len;
 Model1_skb_set_tail_pointer(Model1_skb, Model1_len);
}

void Model1_skb_trim(struct Model1_sk_buff *Model1_skb, unsigned int Model1_len);

static inline __attribute__((no_instrument_function)) int Model1___pskb_trim(struct Model1_sk_buff *Model1_skb, unsigned int Model1_len)
{
 if (Model1_skb->Model1_data_len)
  return Model1____pskb_trim(Model1_skb, Model1_len);
 Model1___skb_trim(Model1_skb, Model1_len);
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_pskb_trim(struct Model1_sk_buff *Model1_skb, unsigned int Model1_len)
{
 return (Model1_len < Model1_skb->Model1_len) ? Model1___pskb_trim(Model1_skb, Model1_len) : 0;
}

/**
 *	pskb_trim_unique - remove end from a paged unique (not cloned) buffer
 *	@skb: buffer to alter
 *	@len: new length
 *
 *	This is identical to pskb_trim except that the caller knows that
 *	the skb is not cloned so we should never get an error due to out-
 *	of-memory.
 */
static inline __attribute__((no_instrument_function)) void Model1_pskb_trim_unique(struct Model1_sk_buff *Model1_skb, unsigned int Model1_len)
{
 int err = Model1_pskb_trim(Model1_skb, Model1_len);
 do { if (__builtin_expect(!!(err), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/skbuff.h"), "i" (2335), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);
}

/**
 *	skb_orphan - orphan a buffer
 *	@skb: buffer to orphan
 *
 *	If a buffer currently has an owner then we call the owner's
 *	destructor function and make the @skb unowned. The buffer continues
 *	to exist but is no longer charged to its former owner.
 */
void Model1_sock_rfree(struct Model1_sk_buff *Model1_skb);
static inline __attribute__((no_instrument_function)) void Model1_skb_orphan(struct Model1_sk_buff *Model1_skb)
{
 if (Model1_skb->Model1_destructor) {
  Model1_skb->Model1_destructor(Model1_skb); //we ignore skb->destructor
  Model1_skb->Model1_destructor = ((void *)0);
  Model1_skb->Model1_sk = ((void *)0);
 } else {
#if CY_ABSTRACT1
     //TODO: confirm the correctness. Actual logic is to print BUG_ON, but that's not expected
     //In what case, it will trigger this branch?
     /*
     printf("Bug_ON inside Model1_skb_orphan\n");
     Model1_sock_rfree(Model1_skb);
     Model1_skb->Model1_destructor = ((void *)0);
     Model1_skb->Model1_sk = ((void *)0);
     */
#else
  do { if (__builtin_expect(!!(Model1_skb->Model1_sk), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/skbuff.h"), "i" (2353), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);
#endif
 }
}

/**
 *	skb_orphan_frags - orphan the frags contained in a buffer
 *	@skb: buffer to orphan frags from
 *	@gfp_mask: allocation mask for replacement pages
 *
 *	For each frag in the SKB which needs a destructor (i.e. has an
 *	owner) create a copy of that frag and release the original
 *	page by calling the destructor.
 */
static inline __attribute__((no_instrument_function)) int Model1_skb_orphan_frags(struct Model1_sk_buff *Model1_skb, Model1_gfp_t Model1_gfp_mask)
{
 if (__builtin_expect(!!(!(((struct Model1_skb_shared_info *)(Model1_skb_end_pointer(Model1_skb)))->Model1_tx_flags & Model1_SKBTX_DEV_ZEROCOPY)), 1))
  return 0;
 return Model1_skb_copy_ubufs(Model1_skb, Model1_gfp_mask);
}

/**
 *	__skb_queue_purge - empty a list
 *	@list: list to empty
 *
 *	Delete all buffers on an &sk_buff list. Each buffer is removed from
 *	the list and one reference dropped. This function does not take the
 *	list lock and the caller must hold the relevant locks to use it.
 */
void Model1_skb_queue_purge(struct Model1_sk_buff_head *Model1_list);
static inline __attribute__((no_instrument_function)) void Model1___skb_queue_purge(struct Model1_sk_buff_head *Model1_list)
{
 struct Model1_sk_buff *Model1_skb;
 while ((Model1_skb = Model1___skb_dequeue(Model1_list)) != ((void *)0))
  Model1_kfree_skb(Model1_skb);
}

void *Model1_netdev_alloc_frag(unsigned int Model1_fragsz);

struct Model1_sk_buff *Model1___netdev_alloc_skb(struct Model1_net_device *Model1_dev, unsigned int Model1_length,
       Model1_gfp_t Model1_gfp_mask);

/**
 *	netdev_alloc_skb - allocate an skbuff for rx on a specific device
 *	@dev: network device to receive on
 *	@length: length to allocate
 *
 *	Allocate a new &sk_buff and assign it a usage count of one. The
 *	buffer has unspecified headroom built in. Users should allocate
 *	the headroom they think they need without accounting for the
 *	built in space. The built in space is used for optimisations.
 *
 *	%NULL is returned if there is no free memory. Although this function
 *	allocates memory it can be called from an interrupt.
 */
static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1_netdev_alloc_skb(struct Model1_net_device *Model1_dev,
            unsigned int Model1_length)
{
 return Model1___netdev_alloc_skb(Model1_dev, Model1_length, ((( Model1_gfp_t)0x20u)|(( Model1_gfp_t)0x80000u)|(( Model1_gfp_t)0x2000000u)));
}

/* legacy helper around __netdev_alloc_skb() */
static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1___dev_alloc_skb(unsigned int Model1_length,
           Model1_gfp_t Model1_gfp_mask)
{
 return Model1___netdev_alloc_skb(((void *)0), Model1_length, Model1_gfp_mask);
}

/* legacy helper around netdev_alloc_skb() */
static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1_dev_alloc_skb(unsigned int Model1_length)
{
 return Model1_netdev_alloc_skb(((void *)0), Model1_length);
}


static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1___netdev_alloc_skb_ip_align(struct Model1_net_device *Model1_dev,
  unsigned int Model1_length, Model1_gfp_t Model1_gfp)
{
 struct Model1_sk_buff *Model1_skb = Model1___netdev_alloc_skb(Model1_dev, Model1_length + 0, Model1_gfp);

 if (0 && Model1_skb)
  Model1_skb_reserve(Model1_skb, 0);
 return Model1_skb;
}

static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1_netdev_alloc_skb_ip_align(struct Model1_net_device *Model1_dev,
  unsigned int Model1_length)
{
 return Model1___netdev_alloc_skb_ip_align(Model1_dev, Model1_length, ((( Model1_gfp_t)0x20u)|(( Model1_gfp_t)0x80000u)|(( Model1_gfp_t)0x2000000u)));
}

static inline __attribute__((no_instrument_function)) void Model1_skb_free_frag(void *Model1_addr)
{
 Model1___free_page_frag(Model1_addr);
}

void *Model1_napi_alloc_frag(unsigned int Model1_fragsz);
struct Model1_sk_buff *Model1___napi_alloc_skb(struct Model1_napi_struct *Model1_napi,
     unsigned int Model1_length, Model1_gfp_t Model1_gfp_mask);
static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1_napi_alloc_skb(struct Model1_napi_struct *Model1_napi,
          unsigned int Model1_length)
{
 return Model1___napi_alloc_skb(Model1_napi, Model1_length, ((( Model1_gfp_t)0x20u)|(( Model1_gfp_t)0x80000u)|(( Model1_gfp_t)0x2000000u)));
}
void Model1_napi_consume_skb(struct Model1_sk_buff *Model1_skb, int Model1_budget);

void Model1___kfree_skb_flush(void);
void Model1___kfree_skb_defer(struct Model1_sk_buff *Model1_skb);

/**
 * __dev_alloc_pages - allocate page for network Rx
 * @gfp_mask: allocation priority. Set __GFP_NOMEMALLOC if not for network Rx
 * @order: size of the allocation
 *
 * Allocate a new page.
 *
 * %NULL is returned if there is no free memory.
*/
static inline __attribute__((no_instrument_function)) struct Model1_page *Model1___dev_alloc_pages(Model1_gfp_t Model1_gfp_mask,
          unsigned int Model1_order)
{
 /* This piece of code contains several assumptions.
	 * 1.  This is for device Rx, therefor a cold page is preferred.
	 * 2.  The expectation is the user wants a compound page.
	 * 3.  If requesting a order 0 page it will not be compound
	 *     due to the check to see if order has a value in prep_new_page
	 * 4.  __GFP_MEMALLOC is ignored if __GFP_NOMEMALLOC is set due to
	 *     code in gfp_to_alloc_flags that should be enforcing this.
	 */
 Model1_gfp_mask |= (( Model1_gfp_t)0x100u) | (( Model1_gfp_t)0x4000u) | (( Model1_gfp_t)0x2000u);

 return Model1_alloc_pages_node((-1), Model1_gfp_mask, Model1_order);
}

static inline __attribute__((no_instrument_function)) struct Model1_page *Model1_dev_alloc_pages(unsigned int Model1_order)
{
 return Model1___dev_alloc_pages(((( Model1_gfp_t)0x20u)|(( Model1_gfp_t)0x80000u)|(( Model1_gfp_t)0x2000000u)) | (( Model1_gfp_t)0x200u), Model1_order);
}

/**
 * __dev_alloc_page - allocate a page for network Rx
 * @gfp_mask: allocation priority. Set __GFP_NOMEMALLOC if not for network Rx
 *
 * Allocate a new page.
 *
 * %NULL is returned if there is no free memory.
 */
static inline __attribute__((no_instrument_function)) struct Model1_page *Model1___dev_alloc_page(Model1_gfp_t Model1_gfp_mask)
{
 return Model1___dev_alloc_pages(Model1_gfp_mask, 0);
}

static inline __attribute__((no_instrument_function)) struct Model1_page *Model1_dev_alloc_page(void)
{
 return Model1_dev_alloc_pages(0);
}

/**
 *	skb_propagate_pfmemalloc - Propagate pfmemalloc if skb is allocated after RX page
 *	@page: The page that was allocated from skb_alloc_page
 *	@skb: The skb that may need pfmemalloc set
 */
static inline __attribute__((no_instrument_function)) void Model1_skb_propagate_pfmemalloc(struct Model1_page *Model1_page,
          struct Model1_sk_buff *Model1_skb)
{
 if (Model1_page_is_pfmemalloc(Model1_page))
  Model1_skb->Model1_pfmemalloc = true;
}

/**
 * skb_frag_page - retrieve the page referred to by a paged fragment
 * @frag: the paged fragment
 *
 * Returns the &struct page associated with @frag.
 */
static inline __attribute__((no_instrument_function)) struct Model1_page *Model1_skb_frag_page(const Model1_skb_frag_t *Model1_frag)
{
 return Model1_frag->Model1_page.Model1_p;
}

/**
 * __skb_frag_ref - take an addition reference on a paged fragment.
 * @frag: the paged fragment
 *
 * Takes an additional reference on the paged fragment @frag.
 */
static inline __attribute__((no_instrument_function)) void Model1___skb_frag_ref(Model1_skb_frag_t *Model1_frag)
{
 Model1_get_page(Model1_skb_frag_page(Model1_frag));
}

/**
 * skb_frag_ref - take an addition reference on a paged fragment of an skb.
 * @skb: the buffer
 * @f: the fragment offset.
 *
 * Takes an additional reference on the @f'th paged fragment of @skb.
 */
static inline __attribute__((no_instrument_function)) void Model1_skb_frag_ref(struct Model1_sk_buff *Model1_skb, int Model1_f)
{
 Model1___skb_frag_ref(&((struct Model1_skb_shared_info *)(Model1_skb_end_pointer(Model1_skb)))->Model1_frags[Model1_f]);
}

/**
 * __skb_frag_unref - release a reference on a paged fragment.
 * @frag: the paged fragment
 *
 * Releases a reference on the paged fragment @frag.
 */
static inline __attribute__((no_instrument_function)) void Model1___skb_frag_unref(Model1_skb_frag_t *Model1_frag)
{
 Model1_put_page(Model1_skb_frag_page(Model1_frag));
}

/**
 * skb_frag_unref - release a reference on a paged fragment of an skb.
 * @skb: the buffer
 * @f: the fragment offset
 *
 * Releases a reference on the @f'th paged fragment of @skb.
 */
static inline __attribute__((no_instrument_function)) void Model1_skb_frag_unref(struct Model1_sk_buff *Model1_skb, int Model1_f)
{
 Model1___skb_frag_unref(&((struct Model1_skb_shared_info *)(Model1_skb_end_pointer(Model1_skb)))->Model1_frags[Model1_f]);
}

/**
 * skb_frag_address - gets the address of the data contained in a paged fragment
 * @frag: the paged fragment buffer
 *
 * Returns the address of the data within @frag. The page must already
 * be mapped.
 */
static inline __attribute__((no_instrument_function)) void *Model1_skb_frag_address(const Model1_skb_frag_t *Model1_frag)
{
 return Model1_lowmem_page_address(Model1_skb_frag_page(Model1_frag)) + Model1_frag->Model1_page_offset;
}

/**
 * skb_frag_address_safe - gets the address of the data contained in a paged fragment
 * @frag: the paged fragment buffer
 *
 * Returns the address of the data within @frag. Checks that the page
 * is mapped and returns %NULL otherwise.
 */
static inline __attribute__((no_instrument_function)) void *Model1_skb_frag_address_safe(const Model1_skb_frag_t *Model1_frag)
{
 void *Model1_ptr = Model1_lowmem_page_address(Model1_skb_frag_page(Model1_frag));
 if (__builtin_expect(!!(!Model1_ptr), 0))
  return ((void *)0);

 return Model1_ptr + Model1_frag->Model1_page_offset;
}

/**
 * __skb_frag_set_page - sets the page contained in a paged fragment
 * @frag: the paged fragment
 * @page: the page to set
 *
 * Sets the fragment @frag to contain @page.
 */
static inline __attribute__((no_instrument_function)) void Model1___skb_frag_set_page(Model1_skb_frag_t *Model1_frag, struct Model1_page *Model1_page)
{
 Model1_frag->Model1_page.Model1_p = Model1_page;
}

/**
 * skb_frag_set_page - sets the page contained in a paged fragment of an skb
 * @skb: the buffer
 * @f: the fragment offset
 * @page: the page to set
 *
 * Sets the @f'th fragment of @skb to contain @page.
 */
static inline __attribute__((no_instrument_function)) void Model1_skb_frag_set_page(struct Model1_sk_buff *Model1_skb, int Model1_f,
         struct Model1_page *Model1_page)
{
 Model1___skb_frag_set_page(&((struct Model1_skb_shared_info *)(Model1_skb_end_pointer(Model1_skb)))->Model1_frags[Model1_f], Model1_page);
}

bool Model1_skb_page_frag_refill(unsigned int Model1_sz, struct Model1_page_frag *Model1_pfrag, Model1_gfp_t Model1_prio);

/**
 * skb_frag_dma_map - maps a paged fragment via the DMA API
 * @dev: the device to map the fragment to
 * @frag: the paged fragment to map
 * @offset: the offset within the fragment (starting at the
 *          fragment's own offset)
 * @size: the number of bytes to map
 * @dir: the direction of the mapping (%PCI_DMA_*)
 *
 * Maps the page associated with @frag to @device.
 */
static inline __attribute__((no_instrument_function)) Model1_dma_addr_t Model1_skb_frag_dma_map(struct Model1_device *Model1_dev,
       const Model1_skb_frag_t *Model1_frag,
       Model1_size_t Model1_offset, Model1_size_t Model1_size,
       enum Model1_dma_data_direction Model1_dir)
{
 return Model1_dma_map_page(Model1_dev, Model1_skb_frag_page(Model1_frag),
       Model1_frag->Model1_page_offset + Model1_offset, Model1_size, Model1_dir);
}

static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1_pskb_copy(struct Model1_sk_buff *Model1_skb,
     Model1_gfp_t Model1_gfp_mask)
{
 return Model1___pskb_copy(Model1_skb, Model1_skb_headroom(Model1_skb), Model1_gfp_mask);
}


static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1_pskb_copy_for_clone(struct Model1_sk_buff *Model1_skb,
        Model1_gfp_t Model1_gfp_mask)
{
 return Model1___pskb_copy_fclone(Model1_skb, Model1_skb_headroom(Model1_skb), Model1_gfp_mask, true);
}


/**
 *	skb_clone_writable - is the header of a clone writable
 *	@skb: buffer to check
 *	@len: length up to which to write
 *
 *	Returns true if modifying the header part of the cloned buffer
 *	does not requires the data to be copied.
 */
static inline __attribute__((no_instrument_function)) int Model1_skb_clone_writable(const struct Model1_sk_buff *Model1_skb, unsigned int Model1_len)
{
 return !Model1_skb_header_cloned(Model1_skb) &&
        Model1_skb_headroom(Model1_skb) + Model1_len <= Model1_skb->Model1_hdr_len;
}

static inline __attribute__((no_instrument_function)) int Model1_skb_try_make_writable(struct Model1_sk_buff *Model1_skb,
     unsigned int Model1_write_len)
{
 return Model1_skb_cloned(Model1_skb) && !Model1_skb_clone_writable(Model1_skb, Model1_write_len) &&
        Model1_pskb_expand_head(Model1_skb, 0, 0, ((( Model1_gfp_t)0x20u)|(( Model1_gfp_t)0x80000u)|(( Model1_gfp_t)0x2000000u)));
}

static inline __attribute__((no_instrument_function)) int Model1___skb_cow(struct Model1_sk_buff *Model1_skb, unsigned int Model1_headroom,
       int Model1_cloned)
{
 int Model1_delta = 0;

 if (Model1_headroom > Model1_skb_headroom(Model1_skb))
  Model1_delta = Model1_headroom - Model1_skb_headroom(Model1_skb);

 if (Model1_delta || Model1_cloned)
  return Model1_pskb_expand_head(Model1_skb, ((((Model1_delta)) + ((typeof((Model1_delta)))((({ typeof(32) Model1__max1 = (32); typeof((1 << (6))) Model1__max2 = ((1 << (6))); (void) (&Model1__max1 == &Model1__max2); Model1__max1 > Model1__max2 ? Model1__max1 : Model1__max2; }))) - 1)) & ~((typeof((Model1_delta)))((({ typeof(32) Model1__max1 = (32); typeof((1 << (6))) Model1__max2 = ((1 << (6))); (void) (&Model1__max1 == &Model1__max2); Model1__max1 > Model1__max2 ? Model1__max1 : Model1__max2; }))) - 1)), 0,
     ((( Model1_gfp_t)0x20u)|(( Model1_gfp_t)0x80000u)|(( Model1_gfp_t)0x2000000u)));
 return 0;
}

/**
 *	skb_cow - copy header of skb when it is required
 *	@skb: buffer to cow
 *	@headroom: needed headroom
 *
 *	If the skb passed lacks sufficient headroom or its data part
 *	is shared, data is reallocated. If reallocation fails, an error
 *	is returned and original skb is not changed.
 *
 *	The result is skb with writable area skb->head...skb->tail
 *	and at least @headroom of space at head.
 */
static inline __attribute__((no_instrument_function)) int Model1_skb_cow(struct Model1_sk_buff *Model1_skb, unsigned int Model1_headroom)
{
 return Model1___skb_cow(Model1_skb, Model1_headroom, Model1_skb_cloned(Model1_skb));
}

/**
 *	skb_cow_head - skb_cow but only making the head writable
 *	@skb: buffer to cow
 *	@headroom: needed headroom
 *
 *	This function is identical to skb_cow except that we replace the
 *	skb_cloned check by skb_header_cloned.  It should be used when
 *	you only need to push on some header and do not need to modify
 *	the data.
 */
static inline __attribute__((no_instrument_function)) int Model1_skb_cow_head(struct Model1_sk_buff *Model1_skb, unsigned int Model1_headroom)
{
 return Model1___skb_cow(Model1_skb, Model1_headroom, Model1_skb_header_cloned(Model1_skb));
}

/**
 *	skb_padto	- pad an skbuff up to a minimal size
 *	@skb: buffer to pad
 *	@len: minimal length
 *
 *	Pads up a buffer to ensure the trailing bytes exist and are
 *	blanked. If the buffer already contains sufficient data it
 *	is untouched. Otherwise it is extended. Returns zero on
 *	success. The skb is freed on error.
 */
static inline __attribute__((no_instrument_function)) int Model1_skb_padto(struct Model1_sk_buff *Model1_skb, unsigned int Model1_len)
{
 unsigned int Model1_size = Model1_skb->Model1_len;
 if (__builtin_expect(!!(Model1_size >= Model1_len), 1))
  return 0;
 return Model1_skb_pad(Model1_skb, Model1_len - Model1_size);
}

/**
 *	skb_put_padto - increase size and pad an skbuff up to a minimal size
 *	@skb: buffer to pad
 *	@len: minimal length
 *
 *	Pads up a buffer to ensure the trailing bytes exist and are
 *	blanked. If the buffer already contains sufficient data it
 *	is untouched. Otherwise it is extended. Returns zero on
 *	success. The skb is freed on error.
 */
static inline __attribute__((no_instrument_function)) int Model1_skb_put_padto(struct Model1_sk_buff *Model1_skb, unsigned int Model1_len)
{
 unsigned int Model1_size = Model1_skb->Model1_len;

 if (__builtin_expect(!!(Model1_size < Model1_len), 0)) {
  Model1_len -= Model1_size;
  if (Model1_skb_pad(Model1_skb, Model1_len))
   return -12;
  Model1___skb_put(Model1_skb, Model1_len);
 }
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_skb_add_data(struct Model1_sk_buff *Model1_skb,
          struct Model1_iov_iter *Model1_from, int Model1_copy)
{
 const int Model1_off = Model1_skb->Model1_len;

 if (Model1_skb->Model1_ip_summed == 0) {
  Model1___wsum Model1_csum = 0;
  if (Model1_csum_and_copy_from_iter(Model1_skb_put(Model1_skb, Model1_copy), Model1_copy,
         &Model1_csum, Model1_from) == Model1_copy) {
   Model1_skb->Model1_csum = Model1_csum_block_add(Model1_skb->Model1_csum, Model1_csum, Model1_off);
   return 0;
  }
 } else if (Model1_copy_from_iter(Model1_skb_put(Model1_skb, Model1_copy), Model1_copy, Model1_from) == Model1_copy)
  return 0;

 Model1___skb_trim(Model1_skb, Model1_off);
 return -14;
}

static inline __attribute__((no_instrument_function)) bool Model1_skb_can_coalesce(struct Model1_sk_buff *Model1_skb, int Model1_i,
        const struct Model1_page *Model1_page, int Model1_off)
{
 if (Model1_i) {
  const struct Model1_skb_frag_struct *Model1_frag = &((struct Model1_skb_shared_info *)(Model1_skb_end_pointer(Model1_skb)))->Model1_frags[Model1_i - 1];

  return Model1_page == Model1_skb_frag_page(Model1_frag) &&
         Model1_off == Model1_frag->Model1_page_offset + Model1_skb_frag_size(Model1_frag);
 }
 return false;
}

static inline __attribute__((no_instrument_function)) int Model1___skb_linearize(struct Model1_sk_buff *Model1_skb)
{
 return Model1___pskb_pull_tail(Model1_skb, Model1_skb->Model1_data_len) ? 0 : -12;
}

/**
 *	skb_linearize - convert paged skb to linear one
 *	@skb: buffer to linarize
 *
 *	If there is no free memory -ENOMEM is returned, otherwise zero
 *	is returned and the old skb data released.
 */
static inline __attribute__((no_instrument_function)) int Model1_skb_linearize(struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb_is_nonlinear(Model1_skb) ? Model1___skb_linearize(Model1_skb) : 0;
}

/**
 * skb_has_shared_frag - can any frag be overwritten
 * @skb: buffer to test
 *
 * Return true if the skb has at least one frag that might be modified
 * by an external entity (as in vmsplice()/sendfile())
 */
static inline __attribute__((no_instrument_function)) bool Model1_skb_has_shared_frag(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb_is_nonlinear(Model1_skb) &&
        ((struct Model1_skb_shared_info *)(Model1_skb_end_pointer(Model1_skb)))->Model1_tx_flags & Model1_SKBTX_SHARED_FRAG;
}

/**
 *	skb_linearize_cow - make sure skb is linear and writable
 *	@skb: buffer to process
 *
 *	If there is no free memory -ENOMEM is returned, otherwise zero
 *	is returned and the old skb data released.
 */
static inline __attribute__((no_instrument_function)) int Model1_skb_linearize_cow(struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb_is_nonlinear(Model1_skb) || Model1_skb_cloned(Model1_skb) ?
        Model1___skb_linearize(Model1_skb) : 0;
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void
Model1___skb_postpull_rcsum(struct Model1_sk_buff *Model1_skb, const void *Model1_start, unsigned int Model1_len,
       unsigned int Model1_off)
{
 if (Model1_skb->Model1_ip_summed == 2)
  Model1_skb->Model1_csum = Model1_csum_block_sub(Model1_skb->Model1_csum,
        Model1_csum_partial(Model1_start, Model1_len, 0), Model1_off);
 else if (Model1_skb->Model1_ip_summed == 3 &&
   Model1_skb_checksum_start_offset(Model1_skb) < 0)
  Model1_skb->Model1_ip_summed = 0;
}

/**
 *	skb_postpull_rcsum - update checksum for received skb after pull
 *	@skb: buffer to update
 *	@start: start of data before pull
 *	@len: length of data pulled
 *
 *	After doing a pull on a received packet, you need to call this to
 *	update the CHECKSUM_COMPLETE checksum, or set ip_summed to
 *	CHECKSUM_NONE so that it can be recomputed from scratch.
 */
static inline __attribute__((no_instrument_function)) void Model1_skb_postpull_rcsum(struct Model1_sk_buff *Model1_skb,
          const void *Model1_start, unsigned int Model1_len)
{
 Model1___skb_postpull_rcsum(Model1_skb, Model1_start, Model1_len, 0);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void
Model1___skb_postpush_rcsum(struct Model1_sk_buff *Model1_skb, const void *Model1_start, unsigned int Model1_len,
       unsigned int Model1_off)
{
 if (Model1_skb->Model1_ip_summed == 2)
  Model1_skb->Model1_csum = Model1_csum_block_add(Model1_skb->Model1_csum,
        Model1_csum_partial(Model1_start, Model1_len, 0), Model1_off);
}

/**
 *	skb_postpush_rcsum - update checksum for received skb after push
 *	@skb: buffer to update
 *	@start: start of data after push
 *	@len: length of data pushed
 *
 *	After doing a push on a received packet, you need to call this to
 *	update the CHECKSUM_COMPLETE checksum.
 */
static inline __attribute__((no_instrument_function)) void Model1_skb_postpush_rcsum(struct Model1_sk_buff *Model1_skb,
          const void *Model1_start, unsigned int Model1_len)
{
 Model1___skb_postpush_rcsum(Model1_skb, Model1_start, Model1_len, 0);
}

unsigned char *Model1_skb_pull_rcsum(struct Model1_sk_buff *Model1_skb, unsigned int Model1_len);

/**
 *	skb_push_rcsum - push skb and update receive checksum
 *	@skb: buffer to update
 *	@len: length of data pulled
 *
 *	This function performs an skb_push on the packet and updates
 *	the CHECKSUM_COMPLETE checksum.  It should be used on
 *	receive path processing instead of skb_push unless you know
 *	that the checksum difference is zero (e.g., a valid IP header)
 *	or you are setting ip_summed to CHECKSUM_NONE.
 */
static inline __attribute__((no_instrument_function)) unsigned char *Model1_skb_push_rcsum(struct Model1_sk_buff *Model1_skb,
         unsigned int Model1_len)
{
 Model1_skb_push(Model1_skb, Model1_len);
 Model1_skb_postpush_rcsum(Model1_skb, Model1_skb->Model1_data, Model1_len);
 return Model1_skb->Model1_data;
}

/**
 *	pskb_trim_rcsum - trim received skb and update checksum
 *	@skb: buffer to trim
 *	@len: new length
 *
 *	This is exactly the same as pskb_trim except that it ensures the
 *	checksum of received packets are still valid after the operation.
 */

static inline __attribute__((no_instrument_function)) int Model1_pskb_trim_rcsum(struct Model1_sk_buff *Model1_skb, unsigned int Model1_len)
{
 if (__builtin_expect(!!(Model1_len >= Model1_skb->Model1_len), 1))
  return 0;
 if (Model1_skb->Model1_ip_summed == 2)
  Model1_skb->Model1_ip_summed = 0;
 return Model1___pskb_trim(Model1_skb, Model1_len);
}
static inline __attribute__((no_instrument_function)) bool Model1_skb_has_frag_list(const struct Model1_sk_buff *Model1_skb)
{
 return ((struct Model1_skb_shared_info *)(Model1_skb_end_pointer(Model1_skb)))->Model1_frag_list != ((void *)0);
}

static inline __attribute__((no_instrument_function)) void Model1_skb_frag_list_init(struct Model1_sk_buff *Model1_skb)
{
 ((struct Model1_skb_shared_info *)(Model1_skb_end_pointer(Model1_skb)))->Model1_frag_list = ((void *)0);
}





int Model1___skb_wait_for_more_packets(struct Model1_sock *Model1_sk, int *err, long *Model1_timeo_p,
    const struct Model1_sk_buff *Model1_skb);
struct Model1_sk_buff *Model1___skb_try_recv_datagram(struct Model1_sock *Model1_sk, unsigned Model1_flags,
     int *Model1_peeked, int *Model1_off, int *err,
     struct Model1_sk_buff **Model1_last);
struct Model1_sk_buff *Model1___skb_recv_datagram(struct Model1_sock *Model1_sk, unsigned Model1_flags,
        int *Model1_peeked, int *Model1_off, int *err);
struct Model1_sk_buff *Model1_skb_recv_datagram(struct Model1_sock *Model1_sk, unsigned Model1_flags, int Model1_noblock,
      int *err);
unsigned int Model1_datagram_poll(struct Model1_file *Model1_file, struct Model1_socket *Model1_sock,
      struct Model1_poll_table_struct *Model1_wait);
int Model1_skb_copy_datagram_iter(const struct Model1_sk_buff *Model1_from, int Model1_offset,
      struct Model1_iov_iter *Model1_to, int Model1_size);
static inline __attribute__((no_instrument_function)) int Model1_skb_copy_datagram_msg(const struct Model1_sk_buff *Model1_from, int Model1_offset,
     struct Model1_msghdr *Model1_msg, int Model1_size)
{
 return Model1_skb_copy_datagram_iter(Model1_from, Model1_offset, &Model1_msg->Model1_msg_iter, Model1_size);
}
int Model1_skb_copy_and_csum_datagram_msg(struct Model1_sk_buff *Model1_skb, int Model1_hlen,
       struct Model1_msghdr *Model1_msg);
int Model1_skb_copy_datagram_from_iter(struct Model1_sk_buff *Model1_skb, int Model1_offset,
     struct Model1_iov_iter *Model1_from, int Model1_len);
int Model1_zerocopy_sg_from_iter(struct Model1_sk_buff *Model1_skb, struct Model1_iov_iter *Model1_frm);
void Model1_skb_free_datagram(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);
void Model1___skb_free_datagram_locked(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb, int Model1_len);
static inline __attribute__((no_instrument_function)) void Model1_skb_free_datagram_locked(struct Model1_sock *Model1_sk,
         struct Model1_sk_buff *Model1_skb)
{
 Model1___skb_free_datagram_locked(Model1_sk, Model1_skb, 0);
}
int Model1_skb_kill_datagram(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb, unsigned int Model1_flags);
int Model1_skb_copy_bits(const struct Model1_sk_buff *Model1_skb, int Model1_offset, void *Model1_to, int Model1_len);
int Model1_skb_store_bits(struct Model1_sk_buff *Model1_skb, int Model1_offset, const void *Model1_from, int Model1_len);
Model1___wsum Model1_skb_copy_and_csum_bits(const struct Model1_sk_buff *Model1_skb, int Model1_offset, Model1_u8 *Model1_to,
         int Model1_len, Model1___wsum Model1_csum);
Model1_ssize_t Model1_skb_socket_splice(struct Model1_sock *Model1_sk,
     struct Model1_pipe_inode_info *Model1_pipe,
     struct Model1_splice_pipe_desc *Model1_spd);
int Model1_skb_splice_bits(struct Model1_sk_buff *Model1_skb, struct Model1_sock *Model1_sk, unsigned int Model1_offset,
      struct Model1_pipe_inode_info *Model1_pipe, unsigned int Model1_len,
      unsigned int Model1_flags,
      Model1_ssize_t (*Model1_splice_cb)(struct Model1_sock *,
      struct Model1_pipe_inode_info *,
      struct Model1_splice_pipe_desc *));
void Model1_skb_copy_and_csum_dev(const struct Model1_sk_buff *Model1_skb, Model1_u8 *Model1_to);
unsigned int Model1_skb_zerocopy_headlen(const struct Model1_sk_buff *Model1_from);
int Model1_skb_zerocopy(struct Model1_sk_buff *Model1_to, struct Model1_sk_buff *Model1_from,
   int Model1_len, int Model1_hlen);
void Model1_skb_split(struct Model1_sk_buff *Model1_skb, struct Model1_sk_buff *Model1_skb1, const Model1_u32 Model1_len);
int Model1_skb_shift(struct Model1_sk_buff *Model1_tgt, struct Model1_sk_buff *Model1_skb, int Model1_shiftlen);
void Model1_skb_scrub_packet(struct Model1_sk_buff *Model1_skb, bool Model1_xnet);
unsigned int Model1_skb_gso_transport_seglen(const struct Model1_sk_buff *Model1_skb);
bool Model1_skb_gso_validate_mtu(const struct Model1_sk_buff *Model1_skb, unsigned int Model1_mtu);
struct Model1_sk_buff *Model1_skb_segment(struct Model1_sk_buff *Model1_skb, Model1_netdev_features_t Model1_features);
struct Model1_sk_buff *Model1_skb_vlan_untag(struct Model1_sk_buff *Model1_skb);
int Model1_skb_ensure_writable(struct Model1_sk_buff *Model1_skb, int Model1_write_len);
int Model1_skb_vlan_pop(struct Model1_sk_buff *Model1_skb);
int Model1_skb_vlan_push(struct Model1_sk_buff *Model1_skb, Model1___be16 Model1_vlan_proto, Model1_u16 Model1_vlan_tci);
struct Model1_sk_buff *Model1_pskb_extract(struct Model1_sk_buff *Model1_skb, int Model1_off, int Model1_to_copy,
        Model1_gfp_t Model1_gfp);

static inline __attribute__((no_instrument_function)) int Model1_memcpy_from_msg(void *Model1_data, struct Model1_msghdr *Model1_msg, int Model1_len)
{
 return Model1_copy_from_iter(Model1_data, Model1_len, &Model1_msg->Model1_msg_iter) == Model1_len ? 0 : -14;
}

static inline __attribute__((no_instrument_function)) int Model1_memcpy_to_msg(struct Model1_msghdr *Model1_msg, void *Model1_data, int Model1_len)
{
 return Model1_copy_to_iter(Model1_data, Model1_len, &Model1_msg->Model1_msg_iter) == Model1_len ? 0 : -14;
}

struct Model1_skb_checksum_ops {
 Model1___wsum (*Model1_update)(const void *Model1_mem, int Model1_len, Model1___wsum Model1_wsum);
 Model1___wsum (*Model1_combine)(Model1___wsum Model1_csum, Model1___wsum Model1_csum2, int Model1_offset, int Model1_len);
};

Model1___wsum Model1___skb_checksum(const struct Model1_sk_buff *Model1_skb, int Model1_offset, int Model1_len,
        Model1___wsum Model1_csum, const struct Model1_skb_checksum_ops *Model1_ops);
Model1___wsum Model1_skb_checksum(const struct Model1_sk_buff *Model1_skb, int Model1_offset, int Model1_len,
      Model1___wsum Model1_csum);

static inline __attribute__((no_instrument_function)) void * __attribute__((warn_unused_result))
Model1___skb_header_pointer(const struct Model1_sk_buff *Model1_skb, int Model1_offset,
       int Model1_len, void *Model1_data, int Model1_hlen, void *Model1_buffer)
{
 if (Model1_hlen - Model1_offset >= Model1_len)
  return Model1_data + Model1_offset;

 if (!Model1_skb ||
     Model1_skb_copy_bits(Model1_skb, Model1_offset, Model1_buffer, Model1_len) < 0)
  return ((void *)0);

 return Model1_buffer;
}

static inline __attribute__((no_instrument_function)) void * __attribute__((warn_unused_result))
Model1_skb_header_pointer(const struct Model1_sk_buff *Model1_skb, int Model1_offset, int Model1_len, void *Model1_buffer)
{
 return Model1___skb_header_pointer(Model1_skb, Model1_offset, Model1_len, Model1_skb->Model1_data,
        Model1_skb_headlen(Model1_skb), Model1_buffer);
}

/**
 *	skb_needs_linearize - check if we need to linearize a given skb
 *			      depending on the given device features.
 *	@skb: socket buffer to check
 *	@features: net device features
 *
 *	Returns true if either:
 *	1. skb has frag_list and the device doesn't support FRAGLIST, or
 *	2. skb is fragmented and the device does not support SG.
 */
static inline __attribute__((no_instrument_function)) bool Model1_skb_needs_linearize(struct Model1_sk_buff *Model1_skb,
           Model1_netdev_features_t Model1_features)
{
 return Model1_skb_is_nonlinear(Model1_skb) &&
        ((Model1_skb_has_frag_list(Model1_skb) && !(Model1_features & ((Model1_netdev_features_t)1 << (Model1_NETIF_F_FRAGLIST_BIT)))) ||
  (((struct Model1_skb_shared_info *)(Model1_skb_end_pointer(Model1_skb)))->Model1_nr_frags && !(Model1_features & ((Model1_netdev_features_t)1 << (Model1_NETIF_F_SG_BIT)))));
}

static inline __attribute__((no_instrument_function)) void Model1_skb_copy_from_linear_data(const struct Model1_sk_buff *Model1_skb,
          void *Model1_to,
          const unsigned int Model1_len)
{
 ({ Model1_size_t Model1___len = (Model1_len); void *Model1___ret; if (__builtin_constant_p(Model1_len) && Model1___len >= 64) Model1___ret = Model1___memcpy((Model1_to), (Model1_skb->Model1_data), Model1___len); else Model1___ret = __builtin_memcpy((Model1_to), (Model1_skb->Model1_data), Model1___len); Model1___ret; });
}

static inline __attribute__((no_instrument_function)) void Model1_skb_copy_from_linear_data_offset(const struct Model1_sk_buff *Model1_skb,
          const int Model1_offset, void *Model1_to,
          const unsigned int Model1_len)
{
 ({ Model1_size_t Model1___len = (Model1_len); void *Model1___ret; if (__builtin_constant_p(Model1_len) && Model1___len >= 64) Model1___ret = Model1___memcpy((Model1_to), (Model1_skb->Model1_data + Model1_offset), Model1___len); else Model1___ret = __builtin_memcpy((Model1_to), (Model1_skb->Model1_data + Model1_offset), Model1___len); Model1___ret; });
}

static inline __attribute__((no_instrument_function)) void Model1_skb_copy_to_linear_data(struct Model1_sk_buff *Model1_skb,
        const void *Model1_from,
        const unsigned int Model1_len)
{
 ({ Model1_size_t Model1___len = (Model1_len); void *Model1___ret; if (__builtin_constant_p(Model1_len) && Model1___len >= 64) Model1___ret = Model1___memcpy((Model1_skb->Model1_data), (Model1_from), Model1___len); else Model1___ret = __builtin_memcpy((Model1_skb->Model1_data), (Model1_from), Model1___len); Model1___ret; });
}

static inline __attribute__((no_instrument_function)) void Model1_skb_copy_to_linear_data_offset(struct Model1_sk_buff *Model1_skb,
        const int Model1_offset,
        const void *Model1_from,
        const unsigned int Model1_len)
{
 ({ Model1_size_t Model1___len = (Model1_len); void *Model1___ret; if (__builtin_constant_p(Model1_len) && Model1___len >= 64) Model1___ret = Model1___memcpy((Model1_skb->Model1_data + Model1_offset), (Model1_from), Model1___len); else Model1___ret = __builtin_memcpy((Model1_skb->Model1_data + Model1_offset), (Model1_from), Model1___len); Model1___ret; });
}

void Model1_skb_init(void);

static inline __attribute__((no_instrument_function)) Model1_ktime_t Model1_skb_get_ktime(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb->Model1_tstamp;
}

/**
 *	skb_get_timestamp - get timestamp from a skb
 *	@skb: skb to get stamp from
 *	@stamp: pointer to struct timeval to store stamp in
 *
 *	Timestamps are stored in the skb as offsets to a base timestamp.
 *	This function converts the offset back to a struct timeval and stores
 *	it in stamp.
 */
static inline __attribute__((no_instrument_function)) void Model1_skb_get_timestamp(const struct Model1_sk_buff *Model1_skb,
         struct Model1_timeval *Model1_stamp)
{
 *Model1_stamp = Model1_ns_to_timeval((Model1_skb->Model1_tstamp).Model1_tv64);
}

static inline __attribute__((no_instrument_function)) void Model1_skb_get_timestampns(const struct Model1_sk_buff *Model1_skb,
           struct Model1_timespec *Model1_stamp)
{
 *Model1_stamp = Model1_ns_to_timespec((Model1_skb->Model1_tstamp).Model1_tv64);
}

static inline __attribute__((no_instrument_function)) void Model1___net_timestamp(struct Model1_sk_buff *Model1_skb)
{
 Model1_skb->Model1_tstamp = Model1_ktime_get_real();
}

static inline __attribute__((no_instrument_function)) Model1_ktime_t Model1_net_timedelta(Model1_ktime_t Model1_t)
{
 return ({ (Model1_ktime_t){ .Model1_tv64 = (Model1_ktime_get_real()).Model1_tv64 - (Model1_t).Model1_tv64 }; });
}

static inline __attribute__((no_instrument_function)) Model1_ktime_t Model1_net_invalid_timestamp(void)
{
 return Model1_ktime_set(0, 0);
}

struct Model1_sk_buff *Model1_skb_clone_sk(struct Model1_sk_buff *Model1_skb);
static inline __attribute__((no_instrument_function)) void Model1_skb_clone_tx_timestamp(struct Model1_sk_buff *Model1_skb)
{
}

static inline __attribute__((no_instrument_function)) bool Model1_skb_defer_rx_timestamp(struct Model1_sk_buff *Model1_skb)
{
 return false;
}



/**
 * skb_complete_tx_timestamp() - deliver cloned skb with tx timestamps
 *
 * PHY drivers may accept clones of transmitted packets for
 * timestamping via their phy_driver.txtstamp method. These drivers
 * must call this function to return the skb back to the stack with a
 * timestamp.
 *
 * @skb: clone of the the original outgoing packet
 * @hwtstamps: hardware time stamps
 *
 */
void Model1_skb_complete_tx_timestamp(struct Model1_sk_buff *Model1_skb,
          struct Model1_skb_shared_hwtstamps *Model1_hwtstamps);

void Model1___skb_tstamp_tx(struct Model1_sk_buff *Model1_orig_skb,
       struct Model1_skb_shared_hwtstamps *Model1_hwtstamps,
       struct Model1_sock *Model1_sk, int Model1_tstype);

/**
 * skb_tstamp_tx - queue clone of skb with send time stamps
 * @orig_skb:	the original outgoing packet
 * @hwtstamps:	hardware time stamps, may be NULL if not available
 *
 * If the skb has a socket associated, then this function clones the
 * skb (thus sharing the actual data and optional structures), stores
 * the optional hardware time stamping information (if non NULL) or
 * generates a software time stamp (otherwise), then queues the clone
 * to the error queue of the socket.  Errors are silently ignored.
 */
void Model1_skb_tstamp_tx(struct Model1_sk_buff *Model1_orig_skb,
     struct Model1_skb_shared_hwtstamps *Model1_hwtstamps);

static inline __attribute__((no_instrument_function)) void Model1_sw_tx_timestamp(struct Model1_sk_buff *Model1_skb)
{
 if (((struct Model1_skb_shared_info *)(Model1_skb_end_pointer(Model1_skb)))->Model1_tx_flags & Model1_SKBTX_SW_TSTAMP &&
     !(((struct Model1_skb_shared_info *)(Model1_skb_end_pointer(Model1_skb)))->Model1_tx_flags & Model1_SKBTX_IN_PROGRESS))
  Model1_skb_tstamp_tx(Model1_skb, ((void *)0));
}

/**
 * skb_tx_timestamp() - Driver hook for transmit timestamping
 *
 * Ethernet MAC Drivers should call this function in their hard_xmit()
 * function immediately before giving the sk_buff to the MAC hardware.
 *
 * Specifically, one should make absolutely sure that this function is
 * called before TX completion of this packet can trigger.  Otherwise
 * the packet could potentially already be freed.
 *
 * @skb: A socket buffer.
 */
static inline __attribute__((no_instrument_function)) void Model1_skb_tx_timestamp(struct Model1_sk_buff *Model1_skb)
{
 Model1_skb_clone_tx_timestamp(Model1_skb);
 Model1_sw_tx_timestamp(Model1_skb);
}

/**
 * skb_complete_wifi_ack - deliver skb with wifi status
 *
 * @skb: the original outgoing packet
 * @acked: ack status
 *
 */
void Model1_skb_complete_wifi_ack(struct Model1_sk_buff *Model1_skb, bool Model1_acked);

Model1___sum16 Model1___skb_checksum_complete_head(struct Model1_sk_buff *Model1_skb, int Model1_len);
Model1___sum16 Model1___skb_checksum_complete(struct Model1_sk_buff *Model1_skb);

static inline __attribute__((no_instrument_function)) int Model1_skb_csum_unnecessary(const struct Model1_sk_buff *Model1_skb)
{
#if CY_ABSTRACT1
     return 1; //csum is always unnecessary
#else
 return ((Model1_skb->Model1_ip_summed == 1) ||
  Model1_skb->Model1_csum_valid ||
  (Model1_skb->Model1_ip_summed == 3 &&
   Model1_skb_checksum_start_offset(Model1_skb) >= 0));
#endif
}

/**
 *	skb_checksum_complete - Calculate checksum of an entire packet
 *	@skb: packet to process
 *
 *	This function calculates the checksum over the entire packet plus
 *	the value of skb->csum.  The latter can be used to supply the
 *	checksum of a pseudo header as used by TCP/UDP.  It returns the
 *	checksum.
 *
 *	For protocols that contain complete checksums such as ICMP/TCP/UDP,
 *	this function can be used to verify that checksum on received
 *	packets.  In that case the function should return zero if the
 *	checksum is correct.  In particular, this function will return zero
 *	if skb->ip_summed is CHECKSUM_UNNECESSARY which indicates that the
 *	hardware has already verified the correctness of the checksum.
 */
static inline __attribute__((no_instrument_function)) Model1___sum16 Model1_skb_checksum_complete(struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb_csum_unnecessary(Model1_skb) ?
        0 : Model1___skb_checksum_complete(Model1_skb);
}

static inline __attribute__((no_instrument_function)) void Model1___skb_decr_checksum_unnecessary(struct Model1_sk_buff *Model1_skb)
{
 if (Model1_skb->Model1_ip_summed == 1) {
  if (Model1_skb->Model1_csum_level == 0)
   Model1_skb->Model1_ip_summed = 0;
  else
   Model1_skb->Model1_csum_level--;
 }
}

static inline __attribute__((no_instrument_function)) void Model1___skb_incr_checksum_unnecessary(struct Model1_sk_buff *Model1_skb)
{
 if (Model1_skb->Model1_ip_summed == 1) {
  if (Model1_skb->Model1_csum_level < 3)
   Model1_skb->Model1_csum_level++;
 } else if (Model1_skb->Model1_ip_summed == 0) {
  Model1_skb->Model1_ip_summed = 1;
  Model1_skb->Model1_csum_level = 0;
 }
}

static inline __attribute__((no_instrument_function)) void Model1___skb_mark_checksum_bad(struct Model1_sk_buff *Model1_skb)
{
 /* Mark current checksum as bad (typically called from GRO
	 * path). In the case that ip_summed is CHECKSUM_NONE
	 * this must be the first checksum encountered in the packet.
	 * When ip_summed is CHECKSUM_UNNECESSARY, this is the first
	 * checksum after the last one validated. For UDP, a zero
	 * checksum can not be marked as bad.
	 */

 if (Model1_skb->Model1_ip_summed == 0 ||
     Model1_skb->Model1_ip_summed == 1)
  Model1_skb->Model1_csum_bad = 1;
}

/* Check if we need to perform checksum complete validation.
 *
 * Returns true if checksum complete is needed, false otherwise
 * (either checksum is unnecessary or zero checksum is allowed).
 */
static inline __attribute__((no_instrument_function)) bool Model1___skb_checksum_validate_needed(struct Model1_sk_buff *Model1_skb,
        bool Model1_zero_okay,
        Model1___sum16 Model1_check)
{
 if (Model1_skb_csum_unnecessary(Model1_skb) || (Model1_zero_okay && !Model1_check)) {
  Model1_skb->Model1_csum_valid = 1;
  Model1___skb_decr_checksum_unnecessary(Model1_skb);
  return false;
 }

 return true;
}

/* For small packets <= CHECKSUM_BREAK peform checksum complete directly
 * in checksum_init.
 */


/* Unset checksum-complete
 *
 * Unset checksum complete can be done when packet is being modified
 * (uncompressed for instance) and checksum-complete value is
 * invalidated.
 */
static inline __attribute__((no_instrument_function)) void Model1_skb_checksum_complete_unset(struct Model1_sk_buff *Model1_skb)
{
 if (Model1_skb->Model1_ip_summed == 2)
  Model1_skb->Model1_ip_summed = 0;
}

/* Validate (init) checksum based on checksum complete.
 *
 * Return values:
 *   0: checksum is validated or try to in skb_checksum_complete. In the latter
 *	case the ip_summed will not be CHECKSUM_UNNECESSARY and the pseudo
 *	checksum is stored in skb->csum for use in __skb_checksum_complete
 *   non-zero: value of invalid checksum
 *
 */
static inline __attribute__((no_instrument_function)) Model1___sum16 Model1___skb_checksum_validate_complete(struct Model1_sk_buff *Model1_skb,
             bool Model1_complete,
             Model1___wsum Model1_psum)
{
 if (Model1_skb->Model1_ip_summed == 2) {
  if (!Model1_csum_fold(Model1_csum_add(Model1_psum, Model1_skb->Model1_csum))) {
   Model1_skb->Model1_csum_valid = 1;
   return 0;
  }
 } else if (Model1_skb->Model1_csum_bad) {
  /* ip_summed == CHECKSUM_NONE in this case */
  return ( Model1___sum16)1;
 }

 Model1_skb->Model1_csum = Model1_psum;

 if (Model1_complete || Model1_skb->Model1_len <= 76) {
  Model1___sum16 Model1_csum;

  Model1_csum = Model1___skb_checksum_complete(Model1_skb);
  Model1_skb->Model1_csum_valid = !Model1_csum;
  return Model1_csum;
 }

 return 0;
}

static inline __attribute__((no_instrument_function)) Model1___wsum Model1_null_compute_pseudo(struct Model1_sk_buff *Model1_skb, int Model1_proto)
{
 return 0;
}

/* Perform checksum validate (init). Note that this is a macro since we only
 * want to calculate the pseudo header which is an input function if necessary.
 * First we try to validate without any computation (checksum unnecessary) and
 * then calculate based on checksum complete calling the function to compute
 * pseudo header.
 *
 * Return values:
 *   0: checksum is validated or try to in skb_checksum_complete
 *   non-zero: value of invalid checksum
 */
static inline __attribute__((no_instrument_function)) bool Model1___skb_checksum_convert_check(struct Model1_sk_buff *Model1_skb)
{
 return (Model1_skb->Model1_ip_summed == 0 &&
  Model1_skb->Model1_csum_valid && !Model1_skb->Model1_csum_bad);
}

static inline __attribute__((no_instrument_function)) void Model1___skb_checksum_convert(struct Model1_sk_buff *Model1_skb,
       Model1___sum16 Model1_check, Model1___wsum Model1_pseudo)
{
 Model1_skb->Model1_csum = ~Model1_pseudo;
 Model1_skb->Model1_ip_summed = 2;
}
static inline __attribute__((no_instrument_function)) void Model1_skb_remcsum_adjust_partial(struct Model1_sk_buff *Model1_skb, void *Model1_ptr,
           Model1_u16 Model1_start, Model1_u16 Model1_offset)
{
 Model1_skb->Model1_ip_summed = 3;
 Model1_skb->Model1_csum_start = ((unsigned char *)Model1_ptr + Model1_start) - Model1_skb->Model1_head;
 Model1_skb->Model1_csum_offset = Model1_offset - Model1_start;
}

/* Update skbuf and packet to reflect the remote checksum offload operation.
 * When called, ptr indicates the starting point for skb->csum when
 * ip_summed is CHECKSUM_COMPLETE. If we need create checksum complete
 * here, skb_postpull_rcsum is done so skb->csum start is ptr.
 */
static inline __attribute__((no_instrument_function)) void Model1_skb_remcsum_process(struct Model1_sk_buff *Model1_skb, void *Model1_ptr,
           int Model1_start, int Model1_offset, bool Model1_nopartial)
{
 Model1___wsum Model1_delta;

 if (!Model1_nopartial) {
  Model1_skb_remcsum_adjust_partial(Model1_skb, Model1_ptr, Model1_start, Model1_offset);
  return;
 }

  if (__builtin_expect(!!(Model1_skb->Model1_ip_summed != 2), 0)) {
  Model1___skb_checksum_complete(Model1_skb);
  Model1_skb_postpull_rcsum(Model1_skb, Model1_skb->Model1_data, Model1_ptr - (void *)Model1_skb->Model1_data);
 }

 Model1_delta = Model1_remcsum_adjust(Model1_ptr, Model1_skb->Model1_csum, Model1_start, Model1_offset);

 /* Adjust skb->csum since we changed the packet */
 Model1_skb->Model1_csum = Model1_csum_add(Model1_skb->Model1_csum, Model1_delta);
}


void Model1_nf_conntrack_destroy(struct Model1_nf_conntrack *Model1_nfct);
static inline __attribute__((no_instrument_function)) void Model1_nf_conntrack_put(struct Model1_nf_conntrack *Model1_nfct)
{
 if (Model1_nfct && Model1_atomic_dec_and_test(&Model1_nfct->Model1_use))
  Model1_nf_conntrack_destroy(Model1_nfct);
}
static inline __attribute__((no_instrument_function)) void Model1_nf_conntrack_get(struct Model1_nf_conntrack *Model1_nfct)
{
 if (Model1_nfct)
  Model1_atomic_inc(&Model1_nfct->Model1_use);
}
static inline __attribute__((no_instrument_function)) void Model1_nf_reset(struct Model1_sk_buff *Model1_skb)
{

 Model1_nf_conntrack_put(Model1_skb->Model1_nfct);
 Model1_skb->Model1_nfct = ((void *)0);





}

static inline __attribute__((no_instrument_function)) void Model1_nf_reset_trace(struct Model1_sk_buff *Model1_skb)
{



}

/* Note: This doesn't put any conntrack and bridge info in dst. */
static inline __attribute__((no_instrument_function)) void Model1___nf_copy(struct Model1_sk_buff *Model1_dst, const struct Model1_sk_buff *Model1_src,
        bool Model1_copy)
{

 Model1_dst->Model1_nfct = Model1_src->Model1_nfct;
 Model1_nf_conntrack_get(Model1_src->Model1_nfct);
 if (Model1_copy)
  Model1_dst->Model1_nfctinfo = Model1_src->Model1_nfctinfo;
}

static inline __attribute__((no_instrument_function)) void Model1_nf_copy(struct Model1_sk_buff *Model1_dst, const struct Model1_sk_buff *Model1_src)
{

 Model1_nf_conntrack_put(Model1_dst->Model1_nfct);




 Model1___nf_copy(Model1_dst, Model1_src, true);
}


static inline __attribute__((no_instrument_function)) void Model1_skb_copy_secmark(struct Model1_sk_buff *Model1_to, const struct Model1_sk_buff *Model1_from)
{
 Model1_to->Model1_secmark = Model1_from->Model1_secmark;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_init_secmark(struct Model1_sk_buff *Model1_skb)
{
 Model1_skb->Model1_secmark = 0;
}
static inline __attribute__((no_instrument_function)) bool Model1_skb_irq_freeable(const struct Model1_sk_buff *Model1_skb)
{
 return !Model1_skb->Model1_destructor &&

  !Model1_skb->Model1_sp &&


  !Model1_skb->Model1_nfct &&

  !Model1_skb->Model1__skb_refdst &&
  !Model1_skb_has_frag_list(Model1_skb);
}

static inline __attribute__((no_instrument_function)) void Model1_skb_set_queue_mapping(struct Model1_sk_buff *Model1_skb, Model1_u16 Model1_queue_mapping)
{
 Model1_skb->Model1_queue_mapping = Model1_queue_mapping;
}

static inline __attribute__((no_instrument_function)) Model1_u16 Model1_skb_get_queue_mapping(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb->Model1_queue_mapping;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_copy_queue_mapping(struct Model1_sk_buff *Model1_to, const struct Model1_sk_buff *Model1_from)
{
 Model1_to->Model1_queue_mapping = Model1_from->Model1_queue_mapping;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_record_rx_queue(struct Model1_sk_buff *Model1_skb, Model1_u16 Model1_rx_queue)
{
 Model1_skb->Model1_queue_mapping = Model1_rx_queue + 1;
}

static inline __attribute__((no_instrument_function)) Model1_u16 Model1_skb_get_rx_queue(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb->Model1_queue_mapping - 1;
}

static inline __attribute__((no_instrument_function)) bool Model1_skb_rx_queue_recorded(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb->Model1_queue_mapping != 0;
}

static inline __attribute__((no_instrument_function)) struct Model1_sec_path *Model1_skb_sec_path(struct Model1_sk_buff *Model1_skb)
{

 return Model1_skb->Model1_sp;



}

/* Keeps track of mac header offset relative to skb->head.
 * It is useful for TSO of Tunneling protocol. e.g. GRE.
 * For non-tunnel skb it points to skb_mac_header() and for
 * tunnel skb it points to outer mac header.
 * Keeps track of level of encapsulation of network headers.
 */
struct Model1_skb_gso_cb {
 union {
  int Model1_mac_offset;
  int Model1_data_offset;
 };
 int Model1_encap_level;
 Model1___wsum Model1_csum;
 Model1___u16 Model1_csum_start;
};



static inline __attribute__((no_instrument_function)) int Model1_skb_tnl_header_len(const struct Model1_sk_buff *Model1_inner_skb)
{
 return (Model1_skb_mac_header(Model1_inner_skb) - Model1_inner_skb->Model1_head) -
  ((struct Model1_skb_gso_cb *)((Model1_inner_skb)->Model1_cb + 32))->Model1_mac_offset;
}

static inline __attribute__((no_instrument_function)) int Model1_gso_pskb_expand_head(struct Model1_sk_buff *Model1_skb, int Model1_extra)
{
 int Model1_new_headroom, Model1_headroom;
 int Model1_ret;

 Model1_headroom = Model1_skb_headroom(Model1_skb);
 Model1_ret = Model1_pskb_expand_head(Model1_skb, Model1_extra, 0, ((( Model1_gfp_t)0x20u)|(( Model1_gfp_t)0x80000u)|(( Model1_gfp_t)0x2000000u)));
 if (Model1_ret)
  return Model1_ret;

 Model1_new_headroom = Model1_skb_headroom(Model1_skb);
 ((struct Model1_skb_gso_cb *)((Model1_skb)->Model1_cb + 32))->Model1_mac_offset += (Model1_new_headroom - Model1_headroom);
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model1_gso_reset_checksum(struct Model1_sk_buff *Model1_skb, Model1___wsum Model1_res)
{
 /* Do not update partial checksums if remote checksum is enabled. */
 if (Model1_skb->Model1_remcsum_offload)
  return;

 ((struct Model1_skb_gso_cb *)((Model1_skb)->Model1_cb + 32))->Model1_csum = Model1_res;
 ((struct Model1_skb_gso_cb *)((Model1_skb)->Model1_cb + 32))->Model1_csum_start = Model1_skb_checksum_start(Model1_skb) - Model1_skb->Model1_head;
}

/* Compute the checksum for a gso segment. First compute the checksum value
 * from the start of transport header to SKB_GSO_CB(skb)->csum_start, and
 * then add in skb->csum (checksum from csum_start to end of packet).
 * skb->csum and csum_start are then updated to reflect the checksum of the
 * resultant packet starting from the transport header-- the resultant checksum
 * is in the res argument (i.e. normally zero or ~ of checksum of a pseudo
 * header.
 */
static inline __attribute__((no_instrument_function)) Model1___sum16 Model1_gso_make_checksum(struct Model1_sk_buff *Model1_skb, Model1___wsum Model1_res)
{
 unsigned char *Model1_csum_start = Model1_skb_transport_header(Model1_skb);
 int Model1_plen = (Model1_skb->Model1_head + ((struct Model1_skb_gso_cb *)((Model1_skb)->Model1_cb + 32))->Model1_csum_start) - Model1_csum_start;
 Model1___wsum Model1_partial = ((struct Model1_skb_gso_cb *)((Model1_skb)->Model1_cb + 32))->Model1_csum;

 ((struct Model1_skb_gso_cb *)((Model1_skb)->Model1_cb + 32))->Model1_csum = Model1_res;
 ((struct Model1_skb_gso_cb *)((Model1_skb)->Model1_cb + 32))->Model1_csum_start = Model1_csum_start - Model1_skb->Model1_head;

 return Model1_csum_fold(Model1_csum_partial(Model1_csum_start, Model1_plen, Model1_partial));
}

static inline __attribute__((no_instrument_function)) bool Model1_skb_is_gso(const struct Model1_sk_buff *Model1_skb)
{
 return ((struct Model1_skb_shared_info *)(Model1_skb_end_pointer(Model1_skb)))->Model1_gso_size;
}

/* Note: Should be called only if skb_is_gso(skb) is true */
static inline __attribute__((no_instrument_function)) bool Model1_skb_is_gso_v6(const struct Model1_sk_buff *Model1_skb)
{
 return ((struct Model1_skb_shared_info *)(Model1_skb_end_pointer(Model1_skb)))->Model1_gso_type & Model1_SKB_GSO_TCPV6;
}

void Model1___skb_warn_lro_forwarding(const struct Model1_sk_buff *Model1_skb);

static inline __attribute__((no_instrument_function)) bool Model1_skb_warn_if_lro(const struct Model1_sk_buff *Model1_skb)
{
 /* LRO sets gso_size but not gso_type, whereas if GSO is really
	 * wanted then gso_type will be set. */
 const struct Model1_skb_shared_info *Model1_shinfo = ((struct Model1_skb_shared_info *)(Model1_skb_end_pointer(Model1_skb)));

 if (Model1_skb_is_nonlinear(Model1_skb) && Model1_shinfo->Model1_gso_size != 0 &&
     __builtin_expect(!!(Model1_shinfo->Model1_gso_type == 0), 0)) {
  Model1___skb_warn_lro_forwarding(Model1_skb);
  return true;
 }
 return false;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_forward_csum(struct Model1_sk_buff *Model1_skb)
{
 /* Unfortunately we don't support this one.  Any brave souls? */
 if (Model1_skb->Model1_ip_summed == 2)
  Model1_skb->Model1_ip_summed = 0;
}

/**
 * skb_checksum_none_assert - make sure skb ip_summed is CHECKSUM_NONE
 * @skb: skb to check
 *
 * fresh skbs have their ip_summed set to CHECKSUM_NONE.
 * Instead of forcing ip_summed to CHECKSUM_NONE, we can
 * use this helper, to document places where we make this assertion.
 */
static inline __attribute__((no_instrument_function)) void Model1_skb_checksum_none_assert(const struct Model1_sk_buff *Model1_skb)
{



}

bool Model1_skb_partial_csum_set(struct Model1_sk_buff *Model1_skb, Model1_u16 Model1_start, Model1_u16 Model1_off);

int Model1_skb_checksum_setup(struct Model1_sk_buff *Model1_skb, bool Model1_recalculate);
struct Model1_sk_buff *Model1_skb_checksum_trimmed(struct Model1_sk_buff *Model1_skb,
         unsigned int Model1_transport_len,
         Model1___sum16(*Model1_skb_chkf)(struct Model1_sk_buff *Model1_skb));

/**
 * skb_head_is_locked - Determine if the skb->head is locked down
 * @skb: skb to check
 *
 * The head on skbs build around a head frag can be removed if they are
 * not cloned.  This function returns true if the skb head is locked down
 * due to either being allocated via kmalloc, or by being a clone with
 * multiple references to the head.
 */
static inline __attribute__((no_instrument_function)) bool Model1_skb_head_is_locked(const struct Model1_sk_buff *Model1_skb)
{
 return !Model1_skb->Model1_head_frag || Model1_skb_cloned(Model1_skb);
}

/**
 * skb_gso_network_seglen - Return length of individual segments of a gso packet
 *
 * @skb: GSO skb
 *
 * skb_gso_network_seglen is used to determine the real size of the
 * individual segments, including Layer3 (IP, IPv6) and L4 headers (TCP/UDP).
 *
 * The MAC/L2 header is not accounted for.
 */
static inline __attribute__((no_instrument_function)) unsigned int Model1_skb_gso_network_seglen(const struct Model1_sk_buff *Model1_skb)
{
 unsigned int Model1_hdr_len = Model1_skb_transport_header(Model1_skb) -
          Model1_skb_network_header(Model1_skb);
 return Model1_hdr_len + Model1_skb_gso_transport_seglen(Model1_skb);
}

/* Local Checksum Offload.
 * Compute outer checksum based on the assumption that the
 * inner checksum will be offloaded later.
 * See Documentation/networking/checksum-offloads.txt for
 * explanation of how this works.
 * Fill in outer checksum adjustment (e.g. with sum of outer
 * pseudo-header) before calling.
 * Also ensure that inner checksum is in linear data area.
 */
static inline __attribute__((no_instrument_function)) Model1___wsum Model1_lco_csum(struct Model1_sk_buff *Model1_skb)
{
 unsigned char *Model1_csum_start = Model1_skb_checksum_start(Model1_skb);
 unsigned char *Model1_l4_hdr = Model1_skb_transport_header(Model1_skb);
 Model1___wsum Model1_partial;

 /* Start with complement of inner checksum adjustment */
 Model1_partial = ~Model1_csum_unfold(*( Model1___sum16 *)(Model1_csum_start +
          Model1_skb->Model1_csum_offset));

 /* Add in checksum of our headers (incl. outer checksum
	 * adjustment filled in by caller) and return result.
	 */
 return Model1_csum_partial(Model1_l4_hdr, Model1_csum_start - Model1_l4_hdr, Model1_partial);
}


static inline __attribute__((no_instrument_function)) struct Model1_ethhdr *Model1_eth_hdr(const struct Model1_sk_buff *Model1_skb)
{
 return (struct Model1_ethhdr *)Model1_skb_mac_header(Model1_skb);
}

static inline __attribute__((no_instrument_function)) struct Model1_ethhdr *Model1_inner_eth_hdr(const struct Model1_sk_buff *Model1_skb)
{
 return (struct Model1_ethhdr *)Model1_skb_inner_mac_header(Model1_skb);
}

int Model1_eth_header_parse(const struct Model1_sk_buff *Model1_skb, unsigned char *Model1_haddr);

extern Model1_ssize_t Model1_sysfs_format_mac(char *Model1_buf, const unsigned char *Model1_addr, int Model1_len);





/* All structures exposed to userland should be defined such that they
 * have the same layout for 32-bit and 64-bit userland.
 */

/**
 * struct ethtool_cmd - DEPRECATED, link control and status
 * This structure is DEPRECATED, please use struct ethtool_link_settings.
 * @cmd: Command number = %ETHTOOL_GSET or %ETHTOOL_SSET
 * @supported: Bitmask of %SUPPORTED_* flags for the link modes,
 *	physical connectors and other link features for which the
 *	interface supports autonegotiation or auto-detection.
 *	Read-only.
 * @advertising: Bitmask of %ADVERTISED_* flags for the link modes,
 *	physical connectors and other link features that are
 *	advertised through autonegotiation or enabled for
 *	auto-detection.
 * @speed: Low bits of the speed, 1Mb units, 0 to INT_MAX or SPEED_UNKNOWN
 * @duplex: Duplex mode; one of %DUPLEX_*
 * @port: Physical connector type; one of %PORT_*
 * @phy_address: MDIO address of PHY (transceiver); 0 or 255 if not
 *	applicable.  For clause 45 PHYs this is the PRTAD.
 * @transceiver: Historically used to distinguish different possible
 *	PHY types, but not in a consistent way.  Deprecated.
 * @autoneg: Enable/disable autonegotiation and auto-detection;
 *	either %AUTONEG_DISABLE or %AUTONEG_ENABLE
 * @mdio_support: Bitmask of %ETH_MDIO_SUPPORTS_* flags for the MDIO
 *	protocols supported by the interface; 0 if unknown.
 *	Read-only.
 * @maxtxpkt: Historically used to report TX IRQ coalescing; now
 *	obsoleted by &struct ethtool_coalesce.  Read-only; deprecated.
 * @maxrxpkt: Historically used to report RX IRQ coalescing; now
 *	obsoleted by &struct ethtool_coalesce.  Read-only; deprecated.
 * @speed_hi: High bits of the speed, 1Mb units, 0 to INT_MAX or SPEED_UNKNOWN
 * @eth_tp_mdix: Ethernet twisted-pair MDI(-X) status; one of
 *	%ETH_TP_MDI_*.  If the status is unknown or not applicable, the
 *	value will be %ETH_TP_MDI_INVALID.  Read-only.
 * @eth_tp_mdix_ctrl: Ethernet twisted pair MDI(-X) control; one of
 *	%ETH_TP_MDI_*.  If MDI(-X) control is not implemented, reads
 *	yield %ETH_TP_MDI_INVALID and writes may be ignored or rejected.
 *	When written successfully, the link should be renegotiated if
 *	necessary.
 * @lp_advertising: Bitmask of %ADVERTISED_* flags for the link modes
 *	and other link features that the link partner advertised
 *	through autonegotiation; 0 if unknown or not applicable.
 *	Read-only.
 *
 * The link speed in Mbps is split between @speed and @speed_hi.  Use
 * the ethtool_cmd_speed() and ethtool_cmd_speed_set() functions to
 * access it.
 *
 * If autonegotiation is disabled, the speed and @duplex represent the
 * fixed link mode and are writable if the driver supports multiple
 * link modes.  If it is enabled then they are read-only; if the link
 * is up they represent the negotiated link mode; if the link is down,
 * the speed is 0, %SPEED_UNKNOWN or the highest enabled speed and
 * @duplex is %DUPLEX_UNKNOWN or the best enabled duplex mode.
 *
 * Some hardware interfaces may have multiple PHYs and/or physical
 * connectors fitted or do not allow the driver to detect which are
 * fitted.  For these interfaces @port and/or @phy_address may be
 * writable, possibly dependent on @autoneg being %AUTONEG_DISABLE.
 * Otherwise, attempts to write different values may be ignored or
 * rejected.
 *
 * Users should assume that all fields not marked read-only are
 * writable and subject to validation by the driver.  They should use
 * %ETHTOOL_GSET to get the current values before making specific
 * changes and then applying them with %ETHTOOL_SSET.
 *
 * Drivers that implement set_settings() should validate all fields
 * other than @cmd that are not described as read-only or deprecated,
 * and must ignore all fields described as read-only.
 *
 * Deprecated fields should be ignored by both users and drivers.
 */
struct Model1_ethtool_cmd {
 __u32 Model1_cmd;
 __u32 Model1_supported;
 __u32 Model1_advertising;
 Model1___u16 Model1_speed;
 __u8 Model1_duplex;
 __u8 Model1_port;
 __u8 Model1_phy_address;
 __u8 Model1_transceiver;
 __u8 Model1_autoneg;
 __u8 Model1_mdio_support;
 __u32 Model1_maxtxpkt;
 __u32 Model1_maxrxpkt;
 Model1___u16 Model1_speed_hi;
 __u8 Model1_eth_tp_mdix;
 __u8 Model1_eth_tp_mdix_ctrl;
 __u32 Model1_lp_advertising;
 __u32 Model1_reserved[2];
};

static inline __attribute__((no_instrument_function)) void Model1_ethtool_cmd_speed_set(struct Model1_ethtool_cmd *Model1_ep,
      __u32 Model1_speed)
{

 Model1_ep->Model1_speed = (Model1___u16)Model1_speed;
 Model1_ep->Model1_speed_hi = (Model1___u16)(Model1_speed >> 16);
}

static inline __attribute__((no_instrument_function)) __u32 Model1_ethtool_cmd_speed(const struct Model1_ethtool_cmd *Model1_ep)
{
 return (Model1_ep->Model1_speed_hi << 16) | Model1_ep->Model1_speed;
}

/* Device supports clause 22 register access to PHY or peripherals
 * using the interface defined in <linux/mii.h>.  This should not be
 * set if there are known to be no such peripherals present or if
 * the driver only emulates clause 22 registers for compatibility.
 */


/* Device supports clause 45 register access to PHY or peripherals
 * using the interface defined in <linux/mii.h> and <linux/mdio.h>.
 * This should not be set if there are known to be no such peripherals
 * present.
 */






/**
 * struct ethtool_drvinfo - general driver and device information
 * @cmd: Command number = %ETHTOOL_GDRVINFO
 * @driver: Driver short name.  This should normally match the name
 *	in its bus driver structure (e.g. pci_driver::name).  Must
 *	not be an empty string.
 * @version: Driver version string; may be an empty string
 * @fw_version: Firmware version string; may be an empty string
 * @erom_version: Expansion ROM version string; may be an empty string
 * @bus_info: Device bus address.  This should match the dev_name()
 *	string for the underlying bus device, if there is one.  May be
 *	an empty string.
 * @n_priv_flags: Number of flags valid for %ETHTOOL_GPFLAGS and
 *	%ETHTOOL_SPFLAGS commands; also the number of strings in the
 *	%ETH_SS_PRIV_FLAGS set
 * @n_stats: Number of u64 statistics returned by the %ETHTOOL_GSTATS
 *	command; also the number of strings in the %ETH_SS_STATS set
 * @testinfo_len: Number of results returned by the %ETHTOOL_TEST
 *	command; also the number of strings in the %ETH_SS_TEST set
 * @eedump_len: Size of EEPROM accessible through the %ETHTOOL_GEEPROM
 *	and %ETHTOOL_SEEPROM commands, in bytes
 * @regdump_len: Size of register dump returned by the %ETHTOOL_GREGS
 *	command, in bytes
 *
 * Users can use the %ETHTOOL_GSSET_INFO command to get the number of
 * strings in any string set (from Linux 2.6.34).
 *
 * Drivers should set at most @driver, @version, @fw_version and
 * @bus_info in their get_drvinfo() implementation.  The ethtool
 * core fills in the other fields using other driver operations.
 */
struct Model1_ethtool_drvinfo {
 __u32 Model1_cmd;
 char Model1_driver[32];
 char Model1_version[32];
 char Model1_fw_version[32];
 char Model1_bus_info[32];
 char Model1_erom_version[32];
 char Model1_reserved2[12];
 __u32 Model1_n_priv_flags;
 __u32 Model1_n_stats;
 __u32 Model1_testinfo_len;
 __u32 Model1_eedump_len;
 __u32 Model1_regdump_len;
};



/**
 * struct ethtool_wolinfo - Wake-On-Lan configuration
 * @cmd: Command number = %ETHTOOL_GWOL or %ETHTOOL_SWOL
 * @supported: Bitmask of %WAKE_* flags for supported Wake-On-Lan modes.
 *	Read-only.
 * @wolopts: Bitmask of %WAKE_* flags for enabled Wake-On-Lan modes.
 * @sopass: SecureOn(tm) password; meaningful only if %WAKE_MAGICSECURE
 *	is set in @wolopts.
 */
struct Model1_ethtool_wolinfo {
 __u32 Model1_cmd;
 __u32 Model1_supported;
 __u32 Model1_wolopts;
 __u8 Model1_sopass[6];
};

/* for passing single values */
struct Model1_ethtool_value {
 __u32 Model1_cmd;
 __u32 Model1_data;
};

enum Model1_tunable_id {
 Model1_ETHTOOL_ID_UNSPEC,
 Model1_ETHTOOL_RX_COPYBREAK,
 Model1_ETHTOOL_TX_COPYBREAK,
 /*
	 * Add your fresh new tubale attribute above and remember to update
	 * tunable_strings[] in net/core/ethtool.c
	 */
 Model1___ETHTOOL_TUNABLE_COUNT,
};

enum Model1_tunable_type_id {
 Model1_ETHTOOL_TUNABLE_UNSPEC,
 Model1_ETHTOOL_TUNABLE_U8,
 Model1_ETHTOOL_TUNABLE_U16,
 Model1_ETHTOOL_TUNABLE_U32,
 Model1_ETHTOOL_TUNABLE_U64,
 Model1_ETHTOOL_TUNABLE_STRING,
 Model1_ETHTOOL_TUNABLE_S8,
 Model1_ETHTOOL_TUNABLE_S16,
 Model1_ETHTOOL_TUNABLE_S32,
 Model1_ETHTOOL_TUNABLE_S64,
};

struct Model1_ethtool_tunable {
 __u32 Model1_cmd;
 __u32 Model1_id;
 __u32 Model1_type_id;
 __u32 Model1_len;
 void *Model1_data[0];
};

/**
 * struct ethtool_regs - hardware register dump
 * @cmd: Command number = %ETHTOOL_GREGS
 * @version: Dump format version.  This is driver-specific and may
 *	distinguish different chips/revisions.  Drivers must use new
 *	version numbers whenever the dump format changes in an
 *	incompatible way.
 * @len: On entry, the real length of @data.  On return, the number of
 *	bytes used.
 * @data: Buffer for the register dump
 *
 * Users should use %ETHTOOL_GDRVINFO to find the maximum length of
 * a register dump for the interface.  They must allocate the buffer
 * immediately following this structure.
 */
struct Model1_ethtool_regs {
 __u32 Model1_cmd;
 __u32 Model1_version;
 __u32 Model1_len;
 __u8 Model1_data[0];
};

/**
 * struct ethtool_eeprom - EEPROM dump
 * @cmd: Command number = %ETHTOOL_GEEPROM, %ETHTOOL_GMODULEEEPROM or
 *	%ETHTOOL_SEEPROM
 * @magic: A 'magic cookie' value to guard against accidental changes.
 *	The value passed in to %ETHTOOL_SEEPROM must match the value
 *	returned by %ETHTOOL_GEEPROM for the same device.  This is
 *	unused when @cmd is %ETHTOOL_GMODULEEEPROM.
 * @offset: Offset within the EEPROM to begin reading/writing, in bytes
 * @len: On entry, number of bytes to read/write.  On successful
 *	return, number of bytes actually read/written.  In case of
 *	error, this may indicate at what point the error occurred.
 * @data: Buffer to read/write from
 *
 * Users may use %ETHTOOL_GDRVINFO or %ETHTOOL_GMODULEINFO to find
 * the length of an on-board or module EEPROM, respectively.  They
 * must allocate the buffer immediately following this structure.
 */
struct Model1_ethtool_eeprom {
 __u32 Model1_cmd;
 __u32 Model1_magic;
 __u32 Model1_offset;
 __u32 Model1_len;
 __u8 Model1_data[0];
};

/**
 * struct ethtool_eee - Energy Efficient Ethernet information
 * @cmd: ETHTOOL_{G,S}EEE
 * @supported: Mask of %SUPPORTED_* flags for the speed/duplex combinations
 *	for which there is EEE support.
 * @advertised: Mask of %ADVERTISED_* flags for the speed/duplex combinations
 *	advertised as eee capable.
 * @lp_advertised: Mask of %ADVERTISED_* flags for the speed/duplex
 *	combinations advertised by the link partner as eee capable.
 * @eee_active: Result of the eee auto negotiation.
 * @eee_enabled: EEE configured mode (enabled/disabled).
 * @tx_lpi_enabled: Whether the interface should assert its tx lpi, given
 *	that eee was negotiated.
 * @tx_lpi_timer: Time in microseconds the interface delays prior to asserting
 *	its tx lpi (after reaching 'idle' state). Effective only when eee
 *	was negotiated and tx_lpi_enabled was set.
 */
struct Model1_ethtool_eee {
 __u32 Model1_cmd;
 __u32 Model1_supported;
 __u32 Model1_advertised;
 __u32 Model1_lp_advertised;
 __u32 Model1_eee_active;
 __u32 Model1_eee_enabled;
 __u32 Model1_tx_lpi_enabled;
 __u32 Model1_tx_lpi_timer;
 __u32 Model1_reserved[2];
};

/**
 * struct ethtool_modinfo - plugin module eeprom information
 * @cmd: %ETHTOOL_GMODULEINFO
 * @type: Standard the module information conforms to %ETH_MODULE_SFF_xxxx
 * @eeprom_len: Length of the eeprom
 *
 * This structure is used to return the information to
 * properly size memory for a subsequent call to %ETHTOOL_GMODULEEEPROM.
 * The type code indicates the eeprom data format
 */
struct Model1_ethtool_modinfo {
 __u32 Model1_cmd;
 __u32 Model1_type;
 __u32 Model1_eeprom_len;
 __u32 Model1_reserved[8];
};

/**
 * struct ethtool_coalesce - coalescing parameters for IRQs and stats updates
 * @cmd: ETHTOOL_{G,S}COALESCE
 * @rx_coalesce_usecs: How many usecs to delay an RX interrupt after
 *	a packet arrives.
 * @rx_max_coalesced_frames: Maximum number of packets to receive
 *	before an RX interrupt.
 * @rx_coalesce_usecs_irq: Same as @rx_coalesce_usecs, except that
 *	this value applies while an IRQ is being serviced by the host.
 * @rx_max_coalesced_frames_irq: Same as @rx_max_coalesced_frames,
 *	except that this value applies while an IRQ is being serviced
 *	by the host.
 * @tx_coalesce_usecs: How many usecs to delay a TX interrupt after
 *	a packet is sent.
 * @tx_max_coalesced_frames: Maximum number of packets to be sent
 *	before a TX interrupt.
 * @tx_coalesce_usecs_irq: Same as @tx_coalesce_usecs, except that
 *	this value applies while an IRQ is being serviced by the host.
 * @tx_max_coalesced_frames_irq: Same as @tx_max_coalesced_frames,
 *	except that this value applies while an IRQ is being serviced
 *	by the host.
 * @stats_block_coalesce_usecs: How many usecs to delay in-memory
 *	statistics block updates.  Some drivers do not have an
 *	in-memory statistic block, and in such cases this value is
 *	ignored.  This value must not be zero.
 * @use_adaptive_rx_coalesce: Enable adaptive RX coalescing.
 * @use_adaptive_tx_coalesce: Enable adaptive TX coalescing.
 * @pkt_rate_low: Threshold for low packet rate (packets per second).
 * @rx_coalesce_usecs_low: How many usecs to delay an RX interrupt after
 *	a packet arrives, when the packet rate is below @pkt_rate_low.
 * @rx_max_coalesced_frames_low: Maximum number of packets to be received
 *	before an RX interrupt, when the packet rate is below @pkt_rate_low.
 * @tx_coalesce_usecs_low: How many usecs to delay a TX interrupt after
 *	a packet is sent, when the packet rate is below @pkt_rate_low.
 * @tx_max_coalesced_frames_low: Maximum nuumber of packets to be sent before
 *	a TX interrupt, when the packet rate is below @pkt_rate_low.
 * @pkt_rate_high: Threshold for high packet rate (packets per second).
 * @rx_coalesce_usecs_high: How many usecs to delay an RX interrupt after
 *	a packet arrives, when the packet rate is above @pkt_rate_high.
 * @rx_max_coalesced_frames_high: Maximum number of packets to be received
 *	before an RX interrupt, when the packet rate is above @pkt_rate_high.
 * @tx_coalesce_usecs_high: How many usecs to delay a TX interrupt after
 *	a packet is sent, when the packet rate is above @pkt_rate_high.
 * @tx_max_coalesced_frames_high: Maximum number of packets to be sent before
 *	a TX interrupt, when the packet rate is above @pkt_rate_high.
 * @rate_sample_interval: How often to do adaptive coalescing packet rate
 *	sampling, measured in seconds.  Must not be zero.
 *
 * Each pair of (usecs, max_frames) fields specifies that interrupts
 * should be coalesced until
 *	(usecs > 0 && time_since_first_completion >= usecs) ||
 *	(max_frames > 0 && completed_frames >= max_frames)
 *
 * It is illegal to set both usecs and max_frames to zero as this
 * would cause interrupts to never be generated.  To disable
 * coalescing, set usecs = 0 and max_frames = 1.
 *
 * Some implementations ignore the value of max_frames and use the
 * condition time_since_first_completion >= usecs
 *
 * This is deprecated.  Drivers for hardware that does not support
 * counting completions should validate that max_frames == !rx_usecs.
 *
 * Adaptive RX/TX coalescing is an algorithm implemented by some
 * drivers to improve latency under low packet rates and improve
 * throughput under high packet rates.  Some drivers only implement
 * one of RX or TX adaptive coalescing.  Anything not implemented by
 * the driver causes these values to be silently ignored.
 *
 * When the packet rate is below @pkt_rate_high but above
 * @pkt_rate_low (both measured in packets per second) the
 * normal {rx,tx}_* coalescing parameters are used.
 */
struct Model1_ethtool_coalesce {
 __u32 Model1_cmd;
 __u32 Model1_rx_coalesce_usecs;
 __u32 Model1_rx_max_coalesced_frames;
 __u32 Model1_rx_coalesce_usecs_irq;
 __u32 Model1_rx_max_coalesced_frames_irq;
 __u32 Model1_tx_coalesce_usecs;
 __u32 Model1_tx_max_coalesced_frames;
 __u32 Model1_tx_coalesce_usecs_irq;
 __u32 Model1_tx_max_coalesced_frames_irq;
 __u32 Model1_stats_block_coalesce_usecs;
 __u32 Model1_use_adaptive_rx_coalesce;
 __u32 Model1_use_adaptive_tx_coalesce;
 __u32 Model1_pkt_rate_low;
 __u32 Model1_rx_coalesce_usecs_low;
 __u32 Model1_rx_max_coalesced_frames_low;
 __u32 Model1_tx_coalesce_usecs_low;
 __u32 Model1_tx_max_coalesced_frames_low;
 __u32 Model1_pkt_rate_high;
 __u32 Model1_rx_coalesce_usecs_high;
 __u32 Model1_rx_max_coalesced_frames_high;
 __u32 Model1_tx_coalesce_usecs_high;
 __u32 Model1_tx_max_coalesced_frames_high;
 __u32 Model1_rate_sample_interval;
};

/**
 * struct ethtool_ringparam - RX/TX ring parameters
 * @cmd: Command number = %ETHTOOL_GRINGPARAM or %ETHTOOL_SRINGPARAM
 * @rx_max_pending: Maximum supported number of pending entries per
 *	RX ring.  Read-only.
 * @rx_mini_max_pending: Maximum supported number of pending entries
 *	per RX mini ring.  Read-only.
 * @rx_jumbo_max_pending: Maximum supported number of pending entries
 *	per RX jumbo ring.  Read-only.
 * @tx_max_pending: Maximum supported number of pending entries per
 *	TX ring.  Read-only.
 * @rx_pending: Current maximum number of pending entries per RX ring
 * @rx_mini_pending: Current maximum number of pending entries per RX
 *	mini ring
 * @rx_jumbo_pending: Current maximum number of pending entries per RX
 *	jumbo ring
 * @tx_pending: Current maximum supported number of pending entries
 *	per TX ring
 *
 * If the interface does not have separate RX mini and/or jumbo rings,
 * @rx_mini_max_pending and/or @rx_jumbo_max_pending will be 0.
 *
 * There may also be driver-dependent minimum values for the number
 * of entries per ring.
 */
struct Model1_ethtool_ringparam {
 __u32 Model1_cmd;
 __u32 Model1_rx_max_pending;
 __u32 Model1_rx_mini_max_pending;
 __u32 Model1_rx_jumbo_max_pending;
 __u32 Model1_tx_max_pending;
 __u32 Model1_rx_pending;
 __u32 Model1_rx_mini_pending;
 __u32 Model1_rx_jumbo_pending;
 __u32 Model1_tx_pending;
};

/**
 * struct ethtool_channels - configuring number of network channel
 * @cmd: ETHTOOL_{G,S}CHANNELS
 * @max_rx: Read only. Maximum number of receive channel the driver support.
 * @max_tx: Read only. Maximum number of transmit channel the driver support.
 * @max_other: Read only. Maximum number of other channel the driver support.
 * @max_combined: Read only. Maximum number of combined channel the driver
 *	support. Set of queues RX, TX or other.
 * @rx_count: Valid values are in the range 1 to the max_rx.
 * @tx_count: Valid values are in the range 1 to the max_tx.
 * @other_count: Valid values are in the range 1 to the max_other.
 * @combined_count: Valid values are in the range 1 to the max_combined.
 *
 * This can be used to configure RX, TX and other channels.
 */

struct Model1_ethtool_channels {
 __u32 Model1_cmd;
 __u32 Model1_max_rx;
 __u32 Model1_max_tx;
 __u32 Model1_max_other;
 __u32 Model1_max_combined;
 __u32 Model1_rx_count;
 __u32 Model1_tx_count;
 __u32 Model1_other_count;
 __u32 Model1_combined_count;
};

/**
 * struct ethtool_pauseparam - Ethernet pause (flow control) parameters
 * @cmd: Command number = %ETHTOOL_GPAUSEPARAM or %ETHTOOL_SPAUSEPARAM
 * @autoneg: Flag to enable autonegotiation of pause frame use
 * @rx_pause: Flag to enable reception of pause frames
 * @tx_pause: Flag to enable transmission of pause frames
 *
 * Drivers should reject a non-zero setting of @autoneg when
 * autoneogotiation is disabled (or not supported) for the link.
 *
 * If the link is autonegotiated, drivers should use
 * mii_advertise_flowctrl() or similar code to set the advertised
 * pause frame capabilities based on the @rx_pause and @tx_pause flags,
 * even if @autoneg is zero.  They should also allow the advertised
 * pause frame capabilities to be controlled directly through the
 * advertising field of &struct ethtool_cmd.
 *
 * If @autoneg is non-zero, the MAC is configured to send and/or
 * receive pause frames according to the result of autonegotiation.
 * Otherwise, it is configured directly based on the @rx_pause and
 * @tx_pause flags.
 */
struct Model1_ethtool_pauseparam {
 __u32 Model1_cmd;
 __u32 Model1_autoneg;
 __u32 Model1_rx_pause;
 __u32 Model1_tx_pause;
};



/**
 * enum ethtool_stringset - string set ID
 * @ETH_SS_TEST: Self-test result names, for use with %ETHTOOL_TEST
 * @ETH_SS_STATS: Statistic names, for use with %ETHTOOL_GSTATS
 * @ETH_SS_PRIV_FLAGS: Driver private flag names, for use with
 *	%ETHTOOL_GPFLAGS and %ETHTOOL_SPFLAGS
 * @ETH_SS_NTUPLE_FILTERS: Previously used with %ETHTOOL_GRXNTUPLE;
 *	now deprecated
 * @ETH_SS_FEATURES: Device feature names
 * @ETH_SS_RSS_HASH_FUNCS: RSS hush function names
 * @ETH_SS_PHY_STATS: Statistic names, for use with %ETHTOOL_GPHYSTATS
 */
enum Model1_ethtool_stringset {
 Model1_ETH_SS_TEST = 0,
 Model1_ETH_SS_STATS,
 Model1_ETH_SS_PRIV_FLAGS,
 Model1_ETH_SS_NTUPLE_FILTERS,
 Model1_ETH_SS_FEATURES,
 Model1_ETH_SS_RSS_HASH_FUNCS,
 Model1_ETH_SS_TUNABLES,
 Model1_ETH_SS_PHY_STATS,
};

/**
 * struct ethtool_gstrings - string set for data tagging
 * @cmd: Command number = %ETHTOOL_GSTRINGS
 * @string_set: String set ID; one of &enum ethtool_stringset
 * @len: On return, the number of strings in the string set
 * @data: Buffer for strings.  Each string is null-padded to a size of
 *	%ETH_GSTRING_LEN.
 *
 * Users must use %ETHTOOL_GSSET_INFO to find the number of strings in
 * the string set.  They must allocate a buffer of the appropriate
 * size immediately following this structure.
 */
struct Model1_ethtool_gstrings {
 __u32 Model1_cmd;
 __u32 Model1_string_set;
 __u32 Model1_len;
 __u8 Model1_data[0];
};

/**
 * struct ethtool_sset_info - string set information
 * @cmd: Command number = %ETHTOOL_GSSET_INFO
 * @sset_mask: On entry, a bitmask of string sets to query, with bits
 *	numbered according to &enum ethtool_stringset.  On return, a
 *	bitmask of those string sets queried that are supported.
 * @data: Buffer for string set sizes.  On return, this contains the
 *	size of each string set that was queried and supported, in
 *	order of ID.
 *
 * Example: The user passes in @sset_mask = 0x7 (sets 0, 1, 2) and on
 * return @sset_mask == 0x6 (sets 1, 2).  Then @data[0] contains the
 * size of set 1 and @data[1] contains the size of set 2.
 *
 * Users must allocate a buffer of the appropriate size (4 * number of
 * sets queried) immediately following this structure.
 */
struct Model1_ethtool_sset_info {
 __u32 Model1_cmd;
 __u32 Model1_reserved;
 __u64 Model1_sset_mask;
 __u32 Model1_data[0];
};

/**
 * enum ethtool_test_flags - flags definition of ethtool_test
 * @ETH_TEST_FL_OFFLINE: if set perform online and offline tests, otherwise
 *	only online tests.
 * @ETH_TEST_FL_FAILED: Driver set this flag if test fails.
 * @ETH_TEST_FL_EXTERNAL_LB: Application request to perform external loopback
 *	test.
 * @ETH_TEST_FL_EXTERNAL_LB_DONE: Driver performed the external loopback test
 */

enum Model1_ethtool_test_flags {
 Model1_ETH_TEST_FL_OFFLINE = (1 << 0),
 Model1_ETH_TEST_FL_FAILED = (1 << 1),
 Model1_ETH_TEST_FL_EXTERNAL_LB = (1 << 2),
 Model1_ETH_TEST_FL_EXTERNAL_LB_DONE = (1 << 3),
};

/**
 * struct ethtool_test - device self-test invocation
 * @cmd: Command number = %ETHTOOL_TEST
 * @flags: A bitmask of flags from &enum ethtool_test_flags.  Some
 *	flags may be set by the user on entry; others may be set by
 *	the driver on return.
 * @len: On return, the number of test results
 * @data: Array of test results
 *
 * Users must use %ETHTOOL_GSSET_INFO or %ETHTOOL_GDRVINFO to find the
 * number of test results that will be returned.  They must allocate a
 * buffer of the appropriate size (8 * number of results) immediately
 * following this structure.
 */
struct Model1_ethtool_test {
 __u32 Model1_cmd;
 __u32 Model1_flags;
 __u32 Model1_reserved;
 __u32 Model1_len;
 __u64 Model1_data[0];
};

/**
 * struct ethtool_stats - device-specific statistics
 * @cmd: Command number = %ETHTOOL_GSTATS
 * @n_stats: On return, the number of statistics
 * @data: Array of statistics
 *
 * Users must use %ETHTOOL_GSSET_INFO or %ETHTOOL_GDRVINFO to find the
 * number of statistics that will be returned.  They must allocate a
 * buffer of the appropriate size (8 * number of statistics)
 * immediately following this structure.
 */
struct Model1_ethtool_stats {
 __u32 Model1_cmd;
 __u32 Model1_n_stats;
 __u64 Model1_data[0];
};

/**
 * struct ethtool_perm_addr - permanent hardware address
 * @cmd: Command number = %ETHTOOL_GPERMADDR
 * @size: On entry, the size of the buffer.  On return, the size of the
 *	address.  The command fails if the buffer is too small.
 * @data: Buffer for the address
 *
 * Users must allocate the buffer immediately following this structure.
 * A buffer size of %MAX_ADDR_LEN should be sufficient for any address
 * type.
 */
struct Model1_ethtool_perm_addr {
 __u32 Model1_cmd;
 __u32 Model1_size;
 __u8 Model1_data[0];
};

/* boolean flags controlling per-interface behavior characteristics.
 * When reading, the flag indicates whether or not a certain behavior
 * is enabled/present.  When writing, the flag indicates whether
 * or not the driver should turn on (set) or off (clear) a behavior.
 *
 * Some behaviors may read-only (unconditionally absent or present).
 * If such is the case, return EINVAL in the set-flags operation if the
 * flag differs from the read-only value.
 */
enum Model1_ethtool_flags {
 Model1_ETH_FLAG_TXVLAN = (1 << 7), /* TX VLAN offload enabled */
 Model1_ETH_FLAG_RXVLAN = (1 << 8), /* RX VLAN offload enabled */
 Model1_ETH_FLAG_LRO = (1 << 15), /* LRO is enabled */
 Model1_ETH_FLAG_NTUPLE = (1 << 27), /* N-tuple filters enabled */
 Model1_ETH_FLAG_RXHASH = (1 << 28),
};

/* The following structures are for supporting RX network flow
 * classification and RX n-tuple configuration. Note, all multibyte
 * fields, e.g., ip4src, ip4dst, psrc, pdst, spi, etc. are expected to
 * be in network byte order.
 */

/**
 * struct ethtool_tcpip4_spec - flow specification for TCP/IPv4 etc.
 * @ip4src: Source host
 * @ip4dst: Destination host
 * @psrc: Source port
 * @pdst: Destination port
 * @tos: Type-of-service
 *
 * This can be used to specify a TCP/IPv4, UDP/IPv4 or SCTP/IPv4 flow.
 */
struct Model1_ethtool_tcpip4_spec {
 Model1___be32 Model1_ip4src;
 Model1___be32 Model1_ip4dst;
 Model1___be16 Model1_psrc;
 Model1___be16 Model1_pdst;
 __u8 Model1_tos;
};

/**
 * struct ethtool_ah_espip4_spec - flow specification for IPsec/IPv4
 * @ip4src: Source host
 * @ip4dst: Destination host
 * @spi: Security parameters index
 * @tos: Type-of-service
 *
 * This can be used to specify an IPsec transport or tunnel over IPv4.
 */
struct Model1_ethtool_ah_espip4_spec {
 Model1___be32 Model1_ip4src;
 Model1___be32 Model1_ip4dst;
 Model1___be32 Model1_spi;
 __u8 Model1_tos;
};



/**
 * struct ethtool_usrip4_spec - general flow specification for IPv4
 * @ip4src: Source host
 * @ip4dst: Destination host
 * @l4_4_bytes: First 4 bytes of transport (layer 4) header
 * @tos: Type-of-service
 * @ip_ver: Value must be %ETH_RX_NFC_IP4; mask must be 0
 * @proto: Transport protocol number; mask must be 0
 */
struct Model1_ethtool_usrip4_spec {
 Model1___be32 Model1_ip4src;
 Model1___be32 Model1_ip4dst;
 Model1___be32 Model1_l4_4_bytes;
 __u8 Model1_tos;
 __u8 Model1_ip_ver;
 __u8 Model1_proto;
};

/**
 * struct ethtool_tcpip6_spec - flow specification for TCP/IPv6 etc.
 * @ip6src: Source host
 * @ip6dst: Destination host
 * @psrc: Source port
 * @pdst: Destination port
 * @tclass: Traffic Class
 *
 * This can be used to specify a TCP/IPv6, UDP/IPv6 or SCTP/IPv6 flow.
 */
struct Model1_ethtool_tcpip6_spec {
 Model1___be32 Model1_ip6src[4];
 Model1___be32 Model1_ip6dst[4];
 Model1___be16 Model1_psrc;
 Model1___be16 Model1_pdst;
 __u8 Model1_tclass;
};

/**
 * struct ethtool_ah_espip6_spec - flow specification for IPsec/IPv6
 * @ip6src: Source host
 * @ip6dst: Destination host
 * @spi: Security parameters index
 * @tclass: Traffic Class
 *
 * This can be used to specify an IPsec transport or tunnel over IPv6.
 */
struct Model1_ethtool_ah_espip6_spec {
 Model1___be32 Model1_ip6src[4];
 Model1___be32 Model1_ip6dst[4];
 Model1___be32 Model1_spi;
 __u8 Model1_tclass;
};

/**
 * struct ethtool_usrip6_spec - general flow specification for IPv6
 * @ip6src: Source host
 * @ip6dst: Destination host
 * @l4_4_bytes: First 4 bytes of transport (layer 4) header
 * @tclass: Traffic Class
 * @l4_proto: Transport protocol number (nexthdr after any Extension Headers)
 */
struct Model1_ethtool_usrip6_spec {
 Model1___be32 Model1_ip6src[4];
 Model1___be32 Model1_ip6dst[4];
 Model1___be32 Model1_l4_4_bytes;
 __u8 Model1_tclass;
 __u8 Model1_l4_proto;
};

union Model1_ethtool_flow_union {
 struct Model1_ethtool_tcpip4_spec Model1_tcp_ip4_spec;
 struct Model1_ethtool_tcpip4_spec Model1_udp_ip4_spec;
 struct Model1_ethtool_tcpip4_spec Model1_sctp_ip4_spec;
 struct Model1_ethtool_ah_espip4_spec Model1_ah_ip4_spec;
 struct Model1_ethtool_ah_espip4_spec Model1_esp_ip4_spec;
 struct Model1_ethtool_usrip4_spec Model1_usr_ip4_spec;
 struct Model1_ethtool_tcpip6_spec Model1_tcp_ip6_spec;
 struct Model1_ethtool_tcpip6_spec Model1_udp_ip6_spec;
 struct Model1_ethtool_tcpip6_spec Model1_sctp_ip6_spec;
 struct Model1_ethtool_ah_espip6_spec Model1_ah_ip6_spec;
 struct Model1_ethtool_ah_espip6_spec Model1_esp_ip6_spec;
 struct Model1_ethtool_usrip6_spec Model1_usr_ip6_spec;
 struct Model1_ethhdr Model1_ether_spec;
 __u8 Model1_hdata[52];
};

/**
 * struct ethtool_flow_ext - additional RX flow fields
 * @h_dest: destination MAC address
 * @vlan_etype: VLAN EtherType
 * @vlan_tci: VLAN tag control information
 * @data: user defined data
 *
 * Note, @vlan_etype, @vlan_tci, and @data are only valid if %FLOW_EXT
 * is set in &struct ethtool_rx_flow_spec @flow_type.
 * @h_dest is valid if %FLOW_MAC_EXT is set.
 */
struct Model1_ethtool_flow_ext {
 __u8 Model1_padding[2];
 unsigned char Model1_h_dest[6];
 Model1___be16 Model1_vlan_etype;
 Model1___be16 Model1_vlan_tci;
 Model1___be32 Model1_data[2];
};

/**
 * struct ethtool_rx_flow_spec - classification rule for RX flows
 * @flow_type: Type of match to perform, e.g. %TCP_V4_FLOW
 * @h_u: Flow fields to match (dependent on @flow_type)
 * @h_ext: Additional fields to match
 * @m_u: Masks for flow field bits to be matched
 * @m_ext: Masks for additional field bits to be matched
 *	Note, all additional fields must be ignored unless @flow_type
 *	includes the %FLOW_EXT or %FLOW_MAC_EXT flag
 *	(see &struct ethtool_flow_ext description).
 * @ring_cookie: RX ring/queue index to deliver to, or %RX_CLS_FLOW_DISC
 *	if packets should be discarded
 * @location: Location of rule in the table.  Locations must be
 *	numbered such that a flow matching multiple rules will be
 *	classified according to the first (lowest numbered) rule.
 */
struct Model1_ethtool_rx_flow_spec {
 __u32 Model1_flow_type;
 union Model1_ethtool_flow_union Model1_h_u;
 struct Model1_ethtool_flow_ext Model1_h_ext;
 union Model1_ethtool_flow_union Model1_m_u;
 struct Model1_ethtool_flow_ext Model1_m_ext;
 __u64 Model1_ring_cookie;
 __u32 Model1_location;
};

/* How rings are layed out when accessing virtual functions or
 * offloaded queues is device specific. To allow users to do flow
 * steering and specify these queues the ring cookie is partitioned
 * into a 32bit queue index with an 8 bit virtual function id.
 * This also leaves the 3bytes for further specifiers. It is possible
 * future devices may support more than 256 virtual functions if
 * devices start supporting PCIe w/ARI. However at the moment I
 * do not know of any devices that support this so I do not reserve
 * space for this at this time. If a future patch consumes the next
 * byte it should be aware of this possiblity.
 */



static inline __attribute__((no_instrument_function)) __u64 Model1_ethtool_get_flow_spec_ring(__u64 Model1_ring_cookie)
{
 return 0x00000000FFFFFFFFLL & Model1_ring_cookie;
};

static inline __attribute__((no_instrument_function)) __u64 Model1_ethtool_get_flow_spec_ring_vf(__u64 Model1_ring_cookie)
{
 return (0x000000FF00000000LL & Model1_ring_cookie) >>
    32;
};

/**
 * struct ethtool_rxnfc - command to get or set RX flow classification rules
 * @cmd: Specific command number - %ETHTOOL_GRXFH, %ETHTOOL_SRXFH,
 *	%ETHTOOL_GRXRINGS, %ETHTOOL_GRXCLSRLCNT, %ETHTOOL_GRXCLSRULE,
 *	%ETHTOOL_GRXCLSRLALL, %ETHTOOL_SRXCLSRLDEL or %ETHTOOL_SRXCLSRLINS
 * @flow_type: Type of flow to be affected, e.g. %TCP_V4_FLOW
 * @data: Command-dependent value
 * @fs: Flow classification rule
 * @rule_cnt: Number of rules to be affected
 * @rule_locs: Array of used rule locations
 *
 * For %ETHTOOL_GRXFH and %ETHTOOL_SRXFH, @data is a bitmask indicating
 * the fields included in the flow hash, e.g. %RXH_IP_SRC.  The following
 * structure fields must not be used.
 *
 * For %ETHTOOL_GRXRINGS, @data is set to the number of RX rings/queues
 * on return.
 *
 * For %ETHTOOL_GRXCLSRLCNT, @rule_cnt is set to the number of defined
 * rules on return.  If @data is non-zero on return then it is the
 * size of the rule table, plus the flag %RX_CLS_LOC_SPECIAL if the
 * driver supports any special location values.  If that flag is not
 * set in @data then special location values should not be used.
 *
 * For %ETHTOOL_GRXCLSRULE, @fs.@location specifies the location of an
 * existing rule on entry and @fs contains the rule on return.
 *
 * For %ETHTOOL_GRXCLSRLALL, @rule_cnt specifies the array size of the
 * user buffer for @rule_locs on entry.  On return, @data is the size
 * of the rule table, @rule_cnt is the number of defined rules, and
 * @rule_locs contains the locations of the defined rules.  Drivers
 * must use the second parameter to get_rxnfc() instead of @rule_locs.
 *
 * For %ETHTOOL_SRXCLSRLINS, @fs specifies the rule to add or update.
 * @fs.@location either specifies the location to use or is a special
 * location value with %RX_CLS_LOC_SPECIAL flag set.  On return,
 * @fs.@location is the actual rule location.
 *
 * For %ETHTOOL_SRXCLSRLDEL, @fs.@location specifies the location of an
 * existing rule on entry.
 *
 * A driver supporting the special location values for
 * %ETHTOOL_SRXCLSRLINS may add the rule at any suitable unused
 * location, and may remove a rule at a later location (lower
 * priority) that matches exactly the same set of flows.  The special
 * values are %RX_CLS_LOC_ANY, selecting any location;
 * %RX_CLS_LOC_FIRST, selecting the first suitable location (maximum
 * priority); and %RX_CLS_LOC_LAST, selecting the last suitable
 * location (minimum priority).  Additional special values may be
 * defined in future and drivers must return -%EINVAL for any
 * unrecognised value.
 */
struct Model1_ethtool_rxnfc {
 __u32 Model1_cmd;
 __u32 Model1_flow_type;
 __u64 Model1_data;
 struct Model1_ethtool_rx_flow_spec Model1_fs;
 __u32 Model1_rule_cnt;
 __u32 Model1_rule_locs[0];
};


/**
 * struct ethtool_rxfh_indir - command to get or set RX flow hash indirection
 * @cmd: Specific command number - %ETHTOOL_GRXFHINDIR or %ETHTOOL_SRXFHINDIR
 * @size: On entry, the array size of the user buffer, which may be zero.
 *	On return from %ETHTOOL_GRXFHINDIR, the array size of the hardware
 *	indirection table.
 * @ring_index: RX ring/queue index for each hash value
 *
 * For %ETHTOOL_GRXFHINDIR, a @size of zero means that only the size
 * should be returned.  For %ETHTOOL_SRXFHINDIR, a @size of zero means
 * the table should be reset to default values.  This last feature
 * is not supported by the original implementations.
 */
struct Model1_ethtool_rxfh_indir {
 __u32 Model1_cmd;
 __u32 Model1_size;
 __u32 Model1_ring_index[0];
};

/**
 * struct ethtool_rxfh - command to get/set RX flow hash indir or/and hash key.
 * @cmd: Specific command number - %ETHTOOL_GRSSH or %ETHTOOL_SRSSH
 * @rss_context: RSS context identifier.
 * @indir_size: On entry, the array size of the user buffer for the
 *	indirection table, which may be zero, or (for %ETHTOOL_SRSSH),
 *	%ETH_RXFH_INDIR_NO_CHANGE.  On return from %ETHTOOL_GRSSH,
 *	the array size of the hardware indirection table.
 * @key_size: On entry, the array size of the user buffer for the hash key,
 *	which may be zero.  On return from %ETHTOOL_GRSSH, the size of the
 *	hardware hash key.
 * @hfunc: Defines the current RSS hash function used by HW (or to be set to).
 *	Valid values are one of the %ETH_RSS_HASH_*.
 * @rsvd:	Reserved for future extensions.
 * @rss_config: RX ring/queue index for each hash value i.e., indirection table
 *	of @indir_size __u32 elements, followed by hash key of @key_size
 *	bytes.
 *
 * For %ETHTOOL_GRSSH, a @indir_size and key_size of zero means that only the
 * size should be returned.  For %ETHTOOL_SRSSH, an @indir_size of
 * %ETH_RXFH_INDIR_NO_CHANGE means that indir table setting is not requested
 * and a @indir_size of zero means the indir table should be reset to default
 * values. An hfunc of zero means that hash function setting is not requested.
 */
struct Model1_ethtool_rxfh {
 __u32 Model1_cmd;
 __u32 Model1_rss_context;
 __u32 Model1_indir_size;
 __u32 Model1_key_size;
 __u8 Model1_hfunc;
 __u8 Model1_rsvd8[3];
 __u32 Model1_rsvd32;
 __u32 Model1_rss_config[0];
};


/**
 * struct ethtool_rx_ntuple_flow_spec - specification for RX flow filter
 * @flow_type: Type of match to perform, e.g. %TCP_V4_FLOW
 * @h_u: Flow field values to match (dependent on @flow_type)
 * @m_u: Masks for flow field value bits to be ignored
 * @vlan_tag: VLAN tag to match
 * @vlan_tag_mask: Mask for VLAN tag bits to be ignored
 * @data: Driver-dependent data to match
 * @data_mask: Mask for driver-dependent data bits to be ignored
 * @action: RX ring/queue index to deliver to (non-negative) or other action
 *	(negative, e.g. %ETHTOOL_RXNTUPLE_ACTION_DROP)
 *
 * For flow types %TCP_V4_FLOW, %UDP_V4_FLOW and %SCTP_V4_FLOW, where
 * a field value and mask are both zero this is treated as if all mask
 * bits are set i.e. the field is ignored.
 */
struct Model1_ethtool_rx_ntuple_flow_spec {
 __u32 Model1_flow_type;
 union {
  struct Model1_ethtool_tcpip4_spec Model1_tcp_ip4_spec;
  struct Model1_ethtool_tcpip4_spec Model1_udp_ip4_spec;
  struct Model1_ethtool_tcpip4_spec Model1_sctp_ip4_spec;
  struct Model1_ethtool_ah_espip4_spec Model1_ah_ip4_spec;
  struct Model1_ethtool_ah_espip4_spec Model1_esp_ip4_spec;
  struct Model1_ethtool_usrip4_spec Model1_usr_ip4_spec;
  struct Model1_ethhdr Model1_ether_spec;
  __u8 Model1_hdata[72];
 } Model1_h_u, Model1_m_u;

 Model1___u16 Model1_vlan_tag;
 Model1___u16 Model1_vlan_tag_mask;
 __u64 Model1_data;
 __u64 Model1_data_mask;

 Model1___s32 Model1_action;


};

/**
 * struct ethtool_rx_ntuple - command to set or clear RX flow filter
 * @cmd: Command number - %ETHTOOL_SRXNTUPLE
 * @fs: Flow filter specification
 */
struct Model1_ethtool_rx_ntuple {
 __u32 Model1_cmd;
 struct Model1_ethtool_rx_ntuple_flow_spec Model1_fs;
};


enum Model1_ethtool_flash_op_type {
 Model1_ETHTOOL_FLASH_ALL_REGIONS = 0,
};

/* for passing firmware flashing related parameters */
struct Model1_ethtool_flash {
 __u32 Model1_cmd;
 __u32 Model1_region;
 char Model1_data[128];
};

/**
 * struct ethtool_dump - used for retrieving, setting device dump
 * @cmd: Command number - %ETHTOOL_GET_DUMP_FLAG, %ETHTOOL_GET_DUMP_DATA, or
 * 	%ETHTOOL_SET_DUMP
 * @version: FW version of the dump, filled in by driver
 * @flag: driver dependent flag for dump setting, filled in by driver during
 *        get and filled in by ethtool for set operation.
 *        flag must be initialized by macro ETH_FW_DUMP_DISABLE value when
 *        firmware dump is disabled.
 * @len: length of dump data, used as the length of the user buffer on entry to
 * 	 %ETHTOOL_GET_DUMP_DATA and this is returned as dump length by driver
 * 	 for %ETHTOOL_GET_DUMP_FLAG command
 * @data: data collected for get dump data operation
 */
struct Model1_ethtool_dump {
 __u32 Model1_cmd;
 __u32 Model1_version;
 __u32 Model1_flag;
 __u32 Model1_len;
 __u8 Model1_data[0];
};



/* for returning and changing feature sets */

/**
 * struct ethtool_get_features_block - block with state of 32 features
 * @available: mask of changeable features
 * @requested: mask of features requested to be enabled if possible
 * @active: mask of currently enabled features
 * @never_changed: mask of features not changeable for any device
 */
struct Model1_ethtool_get_features_block {
 __u32 Model1_available;
 __u32 Model1_requested;
 __u32 Model1_active;
 __u32 Model1_never_changed;
};

/**
 * struct ethtool_gfeatures - command to get state of device's features
 * @cmd: command number = %ETHTOOL_GFEATURES
 * @size: On entry, the number of elements in the features[] array;
 *	on return, the number of elements in features[] needed to hold
 *	all features
 * @features: state of features
 */
struct Model1_ethtool_gfeatures {
 __u32 Model1_cmd;
 __u32 Model1_size;
 struct Model1_ethtool_get_features_block Model1_features[0];
};

/**
 * struct ethtool_set_features_block - block with request for 32 features
 * @valid: mask of features to be changed
 * @requested: values of features to be changed
 */
struct Model1_ethtool_set_features_block {
 __u32 Model1_valid;
 __u32 Model1_requested;
};

/**
 * struct ethtool_sfeatures - command to request change in device's features
 * @cmd: command number = %ETHTOOL_SFEATURES
 * @size: array size of the features[] array
 * @features: feature change masks
 */
struct Model1_ethtool_sfeatures {
 __u32 Model1_cmd;
 __u32 Model1_size;
 struct Model1_ethtool_set_features_block Model1_features[0];
};

/**
 * struct ethtool_ts_info - holds a device's timestamping and PHC association
 * @cmd: command number = %ETHTOOL_GET_TS_INFO
 * @so_timestamping: bit mask of the sum of the supported SO_TIMESTAMPING flags
 * @phc_index: device index of the associated PHC, or -1 if there is none
 * @tx_types: bit mask of the supported hwtstamp_tx_types enumeration values
 * @rx_filters: bit mask of the supported hwtstamp_rx_filters enumeration values
 *
 * The bits in the 'tx_types' and 'rx_filters' fields correspond to
 * the 'hwtstamp_tx_types' and 'hwtstamp_rx_filters' enumeration values,
 * respectively.  For example, if the device supports HWTSTAMP_TX_ON,
 * then (1 << HWTSTAMP_TX_ON) in 'tx_types' will be set.
 *
 * Drivers should only report the filters they actually support without
 * upscaling in the SIOCSHWTSTAMP ioctl. If the SIOCSHWSTAMP request for
 * HWTSTAMP_FILTER_V1_SYNC is supported by HWTSTAMP_FILTER_V1_EVENT, then the
 * driver should only report HWTSTAMP_FILTER_V1_EVENT in this op.
 */
struct Model1_ethtool_ts_info {
 __u32 Model1_cmd;
 __u32 Model1_so_timestamping;
 Model1___s32 Model1_phc_index;
 __u32 Model1_tx_types;
 __u32 Model1_tx_reserved[3];
 __u32 Model1_rx_filters;
 __u32 Model1_rx_reserved[3];
};

/*
 * %ETHTOOL_SFEATURES changes features present in features[].valid to the
 * values of corresponding bits in features[].requested. Bits in .requested
 * not set in .valid or not changeable are ignored.
 *
 * Returns %EINVAL when .valid contains undefined or never-changeable bits
 * or size is not equal to required number of features words (32-bit blocks).
 * Returns >= 0 if request was completed; bits set in the value mean:
 *   %ETHTOOL_F_UNSUPPORTED - there were bits set in .valid that are not
 *	changeable (not present in %ETHTOOL_GFEATURES' features[].available)
 *	those bits were ignored.
 *   %ETHTOOL_F_WISH - some or all changes requested were recorded but the
 *      resulting state of bits masked by .valid is not equal to .requested.
 *      Probably there are other device-specific constraints on some features
 *      in the set. When %ETHTOOL_F_UNSUPPORTED is set, .valid is considered
 *      here as though ignored bits were cleared.
 *   %ETHTOOL_F_COMPAT - some or all changes requested were made by calling
 *      compatibility functions. Requested offload state cannot be properly
 *      managed by kernel.
 *
 * Meaning of bits in the masks are obtained by %ETHTOOL_GSSET_INFO (number of
 * bits in the arrays - always multiple of 32) and %ETHTOOL_GSTRINGS commands
 * for ETH_SS_FEATURES string set. First entry in the table corresponds to least
 * significant bit in features[0] fields. Empty strings mark undefined features.
 */
enum Model1_ethtool_sfeatures_retval_bits {
 Model1_ETHTOOL_F_UNSUPPORTED__BIT,
 Model1_ETHTOOL_F_WISH__BIT,
 Model1_ETHTOOL_F_COMPAT__BIT,
};







/**
 * struct ethtool_per_queue_op - apply sub command to the queues in mask.
 * @cmd: ETHTOOL_PERQUEUE
 * @sub_command: the sub command which apply to each queues
 * @queue_mask: Bitmap of the queues which sub command apply to
 * @data: A complete command structure following for each of the queues addressed
 */
struct Model1_ethtool_per_queue_op {
 __u32 Model1_cmd;
 __u32 Model1_sub_command;
 __u32 Model1_queue_mask[(((4096) + (32) - 1) / (32))];
 char Model1_data[];
};

/* CMDs currently supported */
/* Get link status for host, i.e. whether the interface *and* the
 * physical port (if there is one) are up (ethtool_value). */
/* compatibility with older code */



/* Link mode bit indices */
enum Model1_ethtool_link_mode_bit_indices {
 Model1_ETHTOOL_LINK_MODE_10baseT_Half_BIT = 0,
 Model1_ETHTOOL_LINK_MODE_10baseT_Full_BIT = 1,
 Model1_ETHTOOL_LINK_MODE_100baseT_Half_BIT = 2,
 Model1_ETHTOOL_LINK_MODE_100baseT_Full_BIT = 3,
 Model1_ETHTOOL_LINK_MODE_1000baseT_Half_BIT = 4,
 Model1_ETHTOOL_LINK_MODE_1000baseT_Full_BIT = 5,
 Model1_ETHTOOL_LINK_MODE_Autoneg_BIT = 6,
 Model1_ETHTOOL_LINK_MODE_TP_BIT = 7,
 Model1_ETHTOOL_LINK_MODE_AUI_BIT = 8,
 Model1_ETHTOOL_LINK_MODE_MII_BIT = 9,
 Model1_ETHTOOL_LINK_MODE_FIBRE_BIT = 10,
 Model1_ETHTOOL_LINK_MODE_BNC_BIT = 11,
 Model1_ETHTOOL_LINK_MODE_10000baseT_Full_BIT = 12,
 Model1_ETHTOOL_LINK_MODE_Pause_BIT = 13,
 Model1_ETHTOOL_LINK_MODE_Asym_Pause_BIT = 14,
 Model1_ETHTOOL_LINK_MODE_2500baseX_Full_BIT = 15,
 Model1_ETHTOOL_LINK_MODE_Backplane_BIT = 16,
 Model1_ETHTOOL_LINK_MODE_1000baseKX_Full_BIT = 17,
 Model1_ETHTOOL_LINK_MODE_10000baseKX4_Full_BIT = 18,
 Model1_ETHTOOL_LINK_MODE_10000baseKR_Full_BIT = 19,
 Model1_ETHTOOL_LINK_MODE_10000baseR_FEC_BIT = 20,
 Model1_ETHTOOL_LINK_MODE_20000baseMLD2_Full_BIT = 21,
 Model1_ETHTOOL_LINK_MODE_20000baseKR2_Full_BIT = 22,
 Model1_ETHTOOL_LINK_MODE_40000baseKR4_Full_BIT = 23,
 Model1_ETHTOOL_LINK_MODE_40000baseCR4_Full_BIT = 24,
 Model1_ETHTOOL_LINK_MODE_40000baseSR4_Full_BIT = 25,
 Model1_ETHTOOL_LINK_MODE_40000baseLR4_Full_BIT = 26,
 Model1_ETHTOOL_LINK_MODE_56000baseKR4_Full_BIT = 27,
 Model1_ETHTOOL_LINK_MODE_56000baseCR4_Full_BIT = 28,
 Model1_ETHTOOL_LINK_MODE_56000baseSR4_Full_BIT = 29,
 Model1_ETHTOOL_LINK_MODE_56000baseLR4_Full_BIT = 30,
 Model1_ETHTOOL_LINK_MODE_25000baseCR_Full_BIT = 31,
 Model1_ETHTOOL_LINK_MODE_25000baseKR_Full_BIT = 32,
 Model1_ETHTOOL_LINK_MODE_25000baseSR_Full_BIT = 33,
 Model1_ETHTOOL_LINK_MODE_50000baseCR2_Full_BIT = 34,
 Model1_ETHTOOL_LINK_MODE_50000baseKR2_Full_BIT = 35,
 Model1_ETHTOOL_LINK_MODE_100000baseKR4_Full_BIT = 36,
 Model1_ETHTOOL_LINK_MODE_100000baseSR4_Full_BIT = 37,
 Model1_ETHTOOL_LINK_MODE_100000baseCR4_Full_BIT = 38,
 Model1_ETHTOOL_LINK_MODE_100000baseLR4_ER4_Full_BIT = 39,
 Model1_ETHTOOL_LINK_MODE_50000baseSR2_Full_BIT = 40,

 /* Last allowed bit for __ETHTOOL_LINK_MODE_LEGACY_MASK is bit
	 * 31. Please do NOT define any SUPPORTED_* or ADVERTISED_*
	 * macro for bits > 31. The only way to use indices > 31 is to
	 * use the new ETHTOOL_GLINKSETTINGS/ETHTOOL_SLINKSETTINGS API.
	 */

 Model1___ETHTOOL_LINK_MODE_LAST
   = Model1_ETHTOOL_LINK_MODE_50000baseSR2_Full_BIT,
};




/* DEPRECATED macros. Please migrate to
 * ETHTOOL_GLINKSETTINGS/ETHTOOL_SLINKSETTINGS API. Please do NOT
 * define any new SUPPORTED_* macro for bits > 31.
 */
/* Please do not define any new SUPPORTED_* macro for bits > 31, see
 * notice above.
 */

/*
 * DEPRECATED macros. Please migrate to
 * ETHTOOL_GLINKSETTINGS/ETHTOOL_SLINKSETTINGS API. Please do NOT
 * define any new ADERTISE_* macro for bits > 31.
 */
/* Please do not define any new ADVERTISED_* macro for bits > 31, see
 * notice above.
 */

/* The following are all involved in forcing a particular link
 * mode for the device for setting things.  When getting the
 * devices settings, these indicate the current mode and whether
 * it was forced up into this mode or autonegotiated.
 */

/* The forced speed, in units of 1Mb. All values 0 to INT_MAX are legal. */
static inline __attribute__((no_instrument_function)) int Model1_ethtool_validate_speed(__u32 Model1_speed)
{
 return Model1_speed <= ((int)(~0U>>1)) || Model1_speed == -1;
}

/* Duplex, half or full. */




static inline __attribute__((no_instrument_function)) int Model1_ethtool_validate_duplex(__u8 Model1_duplex)
{
 switch (Model1_duplex) {
 case 0x00:
 case 0x01:
 case 0xff:
  return 1;
 }

 return 0;
}

/* Which connector port. */
/* Which transceiver to use. */






/* Enable or disable autonegotiation. */



/* MDI or MDI-X status/control - if MDI/MDI_X/AUTO is set then
 * the driver is required to renegotiate link
 */





/* Wake-On-Lan options. */
/* L2-L4 network traffic flow types */
/* Flag to enable additional fields in struct ethtool_rx_flow_spec */



/* L3-L4 network traffic flow hash options */
/* Special RX classification rule insert location values */





/* EEPROM Standards for plug in modules */
/* Reset flags */
/* The reset() operation must clear the flags for the components which
 * were actually reset.  On successful return, the flags indicate the
 * components which were not reset, either because they do not exist
 * in the hardware or because they cannot be reset independently.  The
 * driver must never reset any components that were not requested.
 */
enum Model1_ethtool_reset_flags {
 /* These flags represent components dedicated to the interface
	 * the command is addressed to.  Shift any flag left by
	 * ETH_RESET_SHARED_SHIFT to reset a shared component of the
	 * same type.
	 */
 Model1_ETH_RESET_MGMT = 1 << 0, /* Management processor */
 Model1_ETH_RESET_IRQ = 1 << 1, /* Interrupt requester */
 Model1_ETH_RESET_DMA = 1 << 2, /* DMA engine */
 Model1_ETH_RESET_FILTER = 1 << 3, /* Filtering/flow direction */
 Model1_ETH_RESET_OFFLOAD = 1 << 4, /* Protocol offload */
 Model1_ETH_RESET_MAC = 1 << 5, /* Media access controller */
 Model1_ETH_RESET_PHY = 1 << 6, /* Transceiver/PHY */
 Model1_ETH_RESET_RAM = 1 << 7, /* RAM shared between
						 * multiple components */

 Model1_ETH_RESET_DEDICATED = 0x0000ffff, /* All components dedicated to
						 * this interface */
 Model1_ETH_RESET_ALL = 0xffffffff, /* All components used by this
						 * interface, even if shared */
};



/**
 * struct ethtool_link_settings - link control and status
 *
 * IMPORTANT, Backward compatibility notice: When implementing new
 *	user-space tools, please first try %ETHTOOL_GLINKSETTINGS, and
 *	if it succeeds use %ETHTOOL_SLINKSETTINGS to change link
 *	settings; do not use %ETHTOOL_SSET if %ETHTOOL_GLINKSETTINGS
 *	succeeded: stick to %ETHTOOL_GLINKSETTINGS/%SLINKSETTINGS in
 *	that case.  Conversely, if %ETHTOOL_GLINKSETTINGS fails, use
 *	%ETHTOOL_GSET to query and %ETHTOOL_SSET to change link
 *	settings; do not use %ETHTOOL_SLINKSETTINGS if
 *	%ETHTOOL_GLINKSETTINGS failed: stick to
 *	%ETHTOOL_GSET/%ETHTOOL_SSET in that case.
 *
 * @cmd: Command number = %ETHTOOL_GLINKSETTINGS or %ETHTOOL_SLINKSETTINGS
 * @speed: Link speed (Mbps)
 * @duplex: Duplex mode; one of %DUPLEX_*
 * @port: Physical connector type; one of %PORT_*
 * @phy_address: MDIO address of PHY (transceiver); 0 or 255 if not
 *	applicable.  For clause 45 PHYs this is the PRTAD.
 * @autoneg: Enable/disable autonegotiation and auto-detection;
 *	either %AUTONEG_DISABLE or %AUTONEG_ENABLE
 * @mdio_support: Bitmask of %ETH_MDIO_SUPPORTS_* flags for the MDIO
 *	protocols supported by the interface; 0 if unknown.
 *	Read-only.
 * @eth_tp_mdix: Ethernet twisted-pair MDI(-X) status; one of
 *	%ETH_TP_MDI_*.  If the status is unknown or not applicable, the
 *	value will be %ETH_TP_MDI_INVALID.  Read-only.
 * @eth_tp_mdix_ctrl: Ethernet twisted pair MDI(-X) control; one of
 *	%ETH_TP_MDI_*.  If MDI(-X) control is not implemented, reads
 *	yield %ETH_TP_MDI_INVALID and writes may be ignored or rejected.
 *	When written successfully, the link should be renegotiated if
 *	necessary.
 * @link_mode_masks_nwords: Number of 32-bit words for each of the
 *	supported, advertising, lp_advertising link mode bitmaps. For
 *	%ETHTOOL_GLINKSETTINGS: on entry, number of words passed by user
 *	(>= 0); on return, if handshake in progress, negative if
 *	request size unsupported by kernel: absolute value indicates
 *	kernel expected size and all the other fields but cmd
 *	are 0; otherwise (handshake completed), strictly positive
 *	to indicate size used by kernel and cmd field stays
 *	%ETHTOOL_GLINKSETTINGS, all other fields populated by driver. For
 *	%ETHTOOL_SLINKSETTINGS: must be valid on entry, ie. a positive
 *	value returned previously by %ETHTOOL_GLINKSETTINGS, otherwise
 *	refused. For drivers: ignore this field (use kernel's
 *	__ETHTOOL_LINK_MODE_MASK_NBITS instead), any change to it will
 *	be overwritten by kernel.
 * @supported: Bitmap with each bit meaning given by
 *	%ethtool_link_mode_bit_indices for the link modes, physical
 *	connectors and other link features for which the interface
 *	supports autonegotiation or auto-detection.  Read-only.
 * @advertising: Bitmap with each bit meaning given by
 *	%ethtool_link_mode_bit_indices for the link modes, physical
 *	connectors and other link features that are advertised through
 *	autonegotiation or enabled for auto-detection.
 * @lp_advertising: Bitmap with each bit meaning given by
 *	%ethtool_link_mode_bit_indices for the link modes, and other
 *	link features that the link partner advertised through
 *	autonegotiation; 0 if unknown or not applicable.  Read-only.
 *
 * If autonegotiation is disabled, the speed and @duplex represent the
 * fixed link mode and are writable if the driver supports multiple
 * link modes.  If it is enabled then they are read-only; if the link
 * is up they represent the negotiated link mode; if the link is down,
 * the speed is 0, %SPEED_UNKNOWN or the highest enabled speed and
 * @duplex is %DUPLEX_UNKNOWN or the best enabled duplex mode.
 *
 * Some hardware interfaces may have multiple PHYs and/or physical
 * connectors fitted or do not allow the driver to detect which are
 * fitted.  For these interfaces @port and/or @phy_address may be
 * writable, possibly dependent on @autoneg being %AUTONEG_DISABLE.
 * Otherwise, attempts to write different values may be ignored or
 * rejected.
 *
 * Deprecated %ethtool_cmd fields transceiver, maxtxpkt and maxrxpkt
 * are not available in %ethtool_link_settings. Until all drivers are
 * converted to ignore them or to the new %ethtool_link_settings API,
 * for both queries and changes, users should always try
 * %ETHTOOL_GLINKSETTINGS first, and if it fails with -ENOTSUPP stick
 * only to %ETHTOOL_GSET and %ETHTOOL_SSET consistently. If it
 * succeeds, then users should stick to %ETHTOOL_GLINKSETTINGS and
 * %ETHTOOL_SLINKSETTINGS (which would support drivers implementing
 * either %ethtool_cmd or %ethtool_link_settings).
 *
 * Users should assume that all fields not marked read-only are
 * writable and subject to validation by the driver.  They should use
 * %ETHTOOL_GLINKSETTINGS to get the current values before making specific
 * changes and then applying them with %ETHTOOL_SLINKSETTINGS.
 *
 * Drivers that implement %get_link_ksettings and/or
 * %set_link_ksettings should ignore the @cmd
 * and @link_mode_masks_nwords fields (any change to them overwritten
 * by kernel), and rely only on kernel's internal
 * %__ETHTOOL_LINK_MODE_MASK_NBITS and
 * %ethtool_link_mode_mask_t. Drivers that implement
 * %set_link_ksettings() should validate all fields other than @cmd
 * and @link_mode_masks_nwords that are not described as read-only or
 * deprecated, and must ignore all fields described as read-only.
 */
struct Model1_ethtool_link_settings {
 __u32 Model1_cmd;
 __u32 Model1_speed;
 __u8 Model1_duplex;
 __u8 Model1_port;
 __u8 Model1_phy_address;
 __u8 Model1_autoneg;
 __u8 Model1_mdio_support;
 __u8 Model1_eth_tp_mdix;
 __u8 Model1_eth_tp_mdix_ctrl;
 Model1___s8 Model1_link_mode_masks_nwords;
 __u32 Model1_reserved[8];
 __u32 Model1_link_mode_masks[0];
 /* layout of link_mode_masks fields:
	 * __u32 map_supported[link_mode_masks_nwords];
	 * __u32 map_advertising[link_mode_masks_nwords];
	 * __u32 map_lp_advertising[link_mode_masks_nwords];
	 */
};



struct Model1_compat_ethtool_rx_flow_spec {
 Model1_u32 Model1_flow_type;
 union Model1_ethtool_flow_union Model1_h_u;
 struct Model1_ethtool_flow_ext Model1_h_ext;
 union Model1_ethtool_flow_union Model1_m_u;
 struct Model1_ethtool_flow_ext Model1_m_ext;
 Model1_compat_u64 Model1_ring_cookie;
 Model1_u32 Model1_location;
};

struct Model1_compat_ethtool_rxnfc {
 Model1_u32 Model1_cmd;
 Model1_u32 Model1_flow_type;
 Model1_compat_u64 Model1_data;
 struct Model1_compat_ethtool_rx_flow_spec Model1_fs;
 Model1_u32 Model1_rule_cnt;
 Model1_u32 Model1_rule_locs[0];
};





/**
 * enum ethtool_phys_id_state - indicator state for physical identification
 * @ETHTOOL_ID_INACTIVE: Physical ID indicator should be deactivated
 * @ETHTOOL_ID_ACTIVE: Physical ID indicator should be activated
 * @ETHTOOL_ID_ON: LED should be turned on (used iff %ETHTOOL_ID_ACTIVE
 *	is not supported)
 * @ETHTOOL_ID_OFF: LED should be turned off (used iff %ETHTOOL_ID_ACTIVE
 *	is not supported)
 */
enum Model1_ethtool_phys_id_state {
 Model1_ETHTOOL_ID_INACTIVE,
 Model1_ETHTOOL_ID_ACTIVE,
 Model1_ETHTOOL_ID_ON,
 Model1_ETHTOOL_ID_OFF
};

enum {
 Model1_ETH_RSS_HASH_TOP_BIT, /* Configurable RSS hash function - Toeplitz */
 Model1_ETH_RSS_HASH_XOR_BIT, /* Configurable RSS hash function - Xor */

 /*
	 * Add your fresh new hash function bits above and remember to update
	 * rss_hash_func_strings[] in ethtool.c
	 */
 Model1_ETH_RSS_HASH_FUNCS_COUNT
};
struct Model1_net_device;

/* Some generic methods drivers may use in their ethtool_ops */
Model1_u32 Model1_ethtool_op_get_link(struct Model1_net_device *Model1_dev);
int Model1_ethtool_op_get_ts_info(struct Model1_net_device *Model1_dev, struct Model1_ethtool_ts_info *Model1_eti);

/**
 * ethtool_rxfh_indir_default - get default value for RX flow hash indirection
 * @index: Index in RX flow hash indirection table
 * @n_rx_rings: Number of RX rings to use
 *
 * This function provides the default policy for RX flow hash indirection.
 */
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_ethtool_rxfh_indir_default(Model1_u32 Model1_index, Model1_u32 Model1_n_rx_rings)
{
 return Model1_index % Model1_n_rx_rings;
}

/* number of link mode bits/ulongs handled internally by kernel */



/* declare a link mode bitmap */



/* drivers must ignore base.cmd and base.link_mode_masks_nwords
 * fields, but they are allowed to overwrite them (will be ignored).
 */
struct Model1_ethtool_link_ksettings {
 struct Model1_ethtool_link_settings Model1_base;
 struct {
  unsigned long Model1_supported[((((Model1___ETHTOOL_LINK_MODE_LAST + 1)) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];
  unsigned long Model1_advertising[((((Model1___ETHTOOL_LINK_MODE_LAST + 1)) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];
  unsigned long Model1_lp_advertising[((((Model1___ETHTOOL_LINK_MODE_LAST + 1)) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];
 } Model1_link_modes;
};

/**
 * ethtool_link_ksettings_zero_link_mode - clear link_ksettings link mode mask
 *   @ptr : pointer to struct ethtool_link_ksettings
 *   @name : one of supported/advertising/lp_advertising
 */



/**
 * ethtool_link_ksettings_add_link_mode - set bit in link_ksettings
 * link mode mask
 *   @ptr : pointer to struct ethtool_link_ksettings
 *   @name : one of supported/advertising/lp_advertising
 *   @mode : one of the ETHTOOL_LINK_MODE_*_BIT
 * (not atomic, no bound checking)
 */



/**
 * ethtool_link_ksettings_test_link_mode - test bit in ksettings link mode mask
 *   @ptr : pointer to struct ethtool_link_ksettings
 *   @name : one of supported/advertising/lp_advertising
 *   @mode : one of the ETHTOOL_LINK_MODE_*_BIT
 * (not atomic, no bound checking)
 *
 * Returns true/false.
 */



extern int
Model1___ethtool_get_link_ksettings(struct Model1_net_device *Model1_dev,
        struct Model1_ethtool_link_ksettings *Model1_link_ksettings);

void Model1_ethtool_convert_legacy_u32_to_link_mode(unsigned long *Model1_dst,
          Model1_u32 Model1_legacy_u32);

/* return false if src had higher bits set. lower bits always updated. */
bool Model1_ethtool_convert_link_mode_to_legacy_u32(Model1_u32 *Model1_legacy_u32,
         const unsigned long *Model1_src);

/**
 * struct ethtool_ops - optional netdev operations
 * @get_settings: DEPRECATED, use %get_link_ksettings/%set_link_ksettings
 *	API. Get various device settings including Ethernet link
 *	settings. The @cmd parameter is expected to have been cleared
 *	before get_settings is called. Returns a negative error code
 *	or zero.
 * @set_settings: DEPRECATED, use %get_link_ksettings/%set_link_ksettings
 *	API. Set various device settings including Ethernet link
 *	settings.  Returns a negative error code or zero.
 * @get_drvinfo: Report driver/device information.  Should only set the
 *	@driver, @version, @fw_version and @bus_info fields.  If not
 *	implemented, the @driver and @bus_info fields will be filled in
 *	according to the netdev's parent device.
 * @get_regs_len: Get buffer length required for @get_regs
 * @get_regs: Get device registers
 * @get_wol: Report whether Wake-on-Lan is enabled
 * @set_wol: Turn Wake-on-Lan on or off.  Returns a negative error code
 *	or zero.
 * @get_msglevel: Report driver message level.  This should be the value
 *	of the @msg_enable field used by netif logging functions.
 * @set_msglevel: Set driver message level
 * @nway_reset: Restart autonegotiation.  Returns a negative error code
 *	or zero.
 * @get_link: Report whether physical link is up.  Will only be called if
 *	the netdev is up.  Should usually be set to ethtool_op_get_link(),
 *	which uses netif_carrier_ok().
 * @get_eeprom: Read data from the device EEPROM.
 *	Should fill in the magic field.  Don't need to check len for zero
 *	or wraparound.  Fill in the data argument with the eeprom values
 *	from offset to offset + len.  Update len to the amount read.
 *	Returns an error or zero.
 * @set_eeprom: Write data to the device EEPROM.
 *	Should validate the magic field.  Don't need to check len for zero
 *	or wraparound.  Update len to the amount written.  Returns an error
 *	or zero.
 * @get_coalesce: Get interrupt coalescing parameters.  Returns a negative
 *	error code or zero.
 * @set_coalesce: Set interrupt coalescing parameters.  Returns a negative
 *	error code or zero.
 * @get_ringparam: Report ring sizes
 * @set_ringparam: Set ring sizes.  Returns a negative error code or zero.
 * @get_pauseparam: Report pause parameters
 * @set_pauseparam: Set pause parameters.  Returns a negative error code
 *	or zero.
 * @self_test: Run specified self-tests
 * @get_strings: Return a set of strings that describe the requested objects
 * @set_phys_id: Identify the physical devices, e.g. by flashing an LED
 *	attached to it.  The implementation may update the indicator
 *	asynchronously or synchronously, but in either case it must return
 *	quickly.  It is initially called with the argument %ETHTOOL_ID_ACTIVE,
 *	and must either activate asynchronous updates and return zero, return
 *	a negative error or return a positive frequency for synchronous
 *	indication (e.g. 1 for one on/off cycle per second).  If it returns
 *	a frequency then it will be called again at intervals with the
 *	argument %ETHTOOL_ID_ON or %ETHTOOL_ID_OFF and should set the state of
 *	the indicator accordingly.  Finally, it is called with the argument
 *	%ETHTOOL_ID_INACTIVE and must deactivate the indicator.  Returns a
 *	negative error code or zero.
 * @get_ethtool_stats: Return extended statistics about the device.
 *	This is only useful if the device maintains statistics not
 *	included in &struct rtnl_link_stats64.
 * @begin: Function to be called before any other operation.  Returns a
 *	negative error code or zero.
 * @complete: Function to be called after any other operation except
 *	@begin.  Will be called even if the other operation failed.
 * @get_priv_flags: Report driver-specific feature flags.
 * @set_priv_flags: Set driver-specific feature flags.  Returns a negative
 *	error code or zero.
 * @get_sset_count: Get number of strings that @get_strings will write.
 * @get_rxnfc: Get RX flow classification rules.  Returns a negative
 *	error code or zero.
 * @set_rxnfc: Set RX flow classification rules.  Returns a negative
 *	error code or zero.
 * @flash_device: Write a firmware image to device's flash memory.
 *	Returns a negative error code or zero.
 * @reset: Reset (part of) the device, as specified by a bitmask of
 *	flags from &enum ethtool_reset_flags.  Returns a negative
 *	error code or zero.
 * @get_rxfh_key_size: Get the size of the RX flow hash key.
 *	Returns zero if not supported for this specific device.
 * @get_rxfh_indir_size: Get the size of the RX flow hash indirection table.
 *	Returns zero if not supported for this specific device.
 * @get_rxfh: Get the contents of the RX flow hash indirection table, hash key
 *	and/or hash function.
 *	Returns a negative error code or zero.
 * @set_rxfh: Set the contents of the RX flow hash indirection table, hash
 *	key, and/or hash function.  Arguments which are set to %NULL or zero
 *	will remain unchanged.
 *	Returns a negative error code or zero. An error code must be returned
 *	if at least one unsupported change was requested.
 * @get_channels: Get number of channels.
 * @set_channels: Set number of channels.  Returns a negative error code or
 *	zero.
 * @get_dump_flag: Get dump flag indicating current dump length, version,
 * 		   and flag of the device.
 * @get_dump_data: Get dump data.
 * @set_dump: Set dump specific flags to the device.
 * @get_ts_info: Get the time stamping and PTP hardware clock capabilities.
 *	Drivers supporting transmit time stamps in software should set this to
 *	ethtool_op_get_ts_info().
 * @get_module_info: Get the size and type of the eeprom contained within
 *	a plug-in module.
 * @get_module_eeprom: Get the eeprom information from the plug-in module
 * @get_eee: Get Energy-Efficient (EEE) supported and status.
 * @set_eee: Set EEE status (enable/disable) as well as LPI timers.
 * @get_per_queue_coalesce: Get interrupt coalescing parameters per queue.
 *	It must check that the given queue number is valid. If neither a RX nor
 *	a TX queue has this number, return -EINVAL. If only a RX queue or a TX
 *	queue has this number, set the inapplicable fields to ~0 and return 0.
 *	Returns a negative error code or zero.
 * @set_per_queue_coalesce: Set interrupt coalescing parameters per queue.
 *	It must check that the given queue number is valid. If neither a RX nor
 *	a TX queue has this number, return -EINVAL. If only a RX queue or a TX
 *	queue has this number, ignore the inapplicable fields.
 *	Returns a negative error code or zero.
 * @get_link_ksettings: When defined, takes precedence over the
 *	%get_settings method. Get various device settings
 *	including Ethernet link settings. The %cmd and
 *	%link_mode_masks_nwords fields should be ignored (use
 *	%__ETHTOOL_LINK_MODE_MASK_NBITS instead of the latter), any
 *	change to them will be overwritten by kernel. Returns a
 *	negative error code or zero.
 * @set_link_ksettings: When defined, takes precedence over the
 *	%set_settings method. Set various device settings including
 *	Ethernet link settings. The %cmd and %link_mode_masks_nwords
 *	fields should be ignored (use %__ETHTOOL_LINK_MODE_MASK_NBITS
 *	instead of the latter), any change to them will be overwritten
 *	by kernel. Returns a negative error code or zero.
 *
 * All operations are optional (i.e. the function pointer may be set
 * to %NULL) and callers must take this into account.  Callers must
 * hold the RTNL lock.
 *
 * See the structures used by these operations for further documentation.
 * Note that for all operations using a structure ending with a zero-
 * length array, the array is allocated separately in the kernel and
 * is passed to the driver as an additional parameter.
 *
 * See &struct net_device and &struct net_device_ops for documentation
 * of the generic netdev features interface.
 */
struct Model1_ethtool_ops {
 int (*Model1_get_settings)(struct Model1_net_device *, struct Model1_ethtool_cmd *);
 int (*Model1_set_settings)(struct Model1_net_device *, struct Model1_ethtool_cmd *);
 void (*Model1_get_drvinfo)(struct Model1_net_device *, struct Model1_ethtool_drvinfo *);
 int (*Model1_get_regs_len)(struct Model1_net_device *);
 void (*Model1_get_regs)(struct Model1_net_device *, struct Model1_ethtool_regs *, void *);
 void (*Model1_get_wol)(struct Model1_net_device *, struct Model1_ethtool_wolinfo *);
 int (*Model1_set_wol)(struct Model1_net_device *, struct Model1_ethtool_wolinfo *);
 Model1_u32 (*Model1_get_msglevel)(struct Model1_net_device *);
 void (*Model1_set_msglevel)(struct Model1_net_device *, Model1_u32);
 int (*Model1_nway_reset)(struct Model1_net_device *);
 Model1_u32 (*Model1_get_link)(struct Model1_net_device *);
 int (*Model1_get_eeprom_len)(struct Model1_net_device *);
 int (*Model1_get_eeprom)(struct Model1_net_device *,
         struct Model1_ethtool_eeprom *, Model1_u8 *);
 int (*Model1_set_eeprom)(struct Model1_net_device *,
         struct Model1_ethtool_eeprom *, Model1_u8 *);
 int (*Model1_get_coalesce)(struct Model1_net_device *, struct Model1_ethtool_coalesce *);
 int (*Model1_set_coalesce)(struct Model1_net_device *, struct Model1_ethtool_coalesce *);
 void (*Model1_get_ringparam)(struct Model1_net_device *,
     struct Model1_ethtool_ringparam *);
 int (*Model1_set_ringparam)(struct Model1_net_device *,
     struct Model1_ethtool_ringparam *);
 void (*Model1_get_pauseparam)(struct Model1_net_device *,
      struct Model1_ethtool_pauseparam*);
 int (*Model1_set_pauseparam)(struct Model1_net_device *,
      struct Model1_ethtool_pauseparam*);
 void (*Model1_self_test)(struct Model1_net_device *, struct Model1_ethtool_test *, Model1_u64 *);
 void (*Model1_get_strings)(struct Model1_net_device *, Model1_u32 Model1_stringset, Model1_u8 *);
 int (*Model1_set_phys_id)(struct Model1_net_device *, enum Model1_ethtool_phys_id_state);
 void (*Model1_get_ethtool_stats)(struct Model1_net_device *,
         struct Model1_ethtool_stats *, Model1_u64 *);
 int (*Model1_begin)(struct Model1_net_device *);
 void (*Model1_complete)(struct Model1_net_device *);
 Model1_u32 (*Model1_get_priv_flags)(struct Model1_net_device *);
 int (*Model1_set_priv_flags)(struct Model1_net_device *, Model1_u32);
 int (*Model1_get_sset_count)(struct Model1_net_device *, int);
 int (*Model1_get_rxnfc)(struct Model1_net_device *,
        struct Model1_ethtool_rxnfc *, Model1_u32 *Model1_rule_locs);
 int (*Model1_set_rxnfc)(struct Model1_net_device *, struct Model1_ethtool_rxnfc *);
 int (*Model1_flash_device)(struct Model1_net_device *, struct Model1_ethtool_flash *);
 int (*Model1_reset)(struct Model1_net_device *, Model1_u32 *);
 Model1_u32 (*Model1_get_rxfh_key_size)(struct Model1_net_device *);
 Model1_u32 (*Model1_get_rxfh_indir_size)(struct Model1_net_device *);
 int (*Model1_get_rxfh)(struct Model1_net_device *, Model1_u32 *Model1_indir, Model1_u8 *Model1_key,
       Model1_u8 *Model1_hfunc);
 int (*Model1_set_rxfh)(struct Model1_net_device *, const Model1_u32 *Model1_indir,
       const Model1_u8 *Model1_key, const Model1_u8 Model1_hfunc);
 void (*Model1_get_channels)(struct Model1_net_device *, struct Model1_ethtool_channels *);
 int (*Model1_set_channels)(struct Model1_net_device *, struct Model1_ethtool_channels *);
 int (*Model1_get_dump_flag)(struct Model1_net_device *, struct Model1_ethtool_dump *);
 int (*Model1_get_dump_data)(struct Model1_net_device *,
     struct Model1_ethtool_dump *, void *);
 int (*Model1_set_dump)(struct Model1_net_device *, struct Model1_ethtool_dump *);
 int (*Model1_get_ts_info)(struct Model1_net_device *, struct Model1_ethtool_ts_info *);
 int (*Model1_get_module_info)(struct Model1_net_device *,
       struct Model1_ethtool_modinfo *);
 int (*Model1_get_module_eeprom)(struct Model1_net_device *,
         struct Model1_ethtool_eeprom *, Model1_u8 *);
 int (*Model1_get_eee)(struct Model1_net_device *, struct Model1_ethtool_eee *);
 int (*Model1_set_eee)(struct Model1_net_device *, struct Model1_ethtool_eee *);
 int (*Model1_get_tunable)(struct Model1_net_device *,
          const struct Model1_ethtool_tunable *, void *);
 int (*Model1_set_tunable)(struct Model1_net_device *,
          const struct Model1_ethtool_tunable *, const void *);
 int (*Model1_get_per_queue_coalesce)(struct Model1_net_device *, Model1_u32,
       struct Model1_ethtool_coalesce *);
 int (*Model1_set_per_queue_coalesce)(struct Model1_net_device *, Model1_u32,
       struct Model1_ethtool_coalesce *);
 int (*Model1_get_link_ksettings)(struct Model1_net_device *,
          struct Model1_ethtool_link_ksettings *);
 int (*Model1_set_link_ksettings)(struct Model1_net_device *,
          const struct Model1_ethtool_link_ksettings *);
};
/*
 * Operations on the network namespace
 */



struct Model1_ctl_table_header;
struct Model1_prot_inuse;

struct Model1_netns_core {
 /* core sysctls */
 struct Model1_ctl_table_header *Model1_sysctl_hdr;

 int Model1_sysctl_somaxconn;

 struct Model1_prot_inuse *Model1_inuse;
};



/*
 *
 *		SNMP MIB entries for the IP subsystem.
 *		
 *		Alan Cox <gw4pts@gw4pts.ampr.org>
 *
 *		We don't chose to implement SNMP in the kernel (this would
 *		be silly as SNMP is a pain in the backside in places). We do
 *		however need to collect the MIB statistics and export them
 *		out of /proc (eventually)
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 *
 */






/*
 * Definitions for MIBs
 *
 * Author: Hideaki YOSHIFUJI <yoshfuji@linux-ipv6.org>
 */




/* ipstats mib definitions */
/*
 * RFC 1213:  MIB-II
 * RFC 2011 (updates 1213):  SNMPv2-MIB-IP
 * RFC 2863:  Interfaces Group MIB
 * RFC 2465:  IPv6 MIB: General Group
 * draft-ietf-ipv6-rfc2011-update-10.txt: MIB for IP: IP Statistics Tables
 */
enum
{
 Model1_IPSTATS_MIB_NUM = 0,
/* frequently written fields in fast path, kept in same cache line */
 Model1_IPSTATS_MIB_INPKTS, /* InReceives */
 Model1_IPSTATS_MIB_INOCTETS, /* InOctets */
 Model1_IPSTATS_MIB_INDELIVERS, /* InDelivers */
 Model1_IPSTATS_MIB_OUTFORWDATAGRAMS, /* OutForwDatagrams */
 Model1_IPSTATS_MIB_OUTPKTS, /* OutRequests */
 Model1_IPSTATS_MIB_OUTOCTETS, /* OutOctets */
/* other fields */
 Model1_IPSTATS_MIB_INHDRERRORS, /* InHdrErrors */
 Model1_IPSTATS_MIB_INTOOBIGERRORS, /* InTooBigErrors */
 Model1_IPSTATS_MIB_INNOROUTES, /* InNoRoutes */
 Model1_IPSTATS_MIB_INADDRERRORS, /* InAddrErrors */
 Model1_IPSTATS_MIB_INUNKNOWNPROTOS, /* InUnknownProtos */
 Model1_IPSTATS_MIB_INTRUNCATEDPKTS, /* InTruncatedPkts */
 Model1_IPSTATS_MIB_INDISCARDS, /* InDiscards */
 Model1_IPSTATS_MIB_OUTDISCARDS, /* OutDiscards */
 Model1_IPSTATS_MIB_OUTNOROUTES, /* OutNoRoutes */
 Model1_IPSTATS_MIB_REASMTIMEOUT, /* ReasmTimeout */
 Model1_IPSTATS_MIB_REASMREQDS, /* ReasmReqds */
 Model1_IPSTATS_MIB_REASMOKS, /* ReasmOKs */
 Model1_IPSTATS_MIB_REASMFAILS, /* ReasmFails */
 Model1_IPSTATS_MIB_FRAGOKS, /* FragOKs */
 Model1_IPSTATS_MIB_FRAGFAILS, /* FragFails */
 Model1_IPSTATS_MIB_FRAGCREATES, /* FragCreates */
 Model1_IPSTATS_MIB_INMCASTPKTS, /* InMcastPkts */
 Model1_IPSTATS_MIB_OUTMCASTPKTS, /* OutMcastPkts */
 Model1_IPSTATS_MIB_INBCASTPKTS, /* InBcastPkts */
 Model1_IPSTATS_MIB_OUTBCASTPKTS, /* OutBcastPkts */
 Model1_IPSTATS_MIB_INMCASTOCTETS, /* InMcastOctets */
 Model1_IPSTATS_MIB_OUTMCASTOCTETS, /* OutMcastOctets */
 Model1_IPSTATS_MIB_INBCASTOCTETS, /* InBcastOctets */
 Model1_IPSTATS_MIB_OUTBCASTOCTETS, /* OutBcastOctets */
 Model1_IPSTATS_MIB_CSUMERRORS, /* InCsumErrors */
 Model1_IPSTATS_MIB_NOECTPKTS, /* InNoECTPkts */
 Model1_IPSTATS_MIB_ECT1PKTS, /* InECT1Pkts */
 Model1_IPSTATS_MIB_ECT0PKTS, /* InECT0Pkts */
 Model1_IPSTATS_MIB_CEPKTS, /* InCEPkts */
 Model1___IPSTATS_MIB_MAX
};

/* icmp mib definitions */
/*
 * RFC 1213:  MIB-II ICMP Group
 * RFC 2011 (updates 1213):  SNMPv2 MIB for IP: ICMP group
 */
enum
{
 Model1_ICMP_MIB_NUM = 0,
 Model1_ICMP_MIB_INMSGS, /* InMsgs */
 Model1_ICMP_MIB_INERRORS, /* InErrors */
 Model1_ICMP_MIB_INDESTUNREACHS, /* InDestUnreachs */
 Model1_ICMP_MIB_INTIMEEXCDS, /* InTimeExcds */
 Model1_ICMP_MIB_INPARMPROBS, /* InParmProbs */
 Model1_ICMP_MIB_INSRCQUENCHS, /* InSrcQuenchs */
 Model1_ICMP_MIB_INREDIRECTS, /* InRedirects */
 Model1_ICMP_MIB_INECHOS, /* InEchos */
 Model1_ICMP_MIB_INECHOREPS, /* InEchoReps */
 Model1_ICMP_MIB_INTIMESTAMPS, /* InTimestamps */
 Model1_ICMP_MIB_INTIMESTAMPREPS, /* InTimestampReps */
 Model1_ICMP_MIB_INADDRMASKS, /* InAddrMasks */
 Model1_ICMP_MIB_INADDRMASKREPS, /* InAddrMaskReps */
 Model1_ICMP_MIB_OUTMSGS, /* OutMsgs */
 Model1_ICMP_MIB_OUTERRORS, /* OutErrors */
 Model1_ICMP_MIB_OUTDESTUNREACHS, /* OutDestUnreachs */
 Model1_ICMP_MIB_OUTTIMEEXCDS, /* OutTimeExcds */
 Model1_ICMP_MIB_OUTPARMPROBS, /* OutParmProbs */
 Model1_ICMP_MIB_OUTSRCQUENCHS, /* OutSrcQuenchs */
 Model1_ICMP_MIB_OUTREDIRECTS, /* OutRedirects */
 Model1_ICMP_MIB_OUTECHOS, /* OutEchos */
 Model1_ICMP_MIB_OUTECHOREPS, /* OutEchoReps */
 Model1_ICMP_MIB_OUTTIMESTAMPS, /* OutTimestamps */
 Model1_ICMP_MIB_OUTTIMESTAMPREPS, /* OutTimestampReps */
 Model1_ICMP_MIB_OUTADDRMASKS, /* OutAddrMasks */
 Model1_ICMP_MIB_OUTADDRMASKREPS, /* OutAddrMaskReps */
 Model1_ICMP_MIB_CSUMERRORS, /* InCsumErrors */
 Model1___ICMP_MIB_MAX
};



/* icmp6 mib definitions */
/*
 * RFC 2466:  ICMPv6-MIB
 */
enum
{
 Model1_ICMP6_MIB_NUM = 0,
 Model1_ICMP6_MIB_INMSGS, /* InMsgs */
 Model1_ICMP6_MIB_INERRORS, /* InErrors */
 Model1_ICMP6_MIB_OUTMSGS, /* OutMsgs */
 Model1_ICMP6_MIB_OUTERRORS, /* OutErrors */
 Model1_ICMP6_MIB_CSUMERRORS, /* InCsumErrors */
 Model1___ICMP6_MIB_MAX
};



/* tcp mib definitions */
/*
 * RFC 1213:  MIB-II TCP group
 * RFC 2012 (updates 1213):  SNMPv2-MIB-TCP
 */
enum
{
 Model1_TCP_MIB_NUM = 0,
 Model1_TCP_MIB_RTOALGORITHM, /* RtoAlgorithm */
 Model1_TCP_MIB_RTOMIN, /* RtoMin */
 Model1_TCP_MIB_RTOMAX, /* RtoMax */
 Model1_TCP_MIB_MAXCONN, /* MaxConn */
 Model1_TCP_MIB_ACTIVEOPENS, /* ActiveOpens */
 Model1_TCP_MIB_PASSIVEOPENS, /* PassiveOpens */
 Model1_TCP_MIB_ATTEMPTFAILS, /* AttemptFails */
 Model1_TCP_MIB_ESTABRESETS, /* EstabResets */
 Model1_TCP_MIB_CURRESTAB, /* CurrEstab */
 Model1_TCP_MIB_INSEGS, /* InSegs */
 Model1_TCP_MIB_OUTSEGS, /* OutSegs */
 Model1_TCP_MIB_RETRANSSEGS, /* RetransSegs */
 Model1_TCP_MIB_INERRS, /* InErrs */
 Model1_TCP_MIB_OUTRSTS, /* OutRsts */
 Model1_TCP_MIB_CSUMERRORS, /* InCsumErrors */
 Model1___TCP_MIB_MAX
};

/* udp mib definitions */
/*
 * RFC 1213:  MIB-II UDP group
 * RFC 2013 (updates 1213):  SNMPv2-MIB-UDP
 */
enum
{
 Model1_UDP_MIB_NUM = 0,
 Model1_UDP_MIB_INDATAGRAMS, /* InDatagrams */
 Model1_UDP_MIB_NOPORTS, /* NoPorts */
 Model1_UDP_MIB_INERRORS, /* InErrors */
 Model1_UDP_MIB_OUTDATAGRAMS, /* OutDatagrams */
 Model1_UDP_MIB_RCVBUFERRORS, /* RcvbufErrors */
 Model1_UDP_MIB_SNDBUFERRORS, /* SndbufErrors */
 Model1_UDP_MIB_CSUMERRORS, /* InCsumErrors */
 Model1_UDP_MIB_IGNOREDMULTI, /* IgnoredMulti */
 Model1___UDP_MIB_MAX
};

/* linux mib definitions */
enum
{
 Model1_LINUX_MIB_NUM = 0,
 Model1_LINUX_MIB_SYNCOOKIESSENT, /* SyncookiesSent */
 Model1_LINUX_MIB_SYNCOOKIESRECV, /* SyncookiesRecv */
 Model1_LINUX_MIB_SYNCOOKIESFAILED, /* SyncookiesFailed */
 Model1_LINUX_MIB_EMBRYONICRSTS, /* EmbryonicRsts */
 Model1_LINUX_MIB_PRUNECALLED, /* PruneCalled */
 Model1_LINUX_MIB_RCVPRUNED, /* RcvPruned */
 Model1_LINUX_MIB_OFOPRUNED, /* OfoPruned */
 Model1_LINUX_MIB_OUTOFWINDOWICMPS, /* OutOfWindowIcmps */
 Model1_LINUX_MIB_LOCKDROPPEDICMPS, /* LockDroppedIcmps */
 Model1_LINUX_MIB_ARPFILTER, /* ArpFilter */
 Model1_LINUX_MIB_TIMEWAITED, /* TimeWaited */
 Model1_LINUX_MIB_TIMEWAITRECYCLED, /* TimeWaitRecycled */
 Model1_LINUX_MIB_TIMEWAITKILLED, /* TimeWaitKilled */
 Model1_LINUX_MIB_PAWSPASSIVEREJECTED, /* PAWSPassiveRejected */
 Model1_LINUX_MIB_PAWSACTIVEREJECTED, /* PAWSActiveRejected */
 Model1_LINUX_MIB_PAWSESTABREJECTED, /* PAWSEstabRejected */
 Model1_LINUX_MIB_DELAYEDACKS, /* DelayedACKs */
 Model1_LINUX_MIB_DELAYEDACKLOCKED, /* DelayedACKLocked */
 Model1_LINUX_MIB_DELAYEDACKLOST, /* DelayedACKLost */
 Model1_LINUX_MIB_LISTENOVERFLOWS, /* ListenOverflows */
 Model1_LINUX_MIB_LISTENDROPS, /* ListenDrops */
 Model1_LINUX_MIB_TCPPREQUEUED, /* TCPPrequeued */
 Model1_LINUX_MIB_TCPDIRECTCOPYFROMBACKLOG, /* TCPDirectCopyFromBacklog */
 Model1_LINUX_MIB_TCPDIRECTCOPYFROMPREQUEUE, /* TCPDirectCopyFromPrequeue */
 Model1_LINUX_MIB_TCPPREQUEUEDROPPED, /* TCPPrequeueDropped */
 Model1_LINUX_MIB_TCPHPHITS, /* TCPHPHits */
 Model1_LINUX_MIB_TCPHPHITSTOUSER, /* TCPHPHitsToUser */
 Model1_LINUX_MIB_TCPPUREACKS, /* TCPPureAcks */
 Model1_LINUX_MIB_TCPHPACKS, /* TCPHPAcks */
 Model1_LINUX_MIB_TCPRENORECOVERY, /* TCPRenoRecovery */
 Model1_LINUX_MIB_TCPSACKRECOVERY, /* TCPSackRecovery */
 Model1_LINUX_MIB_TCPSACKRENEGING, /* TCPSACKReneging */
 Model1_LINUX_MIB_TCPFACKREORDER, /* TCPFACKReorder */
 Model1_LINUX_MIB_TCPSACKREORDER, /* TCPSACKReorder */
 Model1_LINUX_MIB_TCPRENOREORDER, /* TCPRenoReorder */
 Model1_LINUX_MIB_TCPTSREORDER, /* TCPTSReorder */
 Model1_LINUX_MIB_TCPFULLUNDO, /* TCPFullUndo */
 Model1_LINUX_MIB_TCPPARTIALUNDO, /* TCPPartialUndo */
 Model1_LINUX_MIB_TCPDSACKUNDO, /* TCPDSACKUndo */
 Model1_LINUX_MIB_TCPLOSSUNDO, /* TCPLossUndo */
 Model1_LINUX_MIB_TCPLOSTRETRANSMIT, /* TCPLostRetransmit */
 Model1_LINUX_MIB_TCPRENOFAILURES, /* TCPRenoFailures */
 Model1_LINUX_MIB_TCPSACKFAILURES, /* TCPSackFailures */
 Model1_LINUX_MIB_TCPLOSSFAILURES, /* TCPLossFailures */
 Model1_LINUX_MIB_TCPFASTRETRANS, /* TCPFastRetrans */
 Model1_LINUX_MIB_TCPFORWARDRETRANS, /* TCPForwardRetrans */
 Model1_LINUX_MIB_TCPSLOWSTARTRETRANS, /* TCPSlowStartRetrans */
 Model1_LINUX_MIB_TCPTIMEOUTS, /* TCPTimeouts */
 Model1_LINUX_MIB_TCPLOSSPROBES, /* TCPLossProbes */
 Model1_LINUX_MIB_TCPLOSSPROBERECOVERY, /* TCPLossProbeRecovery */
 Model1_LINUX_MIB_TCPRENORECOVERYFAIL, /* TCPRenoRecoveryFail */
 Model1_LINUX_MIB_TCPSACKRECOVERYFAIL, /* TCPSackRecoveryFail */
 Model1_LINUX_MIB_TCPSCHEDULERFAILED, /* TCPSchedulerFailed */
 Model1_LINUX_MIB_TCPRCVCOLLAPSED, /* TCPRcvCollapsed */
 Model1_LINUX_MIB_TCPDSACKOLDSENT, /* TCPDSACKOldSent */
 Model1_LINUX_MIB_TCPDSACKOFOSENT, /* TCPDSACKOfoSent */
 Model1_LINUX_MIB_TCPDSACKRECV, /* TCPDSACKRecv */
 Model1_LINUX_MIB_TCPDSACKOFORECV, /* TCPDSACKOfoRecv */
 Model1_LINUX_MIB_TCPABORTONDATA, /* TCPAbortOnData */
 Model1_LINUX_MIB_TCPABORTONCLOSE, /* TCPAbortOnClose */
 Model1_LINUX_MIB_TCPABORTONMEMORY, /* TCPAbortOnMemory */
 Model1_LINUX_MIB_TCPABORTONTIMEOUT, /* TCPAbortOnTimeout */
 Model1_LINUX_MIB_TCPABORTONLINGER, /* TCPAbortOnLinger */
 Model1_LINUX_MIB_TCPABORTFAILED, /* TCPAbortFailed */
 Model1_LINUX_MIB_TCPMEMORYPRESSURES, /* TCPMemoryPressures */
 Model1_LINUX_MIB_TCPSACKDISCARD, /* TCPSACKDiscard */
 Model1_LINUX_MIB_TCPDSACKIGNOREDOLD, /* TCPSACKIgnoredOld */
 Model1_LINUX_MIB_TCPDSACKIGNOREDNOUNDO, /* TCPSACKIgnoredNoUndo */
 Model1_LINUX_MIB_TCPSPURIOUSRTOS, /* TCPSpuriousRTOs */
 Model1_LINUX_MIB_TCPMD5NOTFOUND, /* TCPMD5NotFound */
 Model1_LINUX_MIB_TCPMD5UNEXPECTED, /* TCPMD5Unexpected */
 Model1_LINUX_MIB_SACKSHIFTED,
 Model1_LINUX_MIB_SACKMERGED,
 Model1_LINUX_MIB_SACKSHIFTFALLBACK,
 Model1_LINUX_MIB_TCPBACKLOGDROP,
 Model1_LINUX_MIB_TCPMINTTLDROP, /* RFC 5082 */
 Model1_LINUX_MIB_TCPDEFERACCEPTDROP,
 Model1_LINUX_MIB_IPRPFILTER, /* IP Reverse Path Filter (rp_filter) */
 Model1_LINUX_MIB_TCPTIMEWAITOVERFLOW, /* TCPTimeWaitOverflow */
 Model1_LINUX_MIB_TCPREQQFULLDOCOOKIES, /* TCPReqQFullDoCookies */
 Model1_LINUX_MIB_TCPREQQFULLDROP, /* TCPReqQFullDrop */
 Model1_LINUX_MIB_TCPRETRANSFAIL, /* TCPRetransFail */
 Model1_LINUX_MIB_TCPRCVCOALESCE, /* TCPRcvCoalesce */
 Model1_LINUX_MIB_TCPOFOQUEUE, /* TCPOFOQueue */
 Model1_LINUX_MIB_TCPOFODROP, /* TCPOFODrop */
 Model1_LINUX_MIB_TCPOFOMERGE, /* TCPOFOMerge */
 Model1_LINUX_MIB_TCPCHALLENGEACK, /* TCPChallengeACK */
 Model1_LINUX_MIB_TCPSYNCHALLENGE, /* TCPSYNChallenge */
 Model1_LINUX_MIB_TCPFASTOPENACTIVE, /* TCPFastOpenActive */
 Model1_LINUX_MIB_TCPFASTOPENACTIVEFAIL, /* TCPFastOpenActiveFail */
 Model1_LINUX_MIB_TCPFASTOPENPASSIVE, /* TCPFastOpenPassive*/
 Model1_LINUX_MIB_TCPFASTOPENPASSIVEFAIL, /* TCPFastOpenPassiveFail */
 Model1_LINUX_MIB_TCPFASTOPENLISTENOVERFLOW, /* TCPFastOpenListenOverflow */
 Model1_LINUX_MIB_TCPFASTOPENCOOKIEREQD, /* TCPFastOpenCookieReqd */
 Model1_LINUX_MIB_TCPSPURIOUS_RTX_HOSTQUEUES, /* TCPSpuriousRtxHostQueues */
 Model1_LINUX_MIB_BUSYPOLLRXPACKETS, /* BusyPollRxPackets */
 Model1_LINUX_MIB_TCPAUTOCORKING, /* TCPAutoCorking */
 Model1_LINUX_MIB_TCPFROMZEROWINDOWADV, /* TCPFromZeroWindowAdv */
 Model1_LINUX_MIB_TCPTOZEROWINDOWADV, /* TCPToZeroWindowAdv */
 Model1_LINUX_MIB_TCPWANTZEROWINDOWADV, /* TCPWantZeroWindowAdv */
 Model1_LINUX_MIB_TCPSYNRETRANS, /* TCPSynRetrans */
 Model1_LINUX_MIB_TCPORIGDATASENT, /* TCPOrigDataSent */
 Model1_LINUX_MIB_TCPHYSTARTTRAINDETECT, /* TCPHystartTrainDetect */
 Model1_LINUX_MIB_TCPHYSTARTTRAINCWND, /* TCPHystartTrainCwnd */
 Model1_LINUX_MIB_TCPHYSTARTDELAYDETECT, /* TCPHystartDelayDetect */
 Model1_LINUX_MIB_TCPHYSTARTDELAYCWND, /* TCPHystartDelayCwnd */
 Model1_LINUX_MIB_TCPACKSKIPPEDSYNRECV, /* TCPACKSkippedSynRecv */
 Model1_LINUX_MIB_TCPACKSKIPPEDPAWS, /* TCPACKSkippedPAWS */
 Model1_LINUX_MIB_TCPACKSKIPPEDSEQ, /* TCPACKSkippedSeq */
 Model1_LINUX_MIB_TCPACKSKIPPEDFINWAIT2, /* TCPACKSkippedFinWait2 */
 Model1_LINUX_MIB_TCPACKSKIPPEDTIMEWAIT, /* TCPACKSkippedTimeWait */
 Model1_LINUX_MIB_TCPACKSKIPPEDCHALLENGE, /* TCPACKSkippedChallenge */
 Model1_LINUX_MIB_TCPWINPROBE, /* TCPWinProbe */
 Model1_LINUX_MIB_TCPKEEPALIVE, /* TCPKeepAlive */
 Model1_LINUX_MIB_TCPMTUPFAIL, /* TCPMTUPFail */
 Model1_LINUX_MIB_TCPMTUPSUCCESS, /* TCPMTUPSuccess */
 Model1___LINUX_MIB_MAX
};

/* linux Xfrm mib definitions */
enum
{
 Model1_LINUX_MIB_XFRMNUM = 0,
 Model1_LINUX_MIB_XFRMINERROR, /* XfrmInError */
 Model1_LINUX_MIB_XFRMINBUFFERERROR, /* XfrmInBufferError */
 Model1_LINUX_MIB_XFRMINHDRERROR, /* XfrmInHdrError */
 Model1_LINUX_MIB_XFRMINNOSTATES, /* XfrmInNoStates */
 Model1_LINUX_MIB_XFRMINSTATEPROTOERROR, /* XfrmInStateProtoError */
 Model1_LINUX_MIB_XFRMINSTATEMODEERROR, /* XfrmInStateModeError */
 Model1_LINUX_MIB_XFRMINSTATESEQERROR, /* XfrmInStateSeqError */
 Model1_LINUX_MIB_XFRMINSTATEEXPIRED, /* XfrmInStateExpired */
 Model1_LINUX_MIB_XFRMINSTATEMISMATCH, /* XfrmInStateMismatch */
 Model1_LINUX_MIB_XFRMINSTATEINVALID, /* XfrmInStateInvalid */
 Model1_LINUX_MIB_XFRMINTMPLMISMATCH, /* XfrmInTmplMismatch */
 Model1_LINUX_MIB_XFRMINNOPOLS, /* XfrmInNoPols */
 Model1_LINUX_MIB_XFRMINPOLBLOCK, /* XfrmInPolBlock */
 Model1_LINUX_MIB_XFRMINPOLERROR, /* XfrmInPolError */
 Model1_LINUX_MIB_XFRMOUTERROR, /* XfrmOutError */
 Model1_LINUX_MIB_XFRMOUTBUNDLEGENERROR, /* XfrmOutBundleGenError */
 Model1_LINUX_MIB_XFRMOUTBUNDLECHECKERROR, /* XfrmOutBundleCheckError */
 Model1_LINUX_MIB_XFRMOUTNOSTATES, /* XfrmOutNoStates */
 Model1_LINUX_MIB_XFRMOUTSTATEPROTOERROR, /* XfrmOutStateProtoError */
 Model1_LINUX_MIB_XFRMOUTSTATEMODEERROR, /* XfrmOutStateModeError */
 Model1_LINUX_MIB_XFRMOUTSTATESEQERROR, /* XfrmOutStateSeqError */
 Model1_LINUX_MIB_XFRMOUTSTATEEXPIRED, /* XfrmOutStateExpired */
 Model1_LINUX_MIB_XFRMOUTPOLBLOCK, /* XfrmOutPolBlock */
 Model1_LINUX_MIB_XFRMOUTPOLDEAD, /* XfrmOutPolDead */
 Model1_LINUX_MIB_XFRMOUTPOLERROR, /* XfrmOutPolError */
 Model1_LINUX_MIB_XFRMFWDHDRERROR, /* XfrmFwdHdrError*/
 Model1_LINUX_MIB_XFRMOUTSTATEINVALID, /* XfrmOutStateInvalid */
 Model1_LINUX_MIB_XFRMACQUIREERROR, /* XfrmAcquireError */
 Model1___LINUX_MIB_XFRMMAX
};


/*
 * Mibs are stored in array of unsigned long.
 */
/*
 * struct snmp_mib{}
 *  - list of entries for particular API (such as /proc/net/snmp)
 *  - name of entries.
 */
struct Model1_snmp_mib {
 const char *Model1_name;
 int Model1_entry;
};
/*
 * We use unsigned longs for most mibs but u64 for ipstats.
 */




/*
 * To properly implement 64bits network statistics on 32bit and 64bit hosts,
 * we provide a synchronization point, that is a noop on 64bit or UP kernels.
 *
 * Key points :
 * 1) Use a seqcount on SMP 32bits, with low overhead.
 * 2) Whole thing is a noop on 64bit arches or UP kernels.
 * 3) Write side must ensure mutual exclusion or one seqcount update could
 *    be lost, thus blocking readers forever.
 *    If this synchronization point is not a mutex, but a spinlock or
 *    spinlock_bh() or disable_bh() :
 * 3.1) Write side should not sleep.
 * 3.2) Write side should not allow preemption.
 * 3.3) If applicable, interrupts should be disabled.
 *
 * 4) If reader fetches several counters, there is no guarantee the whole values
 *    are consistent (remember point 1) : this is a noop on 64bit arches anyway)
 *
 * 5) readers are allowed to sleep or be preempted/interrupted : They perform
 *    pure reads. But if they have to fetch many values, it's better to not allow
 *    preemptions/interruptions to avoid many retries.
 *
 * 6) If counter might be written by an interrupt, readers should block interrupts.
 *    (On UP, there is no seqcount_t protection, a reader allowing interrupts could
 *     read partial values)
 *
 * 7) For irq and softirq uses, readers can use u64_stats_fetch_begin_irq() and
 *    u64_stats_fetch_retry_irq() helpers
 *
 * Usage :
 *
 * Stats producer (writer) should use following template granted it already got
 * an exclusive access to counters (a lock is already taken, or per cpu
 * data is used [in a non preemptable context])
 *
 *   spin_lock_bh(...) or other synchronization to get exclusive access
 *   ...
 *   u64_stats_update_begin(&stats->syncp);
 *   stats->bytes64 += len; // non atomic operation
 *   stats->packets64++;    // non atomic operation
 *   u64_stats_update_end(&stats->syncp);
 *
 * While a consumer (reader) should use following template to get consistent
 * snapshot for each variable (but no guarantee on several ones)
 *
 * u64 tbytes, tpackets;
 * unsigned int start;
 *
 * do {
 *         start = u64_stats_fetch_begin(&stats->syncp);
 *         tbytes = stats->bytes64; // non atomic operation
 *         tpackets = stats->packets64; // non atomic operation
 * } while (u64_stats_fetch_retry(&stats->syncp, start));
 *
 *
 * Example of use in drivers/net/loopback.c, using per_cpu containers,
 * in BH disabled context.
 */


struct Model1_u64_stats_sync {



};


static inline __attribute__((no_instrument_function)) void Model1_u64_stats_init(struct Model1_u64_stats_sync *Model1_syncp)
{



}

static inline __attribute__((no_instrument_function)) void Model1_u64_stats_update_begin(struct Model1_u64_stats_sync *Model1_syncp)
{



}

static inline __attribute__((no_instrument_function)) void Model1_u64_stats_update_end(struct Model1_u64_stats_sync *Model1_syncp)
{



}

static inline __attribute__((no_instrument_function)) void Model1_u64_stats_update_begin_raw(struct Model1_u64_stats_sync *Model1_syncp)
{



}

static inline __attribute__((no_instrument_function)) void Model1_u64_stats_update_end_raw(struct Model1_u64_stats_sync *Model1_syncp)
{



}

static inline __attribute__((no_instrument_function)) unsigned int Model1_u64_stats_fetch_begin(const struct Model1_u64_stats_sync *Model1_syncp)
{






 return 0;

}

static inline __attribute__((no_instrument_function)) bool Model1_u64_stats_fetch_retry(const struct Model1_u64_stats_sync *Model1_syncp,
      unsigned int Model1_start)
{






 return false;

}

/*
 * In case irq handlers can update u64 counters, readers can use following helpers
 * - SMP 32bit arches use seqcount protection, irq safe.
 * - UP 32bit must disable irqs.
 * - 64bit have no problem atomically reading u64 values, irq safe.
 */
static inline __attribute__((no_instrument_function)) unsigned int Model1_u64_stats_fetch_begin_irq(const struct Model1_u64_stats_sync *Model1_syncp)
{






 return 0;

}

static inline __attribute__((no_instrument_function)) bool Model1_u64_stats_fetch_retry_irq(const struct Model1_u64_stats_sync *Model1_syncp,
      unsigned int Model1_start)
{






 return false;

}

/* IPstats */

struct Model1_ipstats_mib {
 /* mibs[] must be first field of struct ipstats_mib */
 Model1_u64 Model1_mibs[Model1___IPSTATS_MIB_MAX];
 struct Model1_u64_stats_sync Model1_syncp;
};

/* ICMP */

struct Model1_icmp_mib {
 unsigned long Model1_mibs[Model1___ICMP_MIB_MAX];
};


struct Model1_icmpmsg_mib {
 Model1_atomic_long_t Model1_mibs[512];
};

/* ICMP6 (IPv6-ICMP) */

/* per network ns counters */
struct Model1_icmpv6_mib {
 unsigned long Model1_mibs[Model1___ICMP6_MIB_MAX];
};
/* per device counters, (shared on all cpus) */
struct Model1_icmpv6_mib_device {
 Model1_atomic_long_t Model1_mibs[Model1___ICMP6_MIB_MAX];
};


/* per network ns counters */
struct Model1_icmpv6msg_mib {
 Model1_atomic_long_t Model1_mibs[512];
};
/* per device counters, (shared on all cpus) */
struct Model1_icmpv6msg_mib_device {
 Model1_atomic_long_t Model1_mibs[512];
};


/* TCP */

struct Model1_tcp_mib {
 unsigned long Model1_mibs[Model1___TCP_MIB_MAX];
};

/* UDP */

struct Model1_udp_mib {
 unsigned long Model1_mibs[Model1___UDP_MIB_MAX];
};

/* Linux */

struct Model1_linux_mib {
 unsigned long Model1_mibs[Model1___LINUX_MIB_MAX];
};

/* Linux Xfrm */

struct Model1_linux_xfrm_mib {
 unsigned long Model1_mibs[Model1___LINUX_MIB_XFRMMAX];
};

struct Model1_netns_mib {
 __typeof__(struct Model1_tcp_mib) *Model1_tcp_statistics;
 __typeof__(struct Model1_ipstats_mib) *Model1_ip_statistics;
 __typeof__(struct Model1_linux_mib) *Model1_net_statistics;
 __typeof__(struct Model1_udp_mib) *Model1_udp_statistics;
 __typeof__(struct Model1_udp_mib) *Model1_udplite_statistics;
 __typeof__(struct Model1_icmp_mib) *Model1_icmp_statistics;
 __typeof__(struct Model1_icmpmsg_mib) *Model1_icmpmsg_statistics;


 struct Model1_proc_dir_entry *Model1_proc_net_devsnmp6;
 __typeof__(struct Model1_udp_mib) *Model1_udp_stats_in6;
 __typeof__(struct Model1_udp_mib) *Model1_udplite_stats_in6;
 __typeof__(struct Model1_ipstats_mib) *Model1_ipv6_statistics;
 __typeof__(struct Model1_icmpv6_mib) *Model1_icmpv6_statistics;
 __typeof__(struct Model1_icmpv6msg_mib) *Model1_icmpv6msg_statistics;




};
/*
 * Unix network namespace
 */



struct Model1_ctl_table_header;
struct Model1_netns_unix {
 int Model1_sysctl_max_dgram_qlen;
 struct Model1_ctl_table_header *Model1_ctl;
};
/*
 * Packet network namespace
 */






struct Model1_netns_packet {
 struct Model1_mutex Model1_sklist_lock;
 struct Model1_hlist_head Model1_sklist;
};
/*
 * ipv4 in net namespaces
 */











struct Model1_netns_frags {
 /* The percpu_counter "mem" need to be cacheline aligned.
	 *  mem.count must not share cacheline with other writers
	 */
 struct Model1_percpu_counter Model1_mem __attribute__((__aligned__((1 << (6)))));

 /* sysctls */
 int Model1_timeout;
 int Model1_high_thresh;
 int Model1_low_thresh;
 int Model1_max_dist;
};

/**
 * fragment queue flags
 *
 * @INET_FRAG_FIRST_IN: first fragment has arrived
 * @INET_FRAG_LAST_IN: final fragment has arrived
 * @INET_FRAG_COMPLETE: frag queue has been processed and is due for destruction
 */
enum {
 Model1_INET_FRAG_FIRST_IN = (1UL << (0)),
 Model1_INET_FRAG_LAST_IN = (1UL << (1)),
 Model1_INET_FRAG_COMPLETE = (1UL << (2)),
};

/**
 * struct inet_frag_queue - fragment queue
 *
 * @lock: spinlock protecting the queue
 * @timer: queue expiration timer
 * @list: hash bucket list
 * @refcnt: reference count of the queue
 * @fragments: received fragments head
 * @fragments_tail: received fragments tail
 * @stamp: timestamp of the last received fragment
 * @len: total length of the original datagram
 * @meat: length of received fragments so far
 * @flags: fragment queue flags
 * @max_size: maximum received fragment size
 * @net: namespace that this frag belongs to
 * @list_evictor: list of queues to forcefully evict (e.g. due to low memory)
 */
struct Model1_inet_frag_queue {
 Model1_spinlock_t Model1_lock;
 struct Model1_timer_list Model1_timer;
 struct Model1_hlist_node Model1_list;
 Model1_atomic_t Model1_refcnt;
 struct Model1_sk_buff *Model1_fragments;
 struct Model1_sk_buff *Model1_fragments_tail;
 Model1_ktime_t Model1_stamp;
 int Model1_len;
 int Model1_meat;
 __u8 Model1_flags;
 Model1_u16 Model1_max_size;
 struct Model1_netns_frags *Model1_net;
 struct Model1_hlist_node Model1_list_evictor;
};



/* averaged:
 * max_depth = default ipfrag_high_thresh / INETFRAGS_HASHSZ /
 *	       rounded up (SKB_TRUELEN(0) + sizeof(struct ipq or
 *	       struct frag_queue))
 */


struct Model1_inet_frag_bucket {
 struct Model1_hlist_head Model1_chain;
 Model1_spinlock_t Model1_chain_lock;
};

struct Model1_inet_frags {
 struct Model1_inet_frag_bucket Model1_hash[1024];

 struct Model1_work_struct Model1_frags_work;
 unsigned int Model1_next_bucket;
 unsigned long Model1_last_rebuild_jiffies;
 bool Model1_rebuild;

 /* The first call to hashfn is responsible to initialize
	 * rnd. This is best done with net_get_random_once.
	 *
	 * rnd_seqlock is used to let hash insertion detect
	 * when it needs to re-lookup the hash chain to use.
	 */
 Model1_u32 Model1_rnd;
 Model1_seqlock_t Model1_rnd_seqlock;
 int Model1_qsize;

 unsigned int (*Model1_hashfn)(const struct Model1_inet_frag_queue *);
 bool (*Model1_match)(const struct Model1_inet_frag_queue *Model1_q,
      const void *Model1_arg);
 void (*Model1_constructor)(struct Model1_inet_frag_queue *Model1_q,
            const void *Model1_arg);
 void (*Model1_destructor)(struct Model1_inet_frag_queue *);
 void (*Model1_frag_expire)(unsigned long Model1_data);
 struct Model1_kmem_cache *Model1_frags_cachep;
 const char *Model1_frags_cache_name;
};

int Model1_inet_frags_init(struct Model1_inet_frags *);
void Model1_inet_frags_fini(struct Model1_inet_frags *);

static inline __attribute__((no_instrument_function)) int Model1_inet_frags_init_net(struct Model1_netns_frags *Model1_nf)
{
 return ({ static struct Model1_lock_class_key Model1___key; Model1___percpu_counter_init(&Model1_nf->Model1_mem, 0, ((( Model1_gfp_t)(0x400000u|0x2000000u)) | (( Model1_gfp_t)0x40u) | (( Model1_gfp_t)0x80u)), &Model1___key); });
}
static inline __attribute__((no_instrument_function)) void Model1_inet_frags_uninit_net(struct Model1_netns_frags *Model1_nf)
{
 Model1_percpu_counter_destroy(&Model1_nf->Model1_mem);
}

void Model1_inet_frags_exit_net(struct Model1_netns_frags *Model1_nf, struct Model1_inet_frags *Model1_f);

void Model1_inet_frag_kill(struct Model1_inet_frag_queue *Model1_q, struct Model1_inet_frags *Model1_f);
void Model1_inet_frag_destroy(struct Model1_inet_frag_queue *Model1_q, struct Model1_inet_frags *Model1_f);
struct Model1_inet_frag_queue *Model1_inet_frag_find(struct Model1_netns_frags *Model1_nf,
  struct Model1_inet_frags *Model1_f, void *Model1_key, unsigned int Model1_hash);

void Model1_inet_frag_maybe_warn_overflow(struct Model1_inet_frag_queue *Model1_q,
       const char *Model1_prefix);

static inline __attribute__((no_instrument_function)) void Model1_inet_frag_put(struct Model1_inet_frag_queue *Model1_q, struct Model1_inet_frags *Model1_f)
{
 if (Model1_atomic_dec_and_test(&Model1_q->Model1_refcnt))
  Model1_inet_frag_destroy(Model1_q, Model1_f);
}

static inline __attribute__((no_instrument_function)) bool Model1_inet_frag_evicting(struct Model1_inet_frag_queue *Model1_q)
{
 return !Model1_hlist_unhashed(&Model1_q->Model1_list_evictor);
}

/* Memory Tracking Functions. */

/* The default percpu_counter batch size is not big enough to scale to
 * fragmentation mem acct sizes.
 * The mem size of a 64K fragment is approx:
 *  (44 fragments * 2944 truesize) + frag_queue struct(200) = 129736 bytes
 */
static unsigned int Model1_frag_percpu_counter_batch = 130000;

static inline __attribute__((no_instrument_function)) int Model1_frag_mem_limit(struct Model1_netns_frags *Model1_nf)
{
 return Model1_percpu_counter_read(&Model1_nf->Model1_mem);
}

static inline __attribute__((no_instrument_function)) void Model1_sub_frag_mem_limit(struct Model1_netns_frags *Model1_nf, int Model1_i)
{
 Model1___percpu_counter_add(&Model1_nf->Model1_mem, -Model1_i, Model1_frag_percpu_counter_batch);
}

static inline __attribute__((no_instrument_function)) void Model1_add_frag_mem_limit(struct Model1_netns_frags *Model1_nf, int Model1_i)
{
 Model1___percpu_counter_add(&Model1_nf->Model1_mem, Model1_i, Model1_frag_percpu_counter_batch);
}

static inline __attribute__((no_instrument_function)) unsigned int Model1_sum_frag_mem_limit(struct Model1_netns_frags *Model1_nf)
{
 unsigned int Model1_res;

 Model1_local_bh_disable();
 Model1_res = Model1_percpu_counter_sum_positive(&Model1_nf->Model1_mem);
 Model1_local_bh_enable();

 return Model1_res;
}

/* RFC 3168 support :
 * We want to check ECN values of all fragments, do detect invalid combinations.
 * In ipq->ecn, we store the OR value of each ip4_frag_ecn() fragment value.
 */





extern const Model1_u8 Model1_ip_frag_ecn_table[16];


struct Model1_tcpm_hash_bucket;
struct Model1_ctl_table_header;
struct Model1_ipv4_devconf;
struct Model1_fib_rules_ops;
struct Model1_hlist_head;
struct Model1_fib_table;
struct Model1_sock;
struct Model1_local_ports {
 Model1_seqlock_t Model1_lock;
 int Model1_range[2];
 bool Model1_warned;
};

struct Model1_ping_group_range {
 Model1_seqlock_t Model1_lock;
 Model1_kgid_t Model1_range[2];
};

struct Model1_netns_ipv4 {

 struct Model1_ctl_table_header *Model1_forw_hdr;
 struct Model1_ctl_table_header *Model1_frags_hdr;
 struct Model1_ctl_table_header *Model1_ipv4_hdr;
 struct Model1_ctl_table_header *Model1_route_hdr;
 struct Model1_ctl_table_header *Model1_xfrm4_hdr;

 struct Model1_ipv4_devconf *Model1_devconf_all;
 struct Model1_ipv4_devconf *Model1_devconf_dflt;

 struct Model1_fib_rules_ops *Model1_rules_ops;
 bool Model1_fib_has_custom_rules;
 struct Model1_fib_table *Model1_fib_local;
 struct Model1_fib_table *Model1_fib_main;
 struct Model1_fib_table *Model1_fib_default;




 struct Model1_hlist_head *Model1_fib_table_hash;
 bool Model1_fib_offload_disabled;
 struct Model1_sock *Model1_fibnl;

 struct Model1_sock * *Model1_icmp_sk;
 struct Model1_sock *Model1_mc_autojoin_sk;

 struct Model1_inet_peer_base *Model1_peers;
 struct Model1_sock * *Model1_tcp_sk;
 struct Model1_netns_frags Model1_frags;

 struct Model1_xt_table *Model1_iptable_filter;
 struct Model1_xt_table *Model1_iptable_mangle;
 struct Model1_xt_table *Model1_iptable_raw;
 struct Model1_xt_table *Model1_arptable_filter;

 struct Model1_xt_table *Model1_iptable_security;

 struct Model1_xt_table *Model1_nat_table;


 int Model1_sysctl_icmp_echo_ignore_all;
 int Model1_sysctl_icmp_echo_ignore_broadcasts;
 int Model1_sysctl_icmp_ignore_bogus_error_responses;
 int Model1_sysctl_icmp_ratelimit;
 int Model1_sysctl_icmp_ratemask;
 int Model1_sysctl_icmp_errors_use_inbound_ifaddr;

 struct Model1_local_ports Model1_ip_local_ports;

 int Model1_sysctl_tcp_ecn;
 int Model1_sysctl_tcp_ecn_fallback;

 int Model1_sysctl_ip_default_ttl;
 int Model1_sysctl_ip_no_pmtu_disc;
 int Model1_sysctl_ip_fwd_use_pmtu;
 int Model1_sysctl_ip_nonlocal_bind;
 /* Shall we try to damage output packets if routing dev changes? */
 int Model1_sysctl_ip_dynaddr;
 int Model1_sysctl_ip_early_demux;

 int Model1_sysctl_fwmark_reflect;
 int Model1_sysctl_tcp_fwmark_accept;



 int Model1_sysctl_tcp_mtu_probing;
 int Model1_sysctl_tcp_base_mss;
 int Model1_sysctl_tcp_probe_threshold;
 Model1_u32 Model1_sysctl_tcp_probe_interval;

 int Model1_sysctl_tcp_keepalive_time;
 int Model1_sysctl_tcp_keepalive_probes;
 int Model1_sysctl_tcp_keepalive_intvl;

 int Model1_sysctl_tcp_syn_retries;
 int Model1_sysctl_tcp_synack_retries;
 int Model1_sysctl_tcp_syncookies;
 int Model1_sysctl_tcp_reordering;
 int Model1_sysctl_tcp_retries1;
 int Model1_sysctl_tcp_retries2;
 int Model1_sysctl_tcp_orphan_retries;
 int Model1_sysctl_tcp_fin_timeout;
 unsigned int Model1_sysctl_tcp_notsent_lowat;

 int Model1_sysctl_igmp_max_memberships;
 int Model1_sysctl_igmp_max_msf;
 int Model1_sysctl_igmp_llm_reports;
 int Model1_sysctl_igmp_qrv;

 struct Model1_ping_group_range Model1_ping_group_range;

 Model1_atomic_t Model1_dev_addr_genid;


 unsigned long *Model1_sysctl_local_reserved_ports;




 struct Model1_mr_table *Model1_mrt;






 int Model1_sysctl_fib_multipath_use_neigh;

 Model1_atomic_t Model1_rt_genid;
};
/*
 * ipv6 in net namespaces
 */







struct Model1_ctl_table_header;

struct Model1_netns_sysctl_ipv6 {

 struct Model1_ctl_table_header *Model1_hdr;
 struct Model1_ctl_table_header *Model1_route_hdr;
 struct Model1_ctl_table_header *Model1_icmp_hdr;
 struct Model1_ctl_table_header *Model1_frags_hdr;
 struct Model1_ctl_table_header *Model1_xfrm6_hdr;

 int Model1_bindv6only;
 int Model1_flush_delay;
 int Model1_ip6_rt_max_size;
 int Model1_ip6_rt_gc_min_interval;
 int Model1_ip6_rt_gc_timeout;
 int Model1_ip6_rt_gc_interval;
 int Model1_ip6_rt_gc_elasticity;
 int Model1_ip6_rt_mtu_expires;
 int Model1_ip6_rt_min_advmss;
 int Model1_flowlabel_consistency;
 int Model1_auto_flowlabels;
 int Model1_icmpv6_time;
 int Model1_anycast_src_echo_reply;
 int Model1_ip_nonlocal_bind;
 int Model1_fwmark_reflect;
 int Model1_idgen_retries;
 int Model1_idgen_delay;
 int Model1_flowlabel_state_ranges;
};

struct Model1_netns_ipv6 {
 struct Model1_netns_sysctl_ipv6 Model1_sysctl;
 struct Model1_ipv6_devconf *Model1_devconf_all;
 struct Model1_ipv6_devconf *Model1_devconf_dflt;
 struct Model1_inet_peer_base *Model1_peers;
 struct Model1_netns_frags Model1_frags;

 struct Model1_xt_table *Model1_ip6table_filter;
 struct Model1_xt_table *Model1_ip6table_mangle;
 struct Model1_xt_table *Model1_ip6table_raw;

 struct Model1_xt_table *Model1_ip6table_security;

 struct Model1_xt_table *Model1_ip6table_nat;

 struct Model1_rt6_info *Model1_ip6_null_entry;
 struct Model1_rt6_statistics *Model1_rt6_stats;
 struct Model1_timer_list Model1_ip6_fib_timer;
 struct Model1_hlist_head *Model1_fib_table_hash;
 struct Model1_fib6_table *Model1_fib6_main_tbl;
 struct Model1_list_head Model1_fib6_walkers;
 struct Model1_dst_ops Model1_ip6_dst_ops;
 Model1_rwlock_t Model1_fib6_walker_lock;
 Model1_spinlock_t Model1_fib6_gc_lock;
 unsigned int Model1_ip6_rt_gc_expire;
 unsigned long Model1_ip6_rt_last_gc;






 struct Model1_sock **Model1_icmp_sk;
 struct Model1_sock *Model1_ndisc_sk;
 struct Model1_sock *Model1_tcp_sk;
 struct Model1_sock *Model1_igmp_sk;
 struct Model1_sock *Model1_mc_autojoin_sk;
 Model1_atomic_t Model1_dev_addr_genid;
 Model1_atomic_t Model1_fib6_sernum;
};


struct Model1_netns_nf_frag {
 struct Model1_netns_sysctl_ipv6 Model1_sysctl;
 struct Model1_netns_frags Model1_frags;
};
/*
 * ieee802154 6lowpan in net namespaces
 */






struct Model1_netns_sysctl_lowpan {

 struct Model1_ctl_table_header *Model1_frags_hdr;

};

struct Model1_netns_ieee802154_lowpan {
 struct Model1_netns_sysctl_lowpan Model1_sysctl;
 struct Model1_netns_frags Model1_frags;
};



struct Model1_sock;
struct Model1_proc_dir_entry;
struct Model1_sctp_mib;
struct Model1_ctl_table_header;

struct Model1_netns_sctp {
 __typeof__(struct Model1_sctp_mib) *Model1_sctp_statistics;


 struct Model1_proc_dir_entry *Model1_proc_net_sctp;


 struct Model1_ctl_table_header *Model1_sysctl_header;

 /* This is the global socket data structure used for responding to
	 * the Out-of-the-blue (OOTB) packets.  A control sock will be created
	 * for this socket at the initialization time.
	 */
 struct Model1_sock *Model1_ctl_sock;

 /* This is the global local address list.
	 * We actively maintain this complete list of addresses on
	 * the system by catching address add/delete events.
	 *
	 * It is a list of sctp_sockaddr_entry.
	 */
 struct Model1_list_head Model1_local_addr_list;
 struct Model1_list_head Model1_addr_waitq;
 struct Model1_timer_list Model1_addr_wq_timer;
 struct Model1_list_head Model1_auto_asconf_splist;
 /* Lock that protects both addr_waitq and auto_asconf_splist */
 Model1_spinlock_t Model1_addr_wq_lock;

 /* Lock that protects the local_addr_list writers */
 Model1_spinlock_t Model1_local_addr_lock;

 /* RFC2960 Section 14. Suggested SCTP Protocol Parameter Values
	 *
	 * The following protocol parameters are RECOMMENDED:
	 *
	 * RTO.Initial		    - 3	 seconds
	 * RTO.Min		    - 1	 second
	 * RTO.Max		   -  60 seconds
	 * RTO.Alpha		    - 1/8  (3 when converted to right shifts.)
	 * RTO.Beta		    - 1/4  (2 when converted to right shifts.)
	 */
 unsigned int Model1_rto_initial;
 unsigned int Model1_rto_min;
 unsigned int Model1_rto_max;

 /* Note: rto_alpha and rto_beta are really defined as inverse
	 * powers of two to facilitate integer operations.
	 */
 int Model1_rto_alpha;
 int Model1_rto_beta;

 /* Max.Burst		    - 4 */
 int Model1_max_burst;

 /* Whether Cookie Preservative is enabled(1) or not(0) */
 int Model1_cookie_preserve_enable;

 /* The namespace default hmac alg */
 char *Model1_sctp_hmac_alg;

 /* Valid.Cookie.Life	    - 60  seconds  */
 unsigned int Model1_valid_cookie_life;

 /* Delayed SACK timeout  200ms default*/
 unsigned int Model1_sack_timeout;

 /* HB.interval		    - 30 seconds  */
 unsigned int Model1_hb_interval;

 /* Association.Max.Retrans  - 10 attempts
	 * Path.Max.Retrans	    - 5	 attempts (per destination address)
	 * Max.Init.Retransmits	    - 8	 attempts
	 */
 int Model1_max_retrans_association;
 int Model1_max_retrans_path;
 int Model1_max_retrans_init;
 /* Potentially-Failed.Max.Retrans sysctl value
	 * taken from:
	 * http://tools.ietf.org/html/draft-nishida-tsvwg-sctp-failover-05
	 */
 int Model1_pf_retrans;

 /*
	 * Disable Potentially-Failed feature, the feature is enabled by default
	 * pf_enable	-  0  : disable pf
	 *		- >0  : enable pf
	 */
 int Model1_pf_enable;

 /*
	 * Policy for preforming sctp/socket accounting
	 * 0   - do socket level accounting, all assocs share sk_sndbuf
	 * 1   - do sctp accounting, each asoc may use sk_sndbuf bytes
	 */
 int Model1_sndbuf_policy;

 /*
	 * Policy for preforming sctp/socket accounting
	 * 0   - do socket level accounting, all assocs share sk_rcvbuf
	 * 1   - do sctp accounting, each asoc may use sk_rcvbuf bytes
	 */
 int Model1_rcvbuf_policy;

 int Model1_default_auto_asconf;

 /* Flag to indicate if addip is enabled. */
 int Model1_addip_enable;
 int Model1_addip_noauth;

 /* Flag to indicate if PR-SCTP is enabled. */
 int Model1_prsctp_enable;

 /* Flag to idicate if SCTP-AUTH is enabled */
 int Model1_auth_enable;

 /*
	 * Policy to control SCTP IPv4 address scoping
	 * 0   - Disable IPv4 address scoping
	 * 1   - Enable IPv4 address scoping
	 * 2   - Selectively allow only IPv4 private addresses
	 * 3   - Selectively allow only IPv4 link local address
	 */
 int Model1_scope_policy;

 /* Threshold for rwnd update SACKS.  Receive buffer shifted this many
	 * bits is an indicator of when to send and window update SACK.
	 */
 int Model1_rwnd_upd_shift;

 /* Threshold for autoclose timeout, in seconds. */
 unsigned long Model1_max_autoclose;
};



struct Model1_sock;

struct Model1_netns_dccp {
 struct Model1_sock *Model1_v4_ctl_sk;
 struct Model1_sock *Model1_v6_ctl_sk;
};












/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions of the Internet Protocol.
 *
 * Version:	@(#)in.h	1.0.1	04/21/93
 *
 * Authors:	Original taken from the GNU Project <netinet/in.h> file.
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */






/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions of the Internet Protocol.
 *
 * Version:	@(#)in.h	1.0.1	04/21/93
 *
 * Authors:	Original taken from the GNU Project <netinet/in.h> file.
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */
/* Standard well-defined IP protocols.  */
enum {
  Model1_IPPROTO_IP = 0, /* Dummy protocol for TCP		*/

  Model1_IPPROTO_ICMP = 1, /* Internet Control Message Protocol	*/

  Model1_IPPROTO_IGMP = 2, /* Internet Group Management Protocol	*/

  Model1_IPPROTO_IPIP = 4, /* IPIP tunnels (older KA9Q tunnels use 94) */

  Model1_IPPROTO_TCP = 6, /* Transmission Control Protocol	*/

  Model1_IPPROTO_EGP = 8, /* Exterior Gateway Protocol		*/

  Model1_IPPROTO_PUP = 12, /* PUP protocol				*/

  Model1_IPPROTO_UDP = 17, /* User Datagram Protocol		*/

  Model1_IPPROTO_IDP = 22, /* XNS IDP protocol			*/

  Model1_IPPROTO_TP = 29, /* SO Transport Protocol Class 4	*/

  Model1_IPPROTO_DCCP = 33, /* Datagram Congestion Control Protocol */

  Model1_IPPROTO_IPV6 = 41, /* IPv6-in-IPv4 tunnelling		*/

  Model1_IPPROTO_RSVP = 46, /* RSVP Protocol			*/

  Model1_IPPROTO_GRE = 47, /* Cisco GRE tunnels (rfc 1701,1702)	*/

  Model1_IPPROTO_ESP = 50, /* Encapsulation Security Payload protocol */

  Model1_IPPROTO_AH = 51, /* Authentication Header protocol	*/

  Model1_IPPROTO_MTP = 92, /* Multicast Transport Protocol		*/

  Model1_IPPROTO_BEETPH = 94, /* IP option pseudo header for BEET	*/

  Model1_IPPROTO_ENCAP = 98, /* Encapsulation Header			*/

  Model1_IPPROTO_PIM = 103, /* Protocol Independent Multicast	*/

  Model1_IPPROTO_COMP = 108, /* Compression Header Protocol		*/

  Model1_IPPROTO_SCTP = 132, /* Stream Control Transport Protocol	*/

  Model1_IPPROTO_UDPLITE = 136, /* UDP-Lite (RFC 3828)			*/

  Model1_IPPROTO_MPLS = 137, /* MPLS in IP (RFC 4023)		*/

  Model1_IPPROTO_RAW = 255, /* Raw IP packets			*/

  Model1_IPPROTO_MAX
};



/* Internet address. */
struct Model1_in_addr {
 Model1___be32 Model1_s_addr;
};
/* BSD compatibility */


/* TProxy original addresses */
/* IP_MTU_DISCOVER values */




/* Always use interface mtu (ignores dst pmtu) but don't set DF flag.
 * Also incoming ICMP frag_needed notifications will be ignored on
 * this socket to prevent accepting spoofed ones.
 */

/* weaker version of IP_PMTUDISC_INTERFACE, which allos packets to get
 * fragmented if they exeed the interface mtu
 */
/* These need to appear somewhere around here */



/* Request struct for multicast socket ops */


struct Model1_ip_mreq {
 struct Model1_in_addr Model1_imr_multiaddr; /* IP multicast address of group */
 struct Model1_in_addr Model1_imr_interface; /* local IP address of interface */
};

struct Model1_ip_mreqn {
 struct Model1_in_addr Model1_imr_multiaddr; /* IP multicast address of group */
 struct Model1_in_addr Model1_imr_address; /* local IP address of interface */
 int Model1_imr_ifindex; /* Interface index */
};

struct Model1_ip_mreq_source {
 Model1___be32 Model1_imr_multiaddr;
 Model1___be32 Model1_imr_interface;
 Model1___be32 Model1_imr_sourceaddr;
};

struct Model1_ip_msfilter {
 Model1___be32 Model1_imsf_multiaddr;
 Model1___be32 Model1_imsf_interface;
 __u32 Model1_imsf_fmode;
 __u32 Model1_imsf_numsrc;
 Model1___be32 Model1_imsf_slist[1];
};





struct Model1_group_req {
 __u32 Model1_gr_interface; /* interface index */
 struct Model1___kernel_sockaddr_storage Model1_gr_group; /* group address */
};

struct Model1_group_source_req {
 __u32 Model1_gsr_interface; /* interface index */
 struct Model1___kernel_sockaddr_storage Model1_gsr_group; /* group address */
 struct Model1___kernel_sockaddr_storage Model1_gsr_source; /* source address */
};

struct Model1_group_filter {
 __u32 Model1_gf_interface; /* interface index */
 struct Model1___kernel_sockaddr_storage Model1_gf_group; /* multicast address */
 __u32 Model1_gf_fmode; /* filter mode */
 __u32 Model1_gf_numsrc; /* number of sources */
 struct Model1___kernel_sockaddr_storage Model1_gf_slist[1]; /* interface index */
};







struct Model1_in_pktinfo {
 int Model1_ipi_ifindex;
 struct Model1_in_addr Model1_ipi_spec_dst;
 struct Model1_in_addr Model1_ipi_addr;
};


/* Structure describing an Internet (IP) socket address. */


struct Model1_sockaddr_in {
  Model1___kernel_sa_family_t Model1_sin_family; /* Address family		*/
  Model1___be16 Model1_sin_port; /* Port number			*/
  struct Model1_in_addr Model1_sin_addr; /* Internet address		*/

  /* Pad to size of `struct sockaddr'. */
  unsigned char Model1___pad[16 - sizeof(short int) -
   sizeof(unsigned short int) - sizeof(struct Model1_in_addr)];
};




/*
 * Definitions of the bits in an Internet address integer.
 * On subnets, host and network parts are found according
 * to the subnet mask, not these masks.
 */
/* Address to accept any incoming messages. */


/* Address to send to all hosts. */


/* Address indicating an error return. */


/* Network number for local host loopback. */


/* Address to loopback in software to local host.  */



/* Defines for Multicast INADDR */






/* <asm/byteorder.h> contains the htonl type stuff.. */

static inline __attribute__((no_instrument_function)) int Model1_proto_ports_offset(int Model1_proto)
{
 switch (Model1_proto) {
 case Model1_IPPROTO_TCP:
 case Model1_IPPROTO_UDP:
 case Model1_IPPROTO_DCCP:
 case Model1_IPPROTO_ESP: /* SPI */
 case Model1_IPPROTO_SCTP:
 case Model1_IPPROTO_UDPLITE:
  return 0;
 case Model1_IPPROTO_AH: /* SPI */
  return 4;
 default:
  return -22;
 }
}

static inline __attribute__((no_instrument_function)) bool Model1_ipv4_is_loopback(Model1___be32 Model1_addr)
{
 return (Model1_addr & (( Model1___be32)(__builtin_constant_p((__u32)((0xff000000))) ? ((__u32)( (((__u32)((0xff000000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xff000000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xff000000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xff000000)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0xff000000))))) == (( Model1___be32)(__builtin_constant_p((__u32)((0x7f000000))) ? ((__u32)( (((__u32)((0x7f000000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x7f000000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x7f000000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x7f000000)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0x7f000000))));
}

static inline __attribute__((no_instrument_function)) bool Model1_ipv4_is_multicast(Model1___be32 Model1_addr)
{
 return (Model1_addr & (( Model1___be32)(__builtin_constant_p((__u32)((0xf0000000))) ? ((__u32)( (((__u32)((0xf0000000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xf0000000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xf0000000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xf0000000)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0xf0000000))))) == (( Model1___be32)(__builtin_constant_p((__u32)((0xe0000000))) ? ((__u32)( (((__u32)((0xe0000000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xe0000000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xe0000000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xe0000000)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0xe0000000))));
}

static inline __attribute__((no_instrument_function)) bool Model1_ipv4_is_local_multicast(Model1___be32 Model1_addr)
{
 return (Model1_addr & (( Model1___be32)(__builtin_constant_p((__u32)((0xffffff00))) ? ((__u32)( (((__u32)((0xffffff00)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xffffff00)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xffffff00)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xffffff00)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0xffffff00))))) == (( Model1___be32)(__builtin_constant_p((__u32)((0xe0000000))) ? ((__u32)( (((__u32)((0xe0000000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xe0000000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xe0000000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xe0000000)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0xe0000000))));
}

static inline __attribute__((no_instrument_function)) bool Model1_ipv4_is_lbcast(Model1___be32 Model1_addr)
{
 /* limited broadcast */
 return Model1_addr == (( Model1___be32)(__builtin_constant_p((__u32)((((unsigned long int) 0xffffffff)))) ? ((__u32)( (((__u32)((((unsigned long int) 0xffffffff))) & (__u32)0x000000ffUL) << 24) | (((__u32)((((unsigned long int) 0xffffffff))) & (__u32)0x0000ff00UL) << 8) | (((__u32)((((unsigned long int) 0xffffffff))) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((((unsigned long int) 0xffffffff))) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((((unsigned long int) 0xffffffff)))));
}

static inline __attribute__((no_instrument_function)) bool Model1_ipv4_is_zeronet(Model1___be32 Model1_addr)
{
 return (Model1_addr & (( Model1___be32)(__builtin_constant_p((__u32)((0xff000000))) ? ((__u32)( (((__u32)((0xff000000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xff000000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xff000000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xff000000)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0xff000000))))) == (( Model1___be32)(__builtin_constant_p((__u32)((0x00000000))) ? ((__u32)( (((__u32)((0x00000000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x00000000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x00000000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x00000000)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0x00000000))));
}

/* Special-Use IPv4 Addresses (RFC3330) */

static inline __attribute__((no_instrument_function)) bool Model1_ipv4_is_private_10(Model1___be32 Model1_addr)
{
 return (Model1_addr & (( Model1___be32)(__builtin_constant_p((__u32)((0xff000000))) ? ((__u32)( (((__u32)((0xff000000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xff000000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xff000000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xff000000)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0xff000000))))) == (( Model1___be32)(__builtin_constant_p((__u32)((0x0a000000))) ? ((__u32)( (((__u32)((0x0a000000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x0a000000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x0a000000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x0a000000)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0x0a000000))));
}

static inline __attribute__((no_instrument_function)) bool Model1_ipv4_is_private_172(Model1___be32 Model1_addr)
{
 return (Model1_addr & (( Model1___be32)(__builtin_constant_p((__u32)((0xfff00000))) ? ((__u32)( (((__u32)((0xfff00000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xfff00000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xfff00000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xfff00000)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0xfff00000))))) == (( Model1___be32)(__builtin_constant_p((__u32)((0xac100000))) ? ((__u32)( (((__u32)((0xac100000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xac100000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xac100000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xac100000)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0xac100000))));
}

static inline __attribute__((no_instrument_function)) bool Model1_ipv4_is_private_192(Model1___be32 Model1_addr)
{
 return (Model1_addr & (( Model1___be32)(__builtin_constant_p((__u32)((0xffff0000))) ? ((__u32)( (((__u32)((0xffff0000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xffff0000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xffff0000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xffff0000)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0xffff0000))))) == (( Model1___be32)(__builtin_constant_p((__u32)((0xc0a80000))) ? ((__u32)( (((__u32)((0xc0a80000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xc0a80000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xc0a80000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xc0a80000)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0xc0a80000))));
}

static inline __attribute__((no_instrument_function)) bool Model1_ipv4_is_linklocal_169(Model1___be32 Model1_addr)
{
 return (Model1_addr & (( Model1___be32)(__builtin_constant_p((__u32)((0xffff0000))) ? ((__u32)( (((__u32)((0xffff0000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xffff0000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xffff0000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xffff0000)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0xffff0000))))) == (( Model1___be32)(__builtin_constant_p((__u32)((0xa9fe0000))) ? ((__u32)( (((__u32)((0xa9fe0000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xa9fe0000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xa9fe0000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xa9fe0000)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0xa9fe0000))));
}

static inline __attribute__((no_instrument_function)) bool Model1_ipv4_is_anycast_6to4(Model1___be32 Model1_addr)
{
 return (Model1_addr & (( Model1___be32)(__builtin_constant_p((__u32)((0xffffff00))) ? ((__u32)( (((__u32)((0xffffff00)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xffffff00)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xffffff00)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xffffff00)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0xffffff00))))) == (( Model1___be32)(__builtin_constant_p((__u32)((0xc0586300))) ? ((__u32)( (((__u32)((0xc0586300)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xc0586300)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xc0586300)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xc0586300)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0xc0586300))));
}

static inline __attribute__((no_instrument_function)) bool Model1_ipv4_is_test_192(Model1___be32 Model1_addr)
{
 return (Model1_addr & (( Model1___be32)(__builtin_constant_p((__u32)((0xffffff00))) ? ((__u32)( (((__u32)((0xffffff00)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xffffff00)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xffffff00)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xffffff00)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0xffffff00))))) == (( Model1___be32)(__builtin_constant_p((__u32)((0xc0000200))) ? ((__u32)( (((__u32)((0xc0000200)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xc0000200)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xc0000200)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xc0000200)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0xc0000200))));
}

static inline __attribute__((no_instrument_function)) bool Model1_ipv4_is_test_198(Model1___be32 Model1_addr)
{
 return (Model1_addr & (( Model1___be32)(__builtin_constant_p((__u32)((0xfffe0000))) ? ((__u32)( (((__u32)((0xfffe0000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xfffe0000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xfffe0000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xfffe0000)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0xfffe0000))))) == (( Model1___be32)(__builtin_constant_p((__u32)((0xc6120000))) ? ((__u32)( (((__u32)((0xc6120000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xc6120000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xc6120000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xc6120000)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0xc6120000))));
}


/* Responses from hook functions. */
/* we overload the higher bits for encoding auxiliary data such as the queue
 * number or errno values. Not nice, but better than additional function
 * arguments. */


/* extra verdict flags have mask 0x0000ff00 */


/* queue number (NF_QUEUE) or errno (NF_DROP) */







/* only for userspace compatibility */
enum Model1_nf_inet_hooks {
 Model1_NF_INET_PRE_ROUTING,
 Model1_NF_INET_LOCAL_IN,
 Model1_NF_INET_FORWARD,
 Model1_NF_INET_LOCAL_OUT,
 Model1_NF_INET_POST_ROUTING,
 Model1_NF_INET_NUMHOOKS
};

enum Model1_nf_dev_hooks {
 Model1_NF_NETDEV_INGRESS,
 Model1_NF_NETDEV_NUMHOOKS
};

enum {
 Model1_NFPROTO_UNSPEC = 0,
 Model1_NFPROTO_INET = 1,
 Model1_NFPROTO_IPV4 = 2,
 Model1_NFPROTO_ARP = 3,
 Model1_NFPROTO_NETDEV = 5,
 Model1_NFPROTO_BRIDGE = 7,
 Model1_NFPROTO_IPV6 = 10,
 Model1_NFPROTO_DECNET = 12,
 Model1_NFPROTO_NUMPROTO,
};

union Model1_nf_inet_addr {
 __u32 Model1_all[4];
 Model1___be32 Model1_ip;
 Model1___be32 Model1_ip6[4];
 struct Model1_in_addr Model1_in;
 struct Model1_in6_addr Model1_in6;
};

/* Largest hook number + 1, see uapi/linux/netfilter_decnet.h */

struct Model1_proc_dir_entry;
struct Model1_nf_logger;
struct Model1_nf_queue_handler;

struct Model1_netns_nf {

 struct Model1_proc_dir_entry *Model1_proc_netfilter;

 const struct Model1_nf_queue_handler *Model1_queue_handler;
 const struct Model1_nf_logger *Model1_nf_loggers[Model1_NFPROTO_NUMPROTO];

 struct Model1_ctl_table_header *Model1_nf_log_dir_header;

 struct Model1_list_head Model1_hooks[Model1_NFPROTO_NUMPROTO][8];
};






struct Model1_ebt_table;

struct Model1_netns_xt {
 struct Model1_list_head Model1_tables[Model1_NFPROTO_NUMPROTO];
 bool Model1_notrack_deprecated_warning;
 bool Model1_clusterip_deprecated_warning;






};











/*
 * Special version of lists, where end of list is not a NULL pointer,
 * but a 'nulls' marker, which can have many different values.
 * (up to 2^31 different values guaranteed on all platforms)
 *
 * In the standard hlist, termination of a list is the NULL pointer.
 * In this special 'nulls' variant, we use the fact that objects stored in
 * a list are aligned on a word (4 or 8 bytes alignment).
 * We therefore use the last significant bit of 'ptr' :
 * Set to 1 : This is a 'nulls' end-of-list marker (ptr >> 1)
 * Set to 0 : This is a pointer to some object (ptr)
 */

struct Model1_hlist_nulls_head {
 struct Model1_hlist_nulls_node *Model1_first;
};

struct Model1_hlist_nulls_node {
 struct Model1_hlist_nulls_node *Model1_next, **Model1_pprev;
};





/**
 * ptr_is_a_nulls - Test if a ptr is a nulls
 * @ptr: ptr to be tested
 *
 */
static inline __attribute__((no_instrument_function)) int Model1_is_a_nulls(const struct Model1_hlist_nulls_node *Model1_ptr)
{
 return ((unsigned long)Model1_ptr & 1);
}

/**
 * get_nulls_value - Get the 'nulls' value of the end of chain
 * @ptr: end of chain
 *
 * Should be called only if is_a_nulls(ptr);
 */
static inline __attribute__((no_instrument_function)) unsigned long Model1_get_nulls_value(const struct Model1_hlist_nulls_node *Model1_ptr)
{
 return ((unsigned long)Model1_ptr) >> 1;
}

static inline __attribute__((no_instrument_function)) int Model1_hlist_nulls_unhashed(const struct Model1_hlist_nulls_node *Model1_h)
{
 return !Model1_h->Model1_pprev;
}

static inline __attribute__((no_instrument_function)) int Model1_hlist_nulls_empty(const struct Model1_hlist_nulls_head *Model1_h)
{
 return Model1_is_a_nulls(({ union { typeof(Model1_h->Model1_first) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&(Model1_h->Model1_first), Model1___u.Model1___c, sizeof(Model1_h->Model1_first)); else Model1___read_once_size_nocheck(&(Model1_h->Model1_first), Model1___u.Model1___c, sizeof(Model1_h->Model1_first)); Model1___u.Model1___val; }));
}

static inline __attribute__((no_instrument_function)) void Model1_hlist_nulls_add_head(struct Model1_hlist_nulls_node *Model1_n,
     struct Model1_hlist_nulls_head *Model1_h)
{
 struct Model1_hlist_nulls_node *Model1_first = Model1_h->Model1_first;

 Model1_n->Model1_next = Model1_first;
 Model1_n->Model1_pprev = &Model1_h->Model1_first;
 Model1_h->Model1_first = Model1_n;
 if (!Model1_is_a_nulls(Model1_first))
  Model1_first->Model1_pprev = &Model1_n->Model1_next;
}

static inline __attribute__((no_instrument_function)) void Model1___hlist_nulls_del(struct Model1_hlist_nulls_node *Model1_n)
{
 struct Model1_hlist_nulls_node *Model1_next = Model1_n->Model1_next;
 struct Model1_hlist_nulls_node **Model1_pprev = Model1_n->Model1_pprev;

 ({ union { typeof(*Model1_pprev) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(*Model1_pprev)) (Model1_next) }; Model1___write_once_size(&(*Model1_pprev), Model1___u.Model1___c, sizeof(*Model1_pprev)); Model1___u.Model1___val; });
 if (!Model1_is_a_nulls(Model1_next))
  Model1_next->Model1_pprev = Model1_pprev;
}

static inline __attribute__((no_instrument_function)) void Model1_hlist_nulls_del(struct Model1_hlist_nulls_node *Model1_n)
{
 Model1___hlist_nulls_del(Model1_n);
 Model1_n->Model1_pprev = ((void *) 0x200 + (0xdead000000000000UL));
}

/**
 * hlist_nulls_for_each_entry	- iterate over list of given type
 * @tpos:	the type * to use as a loop cursor.
 * @pos:	the &struct hlist_node to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the hlist_node within the struct.
 *
 */






/**
 * hlist_nulls_for_each_entry_from - iterate over a hlist continuing from current point
 * @tpos:	the type * to use as a loop cursor.
 * @pos:	the &struct hlist_node to use as a loop cursor.
 * @member:	the name of the hlist_node within the struct.
 *
 */







/* TCP tracking. */



/* This is exposed to userspace (ctnetlink) */
enum Model1_tcp_conntrack {
 Model1_TCP_CONNTRACK_NONE,
 Model1_TCP_CONNTRACK_SYN_SENT,
 Model1_TCP_CONNTRACK_SYN_RECV,
 Model1_TCP_CONNTRACK_ESTABLISHED,
 Model1_TCP_CONNTRACK_FIN_WAIT,
 Model1_TCP_CONNTRACK_CLOSE_WAIT,
 Model1_TCP_CONNTRACK_LAST_ACK,
 Model1_TCP_CONNTRACK_TIME_WAIT,
 Model1_TCP_CONNTRACK_CLOSE,
 Model1_TCP_CONNTRACK_LISTEN, /* obsolete */

 Model1_TCP_CONNTRACK_MAX,
 Model1_TCP_CONNTRACK_IGNORE,
 Model1_TCP_CONNTRACK_RETRANS,
 Model1_TCP_CONNTRACK_UNACK,
 Model1_TCP_CONNTRACK_TIMEOUT_MAX
};

/* Window scaling is advertised by the sender */


/* SACK is permitted by the sender */


/* This sender sent FIN first */


/* Be liberal in window checking */


/* Has unacknowledged data */


/* The field td_maxack has been set */


/* Marks possibility for expected RFC5961 challenge ACK */


struct Model1_nf_ct_tcp_flags {
 __u8 Model1_flags;
 __u8 Model1_mask;
};


struct Model1_ip_ct_tcp_state {
 Model1_u_int32_t Model1_td_end; /* max of seq + len */
 Model1_u_int32_t Model1_td_maxend; /* max of ack + max(win, 1) */
 Model1_u_int32_t Model1_td_maxwin; /* max(win) */
 Model1_u_int32_t Model1_td_maxack; /* max of ack */
 Model1_u_int8_t Model1_td_scale; /* window scale factor */
 Model1_u_int8_t Model1_flags; /* per direction options */
};

struct Model1_ip_ct_tcp {
 struct Model1_ip_ct_tcp_state Model1_seen[2]; /* connection parameters per direction */
 Model1_u_int8_t Model1_state; /* state of the connection (enum tcp_conntrack) */
 /* For detecting stale connections */
 Model1_u_int8_t Model1_last_dir; /* Direction of the last packet (enum ip_conntrack_dir) */
 Model1_u_int8_t Model1_retrans; /* Number of retransmitted packets */
 Model1_u_int8_t Model1_last_index; /* Index of the last packet */
 Model1_u_int32_t Model1_last_seq; /* Last sequence number seen in dir */
 Model1_u_int32_t Model1_last_ack; /* Last sequence number seen in opposite dir */
 Model1_u_int32_t Model1_last_end; /* Last seq + len */
 Model1_u_int16_t Model1_last_win; /* Last window advertisement seen in dir */
 /* For SYN packets while we may be out-of-sync */
 Model1_u_int8_t Model1_last_wscale; /* Last window scaling factor seen */
 Model1_u_int8_t Model1_last_flags; /* Last flags set */
};


struct Model1_ctl_table_header;
struct Model1_nf_conntrack_ecache;

struct Model1_nf_proto_net {

 struct Model1_ctl_table_header *Model1_ctl_table_header;
 struct Model1_ctl_table *Model1_ctl_table;

 struct Model1_ctl_table_header *Model1_ctl_compat_header;
 struct Model1_ctl_table *Model1_ctl_compat_table;


 unsigned int Model1_users;
};

struct Model1_nf_generic_net {
 struct Model1_nf_proto_net Model1_pn;
 unsigned int Model1_timeout;
};

struct Model1_nf_tcp_net {
 struct Model1_nf_proto_net Model1_pn;
 unsigned int Model1_timeouts[Model1_TCP_CONNTRACK_TIMEOUT_MAX];
 unsigned int Model1_tcp_loose;
 unsigned int Model1_tcp_be_liberal;
 unsigned int Model1_tcp_max_retrans;
};

enum Model1_udp_conntrack {
 Model1_UDP_CT_UNREPLIED,
 Model1_UDP_CT_REPLIED,
 Model1_UDP_CT_MAX
};

struct Model1_nf_udp_net {
 struct Model1_nf_proto_net Model1_pn;
 unsigned int Model1_timeouts[Model1_UDP_CT_MAX];
};

struct Model1_nf_icmp_net {
 struct Model1_nf_proto_net Model1_pn;
 unsigned int Model1_timeout;
};

struct Model1_nf_ip_net {
 struct Model1_nf_generic_net Model1_generic;
 struct Model1_nf_tcp_net Model1_tcp;
 struct Model1_nf_udp_net Model1_udp;
 struct Model1_nf_icmp_net Model1_icmp;
 struct Model1_nf_icmp_net Model1_icmpv6;

 struct Model1_ctl_table_header *Model1_ctl_table_header;
 struct Model1_ctl_table *Model1_ctl_table;

};

struct Model1_ct_pcpu {
 Model1_spinlock_t Model1_lock;
 struct Model1_hlist_nulls_head Model1_unconfirmed;
 struct Model1_hlist_nulls_head Model1_dying;
};

struct Model1_netns_ct {
 Model1_atomic_t Model1_count;
 unsigned int Model1_expect_count;





 struct Model1_ctl_table_header *Model1_sysctl_header;
 struct Model1_ctl_table_header *Model1_acct_sysctl_header;
 struct Model1_ctl_table_header *Model1_tstamp_sysctl_header;
 struct Model1_ctl_table_header *Model1_event_sysctl_header;
 struct Model1_ctl_table_header *Model1_helper_sysctl_header;

 unsigned int Model1_sysctl_log_invalid; /* Log invalid packets */
 int Model1_sysctl_events;
 int Model1_sysctl_acct;
 int Model1_sysctl_auto_assign_helper;
 bool Model1_auto_assign_helper_warned;
 int Model1_sysctl_tstamp;
 int Model1_sysctl_checksum;

 struct Model1_ct_pcpu *Model1_pcpu_lists;
 struct Model1_ip_conntrack_stat *Model1_stat;
 struct Model1_nf_ct_event_notifier *Model1_nf_conntrack_event_cb;
 struct Model1_nf_exp_event_notifier *Model1_nf_expect_event_cb;
 struct Model1_nf_ip_net Model1_nf_ct_proto;




};






struct Model1_nft_af_info;

struct Model1_netns_nftables {
 struct Model1_list_head Model1_af_info;
 struct Model1_list_head Model1_commit_list;
 struct Model1_nft_af_info *Model1_ipv4;
 struct Model1_nft_af_info *Model1_ipv6;
 struct Model1_nft_af_info *Model1_inet;
 struct Model1_nft_af_info *Model1_arp;
 struct Model1_nft_af_info *Model1_bridge;
 struct Model1_nft_af_info *Model1_netdev;
 unsigned int Model1_base_seq;
 Model1_u8 Model1_gencursor;
};












/* All of the structures in this file may not change size as they are
 * passed into the kernel from userspace via netlink sockets.
 */

/* Structure to encapsulate addresses. I do not want to use
 * "standard" structure. My apologies.
 */
typedef union {
 Model1___be32 Model1_a4;
 Model1___be32 Model1_a6[4];
 struct Model1_in6_addr Model1_in6;
} Model1_xfrm_address_t;

/* Ident of a specific xfrm_state. It is used on input to lookup
 * the state by (spi,daddr,ah/esp) or to store information about
 * spi, protocol and tunnel address on output.
 */
struct Model1_xfrm_id {
 Model1_xfrm_address_t Model1_daddr;
 Model1___be32 Model1_spi;
 __u8 Model1_proto;
};

struct Model1_xfrm_sec_ctx {
 __u8 Model1_ctx_doi;
 __u8 Model1_ctx_alg;
 Model1___u16 Model1_ctx_len;
 __u32 Model1_ctx_sid;
 char Model1_ctx_str[0];
};

/* Security Context Domains of Interpretation */



/* Security Context Algorithms */



/* Selector, used as selector both on policy rules (SPD) and SAs. */

struct Model1_xfrm_selector {
 Model1_xfrm_address_t Model1_daddr;
 Model1_xfrm_address_t Model1_saddr;
 Model1___be16 Model1_dport;
 Model1___be16 Model1_dport_mask;
 Model1___be16 Model1_sport;
 Model1___be16 Model1_sport_mask;
 Model1___u16 Model1_family;
 __u8 Model1_prefixlen_d;
 __u8 Model1_prefixlen_s;
 __u8 Model1_proto;
 int Model1_ifindex;
 Model1___kernel_uid32_t Model1_user;
};



struct Model1_xfrm_lifetime_cfg {
 __u64 Model1_soft_byte_limit;
 __u64 Model1_hard_byte_limit;
 __u64 Model1_soft_packet_limit;
 __u64 Model1_hard_packet_limit;
 __u64 Model1_soft_add_expires_seconds;
 __u64 Model1_hard_add_expires_seconds;
 __u64 Model1_soft_use_expires_seconds;
 __u64 Model1_hard_use_expires_seconds;
};

struct Model1_xfrm_lifetime_cur {
 __u64 Model1_bytes;
 __u64 Model1_packets;
 __u64 Model1_add_time;
 __u64 Model1_use_time;
};

struct Model1_xfrm_replay_state {
 __u32 Model1_oseq;
 __u32 Model1_seq;
 __u32 Model1_bitmap;
};



struct Model1_xfrm_replay_state_esn {
 unsigned int Model1_bmp_len;
 __u32 Model1_oseq;
 __u32 Model1_seq;
 __u32 Model1_oseq_hi;
 __u32 Model1_seq_hi;
 __u32 Model1_replay_window;
 __u32 Model1_bmp[0];
};

struct Model1_xfrm_algo {
 char Model1_alg_name[64];
 unsigned int Model1_alg_key_len; /* in bits */
 char Model1_alg_key[0];
};

struct Model1_xfrm_algo_auth {
 char Model1_alg_name[64];
 unsigned int Model1_alg_key_len; /* in bits */
 unsigned int Model1_alg_trunc_len; /* in bits */
 char Model1_alg_key[0];
};

struct Model1_xfrm_algo_aead {
 char Model1_alg_name[64];
 unsigned int Model1_alg_key_len; /* in bits */
 unsigned int Model1_alg_icv_len; /* in bits */
 char Model1_alg_key[0];
};

struct Model1_xfrm_stats {
 __u32 Model1_replay_window;
 __u32 Model1_replay;
 __u32 Model1_integrity_failed;
};

enum {
 Model1_XFRM_POLICY_TYPE_MAIN = 0,
 Model1_XFRM_POLICY_TYPE_SUB = 1,
 Model1_XFRM_POLICY_TYPE_MAX = 2,
 Model1_XFRM_POLICY_TYPE_ANY = 255
};

enum {
 Model1_XFRM_POLICY_IN = 0,
 Model1_XFRM_POLICY_OUT = 1,
 Model1_XFRM_POLICY_FWD = 2,
 Model1_XFRM_POLICY_MASK = 3,
 Model1_XFRM_POLICY_MAX = 3
};

enum {
 Model1_XFRM_SHARE_ANY, /* No limitations */
 Model1_XFRM_SHARE_SESSION, /* For this session only */
 Model1_XFRM_SHARE_USER, /* For this user only */
 Model1_XFRM_SHARE_UNIQUE /* Use once */
};
/* Netlink configuration messages.  */
enum {
 Model1_XFRM_MSG_BASE = 0x10,

 Model1_XFRM_MSG_NEWSA = 0x10,

 Model1_XFRM_MSG_DELSA,

 Model1_XFRM_MSG_GETSA,


 Model1_XFRM_MSG_NEWPOLICY,

 Model1_XFRM_MSG_DELPOLICY,

 Model1_XFRM_MSG_GETPOLICY,


 Model1_XFRM_MSG_ALLOCSPI,

 Model1_XFRM_MSG_ACQUIRE,

 Model1_XFRM_MSG_EXPIRE,


 Model1_XFRM_MSG_UPDPOLICY,

 Model1_XFRM_MSG_UPDSA,


 Model1_XFRM_MSG_POLEXPIRE,


 Model1_XFRM_MSG_FLUSHSA,

 Model1_XFRM_MSG_FLUSHPOLICY,


 Model1_XFRM_MSG_NEWAE,

 Model1_XFRM_MSG_GETAE,


 Model1_XFRM_MSG_REPORT,


 Model1_XFRM_MSG_MIGRATE,


 Model1_XFRM_MSG_NEWSADINFO,

 Model1_XFRM_MSG_GETSADINFO,


 Model1_XFRM_MSG_NEWSPDINFO,

 Model1_XFRM_MSG_GETSPDINFO,


 Model1_XFRM_MSG_MAPPING,

 Model1___XFRM_MSG_MAX
};




/*
 * Generic LSM security context for comunicating to user space
 * NOTE: Same format as sadb_x_sec_ctx
 */
struct Model1_xfrm_user_sec_ctx {
 Model1___u16 Model1_len;
 Model1___u16 Model1_exttype;
 __u8 Model1_ctx_alg; /* LSMs: e.g., selinux == 1 */
 __u8 Model1_ctx_doi;
 Model1___u16 Model1_ctx_len;
};

struct Model1_xfrm_user_tmpl {
 struct Model1_xfrm_id Model1_id;
 Model1___u16 Model1_family;
 Model1_xfrm_address_t Model1_saddr;
 __u32 Model1_reqid;
 __u8 Model1_mode;
 __u8 Model1_share;
 __u8 Model1_optional;
 __u32 Model1_aalgos;
 __u32 Model1_ealgos;
 __u32 Model1_calgos;
};

struct Model1_xfrm_encap_tmpl {
 Model1___u16 Model1_encap_type;
 Model1___be16 Model1_encap_sport;
 Model1___be16 Model1_encap_dport;
 Model1_xfrm_address_t Model1_encap_oa;
};

/* AEVENT flags  */
enum Model1_xfrm_ae_ftype_t {
 Model1_XFRM_AE_UNSPEC,
 Model1_XFRM_AE_RTHR=1, /* replay threshold*/
 Model1_XFRM_AE_RVAL=2, /* replay value */
 Model1_XFRM_AE_LVAL=4, /* lifetime value */
 Model1_XFRM_AE_ETHR=8, /* expiry timer threshold */
 Model1_XFRM_AE_CR=16, /* Event cause is replay update */
 Model1_XFRM_AE_CE=32, /* Event cause is timer expiry */
 Model1_XFRM_AE_CU=64, /* Event cause is policy update */
 Model1___XFRM_AE_MAX


};

struct Model1_xfrm_userpolicy_type {
 __u8 Model1_type;
 Model1___u16 Model1_reserved1;
 __u8 Model1_reserved2;
};

/* Netlink message attributes.  */
enum Model1_xfrm_attr_type_t {
 Model1_XFRMA_UNSPEC,
 Model1_XFRMA_ALG_AUTH, /* struct xfrm_algo */
 Model1_XFRMA_ALG_CRYPT, /* struct xfrm_algo */
 Model1_XFRMA_ALG_COMP, /* struct xfrm_algo */
 Model1_XFRMA_ENCAP, /* struct xfrm_algo + struct xfrm_encap_tmpl */
 Model1_XFRMA_TMPL, /* 1 or more struct xfrm_user_tmpl */
 Model1_XFRMA_SA, /* struct xfrm_usersa_info  */
 Model1_XFRMA_POLICY, /*struct xfrm_userpolicy_info */
 Model1_XFRMA_SEC_CTX, /* struct xfrm_sec_ctx */
 Model1_XFRMA_LTIME_VAL,
 Model1_XFRMA_REPLAY_VAL,
 Model1_XFRMA_REPLAY_THRESH,
 Model1_XFRMA_ETIMER_THRESH,
 Model1_XFRMA_SRCADDR, /* xfrm_address_t */
 Model1_XFRMA_COADDR, /* xfrm_address_t */
 Model1_XFRMA_LASTUSED, /* unsigned long  */
 Model1_XFRMA_POLICY_TYPE, /* struct xfrm_userpolicy_type */
 Model1_XFRMA_MIGRATE,
 Model1_XFRMA_ALG_AEAD, /* struct xfrm_algo_aead */
 Model1_XFRMA_KMADDRESS, /* struct xfrm_user_kmaddress */
 Model1_XFRMA_ALG_AUTH_TRUNC, /* struct xfrm_algo_auth */
 Model1_XFRMA_MARK, /* struct xfrm_mark */
 Model1_XFRMA_TFCPAD, /* __u32 */
 Model1_XFRMA_REPLAY_ESN_VAL, /* struct xfrm_replay_esn */
 Model1_XFRMA_SA_EXTRA_FLAGS, /* __u32 */
 Model1_XFRMA_PROTO, /* __u8 */
 Model1_XFRMA_ADDRESS_FILTER, /* struct xfrm_address_filter */
 Model1_XFRMA_PAD,
 Model1___XFRMA_MAX


};

struct Model1_xfrm_mark {
 __u32 Model1_v; /* value */
 __u32 Model1_m; /* mask */
};

enum Model1_xfrm_sadattr_type_t {
 Model1_XFRMA_SAD_UNSPEC,
 Model1_XFRMA_SAD_CNT,
 Model1_XFRMA_SAD_HINFO,
 Model1___XFRMA_SAD_MAX


};

struct Model1_xfrmu_sadhinfo {
 __u32 Model1_sadhcnt; /* current hash bkts */
 __u32 Model1_sadhmcnt; /* max allowed hash bkts */
};

enum Model1_xfrm_spdattr_type_t {
 Model1_XFRMA_SPD_UNSPEC,
 Model1_XFRMA_SPD_INFO,
 Model1_XFRMA_SPD_HINFO,
 Model1_XFRMA_SPD_IPV4_HTHRESH,
 Model1_XFRMA_SPD_IPV6_HTHRESH,
 Model1___XFRMA_SPD_MAX


};

struct Model1_xfrmu_spdinfo {
 __u32 Model1_incnt;
 __u32 Model1_outcnt;
 __u32 Model1_fwdcnt;
 __u32 Model1_inscnt;
 __u32 Model1_outscnt;
 __u32 Model1_fwdscnt;
};

struct Model1_xfrmu_spdhinfo {
 __u32 Model1_spdhcnt;
 __u32 Model1_spdhmcnt;
};

struct Model1_xfrmu_spdhthresh {
 __u8 Model1_lbits;
 __u8 Model1_rbits;
};

struct Model1_xfrm_usersa_info {
 struct Model1_xfrm_selector Model1_sel;
 struct Model1_xfrm_id Model1_id;
 Model1_xfrm_address_t Model1_saddr;
 struct Model1_xfrm_lifetime_cfg Model1_lft;
 struct Model1_xfrm_lifetime_cur Model1_curlft;
 struct Model1_xfrm_stats Model1_stats;
 __u32 Model1_seq;
 __u32 Model1_reqid;
 Model1___u16 Model1_family;
 __u8 Model1_mode; /* XFRM_MODE_xxx */
 __u8 Model1_replay_window;
 __u8 Model1_flags;
};



struct Model1_xfrm_usersa_id {
 Model1_xfrm_address_t Model1_daddr;
 Model1___be32 Model1_spi;
 Model1___u16 Model1_family;
 __u8 Model1_proto;
};

struct Model1_xfrm_aevent_id {
 struct Model1_xfrm_usersa_id Model1_sa_id;
 Model1_xfrm_address_t Model1_saddr;
 __u32 Model1_flags;
 __u32 Model1_reqid;
};

struct Model1_xfrm_userspi_info {
 struct Model1_xfrm_usersa_info Model1_info;
 __u32 Model1_min;
 __u32 Model1_max;
};

struct Model1_xfrm_userpolicy_info {
 struct Model1_xfrm_selector Model1_sel;
 struct Model1_xfrm_lifetime_cfg Model1_lft;
 struct Model1_xfrm_lifetime_cur Model1_curlft;
 __u32 Model1_priority;
 __u32 Model1_index;
 __u8 Model1_dir;
 __u8 Model1_action;


 __u8 Model1_flags;

 /* Automatically expand selector to include matching ICMP payloads. */

 __u8 Model1_share;
};

struct Model1_xfrm_userpolicy_id {
 struct Model1_xfrm_selector Model1_sel;
 __u32 Model1_index;
 __u8 Model1_dir;
};

struct Model1_xfrm_user_acquire {
 struct Model1_xfrm_id Model1_id;
 Model1_xfrm_address_t Model1_saddr;
 struct Model1_xfrm_selector Model1_sel;
 struct Model1_xfrm_userpolicy_info Model1_policy;
 __u32 Model1_aalgos;
 __u32 Model1_ealgos;
 __u32 Model1_calgos;
 __u32 Model1_seq;
};

struct Model1_xfrm_user_expire {
 struct Model1_xfrm_usersa_info Model1_state;
 __u8 Model1_hard;
};

struct Model1_xfrm_user_polexpire {
 struct Model1_xfrm_userpolicy_info Model1_pol;
 __u8 Model1_hard;
};

struct Model1_xfrm_usersa_flush {
 __u8 Model1_proto;
};

struct Model1_xfrm_user_report {
 __u8 Model1_proto;
 struct Model1_xfrm_selector Model1_sel;
};

/* Used by MIGRATE to pass addresses IKE should use to perform
 * SA negotiation with the peer */
struct Model1_xfrm_user_kmaddress {
 Model1_xfrm_address_t Model1_local;
 Model1_xfrm_address_t Model1_remote;
 __u32 Model1_reserved;
 Model1___u16 Model1_family;
};

struct Model1_xfrm_user_migrate {
 Model1_xfrm_address_t Model1_old_daddr;
 Model1_xfrm_address_t Model1_old_saddr;
 Model1_xfrm_address_t Model1_new_daddr;
 Model1_xfrm_address_t Model1_new_saddr;
 __u8 Model1_proto;
 __u8 Model1_mode;
 Model1___u16 Model1_reserved;
 __u32 Model1_reqid;
 Model1___u16 Model1_old_family;
 Model1___u16 Model1_new_family;
};

struct Model1_xfrm_user_mapping {
 struct Model1_xfrm_usersa_id Model1_id;
 __u32 Model1_reqid;
 Model1_xfrm_address_t Model1_old_saddr;
 Model1_xfrm_address_t Model1_new_saddr;
 Model1___be16 Model1_old_sport;
 Model1___be16 Model1_new_sport;
};

struct Model1_xfrm_address_filter {
 Model1_xfrm_address_t Model1_saddr;
 Model1_xfrm_address_t Model1_daddr;
 Model1___u16 Model1_family;
 __u8 Model1_splen;
 __u8 Model1_dplen;
};
enum Model1_xfrm_nlgroups {
 Model1_XFRMNLGRP_NONE,

 Model1_XFRMNLGRP_ACQUIRE,

 Model1_XFRMNLGRP_EXPIRE,

 Model1_XFRMNLGRP_SA,

 Model1_XFRMNLGRP_POLICY,

 Model1_XFRMNLGRP_AEVENTS,

 Model1_XFRMNLGRP_REPORT,

 Model1_XFRMNLGRP_MIGRATE,

 Model1_XFRMNLGRP_MAPPING,

 Model1___XFRMNLGRP_MAX
};




/* interrupt.h */



/**
 * enum irqreturn
 * @IRQ_NONE		interrupt was not from this device or was not handled
 * @IRQ_HANDLED		interrupt was handled by this device
 * @IRQ_WAKE_THREAD	handler requests to wake the handler thread
 */
enum Model1_irqreturn {
 Model1_IRQ_NONE = (0 << 0),
 Model1_IRQ_HANDLED = (1 << 0),
 Model1_IRQ_WAKE_THREAD = (1 << 1),
};

typedef enum Model1_irqreturn Model1_irqreturn_t;














static inline __attribute__((no_instrument_function)) void Model1_ftrace_nmi_enter(void) { }
static inline __attribute__((no_instrument_function)) void Model1_ftrace_nmi_exit(void) { }








struct Model1_context_tracking {
 /*
	 * When active is false, probes are unset in order
	 * to minimize overhead: TIF flags are cleared
	 * and calls to user_enter/exit are ignored. This
	 * may be further optimized using static keys.
	 */
 bool Model1_active;
 int Model1_recursion;
 enum Model1_ctx_state {
  Model1_CONTEXT_DISABLED = -1, /* returned by ct_state() if unknown */
  Model1_CONTEXT_KERNEL = 0,
  Model1_CONTEXT_USER,
  Model1_CONTEXT_GUEST,
 } Model1_state;
};
static inline __attribute__((no_instrument_function)) bool Model1_context_tracking_in_user(void) { return false; }
static inline __attribute__((no_instrument_function)) bool Model1_context_tracking_active(void) { return false; }
static inline __attribute__((no_instrument_function)) bool Model1_context_tracking_is_enabled(void) { return false; }
static inline __attribute__((no_instrument_function)) bool Model1_context_tracking_cpu_is_enabled(void) { return false; }





struct Model1_task_struct;

/*
 * vtime_accounting_cpu_enabled() definitions/declarations
 */
static inline __attribute__((no_instrument_function)) bool Model1_vtime_accounting_cpu_enabled(void) { return false; }



/*
 * Common vtime APIs
 */
static inline __attribute__((no_instrument_function)) void Model1_vtime_task_switch(struct Model1_task_struct *Model1_prev) { }
static inline __attribute__((no_instrument_function)) void Model1_vtime_account_system(struct Model1_task_struct *Model1_tsk) { }
static inline __attribute__((no_instrument_function)) void Model1_vtime_account_user(struct Model1_task_struct *Model1_tsk) { }
static inline __attribute__((no_instrument_function)) void Model1_vtime_user_enter(struct Model1_task_struct *Model1_tsk) { }
static inline __attribute__((no_instrument_function)) void Model1_vtime_user_exit(struct Model1_task_struct *Model1_tsk) { }
static inline __attribute__((no_instrument_function)) void Model1_vtime_guest_enter(struct Model1_task_struct *Model1_tsk) { }
static inline __attribute__((no_instrument_function)) void Model1_vtime_guest_exit(struct Model1_task_struct *Model1_tsk) { }
static inline __attribute__((no_instrument_function)) void Model1_vtime_init_idle(struct Model1_task_struct *Model1_tsk, int Model1_cpu) { }
static inline __attribute__((no_instrument_function)) void Model1_vtime_account_irq_enter(struct Model1_task_struct *Model1_tsk) { }
static inline __attribute__((no_instrument_function)) void Model1_vtime_account_irq_exit(struct Model1_task_struct *Model1_tsk) { }






static inline __attribute__((no_instrument_function)) void Model1_irqtime_account_irq(struct Model1_task_struct *Model1_tsk) { }


static inline __attribute__((no_instrument_function)) void Model1_account_irq_enter_time(struct Model1_task_struct *Model1_tsk)
{
 Model1_vtime_account_irq_enter(Model1_tsk);
 Model1_irqtime_account_irq(Model1_tsk);
}

static inline __attribute__((no_instrument_function)) void Model1_account_irq_exit_time(struct Model1_task_struct *Model1_tsk)
{
 Model1_vtime_account_irq_exit(Model1_tsk);
 Model1_irqtime_account_irq(Model1_tsk);
}







/*
 * Please do not include this file in generic code.  There is currently
 * no requirement for any architecture to implement anything held
 * within this file.
 *
 * Thanks. --rmk
 */











/*
 * Interrupt flow handler typedefs are defined here to avoid circular
 * include dependencies.
 */

struct Model1_irq_desc;
struct Model1_irq_data;
typedef void (*Model1_irq_flow_handler_t)(struct Model1_irq_desc *Model1_desc);
typedef void (*Model1_irq_preflow_handler_t)(struct Model1_irq_data *Model1_data);





/*
 * Copyright 2006 PathScale, Inc.  All Rights Reserved.
 *
 * This file is free software; you can redistribute it and/or modify
 * it under the terms of version 2 of the GNU General Public License
 * as published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software Foundation,
 * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301, USA.
 */
struct Model1_device;
struct Model1_resource;

          void Model1___iowrite32_copy(void *Model1_to, const void *Model1_from, Model1_size_t Model1_count);
void Model1___ioread32_copy(void *Model1_to, const void *Model1_from, Model1_size_t Model1_count);
void Model1___iowrite64_copy(void *Model1_to, const void *Model1_from, Model1_size_t Model1_count);


int Model1_ioremap_page_range(unsigned long Model1_addr, unsigned long Model1_end,
         Model1_phys_addr_t Model1_phys_addr, Model1_pgprot_t Model1_prot);
void __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_ioremap_huge_init(void);
int Model1_arch_ioremap_pud_supported(void);
int Model1_arch_ioremap_pmd_supported(void);




/*
 * Managed iomap interface
 */

void * Model1_devm_ioport_map(struct Model1_device *Model1_dev, unsigned long Model1_port,
          unsigned int Model1_nr);
void Model1_devm_ioport_unmap(struct Model1_device *Model1_dev, void *Model1_addr);
void *Model1_devm_ioremap(struct Model1_device *Model1_dev, Model1_resource_size_t Model1_offset,
      Model1_resource_size_t Model1_size);
void *Model1_devm_ioremap_nocache(struct Model1_device *Model1_dev, Model1_resource_size_t Model1_offset,
       Model1_resource_size_t Model1_size);
void *Model1_devm_ioremap_wc(struct Model1_device *Model1_dev, Model1_resource_size_t Model1_offset,
       Model1_resource_size_t Model1_size);
void Model1_devm_iounmap(struct Model1_device *Model1_dev, void *Model1_addr);
int Model1_check_signature(const volatile void *Model1_io_addr,
   const unsigned char *Model1_signature, int Model1_length);
void Model1_devm_ioremap_release(struct Model1_device *Model1_dev, void *Model1_res);

void *Model1_devm_memremap(struct Model1_device *Model1_dev, Model1_resource_size_t Model1_offset,
  Model1_size_t Model1_size, unsigned long Model1_flags);
void Model1_devm_memunmap(struct Model1_device *Model1_dev, void *Model1_addr);

void *Model1___devm_memremap_pages(struct Model1_device *Model1_dev, struct Model1_resource *Model1_res);

/*
 * Some systems do not have legacy ISA devices.
 * /dev/port is not a valid interface on these systems.
 * So for those archs, <asm/io.h> should define the following symbol.
 */




/*
 * Some systems (x86 without PAT) have a somewhat reliable way to mark a
 * physical address range such that uncached mappings will actually
 * end up write-combining.  This facility should be used in conjunction
 * with pgprot_writecombine, ioremap-wc, or set_memory_wc, since it has
 * no effect if the per-page mechanisms are functional.
 * (On x86 without PAT, these functions manipulate MTRRs.)
 *
 * arch_phys_del_wc(0) or arch_phys_del_wc(any error code) is guaranteed
 * to have no effect.
 */
enum {
 /* See memremap() kernel-doc for usage description... */
 Model1_MEMREMAP_WB = 1 << 0,
 Model1_MEMREMAP_WT = 1 << 1,
 Model1_MEMREMAP_WC = 1 << 2,
};

void *Model1_memremap(Model1_resource_size_t Model1_offset, Model1_size_t Model1_size, unsigned long Model1_flags);
void Model1_memunmap(void *Model1_addr);



/*
 *	(C) 1992, 1993 Linus Torvalds, (C) 1997 Ingo Molnar
 *
 *	IRQ/IPI changes taken from work by Thomas Radke
 *	<tomsoft@informatik.tu-chemnitz.de>
 */




static inline __attribute__((no_instrument_function)) int Model1_irq_canonicalize(int Model1_irq)
{
 return ((Model1_irq == 2) ? 9 : Model1_irq);
}
struct Model1_irq_desc;



extern int Model1_check_irq_vectors_for_cpu_disable(void);
extern void Model1_fixup_irqs(void);
extern void Model1_irq_force_complete_move(struct Model1_irq_desc *Model1_desc);



extern void Model1_kvm_set_posted_intr_wakeup_handler(void (*Model1_handler)(void));


extern void (*Model1_x86_platform_ipi_callback)(void);
extern void Model1_native_init_IRQ(void);

extern bool Model1_handle_irq(struct Model1_irq_desc *Model1_desc, struct Model1_pt_regs *Model1_regs);

extern unsigned int Model1_do_IRQ(struct Model1_pt_regs *Model1_regs);

/* Interrupt vector management */
extern unsigned long Model1_used_vectors[(((256) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];
extern int Model1_vector_used_by_percpu_irq(unsigned int Model1_vector);

extern void Model1_init_ISA_irqs(void);


void Model1_arch_trigger_all_cpu_backtrace(bool);

/*
 * Per-cpu current frame pointer - the location of the last exception frame on
 * the stack, stored in the per-cpu area.
 *
 * Jeremy Fitzhardinge <jeremy@goop.org>
 */







extern __attribute__((section(".data..percpu" ""))) __typeof__(struct Model1_pt_regs *) Model1_irq_regs;

static inline __attribute__((no_instrument_function)) struct Model1_pt_regs *Model1_get_irq_regs(void)
{
 return ({ typeof(Model1_irq_regs) Model1_pscr_ret__; do { const void *Model1___vpp_verify = (typeof((&(Model1_irq_regs)) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); switch(sizeof(Model1_irq_regs)) { case 1: Model1_pscr_ret__ = ({ typeof(Model1_irq_regs) Model1_pfo_ret__; switch (sizeof(Model1_irq_regs)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_irq_regs)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_irq_regs)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_irq_regs)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_irq_regs)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 2: Model1_pscr_ret__ = ({ typeof(Model1_irq_regs) Model1_pfo_ret__; switch (sizeof(Model1_irq_regs)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_irq_regs)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_irq_regs)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_irq_regs)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_irq_regs)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 4: Model1_pscr_ret__ = ({ typeof(Model1_irq_regs) Model1_pfo_ret__; switch (sizeof(Model1_irq_regs)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_irq_regs)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_irq_regs)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_irq_regs)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_irq_regs)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 8: Model1_pscr_ret__ = ({ typeof(Model1_irq_regs) Model1_pfo_ret__; switch (sizeof(Model1_irq_regs)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_irq_regs)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_irq_regs)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_irq_regs)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_irq_regs)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; default: Model1___bad_size_call_parameter(); break; } Model1_pscr_ret__; });
}

static inline __attribute__((no_instrument_function)) struct Model1_pt_regs *Model1_set_irq_regs(struct Model1_pt_regs *Model1_new_regs)
{
 struct Model1_pt_regs *Model1_old_regs;

 Model1_old_regs = Model1_get_irq_regs();
 do { do { const void *Model1___vpp_verify = (typeof((&(Model1_irq_regs)) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); switch(sizeof(Model1_irq_regs)) { case 1: do { typedef typeof((Model1_irq_regs)) Model1_pto_T__; if (0) { Model1_pto_T__ Model1_pto_tmp__; Model1_pto_tmp__ = (Model1_new_regs); (void)Model1_pto_tmp__; } switch (sizeof((Model1_irq_regs))) { case 1: asm("mov" "b %1,""%%""gs"":" "%" "0" : "+m" ((Model1_irq_regs)) : "qi" ((Model1_pto_T__)(Model1_new_regs))); break; case 2: asm("mov" "w %1,""%%""gs"":" "%" "0" : "+m" ((Model1_irq_regs)) : "ri" ((Model1_pto_T__)(Model1_new_regs))); break; case 4: asm("mov" "l %1,""%%""gs"":" "%" "0" : "+m" ((Model1_irq_regs)) : "ri" ((Model1_pto_T__)(Model1_new_regs))); break; case 8: asm("mov" "q %1,""%%""gs"":" "%" "0" : "+m" ((Model1_irq_regs)) : "re" ((Model1_pto_T__)(Model1_new_regs))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 2: do { typedef typeof((Model1_irq_regs)) Model1_pto_T__; if (0) { Model1_pto_T__ Model1_pto_tmp__; Model1_pto_tmp__ = (Model1_new_regs); (void)Model1_pto_tmp__; } switch (sizeof((Model1_irq_regs))) { case 1: asm("mov" "b %1,""%%""gs"":" "%" "0" : "+m" ((Model1_irq_regs)) : "qi" ((Model1_pto_T__)(Model1_new_regs))); break; case 2: asm("mov" "w %1,""%%""gs"":" "%" "0" : "+m" ((Model1_irq_regs)) : "ri" ((Model1_pto_T__)(Model1_new_regs))); break; case 4: asm("mov" "l %1,""%%""gs"":" "%" "0" : "+m" ((Model1_irq_regs)) : "ri" ((Model1_pto_T__)(Model1_new_regs))); break; case 8: asm("mov" "q %1,""%%""gs"":" "%" "0" : "+m" ((Model1_irq_regs)) : "re" ((Model1_pto_T__)(Model1_new_regs))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 4: do { typedef typeof((Model1_irq_regs)) Model1_pto_T__; if (0) { Model1_pto_T__ Model1_pto_tmp__; Model1_pto_tmp__ = (Model1_new_regs); (void)Model1_pto_tmp__; } switch (sizeof((Model1_irq_regs))) { case 1: asm("mov" "b %1,""%%""gs"":" "%" "0" : "+m" ((Model1_irq_regs)) : "qi" ((Model1_pto_T__)(Model1_new_regs))); break; case 2: asm("mov" "w %1,""%%""gs"":" "%" "0" : "+m" ((Model1_irq_regs)) : "ri" ((Model1_pto_T__)(Model1_new_regs))); break; case 4: asm("mov" "l %1,""%%""gs"":" "%" "0" : "+m" ((Model1_irq_regs)) : "ri" ((Model1_pto_T__)(Model1_new_regs))); break; case 8: asm("mov" "q %1,""%%""gs"":" "%" "0" : "+m" ((Model1_irq_regs)) : "re" ((Model1_pto_T__)(Model1_new_regs))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 8: do { typedef typeof((Model1_irq_regs)) Model1_pto_T__; if (0) { Model1_pto_T__ Model1_pto_tmp__; Model1_pto_tmp__ = (Model1_new_regs); (void)Model1_pto_tmp__; } switch (sizeof((Model1_irq_regs))) { case 1: asm("mov" "b %1,""%%""gs"":" "%" "0" : "+m" ((Model1_irq_regs)) : "qi" ((Model1_pto_T__)(Model1_new_regs))); break; case 2: asm("mov" "w %1,""%%""gs"":" "%" "0" : "+m" ((Model1_irq_regs)) : "ri" ((Model1_pto_T__)(Model1_new_regs))); break; case 4: asm("mov" "l %1,""%%""gs"":" "%" "0" : "+m" ((Model1_irq_regs)) : "ri" ((Model1_pto_T__)(Model1_new_regs))); break; case 8: asm("mov" "q %1,""%%""gs"":" "%" "0" : "+m" ((Model1_irq_regs)) : "re" ((Model1_pto_T__)(Model1_new_regs))); break; default: Model1___bad_percpu_size(); } } while (0);break; default: Model1___bad_size_call_parameter();break; } } while (0);

 return Model1_old_regs;
}

struct Model1_seq_file;
struct Model1_module;
struct Model1_msi_msg;
enum Model1_irqchip_irq_state;

/*
 * IRQ line status.
 *
 * Bits 0-7 are the same as the IRQF_* bits in linux/interrupt.h
 *
 * IRQ_TYPE_NONE		- default, unspecified type
 * IRQ_TYPE_EDGE_RISING		- rising edge triggered
 * IRQ_TYPE_EDGE_FALLING	- falling edge triggered
 * IRQ_TYPE_EDGE_BOTH		- rising and falling edge triggered
 * IRQ_TYPE_LEVEL_HIGH		- high level triggered
 * IRQ_TYPE_LEVEL_LOW		- low level triggered
 * IRQ_TYPE_LEVEL_MASK		- Mask to filter out the level bits
 * IRQ_TYPE_SENSE_MASK		- Mask for all the above bits
 * IRQ_TYPE_DEFAULT		- For use by some PICs to ask irq_set_type
 *				  to setup the HW to a sane default (used
 *                                by irqdomain map() callbacks to synchronize
 *                                the HW state and SW flags for a newly
 *                                allocated descriptor).
 *
 * IRQ_TYPE_PROBE		- Special flag for probing in progress
 *
 * Bits which can be modified via irq_set/clear/modify_status_flags()
 * IRQ_LEVEL			- Interrupt is level type. Will be also
 *				  updated in the code when the above trigger
 *				  bits are modified via irq_set_irq_type()
 * IRQ_PER_CPU			- Mark an interrupt PER_CPU. Will protect
 *				  it from affinity setting
 * IRQ_NOPROBE			- Interrupt cannot be probed by autoprobing
 * IRQ_NOREQUEST		- Interrupt cannot be requested via
 *				  request_irq()
 * IRQ_NOTHREAD			- Interrupt cannot be threaded
 * IRQ_NOAUTOEN			- Interrupt is not automatically enabled in
 *				  request/setup_irq()
 * IRQ_NO_BALANCING		- Interrupt cannot be balanced (affinity set)
 * IRQ_MOVE_PCNTXT		- Interrupt can be migrated from process context
 * IRQ_NESTED_THREAD		- Interrupt nests into another thread
 * IRQ_PER_CPU_DEVID		- Dev_id is a per-cpu variable
 * IRQ_IS_POLLED		- Always polled by another interrupt. Exclude
 *				  it from the spurious interrupt detection
 *				  mechanism and from core side polling.
 * IRQ_DISABLE_UNLAZY		- Disable lazy irq disable
 */
enum {
 Model1_IRQ_TYPE_NONE = 0x00000000,
 Model1_IRQ_TYPE_EDGE_RISING = 0x00000001,
 Model1_IRQ_TYPE_EDGE_FALLING = 0x00000002,
 Model1_IRQ_TYPE_EDGE_BOTH = (Model1_IRQ_TYPE_EDGE_FALLING | Model1_IRQ_TYPE_EDGE_RISING),
 Model1_IRQ_TYPE_LEVEL_HIGH = 0x00000004,
 Model1_IRQ_TYPE_LEVEL_LOW = 0x00000008,
 Model1_IRQ_TYPE_LEVEL_MASK = (Model1_IRQ_TYPE_LEVEL_LOW | Model1_IRQ_TYPE_LEVEL_HIGH),
 Model1_IRQ_TYPE_SENSE_MASK = 0x0000000f,
 Model1_IRQ_TYPE_DEFAULT = Model1_IRQ_TYPE_SENSE_MASK,

 Model1_IRQ_TYPE_PROBE = 0x00000010,

 Model1_IRQ_LEVEL = (1 << 8),
 Model1_IRQ_PER_CPU = (1 << 9),
 Model1_IRQ_NOPROBE = (1 << 10),
 Model1_IRQ_NOREQUEST = (1 << 11),
 Model1_IRQ_NOAUTOEN = (1 << 12),
 Model1_IRQ_NO_BALANCING = (1 << 13),
 Model1_IRQ_MOVE_PCNTXT = (1 << 14),
 Model1_IRQ_NESTED_THREAD = (1 << 15),
 Model1_IRQ_NOTHREAD = (1 << 16),
 Model1_IRQ_PER_CPU_DEVID = (1 << 17),
 Model1_IRQ_IS_POLLED = (1 << 18),
 Model1_IRQ_DISABLE_UNLAZY = (1 << 19),
};
/*
 * Return value for chip->irq_set_affinity()
 *
 * IRQ_SET_MASK_OK	- OK, core updates irq_common_data.affinity
 * IRQ_SET_MASK_NOCPY	- OK, chip did update irq_common_data.affinity
 * IRQ_SET_MASK_OK_DONE	- Same as IRQ_SET_MASK_OK for core. Special code to
 *			  support stacked irqchips, which indicates skipping
 *			  all descendent irqchips.
 */
enum {
 Model1_IRQ_SET_MASK_OK = 0,
 Model1_IRQ_SET_MASK_OK_NOCOPY,
 Model1_IRQ_SET_MASK_OK_DONE,
};

struct Model1_msi_desc;
struct Model1_irq_domain;

/**
 * struct irq_common_data - per irq data shared by all irqchips
 * @state_use_accessors: status information for irq chip functions.
 *			Use accessor functions to deal with it
 * @node:		node index useful for balancing
 * @handler_data:	per-IRQ data for the irq_chip methods
 * @affinity:		IRQ affinity on SMP. If this is an IPI
 *			related irq, then this is the mask of the
 *			CPUs to which an IPI can be sent.
 * @msi_desc:		MSI descriptor
 * @ipi_offset:		Offset of first IPI target cpu in @affinity. Optional.
 */
struct Model1_irq_common_data {
 unsigned int Model1_state_use_accessors;

 unsigned int Model1_node;

 void *Model1_handler_data;
 struct Model1_msi_desc *Model1_msi_desc;
 Model1_cpumask_var_t Model1_affinity;



};

/**
 * struct irq_data - per irq chip data passed down to chip functions
 * @mask:		precomputed bitmask for accessing the chip registers
 * @irq:		interrupt number
 * @hwirq:		hardware interrupt number, local to the interrupt domain
 * @common:		point to data shared by all irqchips
 * @chip:		low level interrupt hardware access
 * @domain:		Interrupt translation domain; responsible for mapping
 *			between hwirq number and linux irq number.
 * @parent_data:	pointer to parent struct irq_data to support hierarchy
 *			irq_domain
 * @chip_data:		platform-specific per-chip private data for the chip
 *			methods, to allow shared chip implementations
 */
struct Model1_irq_data {
 Model1_u32 Model1_mask;
 unsigned int Model1_irq;
 unsigned long Model1_hwirq;
 struct Model1_irq_common_data *Model1_common;
 struct Model1_irq_chip *Model1_chip;
 struct Model1_irq_domain *Model1_domain;

 struct Model1_irq_data *Model1_parent_data;

 void *Model1_chip_data;
};

/*
 * Bit masks for irq_common_data.state_use_accessors
 *
 * IRQD_TRIGGER_MASK		- Mask for the trigger type bits
 * IRQD_SETAFFINITY_PENDING	- Affinity setting is pending
 * IRQD_NO_BALANCING		- Balancing disabled for this IRQ
 * IRQD_PER_CPU			- Interrupt is per cpu
 * IRQD_AFFINITY_SET		- Interrupt affinity was set
 * IRQD_LEVEL			- Interrupt is level triggered
 * IRQD_WAKEUP_STATE		- Interrupt is configured for wakeup
 *				  from suspend
 * IRDQ_MOVE_PCNTXT		- Interrupt can be moved in process
 *				  context
 * IRQD_IRQ_DISABLED		- Disabled state of the interrupt
 * IRQD_IRQ_MASKED		- Masked state of the interrupt
 * IRQD_IRQ_INPROGRESS		- In progress state of the interrupt
 * IRQD_WAKEUP_ARMED		- Wakeup mode armed
 * IRQD_FORWARDED_TO_VCPU	- The interrupt is forwarded to a VCPU
 * IRQD_AFFINITY_MANAGED	- Affinity is auto-managed by the kernel
 */
enum {
 Model1_IRQD_TRIGGER_MASK = 0xf,
 Model1_IRQD_SETAFFINITY_PENDING = (1 << 8),
 Model1_IRQD_NO_BALANCING = (1 << 10),
 Model1_IRQD_PER_CPU = (1 << 11),
 Model1_IRQD_AFFINITY_SET = (1 << 12),
 Model1_IRQD_LEVEL = (1 << 13),
 Model1_IRQD_WAKEUP_STATE = (1 << 14),
 Model1_IRQD_MOVE_PCNTXT = (1 << 15),
 Model1_IRQD_IRQ_DISABLED = (1 << 16),
 Model1_IRQD_IRQ_MASKED = (1 << 17),
 Model1_IRQD_IRQ_INPROGRESS = (1 << 18),
 Model1_IRQD_WAKEUP_ARMED = (1 << 19),
 Model1_IRQD_FORWARDED_TO_VCPU = (1 << 20),
 Model1_IRQD_AFFINITY_MANAGED = (1 << 21),
};



static inline __attribute__((no_instrument_function)) bool Model1_irqd_is_setaffinity_pending(struct Model1_irq_data *Model1_d)
{
 return (((Model1_d)->Model1_common)->Model1_state_use_accessors) & Model1_IRQD_SETAFFINITY_PENDING;
}

static inline __attribute__((no_instrument_function)) bool Model1_irqd_is_per_cpu(struct Model1_irq_data *Model1_d)
{
 return (((Model1_d)->Model1_common)->Model1_state_use_accessors) & Model1_IRQD_PER_CPU;
}

static inline __attribute__((no_instrument_function)) bool Model1_irqd_can_balance(struct Model1_irq_data *Model1_d)
{
 return !((((Model1_d)->Model1_common)->Model1_state_use_accessors) & (Model1_IRQD_PER_CPU | Model1_IRQD_NO_BALANCING));
}

static inline __attribute__((no_instrument_function)) bool Model1_irqd_affinity_was_set(struct Model1_irq_data *Model1_d)
{
 return (((Model1_d)->Model1_common)->Model1_state_use_accessors) & Model1_IRQD_AFFINITY_SET;
}

static inline __attribute__((no_instrument_function)) void Model1_irqd_mark_affinity_was_set(struct Model1_irq_data *Model1_d)
{
 (((Model1_d)->Model1_common)->Model1_state_use_accessors) |= Model1_IRQD_AFFINITY_SET;
}

static inline __attribute__((no_instrument_function)) Model1_u32 Model1_irqd_get_trigger_type(struct Model1_irq_data *Model1_d)
{
 return (((Model1_d)->Model1_common)->Model1_state_use_accessors) & Model1_IRQD_TRIGGER_MASK;
}

/*
 * Must only be called inside irq_chip.irq_set_type() functions.
 */
static inline __attribute__((no_instrument_function)) void Model1_irqd_set_trigger_type(struct Model1_irq_data *Model1_d, Model1_u32 Model1_type)
{
 (((Model1_d)->Model1_common)->Model1_state_use_accessors) &= ~Model1_IRQD_TRIGGER_MASK;
 (((Model1_d)->Model1_common)->Model1_state_use_accessors) |= Model1_type & Model1_IRQD_TRIGGER_MASK;
}

static inline __attribute__((no_instrument_function)) bool Model1_irqd_is_level_type(struct Model1_irq_data *Model1_d)
{
 return (((Model1_d)->Model1_common)->Model1_state_use_accessors) & Model1_IRQD_LEVEL;
}

static inline __attribute__((no_instrument_function)) bool Model1_irqd_is_wakeup_set(struct Model1_irq_data *Model1_d)
{
 return (((Model1_d)->Model1_common)->Model1_state_use_accessors) & Model1_IRQD_WAKEUP_STATE;
}

static inline __attribute__((no_instrument_function)) bool Model1_irqd_can_move_in_process_context(struct Model1_irq_data *Model1_d)
{
 return (((Model1_d)->Model1_common)->Model1_state_use_accessors) & Model1_IRQD_MOVE_PCNTXT;
}

static inline __attribute__((no_instrument_function)) bool Model1_irqd_irq_disabled(struct Model1_irq_data *Model1_d)
{
 return (((Model1_d)->Model1_common)->Model1_state_use_accessors) & Model1_IRQD_IRQ_DISABLED;
}

static inline __attribute__((no_instrument_function)) bool Model1_irqd_irq_masked(struct Model1_irq_data *Model1_d)
{
 return (((Model1_d)->Model1_common)->Model1_state_use_accessors) & Model1_IRQD_IRQ_MASKED;
}

static inline __attribute__((no_instrument_function)) bool Model1_irqd_irq_inprogress(struct Model1_irq_data *Model1_d)
{
 return (((Model1_d)->Model1_common)->Model1_state_use_accessors) & Model1_IRQD_IRQ_INPROGRESS;
}

static inline __attribute__((no_instrument_function)) bool Model1_irqd_is_wakeup_armed(struct Model1_irq_data *Model1_d)
{
 return (((Model1_d)->Model1_common)->Model1_state_use_accessors) & Model1_IRQD_WAKEUP_ARMED;
}

static inline __attribute__((no_instrument_function)) bool Model1_irqd_is_forwarded_to_vcpu(struct Model1_irq_data *Model1_d)
{
 return (((Model1_d)->Model1_common)->Model1_state_use_accessors) & Model1_IRQD_FORWARDED_TO_VCPU;
}

static inline __attribute__((no_instrument_function)) void Model1_irqd_set_forwarded_to_vcpu(struct Model1_irq_data *Model1_d)
{
 (((Model1_d)->Model1_common)->Model1_state_use_accessors) |= Model1_IRQD_FORWARDED_TO_VCPU;
}

static inline __attribute__((no_instrument_function)) void Model1_irqd_clr_forwarded_to_vcpu(struct Model1_irq_data *Model1_d)
{
 (((Model1_d)->Model1_common)->Model1_state_use_accessors) &= ~Model1_IRQD_FORWARDED_TO_VCPU;
}

static inline __attribute__((no_instrument_function)) bool Model1_irqd_affinity_is_managed(struct Model1_irq_data *Model1_d)
{
 return (((Model1_d)->Model1_common)->Model1_state_use_accessors) & Model1_IRQD_AFFINITY_MANAGED;
}



static inline __attribute__((no_instrument_function)) Model1_irq_hw_number_t Model1_irqd_to_hwirq(struct Model1_irq_data *Model1_d)
{
 return Model1_d->Model1_hwirq;
}

/**
 * struct irq_chip - hardware interrupt chip descriptor
 *
 * @parent_device:	pointer to parent device for irqchip
 * @name:		name for /proc/interrupts
 * @irq_startup:	start up the interrupt (defaults to ->enable if NULL)
 * @irq_shutdown:	shut down the interrupt (defaults to ->disable if NULL)
 * @irq_enable:		enable the interrupt (defaults to chip->unmask if NULL)
 * @irq_disable:	disable the interrupt
 * @irq_ack:		start of a new interrupt
 * @irq_mask:		mask an interrupt source
 * @irq_mask_ack:	ack and mask an interrupt source
 * @irq_unmask:		unmask an interrupt source
 * @irq_eoi:		end of interrupt
 * @irq_set_affinity:	set the CPU affinity on SMP machines
 * @irq_retrigger:	resend an IRQ to the CPU
 * @irq_set_type:	set the flow type (IRQ_TYPE_LEVEL/etc.) of an IRQ
 * @irq_set_wake:	enable/disable power-management wake-on of an IRQ
 * @irq_bus_lock:	function to lock access to slow bus (i2c) chips
 * @irq_bus_sync_unlock:function to sync and unlock slow bus (i2c) chips
 * @irq_cpu_online:	configure an interrupt source for a secondary CPU
 * @irq_cpu_offline:	un-configure an interrupt source for a secondary CPU
 * @irq_suspend:	function called from core code on suspend once per
 *			chip, when one or more interrupts are installed
 * @irq_resume:		function called from core code on resume once per chip,
 *			when one ore more interrupts are installed
 * @irq_pm_shutdown:	function called from core code on shutdown once per chip
 * @irq_calc_mask:	Optional function to set irq_data.mask for special cases
 * @irq_print_chip:	optional to print special chip info in show_interrupts
 * @irq_request_resources:	optional to request resources before calling
 *				any other callback related to this irq
 * @irq_release_resources:	optional to release resources acquired with
 *				irq_request_resources
 * @irq_compose_msi_msg:	optional to compose message content for MSI
 * @irq_write_msi_msg:	optional to write message content for MSI
 * @irq_get_irqchip_state:	return the internal state of an interrupt
 * @irq_set_irqchip_state:	set the internal state of a interrupt
 * @irq_set_vcpu_affinity:	optional to target a vCPU in a virtual machine
 * @ipi_send_single:	send a single IPI to destination cpus
 * @ipi_send_mask:	send an IPI to destination cpus in cpumask
 * @flags:		chip specific flags
 */
struct Model1_irq_chip {
 struct Model1_device *Model1_parent_device;
 const char *Model1_name;
 unsigned int (*Model1_irq_startup)(struct Model1_irq_data *Model1_data);
 void (*Model1_irq_shutdown)(struct Model1_irq_data *Model1_data);
 void (*Model1_irq_enable)(struct Model1_irq_data *Model1_data);
 void (*Model1_irq_disable)(struct Model1_irq_data *Model1_data);

 void (*Model1_irq_ack)(struct Model1_irq_data *Model1_data);
 void (*Model1_irq_mask)(struct Model1_irq_data *Model1_data);
 void (*Model1_irq_mask_ack)(struct Model1_irq_data *Model1_data);
 void (*Model1_irq_unmask)(struct Model1_irq_data *Model1_data);
 void (*Model1_irq_eoi)(struct Model1_irq_data *Model1_data);

 int (*Model1_irq_set_affinity)(struct Model1_irq_data *Model1_data, const struct Model1_cpumask *Model1_dest, bool Model1_force);
 int (*Model1_irq_retrigger)(struct Model1_irq_data *Model1_data);
 int (*Model1_irq_set_type)(struct Model1_irq_data *Model1_data, unsigned int Model1_flow_type);
 int (*Model1_irq_set_wake)(struct Model1_irq_data *Model1_data, unsigned int Model1_on);

 void (*Model1_irq_bus_lock)(struct Model1_irq_data *Model1_data);
 void (*Model1_irq_bus_sync_unlock)(struct Model1_irq_data *Model1_data);

 void (*Model1_irq_cpu_online)(struct Model1_irq_data *Model1_data);
 void (*Model1_irq_cpu_offline)(struct Model1_irq_data *Model1_data);

 void (*Model1_irq_suspend)(struct Model1_irq_data *Model1_data);
 void (*Model1_irq_resume)(struct Model1_irq_data *Model1_data);
 void (*Model1_irq_pm_shutdown)(struct Model1_irq_data *Model1_data);

 void (*Model1_irq_calc_mask)(struct Model1_irq_data *Model1_data);

 void (*Model1_irq_print_chip)(struct Model1_irq_data *Model1_data, struct Model1_seq_file *Model1_p);
 int (*Model1_irq_request_resources)(struct Model1_irq_data *Model1_data);
 void (*Model1_irq_release_resources)(struct Model1_irq_data *Model1_data);

 void (*Model1_irq_compose_msi_msg)(struct Model1_irq_data *Model1_data, struct Model1_msi_msg *Model1_msg);
 void (*Model1_irq_write_msi_msg)(struct Model1_irq_data *Model1_data, struct Model1_msi_msg *Model1_msg);

 int (*Model1_irq_get_irqchip_state)(struct Model1_irq_data *Model1_data, enum Model1_irqchip_irq_state Model1_which, bool *Model1_state);
 int (*Model1_irq_set_irqchip_state)(struct Model1_irq_data *Model1_data, enum Model1_irqchip_irq_state Model1_which, bool Model1_state);

 int (*Model1_irq_set_vcpu_affinity)(struct Model1_irq_data *Model1_data, void *Model1_vcpu_info);

 void (*Model1_ipi_send_single)(struct Model1_irq_data *Model1_data, unsigned int Model1_cpu);
 void (*Model1_ipi_send_mask)(struct Model1_irq_data *Model1_data, const struct Model1_cpumask *Model1_dest);

 unsigned long Model1_flags;
};

/*
 * irq_chip specific flags
 *
 * IRQCHIP_SET_TYPE_MASKED:	Mask before calling chip.irq_set_type()
 * IRQCHIP_EOI_IF_HANDLED:	Only issue irq_eoi() when irq was handled
 * IRQCHIP_MASK_ON_SUSPEND:	Mask non wake irqs in the suspend path
 * IRQCHIP_ONOFFLINE_ENABLED:	Only call irq_on/off_line callbacks
 *				when irq enabled
 * IRQCHIP_SKIP_SET_WAKE:	Skip chip.irq_set_wake(), for this irq chip
 * IRQCHIP_ONESHOT_SAFE:	One shot does not require mask/unmask
 * IRQCHIP_EOI_THREADED:	Chip requires eoi() on unmask in threaded mode
 */
enum {
 Model1_IRQCHIP_SET_TYPE_MASKED = (1 << 0),
 Model1_IRQCHIP_EOI_IF_HANDLED = (1 << 1),
 Model1_IRQCHIP_MASK_ON_SUSPEND = (1 << 2),
 Model1_IRQCHIP_ONOFFLINE_ENABLED = (1 << 3),
 Model1_IRQCHIP_SKIP_SET_WAKE = (1 << 4),
 Model1_IRQCHIP_ONESHOT_SAFE = (1 << 5),
 Model1_IRQCHIP_EOI_THREADED = (1 << 6),
};







/*
 * Core internal functions to deal with irq descriptors
 */

struct Model1_irq_affinity_notify;
struct Model1_proc_dir_entry;
struct Model1_module;
struct Model1_irq_desc;
struct Model1_irq_domain;
struct Model1_pt_regs;

/**
 * struct irq_desc - interrupt descriptor
 * @irq_common_data:	per irq and chip data passed down to chip functions
 * @kstat_irqs:		irq stats per cpu
 * @handle_irq:		highlevel irq-events handler
 * @preflow_handler:	handler called before the flow handler (currently used by sparc)
 * @action:		the irq action chain
 * @status:		status information
 * @core_internal_state__do_not_mess_with_it: core internal status information
 * @depth:		disable-depth, for nested irq_disable() calls
 * @wake_depth:		enable depth, for multiple irq_set_irq_wake() callers
 * @irq_count:		stats field to detect stalled irqs
 * @last_unhandled:	aging timer for unhandled count
 * @irqs_unhandled:	stats field for spurious unhandled interrupts
 * @threads_handled:	stats field for deferred spurious detection of threaded handlers
 * @threads_handled_last: comparator field for deferred spurious detection of theraded handlers
 * @lock:		locking for SMP
 * @affinity_hint:	hint to user space for preferred irq affinity
 * @affinity_notify:	context for notification of affinity changes
 * @pending_mask:	pending rebalanced interrupts
 * @threads_oneshot:	bitfield to handle shared oneshot threads
 * @threads_active:	number of irqaction threads currently running
 * @wait_for_threads:	wait queue for sync_irq to wait for threaded handlers
 * @nr_actions:		number of installed actions on this descriptor
 * @no_suspend_depth:	number of irqactions on a irq descriptor with
 *			IRQF_NO_SUSPEND set
 * @force_resume_depth:	number of irqactions on a irq descriptor with
 *			IRQF_FORCE_RESUME set
 * @rcu:		rcu head for delayed free
 * @dir:		/proc/irq/ procfs entry
 * @name:		flow handler name for /proc/interrupts output
 */
struct Model1_irq_desc {
 struct Model1_irq_common_data Model1_irq_common_data;
 struct Model1_irq_data Model1_irq_data;
 unsigned int *Model1_kstat_irqs;
 Model1_irq_flow_handler_t Model1_handle_irq;



 struct Model1_irqaction *Model1_action; /* IRQ action list */
 unsigned int Model1_status_use_accessors;
 unsigned int Model1_core_internal_state__do_not_mess_with_it;
 unsigned int Model1_depth; /* nested irq disables */
 unsigned int Model1_wake_depth; /* nested wake enables */
 unsigned int Model1_irq_count; /* For detecting broken IRQs */
 unsigned long Model1_last_unhandled; /* Aging timer for unhandled count */
 unsigned int Model1_irqs_unhandled;
 Model1_atomic_t Model1_threads_handled;
 int Model1_threads_handled_last;
 Model1_raw_spinlock_t Model1_lock;
 struct Model1_cpumask *Model1_percpu_enabled;
 const struct Model1_cpumask *Model1_percpu_affinity;

 const struct Model1_cpumask *Model1_affinity_hint;
 struct Model1_irq_affinity_notify *Model1_affinity_notify;

 Model1_cpumask_var_t Model1_pending_mask;


 unsigned long Model1_threads_oneshot;
 Model1_atomic_t Model1_threads_active;
 Model1_wait_queue_head_t Model1_wait_for_threads;

 unsigned int Model1_nr_actions;
 unsigned int Model1_no_suspend_depth;
 unsigned int Model1_cond_suspend_depth;
 unsigned int Model1_force_resume_depth;


 struct Model1_proc_dir_entry *Model1_dir;


 struct Model1_callback_head Model1_rcu;

 int Model1_parent_irq;
 struct Model1_module *Model1_owner;
 const char *Model1_name;
} __attribute__((__aligned__(1 << (6))));


extern void Model1_irq_lock_sparse(void);
extern void Model1_irq_unlock_sparse(void);






static inline __attribute__((no_instrument_function)) struct Model1_irq_desc *Model1_irq_data_to_desc(struct Model1_irq_data *Model1_data)
{
 return ({ const typeof( ((struct Model1_irq_desc *)0)->Model1_irq_common_data ) *Model1___mptr = (Model1_data->Model1_common); (struct Model1_irq_desc *)( (char *)Model1___mptr - __builtin_offsetof(struct Model1_irq_desc, Model1_irq_common_data) );});
}

static inline __attribute__((no_instrument_function)) unsigned int Model1_irq_desc_get_irq(struct Model1_irq_desc *Model1_desc)
{
 return Model1_desc->Model1_irq_data.Model1_irq;
}

static inline __attribute__((no_instrument_function)) struct Model1_irq_data *Model1_irq_desc_get_irq_data(struct Model1_irq_desc *Model1_desc)
{
 return &Model1_desc->Model1_irq_data;
}

static inline __attribute__((no_instrument_function)) struct Model1_irq_chip *Model1_irq_desc_get_chip(struct Model1_irq_desc *Model1_desc)
{
 return Model1_desc->Model1_irq_data.Model1_chip;
}

static inline __attribute__((no_instrument_function)) void *Model1_irq_desc_get_chip_data(struct Model1_irq_desc *Model1_desc)
{
 return Model1_desc->Model1_irq_data.Model1_chip_data;
}

static inline __attribute__((no_instrument_function)) void *Model1_irq_desc_get_handler_data(struct Model1_irq_desc *Model1_desc)
{
 return Model1_desc->Model1_irq_common_data.Model1_handler_data;
}

static inline __attribute__((no_instrument_function)) struct Model1_msi_desc *Model1_irq_desc_get_msi_desc(struct Model1_irq_desc *Model1_desc)
{
 return Model1_desc->Model1_irq_common_data.Model1_msi_desc;
}

/*
 * Architectures call this to let the generic IRQ layer
 * handle an interrupt.
 */
static inline __attribute__((no_instrument_function)) void Model1_generic_handle_irq_desc(struct Model1_irq_desc *Model1_desc)
{
 Model1_desc->Model1_handle_irq(Model1_desc);
}

int Model1_generic_handle_irq(unsigned int Model1_irq);
/* Test to see if a driver has successfully requested an irq */
static inline __attribute__((no_instrument_function)) int Model1_irq_desc_has_action(struct Model1_irq_desc *Model1_desc)
{
 return Model1_desc->Model1_action != ((void *)0);
}

static inline __attribute__((no_instrument_function)) int Model1_irq_has_action(unsigned int Model1_irq)
{
 return Model1_irq_desc_has_action(Model1_irq_to_desc(Model1_irq));
}

/**
 * irq_set_handler_locked - Set irq handler from a locked region
 * @data:	Pointer to the irq_data structure which identifies the irq
 * @handler:	Flow control handler function for this interrupt
 *
 * Sets the handler in the irq descriptor associated to @data.
 *
 * Must be called with irq_desc locked and valid parameters. Typical
 * call site is the irq_set_type() callback.
 */
static inline __attribute__((no_instrument_function)) void Model1_irq_set_handler_locked(struct Model1_irq_data *Model1_data,
       Model1_irq_flow_handler_t Model1_handler)
{
 struct Model1_irq_desc *Model1_desc = Model1_irq_data_to_desc(Model1_data);

 Model1_desc->Model1_handle_irq = Model1_handler;
}

/**
 * irq_set_chip_handler_name_locked - Set chip, handler and name from a locked region
 * @data:	Pointer to the irq_data structure for which the chip is set
 * @chip:	Pointer to the new irq chip
 * @handler:	Flow control handler function for this interrupt
 * @name:	Name of the interrupt
 *
 * Replace the irq chip at the proper hierarchy level in @data and
 * sets the handler and name in the associated irq descriptor.
 *
 * Must be called with irq_desc locked and valid parameters.
 */
static inline __attribute__((no_instrument_function)) void
Model1_irq_set_chip_handler_name_locked(struct Model1_irq_data *Model1_data, struct Model1_irq_chip *Model1_chip,
     Model1_irq_flow_handler_t Model1_handler, const char *Model1_name)
{
 struct Model1_irq_desc *Model1_desc = Model1_irq_data_to_desc(Model1_data);

 Model1_desc->Model1_handle_irq = Model1_handler;
 Model1_desc->Model1_name = Model1_name;
 Model1_data->Model1_chip = Model1_chip;
}

static inline __attribute__((no_instrument_function)) int Model1_irq_balancing_disabled(unsigned int Model1_irq)
{
 struct Model1_irq_desc *Model1_desc;

 Model1_desc = Model1_irq_to_desc(Model1_irq);
 return Model1_desc->Model1_status_use_accessors & (Model1_IRQ_PER_CPU | Model1_IRQ_NO_BALANCING);
}

static inline __attribute__((no_instrument_function)) int Model1_irq_is_percpu(unsigned int Model1_irq)
{
 struct Model1_irq_desc *Model1_desc;

 Model1_desc = Model1_irq_to_desc(Model1_irq);
 return Model1_desc->Model1_status_use_accessors & Model1_IRQ_PER_CPU;
}

static inline __attribute__((no_instrument_function)) void
Model1_irq_set_lockdep_class(unsigned int Model1_irq, struct Model1_lock_class_key *Model1_class)
{
 struct Model1_irq_desc *Model1_desc = Model1_irq_to_desc(Model1_irq);

 if (Model1_desc)
  do { (void)(Model1_class); } while (0);
}

/*
 * Pick up the arch-dependent methods:
 */




/*
 * (C) 1992, 1993 Linus Torvalds, (C) 1997 Ingo Molnar
 *
 * moved some of the old arch/i386/kernel/irq.h to here. VY
 *
 * IRQ/IPI changes taken from work by Thomas Radke
 * <tomsoft@informatik.tu-chemnitz.de>
 *
 * hacked by Andi Kleen for x86-64.
 * unified by tglx
 */





















struct Model1_proc_dir_entry;
struct Model1_pt_regs;
struct Model1_notifier_block;


void Model1_create_prof_cpu_mask(void);
int Model1_create_proc_profile(void);
enum Model1_profile_type {
 Model1_PROFILE_TASK_EXIT,
 Model1_PROFILE_MUNMAP
};



extern int Model1_prof_on __attribute__((__section__(".data..read_mostly")));

/* init basic kernel profiler */
int Model1_profile_init(void);
int Model1_profile_setup(char *Model1_str);
void Model1_profile_tick(int Model1_type);
int Model1_setup_profiling_timer(unsigned int Model1_multiplier);

/*
 * Add multiple profiler hits to a given address:
 */
void Model1_profile_hits(int Model1_type, void *Model1_ip, unsigned int Model1_nr_hits);

/*
 * Single profiler hit:
 */
static inline __attribute__((no_instrument_function)) void Model1_profile_hit(int Model1_type, void *Model1_ip)
{
 /*
	 * Speedup for the common (no profiling enabled) case:
	 */
 if (__builtin_expect(!!(Model1_prof_on == Model1_type), 0))
  Model1_profile_hits(Model1_type, Model1_ip, 1);
}

struct Model1_task_struct;
struct Model1_mm_struct;

/* task is in do_exit() */
void Model1_profile_task_exit(struct Model1_task_struct * Model1_task);

/* task is dead, free task struct ? Returns 1 if
 * the task was taken, 0 if the task should be freed.
 */
int Model1_profile_handoff_task(struct Model1_task_struct * Model1_task);

/* sys_munmap */
void Model1_profile_munmap(unsigned long Model1_addr);

int Model1_task_handoff_register(struct Model1_notifier_block * Model1_n);
int Model1_task_handoff_unregister(struct Model1_notifier_block * Model1_n);

int Model1_profile_event_register(enum Model1_profile_type, struct Model1_notifier_block * Model1_n);
int Model1_profile_event_unregister(enum Model1_profile_type, struct Model1_notifier_block * Model1_n);

struct Model1_pt_regs;










/* References to section boundaries */




/*
 * Usage guidelines:
 * _text, _data: architecture specific, don't use them in arch-independent code
 * [_stext, _etext]: contains .text.* sections, may also contain .rodata.*
 *                   and/or .init.* sections
 * [_sdata, _edata]: contains .data.* sections, may also contain .rodata.*
 *                   and/or .init.* sections.
 * [__start_rodata, __end_rodata]: contains .rodata.* sections
 * [__init_begin, __init_end]: contains .init.* sections, but .init.text.*
 *                   may be out of this range on some architectures.
 * [_sinittext, _einittext]: contains .init.text.* sections
 * [__bss_start, __bss_stop]: contains BSS sections
 *
 * Following global variables are optional and may be unavailable on some
 * architectures and/or kernel configurations.
 *	_text, _data
 *	__kprobes_text_start, __kprobes_text_end
 *	__entry_text_start, __entry_text_end
 *	__ctors_start, __ctors_end
 */
extern char Model1__text[], Model1__stext[], Model1__etext[];
extern char Model1__data[], Model1__sdata[], Model1__edata[];
extern char Model1___bss_start[], Model1___bss_stop[];
extern char Model1___init_begin[], Model1___init_end[];
extern char Model1__sinittext[], Model1__einittext[];
extern char Model1__end[];
extern char Model1___per_cpu_load[], Model1___per_cpu_start[], Model1___per_cpu_end[];
extern char Model1___kprobes_text_start[], Model1___kprobes_text_end[];
extern char Model1___entry_text_start[], Model1___entry_text_end[];
extern char Model1___start_rodata[], Model1___end_rodata[];

/* Start and end of .ctors section - used for constructor calls. */
extern char Model1___ctors_start[], Model1___ctors_end[];

extern const void Model1___nosave_begin, Model1___nosave_end;

/* function descriptor handling (if any).  Override
 * in asm/sections.h */




/* random extra sections (if any).  Override
 * in asm/sections.h */

static inline __attribute__((no_instrument_function)) int Model1_arch_is_kernel_text(unsigned long Model1_addr)
{
 return 0;
}



static inline __attribute__((no_instrument_function)) int Model1_arch_is_kernel_data(unsigned long Model1_addr)
{
 return 0;
}


/**
 * memory_contains - checks if an object is contained within a memory region
 * @begin: virtual address of the beginning of the memory region
 * @end: virtual address of the end of the memory region
 * @virt: virtual address of the memory object
 * @size: size of the memory object
 *
 * Returns: true if the object specified by @virt and @size is entirely
 * contained within the memory region defined by @begin and @end, false
 * otherwise.
 */
static inline __attribute__((no_instrument_function)) bool Model1_memory_contains(void *Model1_begin, void *Model1_end, void *Model1_virt,
       Model1_size_t Model1_size)
{
 return Model1_virt >= Model1_begin && Model1_virt + Model1_size <= Model1_end;
}

/**
 * memory_intersects - checks if the region occupied by an object intersects
 *                     with another memory region
 * @begin: virtual address of the beginning of the memory regien
 * @end: virtual address of the end of the memory region
 * @virt: virtual address of the memory object
 * @size: size of the memory object
 *
 * Returns: true if an object's memory region, specified by @virt and @size,
 * intersects with the region specified by @begin and @end, false otherwise.
 */
static inline __attribute__((no_instrument_function)) bool Model1_memory_intersects(void *Model1_begin, void *Model1_end, void *Model1_virt,
         Model1_size_t Model1_size)
{
 void *Model1_vend = Model1_virt + Model1_size;

 return (Model1_virt >= Model1_begin && Model1_virt < Model1_end) || (Model1_vend >= Model1_begin && Model1_vend < Model1_end);
}

/**
 * init_section_contains - checks if an object is contained within the init
 *                         section
 * @virt: virtual address of the memory object
 * @size: size of the memory object
 *
 * Returns: true if the object specified by @virt and @size is entirely
 * contained within the init section, false otherwise.
 */
static inline __attribute__((no_instrument_function)) bool Model1_init_section_contains(void *Model1_virt, Model1_size_t Model1_size)
{
 return Model1_memory_contains(Model1___init_begin, Model1___init_end, Model1_virt, Model1_size);
}

/**
 * init_section_intersects - checks if the region occupied by an object
 *                           intersects with the init section
 * @virt: virtual address of the memory object
 * @size: size of the memory object
 *
 * Returns: true if an object's memory region, specified by @virt and @size,
 * intersects with the init section, false otherwise.
 */
static inline __attribute__((no_instrument_function)) bool Model1_init_section_intersects(void *Model1_virt, Model1_size_t Model1_size)
{
 return Model1_memory_intersects(Model1___init_begin, Model1___init_end, Model1_virt, Model1_size);
}


extern char Model1___brk_base[], Model1___brk_limit[];
extern struct Model1_exception_table_entry Model1___stop___ex_table[];


extern char Model1___end_rodata_hpage_align[];

/* Interrupt handlers registered during init_IRQ */
extern void Model1_apic_timer_interrupt(void);
extern void Model1_x86_platform_ipi(void);
extern void Model1_kvm_posted_intr_ipi(void);
extern void Model1_kvm_posted_intr_wakeup_ipi(void);
extern void Model1_error_interrupt(void);
extern void Model1_irq_work_interrupt(void);

extern void Model1_spurious_interrupt(void);
extern void Model1_thermal_interrupt(void);
extern void Model1_reschedule_interrupt(void);

extern void Model1_irq_move_cleanup_interrupt(void);
extern void Model1_reboot_interrupt(void);
extern void Model1_threshold_interrupt(void);
extern void Model1_deferred_error_interrupt(void);

extern void Model1_call_function_interrupt(void);
extern void Model1_call_function_single_interrupt(void);


/* Interrupt handlers registered during init_IRQ */
extern void Model1_trace_apic_timer_interrupt(void);
extern void Model1_trace_x86_platform_ipi(void);
extern void Model1_trace_error_interrupt(void);
extern void Model1_trace_irq_work_interrupt(void);
extern void Model1_trace_spurious_interrupt(void);
extern void Model1_trace_thermal_interrupt(void);
extern void Model1_trace_reschedule_interrupt(void);
extern void Model1_trace_threshold_interrupt(void);
extern void Model1_trace_deferred_error_interrupt(void);
extern void Model1_trace_call_function_interrupt(void);
extern void Model1_trace_call_function_single_interrupt(void);







struct Model1_irq_data;
struct Model1_pci_dev;
struct Model1_msi_desc;

enum Model1_irq_alloc_type {
 Model1_X86_IRQ_ALLOC_TYPE_IOAPIC = 1,
 Model1_X86_IRQ_ALLOC_TYPE_HPET,
 Model1_X86_IRQ_ALLOC_TYPE_MSI,
 Model1_X86_IRQ_ALLOC_TYPE_MSIX,
 Model1_X86_IRQ_ALLOC_TYPE_DMAR,
 Model1_X86_IRQ_ALLOC_TYPE_UV,
};

struct Model1_irq_alloc_info {
 enum Model1_irq_alloc_type Model1_type;
 Model1_u32 Model1_flags;
 const struct Model1_cpumask *Model1_mask; /* CPU mask for vector allocation */
 union {
  int unused;

  struct {
   int Model1_hpet_id;
   int Model1_hpet_index;
   void *Model1_hpet_data;
  };


  struct {
   struct Model1_pci_dev *Model1_msi_dev;
   Model1_irq_hw_number_t Model1_msi_hwirq;
  };


  struct {
   int Model1_ioapic_id;
   int Model1_ioapic_pin;
   int Model1_ioapic_node;
   Model1_u32 Model1_ioapic_trigger : 1;
   Model1_u32 Model1_ioapic_polarity : 1;
   Model1_u32 Model1_ioapic_valid : 1;
   struct Model1_IO_APIC_route_entry *Model1_ioapic_entry;
  };


  struct {
   int Model1_dmar_id;
   void *Model1_dmar_data;
  };


  struct {
   int Model1_ht_pos;
   int Model1_ht_idx;
   struct Model1_pci_dev *Model1_ht_dev;
   void *Model1_ht_update;
  };
 };
};

struct Model1_irq_cfg {
 unsigned int Model1_dest_apicid;
 Model1_u8 Model1_vector;
 Model1_u8 Model1_old_vector;
};

extern struct Model1_irq_cfg *Model1_irq_cfg(unsigned int Model1_irq);
extern struct Model1_irq_cfg *Model1_irqd_cfg(struct Model1_irq_data *Model1_irq_data);
extern void Model1_lock_vector_lock(void);
extern void Model1_unlock_vector_lock(void);
extern void Model1_setup_vector_irq(int Model1_cpu);

extern void Model1_send_cleanup_vector(struct Model1_irq_cfg *);
extern void Model1_irq_complete_move(struct Model1_irq_cfg *Model1_cfg);





extern void Model1_apic_ack_edge(struct Model1_irq_data *Model1_data);





/* Statistics */
extern Model1_atomic_t Model1_irq_err_count;
extern Model1_atomic_t Model1_irq_mis_count;

extern void Model1_elcr_set_level_irq(unsigned int Model1_irq);

extern char Model1_irq_entries_start[];







typedef struct Model1_irq_desc* Model1_vector_irq_t[256];
extern __attribute__((section(".data..percpu" ""))) __typeof__(Model1_vector_irq_t) Model1_vector_irq;
struct Model1_irqaction;
extern int Model1_setup_irq(unsigned int Model1_irq, struct Model1_irqaction *Model1_new);
extern void Model1_remove_irq(unsigned int Model1_irq, struct Model1_irqaction *Model1_act);
extern int Model1_setup_percpu_irq(unsigned int Model1_irq, struct Model1_irqaction *Model1_new);
extern void Model1_remove_percpu_irq(unsigned int Model1_irq, struct Model1_irqaction *Model1_act);

extern void Model1_irq_cpu_online(void);
extern void Model1_irq_cpu_offline(void);
extern int Model1_irq_set_affinity_locked(struct Model1_irq_data *Model1_data,
       const struct Model1_cpumask *Model1_cpumask, bool Model1_force);
extern int Model1_irq_set_vcpu_affinity(unsigned int Model1_irq, void *Model1_vcpu_info);

extern void Model1_irq_migrate_all_off_this_cpu(void);


void Model1_irq_move_irq(struct Model1_irq_data *Model1_data);
void Model1_irq_move_masked_irq(struct Model1_irq_data *Model1_data);





extern int Model1_no_irq_affinity;




static inline __attribute__((no_instrument_function)) int Model1_irq_set_parent(int Model1_irq, int Model1_parent_irq)
{
 return 0;
}


/*
 * Built-in IRQ handlers for various IRQ types,
 * callable via desc->handle_irq()
 */
extern void Model1_handle_level_irq(struct Model1_irq_desc *Model1_desc);
extern void Model1_handle_fasteoi_irq(struct Model1_irq_desc *Model1_desc);
extern void Model1_handle_edge_irq(struct Model1_irq_desc *Model1_desc);
extern void Model1_handle_edge_eoi_irq(struct Model1_irq_desc *Model1_desc);
extern void Model1_handle_simple_irq(struct Model1_irq_desc *Model1_desc);
extern void Model1_handle_untracked_irq(struct Model1_irq_desc *Model1_desc);
extern void Model1_handle_percpu_irq(struct Model1_irq_desc *Model1_desc);
extern void Model1_handle_percpu_devid_irq(struct Model1_irq_desc *Model1_desc);
extern void Model1_handle_bad_irq(struct Model1_irq_desc *Model1_desc);
extern void Model1_handle_nested_irq(unsigned int Model1_irq);

extern int Model1_irq_chip_compose_msi_msg(struct Model1_irq_data *Model1_data, struct Model1_msi_msg *Model1_msg);
extern int Model1_irq_chip_pm_get(struct Model1_irq_data *Model1_data);
extern int Model1_irq_chip_pm_put(struct Model1_irq_data *Model1_data);

extern void Model1_irq_chip_enable_parent(struct Model1_irq_data *Model1_data);
extern void Model1_irq_chip_disable_parent(struct Model1_irq_data *Model1_data);
extern void Model1_irq_chip_ack_parent(struct Model1_irq_data *Model1_data);
extern int Model1_irq_chip_retrigger_hierarchy(struct Model1_irq_data *Model1_data);
extern void Model1_irq_chip_mask_parent(struct Model1_irq_data *Model1_data);
extern void Model1_irq_chip_unmask_parent(struct Model1_irq_data *Model1_data);
extern void Model1_irq_chip_eoi_parent(struct Model1_irq_data *Model1_data);
extern int Model1_irq_chip_set_affinity_parent(struct Model1_irq_data *Model1_data,
     const struct Model1_cpumask *Model1_dest,
     bool Model1_force);
extern int Model1_irq_chip_set_wake_parent(struct Model1_irq_data *Model1_data, unsigned int Model1_on);
extern int Model1_irq_chip_set_vcpu_affinity_parent(struct Model1_irq_data *Model1_data,
          void *Model1_vcpu_info);
extern int Model1_irq_chip_set_type_parent(struct Model1_irq_data *Model1_data, unsigned int Model1_type);


/* Handling of unhandled and spurious interrupts: */
extern void Model1_note_interrupt(struct Model1_irq_desc *Model1_desc, Model1_irqreturn_t Model1_action_ret);


/* Enable/disable irq debugging output: */
extern int Model1_noirqdebug_setup(char *Model1_str);

/* Checks whether the interrupt can be requested by request_irq(): */
extern int Model1_can_request_irq(unsigned int Model1_irq, unsigned long Model1_irqflags);

/* Dummy irq-chip implementations: */
extern struct Model1_irq_chip Model1_no_irq_chip;
extern struct Model1_irq_chip Model1_dummy_irq_chip;

extern void
Model1_irq_set_chip_and_handler_name(unsigned int Model1_irq, struct Model1_irq_chip *Model1_chip,
         Model1_irq_flow_handler_t Model1_handle, const char *Model1_name);

static inline __attribute__((no_instrument_function)) void Model1_irq_set_chip_and_handler(unsigned int Model1_irq, struct Model1_irq_chip *Model1_chip,
         Model1_irq_flow_handler_t Model1_handle)
{
 Model1_irq_set_chip_and_handler_name(Model1_irq, Model1_chip, Model1_handle, ((void *)0));
}

extern int Model1_irq_set_percpu_devid(unsigned int Model1_irq);
extern int Model1_irq_set_percpu_devid_partition(unsigned int Model1_irq,
       const struct Model1_cpumask *Model1_affinity);
extern int Model1_irq_get_percpu_devid_partition(unsigned int Model1_irq,
       struct Model1_cpumask *Model1_affinity);

extern void
Model1___irq_set_handler(unsigned int Model1_irq, Model1_irq_flow_handler_t Model1_handle, int Model1_is_chained,
    const char *Model1_name);

static inline __attribute__((no_instrument_function)) void
Model1_irq_set_handler(unsigned int Model1_irq, Model1_irq_flow_handler_t Model1_handle)
{
 Model1___irq_set_handler(Model1_irq, Model1_handle, 0, ((void *)0));
}

/*
 * Set a highlevel chained flow handler for a given IRQ.
 * (a chained handler is automatically enabled and set to
 *  IRQ_NOREQUEST, IRQ_NOPROBE, and IRQ_NOTHREAD)
 */
static inline __attribute__((no_instrument_function)) void
Model1_irq_set_chained_handler(unsigned int Model1_irq, Model1_irq_flow_handler_t Model1_handle)
{
 Model1___irq_set_handler(Model1_irq, Model1_handle, 1, ((void *)0));
}

/*
 * Set a highlevel chained flow handler and its data for a given IRQ.
 * (a chained handler is automatically enabled and set to
 *  IRQ_NOREQUEST, IRQ_NOPROBE, and IRQ_NOTHREAD)
 */
void
Model1_irq_set_chained_handler_and_data(unsigned int Model1_irq, Model1_irq_flow_handler_t Model1_handle,
     void *Model1_data);

void Model1_irq_modify_status(unsigned int Model1_irq, unsigned long Model1_clr, unsigned long Model1_set);

static inline __attribute__((no_instrument_function)) void Model1_irq_set_status_flags(unsigned int Model1_irq, unsigned long Model1_set)
{
 Model1_irq_modify_status(Model1_irq, 0, Model1_set);
}

static inline __attribute__((no_instrument_function)) void Model1_irq_clear_status_flags(unsigned int Model1_irq, unsigned long Model1_clr)
{
 Model1_irq_modify_status(Model1_irq, Model1_clr, 0);
}

static inline __attribute__((no_instrument_function)) void Model1_irq_set_noprobe(unsigned int Model1_irq)
{
 Model1_irq_modify_status(Model1_irq, 0, Model1_IRQ_NOPROBE);
}

static inline __attribute__((no_instrument_function)) void Model1_irq_set_probe(unsigned int Model1_irq)
{
 Model1_irq_modify_status(Model1_irq, Model1_IRQ_NOPROBE, 0);
}

static inline __attribute__((no_instrument_function)) void Model1_irq_set_nothread(unsigned int Model1_irq)
{
 Model1_irq_modify_status(Model1_irq, 0, Model1_IRQ_NOTHREAD);
}

static inline __attribute__((no_instrument_function)) void Model1_irq_set_thread(unsigned int Model1_irq)
{
 Model1_irq_modify_status(Model1_irq, Model1_IRQ_NOTHREAD, 0);
}

static inline __attribute__((no_instrument_function)) void Model1_irq_set_nested_thread(unsigned int Model1_irq, bool Model1_nest)
{
 if (Model1_nest)
  Model1_irq_set_status_flags(Model1_irq, Model1_IRQ_NESTED_THREAD);
 else
  Model1_irq_clear_status_flags(Model1_irq, Model1_IRQ_NESTED_THREAD);
}

static inline __attribute__((no_instrument_function)) void Model1_irq_set_percpu_devid_flags(unsigned int Model1_irq)
{
 Model1_irq_set_status_flags(Model1_irq,
        Model1_IRQ_NOAUTOEN | Model1_IRQ_PER_CPU | Model1_IRQ_NOTHREAD |
        Model1_IRQ_NOPROBE | Model1_IRQ_PER_CPU_DEVID);
}

/* Set/get chip/data for an IRQ: */
extern int Model1_irq_set_chip(unsigned int Model1_irq, struct Model1_irq_chip *Model1_chip);
extern int Model1_irq_set_handler_data(unsigned int Model1_irq, void *Model1_data);
extern int Model1_irq_set_chip_data(unsigned int Model1_irq, void *Model1_data);
extern int Model1_irq_set_irq_type(unsigned int Model1_irq, unsigned int Model1_type);
extern int Model1_irq_set_msi_desc(unsigned int Model1_irq, struct Model1_msi_desc *Model1_entry);
extern int Model1_irq_set_msi_desc_off(unsigned int Model1_irq_base, unsigned int Model1_irq_offset,
    struct Model1_msi_desc *Model1_entry);
extern struct Model1_irq_data *Model1_irq_get_irq_data(unsigned int Model1_irq);

static inline __attribute__((no_instrument_function)) struct Model1_irq_chip *Model1_irq_get_chip(unsigned int Model1_irq)
{
 struct Model1_irq_data *Model1_d = Model1_irq_get_irq_data(Model1_irq);
 return Model1_d ? Model1_d->Model1_chip : ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model1_irq_chip *Model1_irq_data_get_irq_chip(struct Model1_irq_data *Model1_d)
{
 return Model1_d->Model1_chip;
}

static inline __attribute__((no_instrument_function)) void *Model1_irq_get_chip_data(unsigned int Model1_irq)
{
 struct Model1_irq_data *Model1_d = Model1_irq_get_irq_data(Model1_irq);
 return Model1_d ? Model1_d->Model1_chip_data : ((void *)0);
}

static inline __attribute__((no_instrument_function)) void *Model1_irq_data_get_irq_chip_data(struct Model1_irq_data *Model1_d)
{
 return Model1_d->Model1_chip_data;
}

static inline __attribute__((no_instrument_function)) void *Model1_irq_get_handler_data(unsigned int Model1_irq)
{
 struct Model1_irq_data *Model1_d = Model1_irq_get_irq_data(Model1_irq);
 return Model1_d ? Model1_d->Model1_common->Model1_handler_data : ((void *)0);
}

static inline __attribute__((no_instrument_function)) void *Model1_irq_data_get_irq_handler_data(struct Model1_irq_data *Model1_d)
{
 return Model1_d->Model1_common->Model1_handler_data;
}

static inline __attribute__((no_instrument_function)) struct Model1_msi_desc *Model1_irq_get_msi_desc(unsigned int Model1_irq)
{
 struct Model1_irq_data *Model1_d = Model1_irq_get_irq_data(Model1_irq);
 return Model1_d ? Model1_d->Model1_common->Model1_msi_desc : ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model1_msi_desc *Model1_irq_data_get_msi_desc(struct Model1_irq_data *Model1_d)
{
 return Model1_d->Model1_common->Model1_msi_desc;
}

static inline __attribute__((no_instrument_function)) Model1_u32 Model1_irq_get_trigger_type(unsigned int Model1_irq)
{
 struct Model1_irq_data *Model1_d = Model1_irq_get_irq_data(Model1_irq);
 return Model1_d ? Model1_irqd_get_trigger_type(Model1_d) : 0;
}

static inline __attribute__((no_instrument_function)) int Model1_irq_common_data_get_node(struct Model1_irq_common_data *Model1_d)
{

 return Model1_d->Model1_node;



}

static inline __attribute__((no_instrument_function)) int Model1_irq_data_get_node(struct Model1_irq_data *Model1_d)
{
 return Model1_irq_common_data_get_node(Model1_d->Model1_common);
}

static inline __attribute__((no_instrument_function)) struct Model1_cpumask *Model1_irq_get_affinity_mask(int Model1_irq)
{
 struct Model1_irq_data *Model1_d = Model1_irq_get_irq_data(Model1_irq);

 return Model1_d ? Model1_d->Model1_common->Model1_affinity : ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model1_cpumask *Model1_irq_data_get_affinity_mask(struct Model1_irq_data *Model1_d)
{
 return Model1_d->Model1_common->Model1_affinity;
}

unsigned int Model1_arch_dynirq_lower_bound(unsigned int Model1_from);

int Model1___irq_alloc_descs(int Model1_irq, unsigned int Model1_from, unsigned int Model1_cnt, int Model1_node,
        struct Model1_module *Model1_owner, const struct Model1_cpumask *Model1_affinity);

/* use macros to avoid needing export.h for THIS_MODULE */
void Model1_irq_free_descs(unsigned int Model1_irq, unsigned int Model1_cnt);
static inline __attribute__((no_instrument_function)) void Model1_irq_free_desc(unsigned int Model1_irq)
{
 Model1_irq_free_descs(Model1_irq, 1);
}
/**
 * struct irq_chip_regs - register offsets for struct irq_gci
 * @enable:	Enable register offset to reg_base
 * @disable:	Disable register offset to reg_base
 * @mask:	Mask register offset to reg_base
 * @ack:	Ack register offset to reg_base
 * @eoi:	Eoi register offset to reg_base
 * @type:	Type configuration register offset to reg_base
 * @polarity:	Polarity configuration register offset to reg_base
 */
struct Model1_irq_chip_regs {
 unsigned long Model1_enable;
 unsigned long Model1_disable;
 unsigned long Model1_mask;
 unsigned long Model1_ack;
 unsigned long Model1_eoi;
 unsigned long Model1_type;
 unsigned long Model1_polarity;
};

/**
 * struct irq_chip_type - Generic interrupt chip instance for a flow type
 * @chip:		The real interrupt chip which provides the callbacks
 * @regs:		Register offsets for this chip
 * @handler:		Flow handler associated with this chip
 * @type:		Chip can handle these flow types
 * @mask_cache_priv:	Cached mask register private to the chip type
 * @mask_cache:		Pointer to cached mask register
 *
 * A irq_generic_chip can have several instances of irq_chip_type when
 * it requires different functions and register offsets for different
 * flow types.
 */
struct Model1_irq_chip_type {
 struct Model1_irq_chip Model1_chip;
 struct Model1_irq_chip_regs Model1_regs;
 Model1_irq_flow_handler_t Model1_handler;
 Model1_u32 Model1_type;
 Model1_u32 Model1_mask_cache_priv;
 Model1_u32 *Model1_mask_cache;
};

/**
 * struct irq_chip_generic - Generic irq chip data structure
 * @lock:		Lock to protect register and cache data access
 * @reg_base:		Register base address (virtual)
 * @reg_readl:		Alternate I/O accessor (defaults to readl if NULL)
 * @reg_writel:		Alternate I/O accessor (defaults to writel if NULL)
 * @suspend:		Function called from core code on suspend once per
 *			chip; can be useful instead of irq_chip::suspend to
 *			handle chip details even when no interrupts are in use
 * @resume:		Function called from core code on resume once per chip;
 *			can be useful instead of irq_chip::suspend to handle
 *			chip details even when no interrupts are in use
 * @irq_base:		Interrupt base nr for this chip
 * @irq_cnt:		Number of interrupts handled by this chip
 * @mask_cache:		Cached mask register shared between all chip types
 * @type_cache:		Cached type register
 * @polarity_cache:	Cached polarity register
 * @wake_enabled:	Interrupt can wakeup from suspend
 * @wake_active:	Interrupt is marked as an wakeup from suspend source
 * @num_ct:		Number of available irq_chip_type instances (usually 1)
 * @private:		Private data for non generic chip callbacks
 * @installed:		bitfield to denote installed interrupts
 * @unused:		bitfield to denote unused interrupts
 * @domain:		irq domain pointer
 * @list:		List head for keeping track of instances
 * @chip_types:		Array of interrupt irq_chip_types
 *
 * Note, that irq_chip_generic can have multiple irq_chip_type
 * implementations which can be associated to a particular irq line of
 * an irq_chip_generic instance. That allows to share and protect
 * state in an irq_chip_generic instance when we need to implement
 * different flow mechanisms (level/edge) for it.
 */
struct Model1_irq_chip_generic {
 Model1_raw_spinlock_t Model1_lock;
 void *Model1_reg_base;
 Model1_u32 (*Model1_reg_readl)(void *Model1_addr);
 void (*Model1_reg_writel)(Model1_u32 Model1_val, void *Model1_addr);
 void (*Model1_suspend)(struct Model1_irq_chip_generic *Model1_gc);
 void (*Model1_resume)(struct Model1_irq_chip_generic *Model1_gc);
 unsigned int Model1_irq_base;
 unsigned int Model1_irq_cnt;
 Model1_u32 Model1_mask_cache;
 Model1_u32 Model1_type_cache;
 Model1_u32 Model1_polarity_cache;
 Model1_u32 Model1_wake_enabled;
 Model1_u32 Model1_wake_active;
 unsigned int Model1_num_ct;
 void *Model1_private;
 unsigned long Model1_installed;
 unsigned long unused;
 struct Model1_irq_domain *Model1_domain;
 struct Model1_list_head Model1_list;
 struct Model1_irq_chip_type Model1_chip_types[0];
};

/**
 * enum irq_gc_flags - Initialization flags for generic irq chips
 * @IRQ_GC_INIT_MASK_CACHE:	Initialize the mask_cache by reading mask reg
 * @IRQ_GC_INIT_NESTED_LOCK:	Set the lock class of the irqs to nested for
 *				irq chips which need to call irq_set_wake() on
 *				the parent irq. Usually GPIO implementations
 * @IRQ_GC_MASK_CACHE_PER_TYPE:	Mask cache is chip type private
 * @IRQ_GC_NO_MASK:		Do not calculate irq_data->mask
 * @IRQ_GC_BE_IO:		Use big-endian register accesses (default: LE)
 */
enum Model1_irq_gc_flags {
 Model1_IRQ_GC_INIT_MASK_CACHE = 1 << 0,
 Model1_IRQ_GC_INIT_NESTED_LOCK = 1 << 1,
 Model1_IRQ_GC_MASK_CACHE_PER_TYPE = 1 << 2,
 Model1_IRQ_GC_NO_MASK = 1 << 3,
 Model1_IRQ_GC_BE_IO = 1 << 4,
};

/*
 * struct irq_domain_chip_generic - Generic irq chip data structure for irq domains
 * @irqs_per_chip:	Number of interrupts per chip
 * @num_chips:		Number of chips
 * @irq_flags_to_set:	IRQ* flags to set on irq setup
 * @irq_flags_to_clear:	IRQ* flags to clear on irq setup
 * @gc_flags:		Generic chip specific setup flags
 * @gc:			Array of pointers to generic interrupt chips
 */
struct Model1_irq_domain_chip_generic {
 unsigned int Model1_irqs_per_chip;
 unsigned int Model1_num_chips;
 unsigned int Model1_irq_flags_to_clear;
 unsigned int Model1_irq_flags_to_set;
 enum Model1_irq_gc_flags Model1_gc_flags;
 struct Model1_irq_chip_generic *Model1_gc[0];
};

/* Generic chip callback functions */
void Model1_irq_gc_noop(struct Model1_irq_data *Model1_d);
void Model1_irq_gc_mask_disable_reg(struct Model1_irq_data *Model1_d);
void Model1_irq_gc_mask_set_bit(struct Model1_irq_data *Model1_d);
void Model1_irq_gc_mask_clr_bit(struct Model1_irq_data *Model1_d);
void Model1_irq_gc_unmask_enable_reg(struct Model1_irq_data *Model1_d);
void Model1_irq_gc_ack_set_bit(struct Model1_irq_data *Model1_d);
void Model1_irq_gc_ack_clr_bit(struct Model1_irq_data *Model1_d);
void Model1_irq_gc_mask_disable_reg_and_ack(struct Model1_irq_data *Model1_d);
void Model1_irq_gc_eoi(struct Model1_irq_data *Model1_d);
int Model1_irq_gc_set_wake(struct Model1_irq_data *Model1_d, unsigned int Model1_on);

/* Setup functions for irq_chip_generic */
int Model1_irq_map_generic_chip(struct Model1_irq_domain *Model1_d, unsigned int Model1_virq,
    Model1_irq_hw_number_t Model1_hw_irq);
struct Model1_irq_chip_generic *
Model1_irq_alloc_generic_chip(const char *Model1_name, int Model1_nr_ct, unsigned int Model1_irq_base,
         void *Model1_reg_base, Model1_irq_flow_handler_t Model1_handler);
void Model1_irq_setup_generic_chip(struct Model1_irq_chip_generic *Model1_gc, Model1_u32 Model1_msk,
       enum Model1_irq_gc_flags Model1_flags, unsigned int Model1_clr,
       unsigned int Model1_set);
int Model1_irq_setup_alt_chip(struct Model1_irq_data *Model1_d, unsigned int Model1_type);
void Model1_irq_remove_generic_chip(struct Model1_irq_chip_generic *Model1_gc, Model1_u32 Model1_msk,
        unsigned int Model1_clr, unsigned int Model1_set);

struct Model1_irq_chip_generic *Model1_irq_get_domain_generic_chip(struct Model1_irq_domain *Model1_d, unsigned int Model1_hw_irq);
int Model1_irq_alloc_domain_generic_chips(struct Model1_irq_domain *Model1_d, int Model1_irqs_per_chip,
       int Model1_num_ct, const char *Model1_name,
       Model1_irq_flow_handler_t Model1_handler,
       unsigned int Model1_clr, unsigned int Model1_set,
       enum Model1_irq_gc_flags Model1_flags);


static inline __attribute__((no_instrument_function)) struct Model1_irq_chip_type *Model1_irq_data_get_chip_type(struct Model1_irq_data *Model1_d)
{
 return ({ const typeof( ((struct Model1_irq_chip_type *)0)->Model1_chip ) *Model1___mptr = (Model1_d->Model1_chip); (struct Model1_irq_chip_type *)( (char *)Model1___mptr - __builtin_offsetof(struct Model1_irq_chip_type, Model1_chip) );});
}




static inline __attribute__((no_instrument_function)) void Model1_irq_gc_lock(struct Model1_irq_chip_generic *Model1_gc)
{
 Model1__raw_spin_lock(&Model1_gc->Model1_lock);
}

static inline __attribute__((no_instrument_function)) void Model1_irq_gc_unlock(struct Model1_irq_chip_generic *Model1_gc)
{
 Model1___raw_spin_unlock(&Model1_gc->Model1_lock);
}





/*
 * The irqsave variants are for usage in non interrupt code. Do not use
 * them in irq_chip callbacks. Use irq_gc_lock() instead.
 */






static inline __attribute__((no_instrument_function)) void Model1_irq_reg_writel(struct Model1_irq_chip_generic *Model1_gc,
      Model1_u32 Model1_val, int Model1_reg_offset)
{
 if (Model1_gc->Model1_reg_writel)
  Model1_gc->Model1_reg_writel(Model1_val, Model1_gc->Model1_reg_base + Model1_reg_offset);
 else
  Model1_writel(Model1_val, Model1_gc->Model1_reg_base + Model1_reg_offset);
}

static inline __attribute__((no_instrument_function)) Model1_u32 Model1_irq_reg_readl(struct Model1_irq_chip_generic *Model1_gc,
    int Model1_reg_offset)
{
 if (Model1_gc->Model1_reg_readl)
  return Model1_gc->Model1_reg_readl(Model1_gc->Model1_reg_base + Model1_reg_offset);
 else
  return Model1_readl(Model1_gc->Model1_reg_base + Model1_reg_offset);
}

/* Contrary to Linux irqs, for hardware irqs the irq number 0 is valid */

Model1_irq_hw_number_t Model1_ipi_get_hwirq(unsigned int Model1_irq, unsigned int Model1_cpu);
int Model1___ipi_send_single(struct Model1_irq_desc *Model1_desc, unsigned int Model1_cpu);
int Model1___ipi_send_mask(struct Model1_irq_desc *Model1_desc, const struct Model1_cpumask *Model1_dest);
int Model1_ipi_send_single(unsigned int Model1_virq, unsigned int Model1_cpu);
int Model1_ipi_send_mask(unsigned int Model1_virq, const struct Model1_cpumask *Model1_dest);

typedef struct {
 unsigned int Model1___softirq_pending;
 unsigned int Model1___nmi_count; /* arch dependent */

 unsigned int Model1_apic_timer_irqs; /* arch dependent */
 unsigned int Model1_irq_spurious_count;
 unsigned int Model1_icr_read_retry_count;


 unsigned int Model1_kvm_posted_intr_ipis;
 unsigned int Model1_kvm_posted_intr_wakeup_ipis;

 unsigned int Model1_x86_platform_ipis; /* arch dependent */
 unsigned int Model1_apic_perf_irqs;
 unsigned int Model1_apic_irq_work_irqs;

 unsigned int Model1_irq_resched_count;
 unsigned int Model1_irq_call_count;
 unsigned int Model1_irq_tlb_count;


 unsigned int Model1_irq_thermal_count;


 unsigned int Model1_irq_threshold_count;


 unsigned int Model1_irq_deferred_error_count;




} __attribute__((__aligned__((1 << (6))))) Model1_irq_cpustat_t;

extern __attribute__((section(".data..percpu" "..shared_aligned"))) __typeof__(Model1_irq_cpustat_t) Model1_irq_stat __attribute__((__aligned__((1 << (6)))));
extern void Model1_ack_bad_irq(unsigned int Model1_irq);

extern Model1_u64 Model1_arch_irq_stat_cpu(unsigned int Model1_cpu);


extern Model1_u64 Model1_arch_irq_stat(void);


extern void Model1_synchronize_irq(unsigned int Model1_irq);
extern bool Model1_synchronize_hardirq(unsigned int Model1_irq);
extern void Model1_rcu_nmi_enter(void);
extern void Model1_rcu_nmi_exit(void);


/*
 * It is safe to do non-atomic ops on ->hardirq_context,
 * because NMI handlers may not preempt and the ops are
 * always balanced, so the interrupted value of ->hardirq_context
 * will always be restored.
 */







/*
 * Enter irq context (on NO_HZ, update jiffies):
 */
extern void Model1_irq_enter(void);

/*
 * Exit irq context without processing softirqs:
 */







/*
 * Exit irq context and process softirqs if needed:
 */
extern void Model1_irq_exit(void);
/*
 * These correspond to the IORESOURCE_IRQ_* defines in
 * linux/ioport.h to select the interrupt line behaviour.  When
 * requesting an interrupt without specifying a IRQF_TRIGGER, the
 * setting should be assumed to be "as already configured", which
 * may be as per machine or firmware initialisation.
 */
/*
 * These flags used only by the kernel as part of the
 * irq handling routines.
 *
 * IRQF_SHARED - allow sharing the irq among several devices
 * IRQF_PROBE_SHARED - set by callers when they expect sharing mismatches to occur
 * IRQF_TIMER - Flag to mark this interrupt as timer interrupt
 * IRQF_PERCPU - Interrupt is per cpu
 * IRQF_NOBALANCING - Flag to exclude this interrupt from irq balancing
 * IRQF_IRQPOLL - Interrupt is used for polling (only the interrupt that is
 *                registered first in an shared interrupt is considered for
 *                performance reasons)
 * IRQF_ONESHOT - Interrupt is not reenabled after the hardirq handler finished.
 *                Used by threaded interrupts which need to keep the
 *                irq line disabled until the threaded handler has been run.
 * IRQF_NO_SUSPEND - Do not disable this IRQ during suspend.  Does not guarantee
 *                   that this interrupt will wake the system from a suspended
 *                   state.  See Documentation/power/suspend-and-interrupts.txt
 * IRQF_FORCE_RESUME - Force enable it on resume even if IRQF_NO_SUSPEND is set
 * IRQF_NO_THREAD - Interrupt cannot be threaded
 * IRQF_EARLY_RESUME - Resume IRQ early during syscore instead of at device
 *                resume time.
 * IRQF_COND_SUSPEND - If the IRQ is shared with a NO_SUSPEND user, execute this
 *                interrupt handler after suspending interrupts. For system
 *                wakeup devices users need to implement wakeup detection in
 *                their interrupt handlers.
 */
/*
 * These values can be returned by request_any_context_irq() and
 * describe the context the interrupt will be run in.
 *
 * IRQC_IS_HARDIRQ - interrupt runs in hardirq context
 * IRQC_IS_NESTED - interrupt runs in a nested threaded context
 */
enum {
 Model1_IRQC_IS_HARDIRQ = 0,
 Model1_IRQC_IS_NESTED,
};

typedef Model1_irqreturn_t (*Model1_irq_handler_t)(int, void *);

/**
 * struct irqaction - per interrupt action descriptor
 * @handler:	interrupt handler function
 * @name:	name of the device
 * @dev_id:	cookie to identify the device
 * @percpu_dev_id:	cookie to identify the device
 * @next:	pointer to the next irqaction for shared interrupts
 * @irq:	interrupt number
 * @flags:	flags (see IRQF_* above)
 * @thread_fn:	interrupt handler function for threaded interrupts
 * @thread:	thread pointer for threaded interrupts
 * @secondary:	pointer to secondary irqaction (force threading)
 * @thread_flags:	flags related to @thread
 * @thread_mask:	bitmask for keeping track of @thread activity
 * @dir:	pointer to the proc/irq/NN/name entry
 */
struct Model1_irqaction {
 Model1_irq_handler_t Model1_handler;
 void *Model1_dev_id;
 void *Model1_percpu_dev_id;
 struct Model1_irqaction *Model1_next;
 Model1_irq_handler_t Model1_thread_fn;
 struct Model1_task_struct *thread;
 struct Model1_irqaction *Model1_secondary;
 unsigned int Model1_irq;
 unsigned int Model1_flags;
 unsigned long Model1_thread_flags;
 unsigned long Model1_thread_mask;
 const char *Model1_name;
 struct Model1_proc_dir_entry *Model1_dir;
} __attribute__((__aligned__(1 << (6))));

extern Model1_irqreturn_t Model1_no_action(int Model1_cpl, void *Model1_dev_id);

/*
 * If a (PCI) device interrupt is not connected we set dev->irq to
 * IRQ_NOTCONNECTED. This causes request_irq() to fail with -ENOTCONN, so we
 * can distingiush that case from other error returns.
 *
 * 0x80000000 is guaranteed to be outside the available range of interrupts
 * and easy to distinguish from other possible incorrect values.
 */


extern int __attribute__((warn_unused_result))
Model1_request_threaded_irq(unsigned int Model1_irq, Model1_irq_handler_t Model1_handler,
       Model1_irq_handler_t Model1_thread_fn,
       unsigned long Model1_flags, const char *Model1_name, void *Model1_dev);

static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result))
Model1_request_irq(unsigned int Model1_irq, Model1_irq_handler_t Model1_handler, unsigned long Model1_flags,
     const char *Model1_name, void *Model1_dev)
{
 return Model1_request_threaded_irq(Model1_irq, Model1_handler, ((void *)0), Model1_flags, Model1_name, Model1_dev);
}

extern int __attribute__((warn_unused_result))
Model1_request_any_context_irq(unsigned int Model1_irq, Model1_irq_handler_t Model1_handler,
   unsigned long Model1_flags, const char *Model1_name, void *Model1_dev_id);

extern int __attribute__((warn_unused_result))
Model1_request_percpu_irq(unsigned int Model1_irq, Model1_irq_handler_t Model1_handler,
     const char *Model1_devname, void *Model1_percpu_dev_id);

extern void Model1_free_irq(unsigned int, void *);
extern void Model1_free_percpu_irq(unsigned int, void *);

struct Model1_device;

extern int __attribute__((warn_unused_result))
Model1_devm_request_threaded_irq(struct Model1_device *Model1_dev, unsigned int Model1_irq,
     Model1_irq_handler_t Model1_handler, Model1_irq_handler_t Model1_thread_fn,
     unsigned long Model1_irqflags, const char *Model1_devname,
     void *Model1_dev_id);

static inline __attribute__((no_instrument_function)) int __attribute__((warn_unused_result))
Model1_devm_request_irq(struct Model1_device *Model1_dev, unsigned int Model1_irq, Model1_irq_handler_t Model1_handler,
   unsigned long Model1_irqflags, const char *Model1_devname, void *Model1_dev_id)
{
 return Model1_devm_request_threaded_irq(Model1_dev, Model1_irq, Model1_handler, ((void *)0), Model1_irqflags,
      Model1_devname, Model1_dev_id);
}

extern int __attribute__((warn_unused_result))
Model1_devm_request_any_context_irq(struct Model1_device *Model1_dev, unsigned int Model1_irq,
   Model1_irq_handler_t Model1_handler, unsigned long Model1_irqflags,
   const char *Model1_devname, void *Model1_dev_id);

extern void Model1_devm_free_irq(struct Model1_device *Model1_dev, unsigned int Model1_irq, void *Model1_dev_id);

/*
 * On lockdep we dont want to enable hardirqs in hardirq
 * context. Use local_irq_enable_in_hardirq() to annotate
 * kernel code that has to do this nevertheless (pretty much
 * the only valid case is for old/broken hardware that is
 * insanely slow).
 *
 * NOTE: in theory this might break fragile code that relies
 * on hardirq delivery - in practice we dont seem to have such
 * places left. So the only effect should be slightly increased
 * irqs-off latencies.
 */






extern void Model1_disable_irq_nosync(unsigned int Model1_irq);
extern bool Model1_disable_hardirq(unsigned int Model1_irq);
extern void Model1_disable_irq(unsigned int Model1_irq);
extern void Model1_disable_percpu_irq(unsigned int Model1_irq);
extern void Model1_enable_irq(unsigned int Model1_irq);
extern void Model1_enable_percpu_irq(unsigned int Model1_irq, unsigned int Model1_type);
extern bool Model1_irq_percpu_is_enabled(unsigned int Model1_irq);
extern void Model1_irq_wake_thread(unsigned int Model1_irq, void *Model1_dev_id);

/* The following three functions are for the core kernel use only. */
extern void Model1_suspend_device_irqs(void);
extern void Model1_resume_device_irqs(void);

/**
 * struct irq_affinity_notify - context for notification of IRQ affinity changes
 * @irq:		Interrupt to which notification applies
 * @kref:		Reference count, for internal use
 * @work:		Work item, for internal use
 * @notify:		Function to be called on change.  This will be
 *			called in process context.
 * @release:		Function to be called on release.  This will be
 *			called in process context.  Once registered, the
 *			structure must only be freed when this function is
 *			called or later.
 */
struct Model1_irq_affinity_notify {
 unsigned int Model1_irq;
 struct Model1_kref Model1_kref;
 struct Model1_work_struct Model1_work;
 void (*Model1_notify)(struct Model1_irq_affinity_notify *, const Model1_cpumask_t *Model1_mask);
 void (*Model1_release)(struct Model1_kref *Model1_ref);
};



extern Model1_cpumask_var_t Model1_irq_default_affinity;

/* Internal implementation. Use the helpers below */
extern int Model1___irq_set_affinity(unsigned int Model1_irq, const struct Model1_cpumask *Model1_cpumask,
         bool Model1_force);

/**
 * irq_set_affinity - Set the irq affinity of a given irq
 * @irq:	Interrupt to set affinity
 * @cpumask:	cpumask
 *
 * Fails if cpumask does not contain an online CPU
 */
static inline __attribute__((no_instrument_function)) int
Model1_irq_set_affinity(unsigned int Model1_irq, const struct Model1_cpumask *Model1_cpumask)
{
 return Model1___irq_set_affinity(Model1_irq, Model1_cpumask, false);
}

/**
 * irq_force_affinity - Force the irq affinity of a given irq
 * @irq:	Interrupt to set affinity
 * @cpumask:	cpumask
 *
 * Same as irq_set_affinity, but without checking the mask against
 * online cpus.
 *
 * Solely for low level cpu hotplug code, where we need to make per
 * cpu interrupts affine before the cpu becomes online.
 */
static inline __attribute__((no_instrument_function)) int
Model1_irq_force_affinity(unsigned int Model1_irq, const struct Model1_cpumask *Model1_cpumask)
{
 return Model1___irq_set_affinity(Model1_irq, Model1_cpumask, true);
}

extern int Model1_irq_can_set_affinity(unsigned int Model1_irq);
extern int Model1_irq_select_affinity(unsigned int Model1_irq);

extern int Model1_irq_set_affinity_hint(unsigned int Model1_irq, const struct Model1_cpumask *Model1_m);

extern int
Model1_irq_set_affinity_notifier(unsigned int Model1_irq, struct Model1_irq_affinity_notify *Model1_notify);

struct Model1_cpumask *Model1_irq_create_affinity_mask(unsigned int *Model1_nr_vecs);
/*
 * Special lockdep variants of irq disabling/enabling.
 * These should be used for locking constructs that
 * know that a particular irq context which is disabled,
 * and which is the only irq-context user of a lock,
 * that it's safe to take the lock in the irq-disabled
 * section without disabling hardirqs.
 *
 * On !CONFIG_LOCKDEP they are equivalent to the normal
 * irq disable/enable methods.
 */
static inline __attribute__((no_instrument_function)) void Model1_disable_irq_nosync_lockdep(unsigned int Model1_irq)
{
 Model1_disable_irq_nosync(Model1_irq);



}

static inline __attribute__((no_instrument_function)) void Model1_disable_irq_nosync_lockdep_irqsave(unsigned int Model1_irq, unsigned long *Model1_flags)
{
 Model1_disable_irq_nosync(Model1_irq);



}

static inline __attribute__((no_instrument_function)) void Model1_disable_irq_lockdep(unsigned int Model1_irq)
{
 Model1_disable_irq(Model1_irq);



}

static inline __attribute__((no_instrument_function)) void Model1_enable_irq_lockdep(unsigned int Model1_irq)
{



 Model1_enable_irq(Model1_irq);
}

static inline __attribute__((no_instrument_function)) void Model1_enable_irq_lockdep_irqrestore(unsigned int Model1_irq, unsigned long *Model1_flags)
{



 Model1_enable_irq(Model1_irq);
}

/* IRQ wakeup (PM) control: */
extern int Model1_irq_set_irq_wake(unsigned int Model1_irq, unsigned int Model1_on);

static inline __attribute__((no_instrument_function)) int Model1_enable_irq_wake(unsigned int Model1_irq)
{
 return Model1_irq_set_irq_wake(Model1_irq, 1);
}

static inline __attribute__((no_instrument_function)) int Model1_disable_irq_wake(unsigned int Model1_irq)
{
 return Model1_irq_set_irq_wake(Model1_irq, 0);
}

/*
 * irq_get_irqchip_state/irq_set_irqchip_state specific flags
 */
enum Model1_irqchip_irq_state {
 Model1_IRQCHIP_STATE_PENDING, /* Is interrupt pending? */
 Model1_IRQCHIP_STATE_ACTIVE, /* Is interrupt in progress? */
 Model1_IRQCHIP_STATE_MASKED, /* Is interrupt masked? */
 Model1_IRQCHIP_STATE_LINE_LEVEL, /* Is IRQ line high? */
};

extern int Model1_irq_get_irqchip_state(unsigned int Model1_irq, enum Model1_irqchip_irq_state Model1_which,
     bool *Model1_state);
extern int Model1_irq_set_irqchip_state(unsigned int Model1_irq, enum Model1_irqchip_irq_state Model1_which,
     bool Model1_state);


extern bool Model1_force_irqthreads;
/* Some architectures might implement lazy enabling/disabling of
 * interrupts. In some cases, such as stop_machine, we might want
 * to ensure that after a local_irq_disable(), interrupts have
 * really been disabled in hardware. Such architectures need to
 * implement the following hook.
 */




/* PLEASE, avoid to allocate new softirqs, if you need not _really_ high
   frequency threaded job scheduling. For almost all the purposes
   tasklets are more than enough. F.e. all serial device BHs et
   al. should be converted to tasklets, not to softirqs.
 */

enum
{
 Model1_HI_SOFTIRQ=0,
 Model1_TIMER_SOFTIRQ,
 Model1_NET_TX_SOFTIRQ,
 Model1_NET_RX_SOFTIRQ,
 Model1_BLOCK_SOFTIRQ,
 Model1_IRQ_POLL_SOFTIRQ,
 Model1_TASKLET_SOFTIRQ,
 Model1_SCHED_SOFTIRQ,
 Model1_HRTIMER_SOFTIRQ, /* Unused, but kept as tools rely on the
			    numbering. Sigh! */
 Model1_RCU_SOFTIRQ, /* Preferable RCU should always be the last softirq */

 Model1_NR_SOFTIRQS
};



/* map softirq index to softirq name. update 'softirq_to_name' in
 * kernel/softirq.c when adding a new softirq.
 */
extern const char * const Model1_softirq_to_name[Model1_NR_SOFTIRQS];

/* softirq mask and active fields moved to irq_cpustat_t in
 * asm/hardirq.h to get better cache usage.  KAO
 */

struct Model1_softirq_action
{
 void (*Model1_action)(struct Model1_softirq_action *);
};

           void Model1_do_softirq(void);
           void Model1___do_softirq(void);


void Model1_do_softirq_own_stack(void);







extern void Model1_open_softirq(int Model1_nr, void (*Model1_action)(struct Model1_softirq_action *));
extern void Model1_softirq_init(void);
extern void Model1___raise_softirq_irqoff(unsigned int Model1_nr);

extern void Model1_raise_softirq_irqoff(unsigned int Model1_nr);
extern void Model1_raise_softirq(unsigned int Model1_nr);

extern __attribute__((section(".data..percpu" ""))) __typeof__(struct Model1_task_struct *) Model1_ksoftirqd;

static inline __attribute__((no_instrument_function)) struct Model1_task_struct *Model1_this_cpu_ksoftirqd(void)
{
 return ({ typeof(Model1_ksoftirqd) Model1_pscr_ret__; do { const void *Model1___vpp_verify = (typeof((&(Model1_ksoftirqd)) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); switch(sizeof(Model1_ksoftirqd)) { case 1: Model1_pscr_ret__ = ({ typeof(Model1_ksoftirqd) Model1_pfo_ret__; switch (sizeof(Model1_ksoftirqd)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_ksoftirqd)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_ksoftirqd)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_ksoftirqd)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_ksoftirqd)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 2: Model1_pscr_ret__ = ({ typeof(Model1_ksoftirqd) Model1_pfo_ret__; switch (sizeof(Model1_ksoftirqd)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_ksoftirqd)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_ksoftirqd)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_ksoftirqd)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_ksoftirqd)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 4: Model1_pscr_ret__ = ({ typeof(Model1_ksoftirqd) Model1_pfo_ret__; switch (sizeof(Model1_ksoftirqd)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_ksoftirqd)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_ksoftirqd)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_ksoftirqd)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_ksoftirqd)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 8: Model1_pscr_ret__ = ({ typeof(Model1_ksoftirqd) Model1_pfo_ret__; switch (sizeof(Model1_ksoftirqd)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_ksoftirqd)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_ksoftirqd)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_ksoftirqd)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_ksoftirqd)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; default: Model1___bad_size_call_parameter(); break; } Model1_pscr_ret__; });
}

/* Tasklets --- multithreaded analogue of BHs.

   Main feature differing them of generic softirqs: tasklet
   is running only on one CPU simultaneously.

   Main feature differing them of BHs: different tasklets
   may be run simultaneously on different CPUs.

   Properties:
   * If tasklet_schedule() is called, then tasklet is guaranteed
     to be executed on some cpu at least once after this.
   * If the tasklet is already scheduled, but its execution is still not
     started, it will be executed only once.
   * If this tasklet is already running on another CPU (or schedule is called
     from tasklet itself), it is rescheduled for later.
   * Tasklet is strictly serialized wrt itself, but not
     wrt another tasklets. If client needs some intertask synchronization,
     he makes it with spinlocks.
 */

struct Model1_tasklet_struct
{
 struct Model1_tasklet_struct *Model1_next;
 unsigned long Model1_state;
 Model1_atomic_t Model1_count;
 void (*func)(unsigned long);
 unsigned long Model1_data;
};
enum
{
 Model1_TASKLET_STATE_SCHED, /* Tasklet is scheduled for execution */
 Model1_TASKLET_STATE_RUN /* Tasklet is running (SMP only) */
};


static inline __attribute__((no_instrument_function)) int Model1_tasklet_trylock(struct Model1_tasklet_struct *Model1_t)
{
 return !Model1_test_and_set_bit(Model1_TASKLET_STATE_RUN, &(Model1_t)->Model1_state);
}

static inline __attribute__((no_instrument_function)) void Model1_tasklet_unlock(struct Model1_tasklet_struct *Model1_t)
{
 __asm__ __volatile__("": : :"memory");
 Model1_clear_bit(Model1_TASKLET_STATE_RUN, &(Model1_t)->Model1_state);
}

static inline __attribute__((no_instrument_function)) void Model1_tasklet_unlock_wait(struct Model1_tasklet_struct *Model1_t)
{
 while ((__builtin_constant_p((Model1_TASKLET_STATE_RUN)) ? Model1_constant_test_bit((Model1_TASKLET_STATE_RUN), (&(Model1_t)->Model1_state)) : Model1_variable_test_bit((Model1_TASKLET_STATE_RUN), (&(Model1_t)->Model1_state)))) { __asm__ __volatile__("": : :"memory"); }
}






extern void Model1___tasklet_schedule(struct Model1_tasklet_struct *Model1_t);

static inline __attribute__((no_instrument_function)) void Model1_tasklet_schedule(struct Model1_tasklet_struct *Model1_t)
{
 if (!Model1_test_and_set_bit(Model1_TASKLET_STATE_SCHED, &Model1_t->Model1_state))
  Model1___tasklet_schedule(Model1_t);
}

extern void Model1___tasklet_hi_schedule(struct Model1_tasklet_struct *Model1_t);

static inline __attribute__((no_instrument_function)) void Model1_tasklet_hi_schedule(struct Model1_tasklet_struct *Model1_t)
{
 if (!Model1_test_and_set_bit(Model1_TASKLET_STATE_SCHED, &Model1_t->Model1_state))
  Model1___tasklet_hi_schedule(Model1_t);
}

extern void Model1___tasklet_hi_schedule_first(struct Model1_tasklet_struct *Model1_t);

/*
 * This version avoids touching any other tasklets. Needed for kmemcheck
 * in order not to take any page faults while enqueueing this tasklet;
 * consider VERY carefully whether you really need this or
 * tasklet_hi_schedule()...
 */
static inline __attribute__((no_instrument_function)) void Model1_tasklet_hi_schedule_first(struct Model1_tasklet_struct *Model1_t)
{
 if (!Model1_test_and_set_bit(Model1_TASKLET_STATE_SCHED, &Model1_t->Model1_state))
  Model1___tasklet_hi_schedule_first(Model1_t);
}


static inline __attribute__((no_instrument_function)) void Model1_tasklet_disable_nosync(struct Model1_tasklet_struct *Model1_t)
{
 Model1_atomic_inc(&Model1_t->Model1_count);
 __asm__ __volatile__("": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model1_tasklet_disable(struct Model1_tasklet_struct *Model1_t)
{
 Model1_tasklet_disable_nosync(Model1_t);
 Model1_tasklet_unlock_wait(Model1_t);
 asm volatile("mfence":::"memory");
}

static inline __attribute__((no_instrument_function)) void Model1_tasklet_enable(struct Model1_tasklet_struct *Model1_t)
{
 __asm__ __volatile__("": : :"memory");
 Model1_atomic_dec(&Model1_t->Model1_count);
}

extern void Model1_tasklet_kill(struct Model1_tasklet_struct *Model1_t);
extern void Model1_tasklet_kill_immediate(struct Model1_tasklet_struct *Model1_t, unsigned int Model1_cpu);
extern void Model1_tasklet_init(struct Model1_tasklet_struct *Model1_t,
    void (*func)(unsigned long), unsigned long Model1_data);

struct Model1_tasklet_hrtimer {
 struct Model1_hrtimer Model1_timer;
 struct Model1_tasklet_struct Model1_tasklet;
 enum Model1_hrtimer_restart (*Model1_function)(struct Model1_hrtimer *);
};

extern void
Model1_tasklet_hrtimer_init(struct Model1_tasklet_hrtimer *Model1_ttimer,
       enum Model1_hrtimer_restart (*Model1_function)(struct Model1_hrtimer *),
       Model1_clockid_t Model1_which_clock, enum Model1_hrtimer_mode Model1_mode);

static inline __attribute__((no_instrument_function))
void Model1_tasklet_hrtimer_start(struct Model1_tasklet_hrtimer *Model1_ttimer, Model1_ktime_t Model1_time,
      const enum Model1_hrtimer_mode Model1_mode)
{
 Model1_hrtimer_start(&Model1_ttimer->Model1_timer, Model1_time, Model1_mode);
}

static inline __attribute__((no_instrument_function))
void Model1_tasklet_hrtimer_cancel(struct Model1_tasklet_hrtimer *Model1_ttimer)
{
 Model1_hrtimer_cancel(&Model1_ttimer->Model1_timer);
 Model1_tasklet_kill(&Model1_ttimer->Model1_tasklet);
}

/*
 * Autoprobing for irqs:
 *
 * probe_irq_on() and probe_irq_off() provide robust primitives
 * for accurate IRQ probing during kernel initialization.  They are
 * reasonably simple to use, are not "fooled" by spurious interrupts,
 * and, unlike other attempts at IRQ probing, they do not get hung on
 * stuck interrupts (such as unused PS2 mouse interfaces on ASUS boards).
 *
 * For reasonably foolproof probing, use them as follows:
 *
 * 1. clear and/or mask the device's internal interrupt.
 * 2. sti();
 * 3. irqs = probe_irq_on();      // "take over" all unassigned idle IRQs
 * 4. enable the device and cause it to trigger an interrupt.
 * 5. wait for the device to interrupt, using non-intrusive polling or a delay.
 * 6. irq = probe_irq_off(irqs);  // get IRQ number, 0=none, negative=multiple
 * 7. service the device to clear its pending interrupt.
 * 8. loop again if paranoia is required.
 *
 * probe_irq_on() returns a mask of allocated irq's.
 *
 * probe_irq_off() takes the mask as a parameter,
 * and returns the irq number which occurred,
 * or zero if none occurred, or a negative irq number
 * if more than one irq occurred.
 */
extern unsigned long Model1_probe_irq_on(void); /* returns 0 on failure */
extern int Model1_probe_irq_off(unsigned long); /* returns 0 or negative on failure */
extern unsigned int Model1_probe_irq_mask(unsigned long); /* returns mask of ISA interrupts */



/* Initialize /proc/irq/ */
extern void Model1_init_irq_proc(void);






struct Model1_seq_file;
int Model1_show_interrupts(struct Model1_seq_file *Model1_p, void *Model1_v);
int Model1_arch_show_interrupts(struct Model1_seq_file *Model1_p, int Model1_prec);

extern int Model1_early_irq_init(void);
extern int Model1_arch_probe_nr_irqs(void);
extern int Model1_arch_early_irq_init(void);




struct Model1_flow_cache_percpu {
 struct Model1_hlist_head *Model1_hash_table;
 int Model1_hash_count;
 Model1_u32 Model1_hash_rnd;
 int Model1_hash_rnd_recalc;
 struct Model1_tasklet_struct Model1_flush_tasklet;
};

struct Model1_flow_cache {
 Model1_u32 Model1_hash_shift;
 struct Model1_flow_cache_percpu *Model1_percpu;
 struct Model1_notifier_block Model1_hotcpu_notifier;
 int Model1_low_watermark;
 int Model1_high_watermark;
 struct Model1_timer_list Model1_rnd_timer;
};

struct Model1_ctl_table_header;

struct Model1_xfrm_policy_hash {
 struct Model1_hlist_head *Model1_table;
 unsigned int Model1_hmask;
 Model1_u8 Model1_dbits4;
 Model1_u8 Model1_sbits4;
 Model1_u8 Model1_dbits6;
 Model1_u8 Model1_sbits6;
};

struct Model1_xfrm_policy_hthresh {
 struct Model1_work_struct Model1_work;
 Model1_seqlock_t Model1_lock;
 Model1_u8 Model1_lbits4;
 Model1_u8 Model1_rbits4;
 Model1_u8 Model1_lbits6;
 Model1_u8 Model1_rbits6;
};

struct Model1_netns_xfrm {
 struct Model1_list_head Model1_state_all;
 /*
	 * Hash table to find appropriate SA towards given target (endpoint of
	 * tunnel or destination of transport mode) allowed by selector.
	 *
	 * Main use is finding SA after policy selected tunnel or transport
	 * mode. Also, it can be used by ah/esp icmp error handler to find
	 * offending SA.
	 */
 struct Model1_hlist_head *Model1_state_bydst;
 struct Model1_hlist_head *Model1_state_bysrc;
 struct Model1_hlist_head *Model1_state_byspi;
 unsigned int Model1_state_hmask;
 unsigned int Model1_state_num;
 struct Model1_work_struct Model1_state_hash_work;
 struct Model1_hlist_head Model1_state_gc_list;
 struct Model1_work_struct Model1_state_gc_work;

 struct Model1_list_head Model1_policy_all;
 struct Model1_hlist_head *Model1_policy_byidx;
 unsigned int Model1_policy_idx_hmask;
 struct Model1_hlist_head Model1_policy_inexact[Model1_XFRM_POLICY_MAX];
 struct Model1_xfrm_policy_hash Model1_policy_bydst[Model1_XFRM_POLICY_MAX];
 unsigned int Model1_policy_count[Model1_XFRM_POLICY_MAX * 2];
 struct Model1_work_struct Model1_policy_hash_work;
 struct Model1_xfrm_policy_hthresh Model1_policy_hthresh;


 struct Model1_sock *Model1_nlsk;
 struct Model1_sock *Model1_nlsk_stash;

 Model1_u32 Model1_sysctl_aevent_etime;
 Model1_u32 Model1_sysctl_aevent_rseqth;
 int Model1_sysctl_larval_drop;
 Model1_u32 Model1_sysctl_acq_expires;

 struct Model1_ctl_table_header *Model1_sysctl_hdr;


 struct Model1_dst_ops Model1_xfrm4_dst_ops;

 struct Model1_dst_ops Model1_xfrm6_dst_ops;

 Model1_spinlock_t Model1_xfrm_state_lock;
 Model1_rwlock_t Model1_xfrm_policy_lock;
 struct Model1_mutex Model1_xfrm_cfg_mutex;

 /* flow cache part */
 struct Model1_flow_cache Model1_flow_cache_global;
 Model1_atomic_t Model1_flow_cache_genid;
 struct Model1_list_head Model1_flow_cache_gc_list;
 Model1_atomic_t Model1_flow_cache_gc_count;
 Model1_spinlock_t Model1_flow_cache_gc_lock;
 struct Model1_work_struct Model1_flow_cache_gc_work;
 struct Model1_work_struct Model1_flow_cache_flush_work;
 struct Model1_mutex Model1_flow_flush_sem;
};
/*
 * mpls in net namespaces
 */




struct Model1_mpls_route;
struct Model1_ctl_table_header;

struct Model1_netns_mpls {
 Model1_size_t Model1_platform_labels;
 struct Model1_mpls_route * *Model1_platform_label;
 struct Model1_ctl_table_header *Model1_ctl;
};



struct Model1_proc_ns_operations;

struct Model1_ns_common {
 Model1_atomic_long_t Model1_stashed;
 const struct Model1_proc_ns_operations *Model1_ops;
 unsigned int Model1_inum;
};



struct Model1_user_namespace;
struct Model1_proc_dir_entry;
struct Model1_net_device;
struct Model1_sock;
struct Model1_ctl_table_header;
struct Model1_net_generic;
struct Model1_sock;
struct Model1_netns_ipvs;





struct Model1_net {
 Model1_atomic_t Model1_passive; /* To decided when the network
						 * namespace should be freed.
						 */
 Model1_atomic_t Model1_count; /* To decided when the network
						 *  namespace should be shut down.
						 */
 Model1_spinlock_t Model1_rules_mod_lock;

 Model1_atomic64_t Model1_cookie_gen;

 struct Model1_list_head Model1_list; /* list of network namespaces */
 struct Model1_list_head Model1_cleanup_list; /* namespaces on death row */
 struct Model1_list_head Model1_exit_list; /* Use only net_mutex */

 struct Model1_user_namespace *Model1_user_ns; /* Owning user namespace */
 Model1_spinlock_t Model1_nsid_lock;
 struct Model1_idr Model1_netns_ids;

 struct Model1_ns_common Model1_ns;

 struct Model1_proc_dir_entry *Model1_proc_net;
 struct Model1_proc_dir_entry *Model1_proc_net_stat;


 struct Model1_ctl_table_set Model1_sysctls;


 struct Model1_sock *Model1_rtnl; /* rtnetlink socket */
 struct Model1_sock *Model1_genl_sock;

 struct Model1_list_head Model1_dev_base_head;
 struct Model1_hlist_head *Model1_dev_name_head;
 struct Model1_hlist_head *Model1_dev_index_head;
 unsigned int Model1_dev_base_seq; /* protected by rtnl_mutex */
 int Model1_ifindex;
 unsigned int Model1_dev_unreg_count;

 /* core fib_rules */
 struct Model1_list_head Model1_rules_ops;


 struct Model1_net_device *Model1_loopback_dev; /* The loopback */
 struct Model1_netns_core Model1_core;
 struct Model1_netns_mib Model1_mib;
 struct Model1_netns_packet Model1_packet;
 struct Model1_netns_unix Model1_unx;
 struct Model1_netns_ipv4 Model1_ipv4;

 struct Model1_netns_ipv6 Model1_ipv6;
 struct Model1_netns_nf Model1_nf;
 struct Model1_netns_xt Model1_xt;

 struct Model1_netns_ct Model1_ct;





 struct Model1_netns_nf_frag Model1_nf_frag;

 struct Model1_sock *Model1_nfnl;
 struct Model1_sock *Model1_nfnl_stash;
 struct Model1_net_generic *Model1_gen;

 /* Note : following structs are cache line aligned */

 struct Model1_netns_xfrm Model1_xfrm;







 struct Model1_sock *Model1_diag_nlsk;
 Model1_atomic_t Model1_fnhe_genid;
};





struct Model1_seq_operations;

struct Model1_seq_file {
 char *Model1_buf;
 Model1_size_t Model1_size;
 Model1_size_t Model1_from;
 Model1_size_t Model1_count;
 Model1_size_t Model1_pad_until;
 Model1_loff_t Model1_index;
 Model1_loff_t Model1_read_pos;
 Model1_u64 Model1_version;
 struct Model1_mutex Model1_lock;
 const struct Model1_seq_operations *Model1_op;
 int Model1_poll_event;
 const struct Model1_file *Model1_file;
 void *Model1_private;
};

struct Model1_seq_operations {
 void * (*Model1_start) (struct Model1_seq_file *Model1_m, Model1_loff_t *Model1_pos);
 void (*Model1_stop) (struct Model1_seq_file *Model1_m, void *Model1_v);
 void * (*Model1_next) (struct Model1_seq_file *Model1_m, void *Model1_v, Model1_loff_t *Model1_pos);
 int (*Model1_show) (struct Model1_seq_file *Model1_m, void *Model1_v);
};



/**
 * seq_has_overflowed - check if the buffer has overflowed
 * @m: the seq_file handle
 *
 * seq_files have a buffer which may overflow. When this happens a larger
 * buffer is reallocated and all the data will be printed again.
 * The overflow state is true when m->count == m->size.
 *
 * Returns true if the buffer received more than it can hold.
 */
static inline __attribute__((no_instrument_function)) bool Model1_seq_has_overflowed(struct Model1_seq_file *Model1_m)
{
 return Model1_m->Model1_count == Model1_m->Model1_size;
}

/**
 * seq_get_buf - get buffer to write arbitrary data to
 * @m: the seq_file handle
 * @bufp: the beginning of the buffer is stored here
 *
 * Return the number of bytes available in the buffer, or zero if
 * there's no space.
 */
static inline __attribute__((no_instrument_function)) Model1_size_t Model1_seq_get_buf(struct Model1_seq_file *Model1_m, char **Model1_bufp)
{
 do { if (__builtin_expect(!!(Model1_m->Model1_count > Model1_m->Model1_size), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/seq_file.h"), "i" (65), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);
 if (Model1_m->Model1_count < Model1_m->Model1_size)
  *Model1_bufp = Model1_m->Model1_buf + Model1_m->Model1_count;
 else
  *Model1_bufp = ((void *)0);

 return Model1_m->Model1_size - Model1_m->Model1_count;
}

/**
 * seq_commit - commit data to the buffer
 * @m: the seq_file handle
 * @num: the number of bytes to commit
 *
 * Commit @num bytes of data written to a buffer previously acquired
 * by seq_buf_get.  To signal an error condition, or that the data
 * didn't fit in the available space, pass a negative @num value.
 */
static inline __attribute__((no_instrument_function)) void Model1_seq_commit(struct Model1_seq_file *Model1_m, int Model1_num)
{
 if (Model1_num < 0) {
  Model1_m->Model1_count = Model1_m->Model1_size;
 } else {
  do { if (__builtin_expect(!!(Model1_m->Model1_count + Model1_num > Model1_m->Model1_size), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/seq_file.h"), "i" (88), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);
  Model1_m->Model1_count += Model1_num;
 }
}

/**
 * seq_setwidth - set padding width
 * @m: the seq_file handle
 * @size: the max number of bytes to pad.
 *
 * Call seq_setwidth() for setting max width, then call seq_printf() etc. and
 * finally call seq_pad() to pad the remaining bytes.
 */
static inline __attribute__((no_instrument_function)) void Model1_seq_setwidth(struct Model1_seq_file *Model1_m, Model1_size_t Model1_size)
{
 Model1_m->Model1_pad_until = Model1_m->Model1_count + Model1_size;
}
void Model1_seq_pad(struct Model1_seq_file *Model1_m, char Model1_c);

char *Model1_mangle_path(char *Model1_s, const char *Model1_p, const char *Model1_esc);
int Model1_seq_open(struct Model1_file *, const struct Model1_seq_operations *);
Model1_ssize_t Model1_seq_read(struct Model1_file *, char *, Model1_size_t, Model1_loff_t *);
Model1_loff_t Model1_seq_lseek(struct Model1_file *, Model1_loff_t, int);
int Model1_seq_release(struct Model1_inode *, struct Model1_file *);
int Model1_seq_write(struct Model1_seq_file *Model1_seq, const void *Model1_data, Model1_size_t Model1_len);

__attribute__((format(printf, 2, 0)))
void Model1_seq_vprintf(struct Model1_seq_file *Model1_m, const char *Model1_fmt, Model1_va_list Model1_args);
__attribute__((format(printf, 2, 3)))
void Model1_seq_printf(struct Model1_seq_file *Model1_m, const char *Model1_fmt, ...);
void Model1_seq_putc(struct Model1_seq_file *Model1_m, char Model1_c);
void Model1_seq_puts(struct Model1_seq_file *Model1_m, const char *Model1_s);
void Model1_seq_put_decimal_ull(struct Model1_seq_file *Model1_m, char Model1_delimiter,
    unsigned long long Model1_num);
void Model1_seq_put_decimal_ll(struct Model1_seq_file *Model1_m, char Model1_delimiter, long long Model1_num);
void Model1_seq_escape(struct Model1_seq_file *Model1_m, const char *Model1_s, const char *Model1_esc);

void Model1_seq_hex_dump(struct Model1_seq_file *Model1_m, const char *Model1_prefix_str, int Model1_prefix_type,
    int Model1_rowsize, int Model1_groupsize, const void *Model1_buf, Model1_size_t Model1_len,
    bool Model1_ascii);

int Model1_seq_path(struct Model1_seq_file *, const struct Model1_path *, const char *);
int Model1_seq_file_path(struct Model1_seq_file *, struct Model1_file *, const char *);
int Model1_seq_dentry(struct Model1_seq_file *, struct Model1_dentry *, const char *);
int Model1_seq_path_root(struct Model1_seq_file *Model1_m, const struct Model1_path *Model1_path,
    const struct Model1_path *Model1_root, const char *Model1_esc);

int Model1_single_open(struct Model1_file *, int (*)(struct Model1_seq_file *, void *), void *);
int Model1_single_open_size(struct Model1_file *, int (*)(struct Model1_seq_file *, void *), void *, Model1_size_t);
int Model1_single_release(struct Model1_inode *, struct Model1_file *);
void *Model1___seq_open_private(struct Model1_file *, const struct Model1_seq_operations *, int);
int Model1_seq_open_private(struct Model1_file *, const struct Model1_seq_operations *, int);
int Model1_seq_release_private(struct Model1_inode *, struct Model1_file *);

static inline __attribute__((no_instrument_function)) struct Model1_user_namespace *Model1_seq_user_ns(struct Model1_seq_file *Model1_seq)
{



 extern struct Model1_user_namespace Model1_init_user_ns;
 return &Model1_init_user_ns;

}

/**
 * seq_show_options - display mount options with appropriate escapes.
 * @m: the seq_file handle
 * @name: the mount option name
 * @value: the mount option name's value, can be NULL
 */
static inline __attribute__((no_instrument_function)) void Model1_seq_show_option(struct Model1_seq_file *Model1_m, const char *Model1_name,
       const char *Model1_value)
{
 Model1_seq_putc(Model1_m, ',');
 Model1_seq_escape(Model1_m, Model1_name, ",= \t\n\\");
 if (Model1_value) {
  Model1_seq_putc(Model1_m, '=');
  Model1_seq_escape(Model1_m, Model1_value, ", \t\n\\");
 }
}

/**
 * seq_show_option_n - display mount options with appropriate escapes
 *		       where @value must be a specific length.
 * @m: the seq_file handle
 * @name: the mount option name
 * @value: the mount option name's value, cannot be NULL
 * @length: the length of @value to display
 *
 * This is a macro since this uses "length" to define the size of the
 * stack buffer.
 */
/*
 * Helpers for iteration over list_head-s in seq_files
 */

extern struct Model1_list_head *Model1_seq_list_start(struct Model1_list_head *Model1_head,
  Model1_loff_t Model1_pos);
extern struct Model1_list_head *Model1_seq_list_start_head(struct Model1_list_head *Model1_head,
  Model1_loff_t Model1_pos);
extern struct Model1_list_head *Model1_seq_list_next(void *Model1_v, struct Model1_list_head *Model1_head,
  Model1_loff_t *Model1_ppos);

/*
 * Helpers for iteration over hlist_head-s in seq_files
 */

extern struct Model1_hlist_node *Model1_seq_hlist_start(struct Model1_hlist_head *Model1_head,
       Model1_loff_t Model1_pos);
extern struct Model1_hlist_node *Model1_seq_hlist_start_head(struct Model1_hlist_head *Model1_head,
            Model1_loff_t Model1_pos);
extern struct Model1_hlist_node *Model1_seq_hlist_next(void *Model1_v, struct Model1_hlist_head *Model1_head,
      Model1_loff_t *Model1_ppos);

extern struct Model1_hlist_node *Model1_seq_hlist_start_rcu(struct Model1_hlist_head *Model1_head,
           Model1_loff_t Model1_pos);
extern struct Model1_hlist_node *Model1_seq_hlist_start_head_rcu(struct Model1_hlist_head *Model1_head,
         Model1_loff_t Model1_pos);
extern struct Model1_hlist_node *Model1_seq_hlist_next_rcu(void *Model1_v,
         struct Model1_hlist_head *Model1_head,
         Model1_loff_t *Model1_ppos);

/* Helpers for iterating over per-cpu hlist_head-s in seq_files */
extern struct Model1_hlist_node *Model1_seq_hlist_start_percpu(struct Model1_hlist_head *Model1_head, int *Model1_cpu, Model1_loff_t Model1_pos);

extern struct Model1_hlist_node *Model1_seq_hlist_next_percpu(void *Model1_v, struct Model1_hlist_head *Model1_head, int *Model1_cpu, Model1_loff_t *Model1_pos);

struct Model1_net;
extern struct Model1_net Model1_init_net;

struct Model1_seq_net_private {

 struct Model1_net *Model1_net;

};

int Model1_seq_open_net(struct Model1_inode *, struct Model1_file *,
   const struct Model1_seq_operations *, int);
int Model1_single_open_net(struct Model1_inode *, struct Model1_file *Model1_file,
  int (*Model1_show)(struct Model1_seq_file *, void *));
int Model1_seq_release_net(struct Model1_inode *, struct Model1_file *);
int Model1_single_release_net(struct Model1_inode *, struct Model1_file *);
static inline __attribute__((no_instrument_function)) struct Model1_net *Model1_seq_file_net(struct Model1_seq_file *Model1_seq)
{

 return ((struct Model1_seq_net_private *)Model1_seq->Model1_private)->Model1_net;



}

/* Init's network namespace */
extern struct Model1_net Model1_init_net;


struct Model1_net *Model1_copy_net_ns(unsigned long Model1_flags, struct Model1_user_namespace *Model1_user_ns,
   struct Model1_net *Model1_old_net);
extern struct Model1_list_head Model1_net_namespace_list;

struct Model1_net *Model1_get_net_ns_by_pid(Model1_pid_t Model1_pid);
struct Model1_net *Model1_get_net_ns_by_fd(int Model1_pid);


void Model1_ipx_register_sysctl(void);
void Model1_ipx_unregister_sysctl(void);






void Model1___put_net(struct Model1_net *Model1_net);

static inline __attribute__((no_instrument_function)) struct Model1_net *Model1_get_net(struct Model1_net *Model1_net)
{
 Model1_atomic_inc(&Model1_net->Model1_count);
 return Model1_net;
}

static inline __attribute__((no_instrument_function)) struct Model1_net *Model1_maybe_get_net(struct Model1_net *Model1_net)
{
 /* Used when we know struct net exists but we
	 * aren't guaranteed a previous reference count
	 * exists.  If the reference count is zero this
	 * function fails and returns NULL.
	 */
 if (!Model1_atomic_add_unless((&Model1_net->Model1_count), 1, 0))
  Model1_net = ((void *)0);
 return Model1_net;
}

static inline __attribute__((no_instrument_function)) void Model1_put_net(struct Model1_net *Model1_net)
{
 if (Model1_atomic_dec_and_test(&Model1_net->Model1_count))
  Model1___put_net(Model1_net);
}

static inline __attribute__((no_instrument_function))
int Model1_net_eq(const struct Model1_net *Model1_net1, const struct Model1_net *Model1_net2)
{
 return Model1_net1 == Model1_net2;
}

void Model1_net_drop_ns(void *);
typedef struct {

 struct Model1_net *Model1_net;

} Model1_possible_net_t;

static inline __attribute__((no_instrument_function)) void Model1_write_pnet(Model1_possible_net_t *Model1_pnet, struct Model1_net *Model1_net)
{

 Model1_pnet->Model1_net = Model1_net;

}

static inline __attribute__((no_instrument_function)) struct Model1_net *Model1_read_pnet(const Model1_possible_net_t *Model1_pnet)
{
 return Model1_pnet->Model1_net;

}
int Model1_peernet2id_alloc(struct Model1_net *Model1_net, struct Model1_net *Model1_peer);
int Model1_peernet2id(struct Model1_net *Model1_net, struct Model1_net *Model1_peer);
bool Model1_peernet_has_id(struct Model1_net *Model1_net, struct Model1_net *Model1_peer);
struct Model1_net *Model1_get_net_ns_by_id(struct Model1_net *Model1_net, int Model1_id);

struct Model1_pernet_operations {
 struct Model1_list_head Model1_list;
 int (*Model1_init)(struct Model1_net *Model1_net);
 void (*Model1_exit)(struct Model1_net *Model1_net);
 void (*Model1_exit_batch)(struct Model1_list_head *Model1_net_exit_list);
 int *Model1_id;
 Model1_size_t Model1_size;
};

/*
 * Use these carefully.  If you implement a network device and it
 * needs per network namespace operations use device pernet operations,
 * otherwise use pernet subsys operations.
 *
 * Network interfaces need to be removed from a dying netns _before_
 * subsys notifiers can be called, as most of the network code cleanup
 * (which is done from subsys notifiers) runs with the assumption that
 * dev_remove_pack has been called so no new packets will arrive during
 * and after the cleanup functions have been called.  dev_remove_pack
 * is not per namespace so instead the guarantee of no more packets
 * arriving in a network namespace is provided by ensuring that all
 * network devices and all sockets have left the network namespace
 * before the cleanup methods are called.
 *
 * For the longest time the ipv4 icmp code was registered as a pernet
 * device which caused kernel oops, and panics during network
 * namespace cleanup.   So please don't get this wrong.
 */
int Model1_register_pernet_subsys(struct Model1_pernet_operations *);
void Model1_unregister_pernet_subsys(struct Model1_pernet_operations *);
int Model1_register_pernet_device(struct Model1_pernet_operations *);
void Model1_unregister_pernet_device(struct Model1_pernet_operations *);

struct Model1_ctl_table;
struct Model1_ctl_table_header;


int Model1_net_sysctl_init(void);
struct Model1_ctl_table_header *Model1_register_net_sysctl(struct Model1_net *Model1_net, const char *Model1_path,
          struct Model1_ctl_table *Model1_table);
void Model1_unregister_net_sysctl_table(struct Model1_ctl_table_header *Model1_header);
static inline __attribute__((no_instrument_function)) int Model1_rt_genid_ipv4(struct Model1_net *Model1_net)
{
 return Model1_atomic_read(&Model1_net->Model1_ipv4.Model1_rt_genid);
}

static inline __attribute__((no_instrument_function)) void Model1_rt_genid_bump_ipv4(struct Model1_net *Model1_net)
{
 Model1_atomic_inc(&Model1_net->Model1_ipv4.Model1_rt_genid);
}

extern void (*Model1___fib6_flush_trees)(struct Model1_net *Model1_net);
static inline __attribute__((no_instrument_function)) void Model1_rt_genid_bump_ipv6(struct Model1_net *Model1_net)
{
 if (Model1___fib6_flush_trees)
  Model1___fib6_flush_trees(Model1_net);
}
/* For callers who don't really care about whether it's IPv4 or IPv6 */
static inline __attribute__((no_instrument_function)) void Model1_rt_genid_bump_all(struct Model1_net *Model1_net)
{
 Model1_rt_genid_bump_ipv4(Model1_net);
 Model1_rt_genid_bump_ipv6(Model1_net);
}

static inline __attribute__((no_instrument_function)) int Model1_fnhe_genid(struct Model1_net *Model1_net)
{
 return Model1_atomic_read(&Model1_net->Model1_fnhe_genid);
}

static inline __attribute__((no_instrument_function)) void Model1_fnhe_genid_bump(struct Model1_net *Model1_net)
{
 Model1_atomic_inc(&Model1_net->Model1_fnhe_genid);
}
/*
 * include/net/dsa.h - Driver for Distributed Switch Architecture switch chips
 * Copyright (c) 2008-2009 Marvell Semiconductor
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 */


/*
 * Definitions for talking to the Open Firmware PROM on
 * Power Macintosh and other computers.
 *
 * Copyright (C) 1996-2005 Paul Mackerras.
 *
 * Updates for PPC64 by Peter Bergner & David Engebretsen, IBM Corp.
 * Updates for SPARC64 by David S. Miller
 * Derived from PowerPC and Sparc prom.h files by Stephen Rothwell, IBM Corp.
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public License
 * as published by the Free Software Foundation; either version
 * 2 of the License, or (at your option) any later version.
 */





/*
 * Device tables which are exported to userspace via
 * scripts/mod/file2alias.c.  You must keep that file in sync with this
 * header.
 */







/*
 * UUID/GUID definition
 *
 * Copyright (C) 2010, 2016 Intel Corp.
 *	Huang Ying <ying.huang@intel.com>
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public License version
 * 2 as published by the Free Software Foundation;
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 */




/*
 * UUID/GUID definition
 *
 * Copyright (C) 2010, Intel Corp.
 *	Huang Ying <ying.huang@intel.com>
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public License version
 * 2 as published by the Free Software Foundation;
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 */







typedef struct {
 __u8 Model1_b[16];
} Model1_uuid_le;

typedef struct {
 __u8 Model1_b[16];
} Model1_uuid_be;

/*
 * The length of a UUID string ("aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee")
 * not including trailing NUL.
 */


static inline __attribute__((no_instrument_function)) int Model1_uuid_le_cmp(const Model1_uuid_le Model1_u1, const Model1_uuid_le Model1_u2)
{
 return Model1_memcmp(&Model1_u1, &Model1_u2, sizeof(Model1_uuid_le));
}

static inline __attribute__((no_instrument_function)) int Model1_uuid_be_cmp(const Model1_uuid_be Model1_u1, const Model1_uuid_be Model1_u2)
{
 return Model1_memcmp(&Model1_u1, &Model1_u2, sizeof(Model1_uuid_be));
}

void Model1_generate_random_uuid(unsigned char Model1_uuid[16]);

extern void Model1_uuid_le_gen(Model1_uuid_le *Model1_u);
extern void Model1_uuid_be_gen(Model1_uuid_be *Model1_u);

bool __attribute__((warn_unused_result)) Model1_uuid_is_valid(const char *Model1_uuid);

extern const Model1_u8 Model1_uuid_le_index[16];
extern const Model1_u8 Model1_uuid_be_index[16];

int Model1_uuid_le_to_bin(const char *Model1_uuid, Model1_uuid_le *Model1_u);
int Model1_uuid_be_to_bin(const char *Model1_uuid, Model1_uuid_be *Model1_u);
typedef unsigned long Model1_kernel_ulong_t;




struct Model1_pci_device_id {
 __u32 Model1_vendor, Model1_device; /* Vendor and device ID or PCI_ANY_ID*/
 __u32 Model1_subvendor, Model1_subdevice; /* Subsystem ID's or PCI_ANY_ID */
 __u32 Model1_class, Model1_class_mask; /* (class,subclass,prog-if) triplet */
 Model1_kernel_ulong_t Model1_driver_data; /* Data private to the driver */
};







struct Model1_ieee1394_device_id {
 __u32 Model1_match_flags;
 __u32 Model1_vendor_id;
 __u32 Model1_model_id;
 __u32 Model1_specifier_id;
 __u32 Model1_version;
 Model1_kernel_ulong_t Model1_driver_data;
};


/*
 * Device table entry for "new style" table-driven USB drivers.
 * User mode code can read these tables to choose which modules to load.
 * Declare the table as a MODULE_DEVICE_TABLE.
 *
 * A probe() parameter will point to a matching entry from this table.
 * Use the driver_info field for each match to hold information tied
 * to that match:  device quirks, etc.
 *
 * Terminate the driver's table with an all-zeroes entry.
 * Use the flag values to control which fields are compared.
 */

/**
 * struct usb_device_id - identifies USB devices for probing and hotplugging
 * @match_flags: Bit mask controlling which of the other fields are used to
 *	match against new devices. Any field except for driver_info may be
 *	used, although some only make sense in conjunction with other fields.
 *	This is usually set by a USB_DEVICE_*() macro, which sets all
 *	other fields in this structure except for driver_info.
 * @idVendor: USB vendor ID for a device; numbers are assigned
 *	by the USB forum to its members.
 * @idProduct: Vendor-assigned product ID.
 * @bcdDevice_lo: Low end of range of vendor-assigned product version numbers.
 *	This is also used to identify individual product versions, for
 *	a range consisting of a single device.
 * @bcdDevice_hi: High end of version number range.  The range of product
 *	versions is inclusive.
 * @bDeviceClass: Class of device; numbers are assigned
 *	by the USB forum.  Products may choose to implement classes,
 *	or be vendor-specific.  Device classes specify behavior of all
 *	the interfaces on a device.
 * @bDeviceSubClass: Subclass of device; associated with bDeviceClass.
 * @bDeviceProtocol: Protocol of device; associated with bDeviceClass.
 * @bInterfaceClass: Class of interface; numbers are assigned
 *	by the USB forum.  Products may choose to implement classes,
 *	or be vendor-specific.  Interface classes specify behavior only
 *	of a given interface; other interfaces may support other classes.
 * @bInterfaceSubClass: Subclass of interface; associated with bInterfaceClass.
 * @bInterfaceProtocol: Protocol of interface; associated with bInterfaceClass.
 * @bInterfaceNumber: Number of interface; composite devices may use
 *	fixed interface numbers to differentiate between vendor-specific
 *	interfaces.
 * @driver_info: Holds information used by the driver.  Usually it holds
 *	a pointer to a descriptor understood by the driver, or perhaps
 *	device flags.
 *
 * In most cases, drivers will create a table of device IDs by using
 * USB_DEVICE(), or similar macros designed for that purpose.
 * They will then export it to userspace using MODULE_DEVICE_TABLE(),
 * and provide it to the USB core through their usb_driver structure.
 *
 * See the usb_match_id() function for information about how matches are
 * performed.  Briefly, you will normally use one of several macros to help
 * construct these entries.  Each entry you provide will either identify
 * one or more specific products, or will identify a class of products
 * which have agreed to behave the same.  You should put the more specific
 * matches towards the beginning of your table, so that driver_info can
 * record quirks of specific products.
 */
struct Model1_usb_device_id {
 /* which fields to match against? */
 Model1___u16 Model1_match_flags;

 /* Used for product specific matches; range is inclusive */
 Model1___u16 Model1_idVendor;
 Model1___u16 Model1_idProduct;
 Model1___u16 Model1_bcdDevice_lo;
 Model1___u16 Model1_bcdDevice_hi;

 /* Used for device class matches */
 __u8 Model1_bDeviceClass;
 __u8 Model1_bDeviceSubClass;
 __u8 Model1_bDeviceProtocol;

 /* Used for interface class matches */
 __u8 Model1_bInterfaceClass;
 __u8 Model1_bInterfaceSubClass;
 __u8 Model1_bInterfaceProtocol;

 /* Used for vendor-specific interface matches */
 __u8 Model1_bInterfaceNumber;

 /* not matched against */
 Model1_kernel_ulong_t Model1_driver_info
  __attribute__((aligned(sizeof(Model1_kernel_ulong_t))));
};

/* Some useful macros to use to create struct usb_device_id */
struct Model1_hid_device_id {
 Model1___u16 Model1_bus;
 Model1___u16 Model1_group;
 __u32 Model1_vendor;
 __u32 Model1_product;
 Model1_kernel_ulong_t Model1_driver_data;
};

/* s390 CCW devices */
struct Model1_ccw_device_id {
 Model1___u16 Model1_match_flags; /* which fields to match against */

 Model1___u16 Model1_cu_type; /* control unit type     */
 Model1___u16 Model1_dev_type; /* device type           */
 __u8 Model1_cu_model; /* control unit model    */
 __u8 Model1_dev_model; /* device model          */

 Model1_kernel_ulong_t Model1_driver_info;
};






/* s390 AP bus devices */
struct Model1_ap_device_id {
 Model1___u16 Model1_match_flags; /* which fields to match against */
 __u8 Model1_dev_type; /* device type */
 Model1_kernel_ulong_t Model1_driver_info;
};



/* s390 css bus devices (subchannels) */
struct Model1_css_device_id {
 __u8 Model1_match_flags;
 __u8 Model1_type; /* subchannel type */
 Model1_kernel_ulong_t Model1_driver_data;
};



struct Model1_acpi_device_id {
 __u8 Model1_id[9];
 Model1_kernel_ulong_t Model1_driver_data;
 __u32 Model1_cls;
 __u32 Model1_cls_msk;
};




struct Model1_pnp_device_id {
 __u8 Model1_id[8];
 Model1_kernel_ulong_t Model1_driver_data;
};

struct Model1_pnp_card_device_id {
 __u8 Model1_id[8];
 Model1_kernel_ulong_t Model1_driver_data;
 struct {
  __u8 Model1_id[8];
 } Model1_devs[8];
};




struct Model1_serio_device_id {
 __u8 Model1_type;
 __u8 Model1_extra;
 __u8 Model1_id;
 __u8 Model1_proto;
};

struct Model1_hda_device_id {
 __u32 Model1_vendor_id;
 __u32 Model1_rev_id;
 __u8 Model1_api_version;
 const char *Model1_name;
 unsigned long Model1_driver_data;
};

/*
 * Struct used for matching a device
 */
struct Model1_of_device_id {
 char Model1_name[32];
 char Model1_type[32];
 char Model1_compatible[128];
 const void *Model1_data;
};

/* VIO */
struct Model1_vio_device_id {
 char Model1_type[32];
 char Model1_compat[32];
};

/* PCMCIA */

struct Model1_pcmcia_device_id {
 Model1___u16 Model1_match_flags;

 Model1___u16 Model1_manf_id;
 Model1___u16 Model1_card_id;

 __u8 Model1_func_id;

 /* for real multi-function devices */
 __u8 Model1_function;

 /* for pseudo multi-function devices */
 __u8 Model1_device_no;

 __u32 Model1_prod_id_hash[4];

 /* not matched against in kernelspace */
 const char * Model1_prod_id[4];

 /* not matched against */
 Model1_kernel_ulong_t Model1_driver_info;
 char * Model1_cisfile;
};
/* Input */
struct Model1_input_device_id {

 Model1_kernel_ulong_t Model1_flags;

 Model1___u16 Model1_bustype;
 Model1___u16 Model1_vendor;
 Model1___u16 Model1_product;
 Model1___u16 Model1_version;

 Model1_kernel_ulong_t Model1_evbit[0x1f / 64 + 1];
 Model1_kernel_ulong_t Model1_keybit[0x2ff / 64 + 1];
 Model1_kernel_ulong_t Model1_relbit[0x0f / 64 + 1];
 Model1_kernel_ulong_t Model1_absbit[0x3f / 64 + 1];
 Model1_kernel_ulong_t Model1_mscbit[0x07 / 64 + 1];
 Model1_kernel_ulong_t Model1_ledbit[0x0f / 64 + 1];
 Model1_kernel_ulong_t Model1_sndbit[0x07 / 64 + 1];
 Model1_kernel_ulong_t Model1_ffbit[0x7f / 64 + 1];
 Model1_kernel_ulong_t Model1_swbit[0x0f / 64 + 1];

 Model1_kernel_ulong_t Model1_driver_info;
};

/* EISA */



/* The EISA signature, in ASCII form, null terminated */
struct Model1_eisa_device_id {
 char Model1_sig[8];
 Model1_kernel_ulong_t Model1_driver_data;
};



struct Model1_parisc_device_id {
 __u8 Model1_hw_type; /* 5 bits used */
 __u8 Model1_hversion_rev; /* 4 bits */
 Model1___u16 Model1_hversion; /* 12 bits */
 __u32 Model1_sversion; /* 20 bits */
};






/* SDIO */



struct Model1_sdio_device_id {
 __u8 Model1_class; /* Standard interface or SDIO_ANY_ID */
 Model1___u16 Model1_vendor; /* Vendor or SDIO_ANY_ID */
 Model1___u16 Model1_device; /* Device ID or SDIO_ANY_ID */
 Model1_kernel_ulong_t Model1_driver_data; /* Data private to the driver */
};

/* SSB core, see drivers/ssb/ */
struct Model1_ssb_device_id {
 Model1___u16 Model1_vendor;
 Model1___u16 Model1_coreid;
 __u8 Model1_revision;
 __u8 Model1___pad;
} __attribute__((packed, aligned(2)));







/* Broadcom's specific AMBA core, see drivers/bcma/ */
struct Model1_bcma_device_id {
 Model1___u16 Model1_manuf;
 Model1___u16 Model1_id;
 __u8 Model1_rev;
 __u8 Model1_class;
} __attribute__((packed,aligned(2)));
struct Model1_virtio_device_id {
 __u32 Model1_device;
 __u32 Model1_vendor;
};


/*
 * For Hyper-V devices we use the device guid as the id.
 */
struct Model1_hv_vmbus_device_id {
 Model1_uuid_le Model1_guid;
 Model1_kernel_ulong_t Model1_driver_data; /* Data private to the driver */
};

/* rpmsg */




struct Model1_rpmsg_device_id {
 char Model1_name[32];
};

/* i2c */




struct Model1_i2c_device_id {
 char Model1_name[20];
 Model1_kernel_ulong_t Model1_driver_data; /* Data private to the driver */
};

/* spi */




struct Model1_spi_device_id {
 char Model1_name[32];
 Model1_kernel_ulong_t Model1_driver_data; /* Data private to the driver */
};




struct Model1_spmi_device_id {
 char Model1_name[32];
 Model1_kernel_ulong_t Model1_driver_data; /* Data private to the driver */
};

/* dmi */
enum Model1_dmi_field {
 Model1_DMI_NONE,
 Model1_DMI_BIOS_VENDOR,
 Model1_DMI_BIOS_VERSION,
 Model1_DMI_BIOS_DATE,
 Model1_DMI_SYS_VENDOR,
 Model1_DMI_PRODUCT_NAME,
 Model1_DMI_PRODUCT_VERSION,
 Model1_DMI_PRODUCT_SERIAL,
 Model1_DMI_PRODUCT_UUID,
 Model1_DMI_BOARD_VENDOR,
 Model1_DMI_BOARD_NAME,
 Model1_DMI_BOARD_VERSION,
 Model1_DMI_BOARD_SERIAL,
 Model1_DMI_BOARD_ASSET_TAG,
 Model1_DMI_CHASSIS_VENDOR,
 Model1_DMI_CHASSIS_TYPE,
 Model1_DMI_CHASSIS_VERSION,
 Model1_DMI_CHASSIS_SERIAL,
 Model1_DMI_CHASSIS_ASSET_TAG,
 Model1_DMI_STRING_MAX,
};

struct Model1_dmi_strmatch {
 unsigned char Model1_slot:7;
 unsigned char Model1_exact_match:1;
 char Model1_substr[79];
};

struct Model1_dmi_system_id {
 int (*Model1_callback)(const struct Model1_dmi_system_id *);
 const char *Model1_ident;
 struct Model1_dmi_strmatch Model1_matches[4];
 void *Model1_driver_data;
};
/*
 * struct dmi_device_id appears during expansion of
 * "MODULE_DEVICE_TABLE(dmi, x)". Compiler doesn't look inside it
 * but this is enough for gcc 3.4.6 to error out:
 *	error: storage size of '__mod_dmi_device_table' isn't known
 */
struct Model1_platform_device_id {
 char Model1_name[20];
 Model1_kernel_ulong_t Model1_driver_data;
};
/**
 * struct mdio_device_id - identifies PHY devices on an MDIO/MII bus
 * @phy_id: The result of
 *     (mdio_read(&MII_PHYSID1) << 16 | mdio_read(&PHYSID2)) & @phy_id_mask
 *     for this PHY type
 * @phy_id_mask: Defines the significant bits of @phy_id.  A value of 0
 *     is used to terminate an array of struct mdio_device_id.
 */
struct Model1_mdio_device_id {
 __u32 Model1_phy_id;
 __u32 Model1_phy_id_mask;
};

struct Model1_zorro_device_id {
 __u32 Model1_id; /* Device ID or ZORRO_WILDCARD */
 Model1_kernel_ulong_t Model1_driver_data; /* Data private to the driver */
};






struct Model1_isapnp_device_id {
 unsigned short Model1_card_vendor, Model1_card_device;
 unsigned short Model1_vendor, Model1_function;
 Model1_kernel_ulong_t Model1_driver_data; /* data private to the driver */
};

/**
 * struct amba_id - identifies a device on an AMBA bus
 * @id: The significant bits if the hardware device ID
 * @mask: Bitmask specifying which bits of the id field are significant when
 *	matching.  A driver binds to a device when ((hardware device ID) & mask)
 *	== id.
 * @data: Private data used by the driver.
 */
struct Model1_amba_id {
 unsigned int Model1_id;
 unsigned int Model1_mask;
 void *Model1_data;
};

/**
 * struct mips_cdmm_device_id - identifies devices in MIPS CDMM bus
 * @type:	Device type identifier.
 */
struct Model1_mips_cdmm_device_id {
 __u8 Model1_type;
};

/*
 * Match x86 CPUs for CPU specific drivers.
 * See documentation of "x86_match_cpu" for details.
 */

/*
 * MODULE_DEVICE_TABLE expects this struct to be called x86cpu_device_id.
 * Although gcc seems to ignore this error, clang fails without this define.
 */

struct Model1_x86_cpu_id {
 Model1___u16 Model1_vendor;
 Model1___u16 Model1_family;
 Model1___u16 Model1_model;
 Model1___u16 Model1_feature; /* bit index */
 Model1_kernel_ulong_t Model1_driver_data;
};
/*
 * Generic table type for matching CPU features.
 * @feature:	the bit number of the feature (0 - 65535)
 */

struct Model1_cpu_feature {
 Model1___u16 Model1_feature;
};



struct Model1_ipack_device_id {
 __u8 format; /* Format version or IPACK_ANY_ID */
 __u32 Model1_vendor; /* Vendor ID or IPACK_ANY_ID */
 __u32 Model1_device; /* Device ID or IPACK_ANY_ID */
};





/**
 * struct mei_cl_device_id - MEI client device identifier
 * @name: helper name
 * @uuid: client uuid
 * @version: client protocol version
 * @driver_info: information used by the driver.
 *
 * identifies mei client device by uuid and name
 */
struct Model1_mei_cl_device_id {
 char Model1_name[32];
 Model1_uuid_le Model1_uuid;
 __u8 Model1_version;
 Model1_kernel_ulong_t Model1_driver_info;
};

/* RapidIO */



/**
 * struct rio_device_id - RIO device identifier
 * @did: RapidIO device ID
 * @vid: RapidIO vendor ID
 * @asm_did: RapidIO assembly device ID
 * @asm_vid: RapidIO assembly vendor ID
 *
 * Identifies a RapidIO device based on both the device/vendor IDs and
 * the assembly device/vendor IDs.
 */
struct Model1_rio_device_id {
 Model1___u16 Model1_did, Model1_vid;
 Model1___u16 Model1_asm_did, Model1_asm_vid;
};

struct Model1_mcb_device_id {
 Model1___u16 Model1_device;
 Model1_kernel_ulong_t Model1_driver_data;
};

struct Model1_ulpi_device_id {
 Model1___u16 Model1_vendor;
 Model1___u16 Model1_product;
 Model1_kernel_ulong_t Model1_driver_data;
};

/**
 * struct fsl_mc_device_id - MC object device identifier
 * @vendor: vendor ID
 * @obj_type: MC object type
 * @ver_major: MC object version major number
 * @ver_minor: MC object version minor number
 *
 * Type of entries in the "device Id" table for MC object devices supported by
 * a MC object device driver. The last entry of the table has vendor set to 0x0
 */
struct Model1_fsl_mc_device_id {
 Model1___u16 Model1_vendor;
 const char Model1_obj_type[16];
};



/*
 * property.h - Unified device property interface.
 *
 * Copyright (C) 2014, Intel Corporation
 * Authors: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
 *          Mika Westerberg <mika.westerberg@linux.intel.com>
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2 as
 * published by the Free Software Foundation.
 */





/*
 * fwnode.h - Firmware device node object handle type definition.
 *
 * Copyright (C) 2015, Intel Corporation
 * Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2 as
 * published by the Free Software Foundation.
 */




enum Model1_fwnode_type {
 Model1_FWNODE_INVALID = 0,
 Model1_FWNODE_OF,
 Model1_FWNODE_ACPI,
 Model1_FWNODE_ACPI_DATA,
 Model1_FWNODE_PDATA,
 Model1_FWNODE_IRQCHIP,
};

struct Model1_fwnode_handle {
 enum Model1_fwnode_type Model1_type;
 struct Model1_fwnode_handle *Model1_secondary;
};


struct Model1_device;

enum Model1_dev_prop_type {
 Model1_DEV_PROP_U8,
 Model1_DEV_PROP_U16,
 Model1_DEV_PROP_U32,
 Model1_DEV_PROP_U64,
 Model1_DEV_PROP_STRING,
 Model1_DEV_PROP_MAX,
};

enum Model1_dev_dma_attr {
 Model1_DEV_DMA_NOT_SUPPORTED,
 Model1_DEV_DMA_NON_COHERENT,
 Model1_DEV_DMA_COHERENT,
};

bool Model1_device_property_present(struct Model1_device *Model1_dev, const char *Model1_propname);
int Model1_device_property_read_u8_array(struct Model1_device *Model1_dev, const char *Model1_propname,
      Model1_u8 *Model1_val, Model1_size_t Model1_nval);
int Model1_device_property_read_u16_array(struct Model1_device *Model1_dev, const char *Model1_propname,
       Model1_u16 *Model1_val, Model1_size_t Model1_nval);
int Model1_device_property_read_u32_array(struct Model1_device *Model1_dev, const char *Model1_propname,
       Model1_u32 *Model1_val, Model1_size_t Model1_nval);
int Model1_device_property_read_u64_array(struct Model1_device *Model1_dev, const char *Model1_propname,
       Model1_u64 *Model1_val, Model1_size_t Model1_nval);
int Model1_device_property_read_string_array(struct Model1_device *Model1_dev, const char *Model1_propname,
          const char **Model1_val, Model1_size_t Model1_nval);
int Model1_device_property_read_string(struct Model1_device *Model1_dev, const char *Model1_propname,
    const char **Model1_val);
int Model1_device_property_match_string(struct Model1_device *Model1_dev,
     const char *Model1_propname, const char *Model1_string);

bool Model1_fwnode_property_present(struct Model1_fwnode_handle *Model1_fwnode, const char *Model1_propname);
int Model1_fwnode_property_read_u8_array(struct Model1_fwnode_handle *Model1_fwnode,
      const char *Model1_propname, Model1_u8 *Model1_val,
      Model1_size_t Model1_nval);
int Model1_fwnode_property_read_u16_array(struct Model1_fwnode_handle *Model1_fwnode,
       const char *Model1_propname, Model1_u16 *Model1_val,
       Model1_size_t Model1_nval);
int Model1_fwnode_property_read_u32_array(struct Model1_fwnode_handle *Model1_fwnode,
       const char *Model1_propname, Model1_u32 *Model1_val,
       Model1_size_t Model1_nval);
int Model1_fwnode_property_read_u64_array(struct Model1_fwnode_handle *Model1_fwnode,
       const char *Model1_propname, Model1_u64 *Model1_val,
       Model1_size_t Model1_nval);
int Model1_fwnode_property_read_string_array(struct Model1_fwnode_handle *Model1_fwnode,
          const char *Model1_propname, const char **Model1_val,
          Model1_size_t Model1_nval);
int Model1_fwnode_property_read_string(struct Model1_fwnode_handle *Model1_fwnode,
    const char *Model1_propname, const char **Model1_val);
int Model1_fwnode_property_match_string(struct Model1_fwnode_handle *Model1_fwnode,
     const char *Model1_propname, const char *Model1_string);

struct Model1_fwnode_handle *Model1_device_get_next_child_node(struct Model1_device *Model1_dev,
       struct Model1_fwnode_handle *Model1_child);





struct Model1_fwnode_handle *Model1_device_get_named_child_node(struct Model1_device *Model1_dev,
        const char *Model1_childname);

void Model1_fwnode_handle_put(struct Model1_fwnode_handle *Model1_fwnode);

unsigned int Model1_device_get_child_node_count(struct Model1_device *Model1_dev);

static inline __attribute__((no_instrument_function)) bool Model1_device_property_read_bool(struct Model1_device *Model1_dev,
          const char *Model1_propname)
{
 return Model1_device_property_present(Model1_dev, Model1_propname);
}

static inline __attribute__((no_instrument_function)) int Model1_device_property_read_u8(struct Model1_device *Model1_dev,
       const char *Model1_propname, Model1_u8 *Model1_val)
{
 return Model1_device_property_read_u8_array(Model1_dev, Model1_propname, Model1_val, 1);
}

static inline __attribute__((no_instrument_function)) int Model1_device_property_read_u16(struct Model1_device *Model1_dev,
        const char *Model1_propname, Model1_u16 *Model1_val)
{
 return Model1_device_property_read_u16_array(Model1_dev, Model1_propname, Model1_val, 1);
}

static inline __attribute__((no_instrument_function)) int Model1_device_property_read_u32(struct Model1_device *Model1_dev,
        const char *Model1_propname, Model1_u32 *Model1_val)
{
 return Model1_device_property_read_u32_array(Model1_dev, Model1_propname, Model1_val, 1);
}

static inline __attribute__((no_instrument_function)) int Model1_device_property_read_u64(struct Model1_device *Model1_dev,
        const char *Model1_propname, Model1_u64 *Model1_val)
{
 return Model1_device_property_read_u64_array(Model1_dev, Model1_propname, Model1_val, 1);
}

static inline __attribute__((no_instrument_function)) bool Model1_fwnode_property_read_bool(struct Model1_fwnode_handle *Model1_fwnode,
          const char *Model1_propname)
{
 return Model1_fwnode_property_present(Model1_fwnode, Model1_propname);
}

static inline __attribute__((no_instrument_function)) int Model1_fwnode_property_read_u8(struct Model1_fwnode_handle *Model1_fwnode,
       const char *Model1_propname, Model1_u8 *Model1_val)
{
 return Model1_fwnode_property_read_u8_array(Model1_fwnode, Model1_propname, Model1_val, 1);
}

static inline __attribute__((no_instrument_function)) int Model1_fwnode_property_read_u16(struct Model1_fwnode_handle *Model1_fwnode,
        const char *Model1_propname, Model1_u16 *Model1_val)
{
 return Model1_fwnode_property_read_u16_array(Model1_fwnode, Model1_propname, Model1_val, 1);
}

static inline __attribute__((no_instrument_function)) int Model1_fwnode_property_read_u32(struct Model1_fwnode_handle *Model1_fwnode,
        const char *Model1_propname, Model1_u32 *Model1_val)
{
 return Model1_fwnode_property_read_u32_array(Model1_fwnode, Model1_propname, Model1_val, 1);
}

static inline __attribute__((no_instrument_function)) int Model1_fwnode_property_read_u64(struct Model1_fwnode_handle *Model1_fwnode,
        const char *Model1_propname, Model1_u64 *Model1_val)
{
 return Model1_fwnode_property_read_u64_array(Model1_fwnode, Model1_propname, Model1_val, 1);
}

/**
 * struct property_entry - "Built-in" device property representation.
 * @name: Name of the property.
 * @length: Length of data making up the value.
 * @is_array: True when the property is an array.
 * @is_string: True when property is a string.
 * @pointer: Pointer to the property (an array of items of the given type).
 * @value: Value of the property (when it is a single item of the given type).
 */
struct Model1_property_entry {
 const char *Model1_name;
 Model1_size_t Model1_length;
 bool Model1_is_array;
 bool Model1_is_string;
 union {
  union {
   void *Model1_raw_data;
   Model1_u8 *Model1_u8_data;
   Model1_u16 *Model1_u16_data;
   Model1_u32 *Model1_u32_data;
   Model1_u64 *Model1_u64_data;
   const char **Model1_str;
  } Model1_pointer;
  union {
   unsigned long long Model1_raw_data;
   Model1_u8 Model1_u8_data;
   Model1_u16 Model1_u16_data;
   Model1_u32 Model1_u32_data;
   Model1_u64 Model1_u64_data;
   const char *Model1_str;
  } Model1_value;
 };
};

/*
 * Note: the below four initializers for the anonymous union are carefully
 * crafted to avoid gcc-4.4.4's problems with initialization of anon unions
 * and structs.
 */
int Model1_device_add_properties(struct Model1_device *Model1_dev,
     struct Model1_property_entry *Model1_properties);
void Model1_device_remove_properties(struct Model1_device *Model1_dev);

bool Model1_device_dma_supported(struct Model1_device *Model1_dev);

enum Model1_dev_dma_attr Model1_device_get_dma_attr(struct Model1_device *Model1_dev);

int Model1_device_get_phy_mode(struct Model1_device *Model1_dev);

void *Model1_device_get_mac_address(struct Model1_device *Model1_dev, char *Model1_addr, int Model1_alen);




typedef Model1_u32 Model1_phandle;
typedef Model1_u32 Model1_ihandle;

struct Model1_property {
 char *Model1_name;
 int Model1_length;
 void *Model1_value;
 struct Model1_property *Model1_next;
 unsigned long Model1__flags;
 unsigned int Model1_unique_id;
 struct Model1_bin_attribute Model1_attr;
};





struct Model1_device_node {
 const char *Model1_name;
 const char *Model1_type;
 Model1_phandle Model1_phandle;
 const char *Model1_full_name;
 struct Model1_fwnode_handle Model1_fwnode;

 struct Model1_property *Model1_properties;
 struct Model1_property *Model1_deadprops; /* removed properties */
 struct Model1_device_node *Model1_parent;
 struct Model1_device_node *Model1_child;
 struct Model1_device_node *Model1_sibling;
 struct Model1_kobject Model1_kobj;
 unsigned long Model1__flags;
 void *Model1_data;





};


struct Model1_of_phandle_args {
 struct Model1_device_node *Model1_np;
 int Model1_args_count;
 Model1_uint32_t Model1_args[16];
};

struct Model1_of_phandle_iterator {
 /* Common iterator information */
 const char *Model1_cells_name;
 int Model1_cell_count;
 const struct Model1_device_node *Model1_parent;

 /* List size information */
 const Model1___be32 *Model1_list_end;
 const Model1___be32 *Model1_phandle_end;

 /* Current position state */
 const Model1___be32 *Model1_cur;
 Model1_uint32_t Model1_cur_count;
 Model1_phandle Model1_phandle;
 struct Model1_device_node *Model1_node;
};

struct Model1_of_reconfig_data {
 struct Model1_device_node *Model1_dn;
 struct Model1_property *Model1_prop;
 struct Model1_property *Model1_old_prop;
};

/* initialize a node */
extern struct Model1_kobj_type Model1_of_node_ktype;
static inline __attribute__((no_instrument_function)) void Model1_of_node_init(struct Model1_device_node *Model1_node)
{
 Model1_kobject_init(&Model1_node->Model1_kobj, &Model1_of_node_ktype);
 Model1_node->Model1_fwnode.Model1_type = Model1_FWNODE_OF;
}

/* true when node is initialized */
static inline __attribute__((no_instrument_function)) int Model1_of_node_is_initialized(struct Model1_device_node *Model1_node)
{
 return Model1_node && Model1_node->Model1_kobj.Model1_state_initialized;
}

/* true when node is attached (i.e. present on sysfs) */
static inline __attribute__((no_instrument_function)) int Model1_of_node_is_attached(struct Model1_device_node *Model1_node)
{
 return Model1_node && Model1_node->Model1_kobj.Model1_state_in_sysfs;
}





/* Dummy ref counting routines - to be implemented later */
static inline __attribute__((no_instrument_function)) struct Model1_device_node *Model1_of_node_get(struct Model1_device_node *Model1_node)
{
 return Model1_node;
}
static inline __attribute__((no_instrument_function)) void Model1_of_node_put(struct Model1_device_node *Model1_node) { }


/* Pointer for first entry in chain of all nodes. */
extern struct Model1_device_node *Model1_of_root;
extern struct Model1_device_node *Model1_of_chosen;
extern struct Model1_device_node *Model1_of_aliases;
extern struct Model1_device_node *Model1_of_stdout;
extern Model1_raw_spinlock_t Model1_devtree_lock;

/* flag descriptions (need to be visible even when !CONFIG_OF) */
static inline __attribute__((no_instrument_function)) void Model1_of_core_init(void)
{
}

static inline __attribute__((no_instrument_function)) bool Model1_is_of_node(struct Model1_fwnode_handle *Model1_fwnode)
{
 return false;
}

static inline __attribute__((no_instrument_function)) struct Model1_device_node *Model1_to_of_node(struct Model1_fwnode_handle *Model1_fwnode)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) const char* Model1_of_node_full_name(const struct Model1_device_node *Model1_np)
{
 return "<no-node>";
}

static inline __attribute__((no_instrument_function)) struct Model1_device_node *Model1_of_find_node_by_name(struct Model1_device_node *Model1_from,
 const char *Model1_name)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model1_device_node *Model1_of_find_node_by_type(struct Model1_device_node *Model1_from,
 const char *Model1_type)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model1_device_node *Model1_of_find_matching_node_and_match(
 struct Model1_device_node *Model1_from,
 const struct Model1_of_device_id *Model1_matches,
 const struct Model1_of_device_id **Model1_match)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model1_device_node *Model1_of_find_node_by_path(const char *Model1_path)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model1_device_node *Model1_of_find_node_opts_by_path(const char *Model1_path,
 const char **Model1_opts)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model1_device_node *Model1_of_find_node_by_phandle(Model1_phandle Model1_handle)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model1_device_node *Model1_of_get_parent(const struct Model1_device_node *Model1_node)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model1_device_node *Model1_of_get_next_child(
 const struct Model1_device_node *Model1_node, struct Model1_device_node *Model1_prev)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model1_device_node *Model1_of_get_next_available_child(
 const struct Model1_device_node *Model1_node, struct Model1_device_node *Model1_prev)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model1_device_node *Model1_of_find_node_with_property(
 struct Model1_device_node *Model1_from, const char *Model1_prop_name)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) bool Model1_of_have_populated_dt(void)
{
 return false;
}

static inline __attribute__((no_instrument_function)) struct Model1_device_node *Model1_of_get_child_by_name(
     const struct Model1_device_node *Model1_node,
     const char *Model1_name)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) int Model1_of_device_is_compatible(const struct Model1_device_node *Model1_device,
       const char *Model1_name)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) bool Model1_of_device_is_available(const struct Model1_device_node *Model1_device)
{
 return false;
}

static inline __attribute__((no_instrument_function)) bool Model1_of_device_is_big_endian(const struct Model1_device_node *Model1_device)
{
 return false;
}

static inline __attribute__((no_instrument_function)) struct Model1_property *Model1_of_find_property(const struct Model1_device_node *Model1_np,
      const char *Model1_name,
      int *Model1_lenp)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model1_device_node *Model1_of_find_compatible_node(
      struct Model1_device_node *Model1_from,
      const char *Model1_type,
      const char *Model1_compat)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) int Model1_of_property_count_elems_of_size(const struct Model1_device_node *Model1_np,
   const char *Model1_propname, int Model1_elem_size)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) int Model1_of_property_read_u32_index(const struct Model1_device_node *Model1_np,
   const char *Model1_propname, Model1_u32 Model1_index, Model1_u32 *Model1_out_value)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) int Model1_of_property_read_u8_array(const struct Model1_device_node *Model1_np,
   const char *Model1_propname, Model1_u8 *Model1_out_values, Model1_size_t Model1_sz)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) int Model1_of_property_read_u16_array(const struct Model1_device_node *Model1_np,
   const char *Model1_propname, Model1_u16 *Model1_out_values, Model1_size_t Model1_sz)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) int Model1_of_property_read_u32_array(const struct Model1_device_node *Model1_np,
          const char *Model1_propname,
          Model1_u32 *Model1_out_values, Model1_size_t Model1_sz)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) int Model1_of_property_read_u64_array(const struct Model1_device_node *Model1_np,
          const char *Model1_propname,
          Model1_u64 *Model1_out_values, Model1_size_t Model1_sz)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) int Model1_of_property_read_string(const struct Model1_device_node *Model1_np,
       const char *Model1_propname,
       const char **Model1_out_string)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) int Model1_of_property_read_string_helper(const struct Model1_device_node *Model1_np,
       const char *Model1_propname,
       const char **Model1_out_strs, Model1_size_t Model1_sz, int Model1_index)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) const void *Model1_of_get_property(const struct Model1_device_node *Model1_node,
    const char *Model1_name,
    int *Model1_lenp)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model1_device_node *Model1_of_get_cpu_node(int Model1_cpu,
     unsigned int *thread)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) int Model1_of_property_read_u64(const struct Model1_device_node *Model1_np,
           const char *Model1_propname, Model1_u64 *Model1_out_value)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) int Model1_of_property_match_string(const struct Model1_device_node *Model1_np,
        const char *Model1_propname,
        const char *Model1_string)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) struct Model1_device_node *Model1_of_parse_phandle(const struct Model1_device_node *Model1_np,
         const char *Model1_phandle_name,
         int Model1_index)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) int Model1_of_parse_phandle_with_args(const struct Model1_device_node *Model1_np,
          const char *Model1_list_name,
          const char *Model1_cells_name,
          int Model1_index,
          struct Model1_of_phandle_args *Model1_out_args)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) int Model1_of_parse_phandle_with_fixed_args(const struct Model1_device_node *Model1_np,
 const char *Model1_list_name, int Model1_cells_count, int Model1_index,
 struct Model1_of_phandle_args *Model1_out_args)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) int Model1_of_count_phandle_with_args(struct Model1_device_node *Model1_np,
          const char *Model1_list_name,
          const char *Model1_cells_name)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) int Model1_of_phandle_iterator_init(struct Model1_of_phandle_iterator *Model1_it,
        const struct Model1_device_node *Model1_np,
        const char *Model1_list_name,
        const char *Model1_cells_name,
        int Model1_cell_count)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) int Model1_of_phandle_iterator_next(struct Model1_of_phandle_iterator *Model1_it)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) int Model1_of_phandle_iterator_args(struct Model1_of_phandle_iterator *Model1_it,
        Model1_uint32_t *Model1_args,
        int Model1_size)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_of_alias_get_id(struct Model1_device_node *Model1_np, const char *Model1_stem)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) int Model1_of_alias_get_highest_id(const char *Model1_stem)
{
 return -38;
}

static inline __attribute__((no_instrument_function)) int Model1_of_machine_is_compatible(const char *Model1_compat)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) bool Model1_of_console_check(const struct Model1_device_node *Model1_dn, const char *Model1_name, int Model1_index)
{
 return false;
}

static inline __attribute__((no_instrument_function)) const Model1___be32 *Model1_of_prop_next_u32(struct Model1_property *Model1_prop,
  const Model1___be32 *Model1_cur, Model1_u32 *Model1_pu)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) const char *Model1_of_prop_next_string(struct Model1_property *Model1_prop,
  const char *Model1_cur)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) int Model1_of_node_check_flag(struct Model1_device_node *Model1_n, unsigned long Model1_flag)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_of_node_test_and_set_flag(struct Model1_device_node *Model1_n,
         unsigned long Model1_flag)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model1_of_node_set_flag(struct Model1_device_node *Model1_n, unsigned long Model1_flag)
{
}

static inline __attribute__((no_instrument_function)) void Model1_of_node_clear_flag(struct Model1_device_node *Model1_n, unsigned long Model1_flag)
{
}

static inline __attribute__((no_instrument_function)) int Model1_of_property_check_flag(struct Model1_property *Model1_p, unsigned long Model1_flag)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model1_of_property_set_flag(struct Model1_property *Model1_p, unsigned long Model1_flag)
{
}

static inline __attribute__((no_instrument_function)) void Model1_of_property_clear_flag(struct Model1_property *Model1_p, unsigned long Model1_flag)
{
}





/* Default string compare functions, Allow arch asm/prom.h to override */
static inline __attribute__((no_instrument_function)) int Model1_of_node_to_nid(struct Model1_device_node *Model1_device)
{
 return (-1);
}





static inline __attribute__((no_instrument_function)) int Model1_of_numa_init(void)
{
 return -38;
}


static inline __attribute__((no_instrument_function)) struct Model1_device_node *Model1_of_find_matching_node(
 struct Model1_device_node *Model1_from,
 const struct Model1_of_device_id *Model1_matches)
{
 return Model1_of_find_matching_node_and_match(Model1_from, Model1_matches, ((void *)0));
}

/**
 * of_property_count_u8_elems - Count the number of u8 elements in a property
 *
 * @np:		device node from which the property value is to be read.
 * @propname:	name of the property to be searched.
 *
 * Search for a property in a device node and count the number of u8 elements
 * in it. Returns number of elements on sucess, -EINVAL if the property does
 * not exist or its length does not match a multiple of u8 and -ENODATA if the
 * property does not have a value.
 */
static inline __attribute__((no_instrument_function)) int Model1_of_property_count_u8_elems(const struct Model1_device_node *Model1_np,
    const char *Model1_propname)
{
 return Model1_of_property_count_elems_of_size(Model1_np, Model1_propname, sizeof(Model1_u8));
}

/**
 * of_property_count_u16_elems - Count the number of u16 elements in a property
 *
 * @np:		device node from which the property value is to be read.
 * @propname:	name of the property to be searched.
 *
 * Search for a property in a device node and count the number of u16 elements
 * in it. Returns number of elements on sucess, -EINVAL if the property does
 * not exist or its length does not match a multiple of u16 and -ENODATA if the
 * property does not have a value.
 */
static inline __attribute__((no_instrument_function)) int Model1_of_property_count_u16_elems(const struct Model1_device_node *Model1_np,
    const char *Model1_propname)
{
 return Model1_of_property_count_elems_of_size(Model1_np, Model1_propname, sizeof(Model1_u16));
}

/**
 * of_property_count_u32_elems - Count the number of u32 elements in a property
 *
 * @np:		device node from which the property value is to be read.
 * @propname:	name of the property to be searched.
 *
 * Search for a property in a device node and count the number of u32 elements
 * in it. Returns number of elements on sucess, -EINVAL if the property does
 * not exist or its length does not match a multiple of u32 and -ENODATA if the
 * property does not have a value.
 */
static inline __attribute__((no_instrument_function)) int Model1_of_property_count_u32_elems(const struct Model1_device_node *Model1_np,
    const char *Model1_propname)
{
 return Model1_of_property_count_elems_of_size(Model1_np, Model1_propname, sizeof(Model1_u32));
}

/**
 * of_property_count_u64_elems - Count the number of u64 elements in a property
 *
 * @np:		device node from which the property value is to be read.
 * @propname:	name of the property to be searched.
 *
 * Search for a property in a device node and count the number of u64 elements
 * in it. Returns number of elements on sucess, -EINVAL if the property does
 * not exist or its length does not match a multiple of u64 and -ENODATA if the
 * property does not have a value.
 */
static inline __attribute__((no_instrument_function)) int Model1_of_property_count_u64_elems(const struct Model1_device_node *Model1_np,
    const char *Model1_propname)
{
 return Model1_of_property_count_elems_of_size(Model1_np, Model1_propname, sizeof(Model1_u64));
}

/**
 * of_property_read_string_array() - Read an array of strings from a multiple
 * strings property.
 * @np:		device node from which the property value is to be read.
 * @propname:	name of the property to be searched.
 * @out_strs:	output array of string pointers.
 * @sz:		number of array elements to read.
 *
 * Search for a property in a device tree node and retrieve a list of
 * terminated string values (pointer to data, not a copy) in that property.
 *
 * If @out_strs is NULL, the number of strings in the property is returned.
 */
static inline __attribute__((no_instrument_function)) int Model1_of_property_read_string_array(const struct Model1_device_node *Model1_np,
      const char *Model1_propname, const char **Model1_out_strs,
      Model1_size_t Model1_sz)
{
 return Model1_of_property_read_string_helper(Model1_np, Model1_propname, Model1_out_strs, Model1_sz, 0);
}

/**
 * of_property_count_strings() - Find and return the number of strings from a
 * multiple strings property.
 * @np:		device node from which the property value is to be read.
 * @propname:	name of the property to be searched.
 *
 * Search for a property in a device tree node and retrieve the number of null
 * terminated string contain in it. Returns the number of strings on
 * success, -EINVAL if the property does not exist, -ENODATA if property
 * does not have a value, and -EILSEQ if the string is not null-terminated
 * within the length of the property data.
 */
static inline __attribute__((no_instrument_function)) int Model1_of_property_count_strings(const struct Model1_device_node *Model1_np,
         const char *Model1_propname)
{
 return Model1_of_property_read_string_helper(Model1_np, Model1_propname, ((void *)0), 0, 0);
}

/**
 * of_property_read_string_index() - Find and read a string from a multiple
 * strings property.
 * @np:		device node from which the property value is to be read.
 * @propname:	name of the property to be searched.
 * @index:	index of the string in the list of strings
 * @out_string:	pointer to null terminated return string, modified only if
 *		return value is 0.
 *
 * Search for a property in a device tree node and retrieve a null
 * terminated string value (pointer to data, not a copy) in the list of strings
 * contained in that property.
 * Returns 0 on success, -EINVAL if the property does not exist, -ENODATA if
 * property does not have a value, and -EILSEQ if the string is not
 * null-terminated within the length of the property data.
 *
 * The out_string pointer is modified only if a valid string can be decoded.
 */
static inline __attribute__((no_instrument_function)) int Model1_of_property_read_string_index(const struct Model1_device_node *Model1_np,
      const char *Model1_propname,
      int Model1_index, const char **Model1_output)
{
 int Model1_rc = Model1_of_property_read_string_helper(Model1_np, Model1_propname, Model1_output, 1, Model1_index);
 return Model1_rc < 0 ? Model1_rc : 0;
}

/**
 * of_property_read_bool - Findfrom a property
 * @np:		device node from which the property value is to be read.
 * @propname:	name of the property to be searched.
 *
 * Search for a property in a device node.
 * Returns true if the property exists false otherwise.
 */
static inline __attribute__((no_instrument_function)) bool Model1_of_property_read_bool(const struct Model1_device_node *Model1_np,
      const char *Model1_propname)
{
 struct Model1_property *Model1_prop = Model1_of_find_property(Model1_np, Model1_propname, ((void *)0));

 return Model1_prop ? true : false;
}

static inline __attribute__((no_instrument_function)) int Model1_of_property_read_u8(const struct Model1_device_node *Model1_np,
           const char *Model1_propname,
           Model1_u8 *Model1_out_value)
{
 return Model1_of_property_read_u8_array(Model1_np, Model1_propname, Model1_out_value, 1);
}

static inline __attribute__((no_instrument_function)) int Model1_of_property_read_u16(const struct Model1_device_node *Model1_np,
           const char *Model1_propname,
           Model1_u16 *Model1_out_value)
{
 return Model1_of_property_read_u16_array(Model1_np, Model1_propname, Model1_out_value, 1);
}

static inline __attribute__((no_instrument_function)) int Model1_of_property_read_u32(const struct Model1_device_node *Model1_np,
           const char *Model1_propname,
           Model1_u32 *Model1_out_value)
{
 return Model1_of_property_read_u32_array(Model1_np, Model1_propname, Model1_out_value, 1);
}

static inline __attribute__((no_instrument_function)) int Model1_of_property_read_s32(const struct Model1_device_node *Model1_np,
           const char *Model1_propname,
           Model1_s32 *Model1_out_value)
{
 return Model1_of_property_read_u32(Model1_np, Model1_propname, (Model1_u32*) Model1_out_value);
}
static inline __attribute__((no_instrument_function)) int Model1_of_get_child_count(const struct Model1_device_node *Model1_np)
{
 struct Model1_device_node *Model1_child;
 int Model1_num = 0;

 for (Model1_child = Model1_of_get_next_child(Model1_np, ((void *)0)); Model1_child != ((void *)0); Model1_child = Model1_of_get_next_child(Model1_np, Model1_child))
  Model1_num++;

 return Model1_num;
}

static inline __attribute__((no_instrument_function)) int Model1_of_get_available_child_count(const struct Model1_device_node *Model1_np)
{
 struct Model1_device_node *Model1_child;
 int Model1_num = 0;

 for (Model1_child = Model1_of_get_next_available_child(Model1_np, ((void *)0)); Model1_child != ((void *)0); Model1_child = Model1_of_get_next_available_child(Model1_np, Model1_child))
  Model1_num++;

 return Model1_num;
}
typedef int (*Model1_of_init_fn_2)(struct Model1_device_node *, struct Model1_device_node *);
typedef int (*Model1_of_init_fn_1_ret)(struct Model1_device_node *);
typedef void (*Model1_of_init_fn_1)(struct Model1_device_node *);
/**
 * struct of_changeset_entry	- Holds a changeset entry
 *
 * @node:	list_head for the log list
 * @action:	notifier action
 * @np:		pointer to the device node affected
 * @prop:	pointer to the property affected
 * @old_prop:	hold a pointer to the original property
 *
 * Every modification of the device tree during a changeset
 * is held in a list of of_changeset_entry structures.
 * That way we can recover from a partial application, or we can
 * revert the changeset
 */
struct Model1_of_changeset_entry {
 struct Model1_list_head Model1_node;
 unsigned long Model1_action;
 struct Model1_device_node *Model1_np;
 struct Model1_property *Model1_prop;
 struct Model1_property *Model1_old_prop;
};

/**
 * struct of_changeset - changeset tracker structure
 *
 * @entries:	list_head for the changeset entries
 *
 * changesets are a convenient way to apply bulk changes to the
 * live tree. In case of an error, changes are rolled-back.
 * changesets live on after initial application, and if not
 * destroyed after use, they can be reverted in one single call.
 */
struct Model1_of_changeset {
 struct Model1_list_head Model1_entries;
};

enum Model1_of_reconfig_change {
 Model1_OF_RECONFIG_NO_CHANGE = 0,
 Model1_OF_RECONFIG_CHANGE_ADD,
 Model1_OF_RECONFIG_CHANGE_REMOVE,
};
static inline __attribute__((no_instrument_function)) int Model1_of_reconfig_notifier_register(struct Model1_notifier_block *Model1_nb)
{
 return -22;
}
static inline __attribute__((no_instrument_function)) int Model1_of_reconfig_notifier_unregister(struct Model1_notifier_block *Model1_nb)
{
 return -22;
}
static inline __attribute__((no_instrument_function)) int Model1_of_reconfig_notify(unsigned long Model1_action,
         struct Model1_of_reconfig_data *Model1_arg)
{
 return -22;
}
static inline __attribute__((no_instrument_function)) int Model1_of_reconfig_get_state_change(unsigned long Model1_action,
      struct Model1_of_reconfig_data *Model1_arg)
{
 return -22;
}


/* CONFIG_OF_RESOLVE api */
extern int Model1_of_resolve_phandles(struct Model1_device_node *Model1_tree);

/**
 * of_device_is_system_power_controller - Tells if system-power-controller is found for device_node
 * @np: Pointer to the given device_node
 *
 * return true if present false otherwise
 */
static inline __attribute__((no_instrument_function)) bool Model1_of_device_is_system_power_controller(const struct Model1_device_node *Model1_np)
{
 return Model1_of_property_read_bool(Model1_np, "system-power-controller");
}

/**
 * Overlay support
 */
static inline __attribute__((no_instrument_function)) int Model1_of_overlay_create(struct Model1_device_node *Model1_tree)
{
 return -524;
}

static inline __attribute__((no_instrument_function)) int Model1_of_overlay_destroy(int Model1_id)
{
 return -524;
}

static inline __attribute__((no_instrument_function)) int Model1_of_overlay_destroy_all(void)
{
 return -524;
}
/*
 * Framework and drivers for configuring and reading different PHYs
 * Based on code in sungem_phy.c and gianfar_phy.c
 *
 * Author: Andy Fleming
 *
 * Copyright (c) 2004 Freescale Semiconductor, Inc.
 *
 * This program is free software; you can redistribute  it and/or modify it
 * under  the terms of  the GNU General  Public License as published by the
 * Free Software Foundation;  either version 2 of the  License, or (at your
 * option) any later version.
 *
 */








/*
 * linux/mdio.h: definitions for MDIO (clause 45) transceivers
 * Copyright 2006-2009 Solarflare Communications Inc.
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License version 2 as published
 * by the Free Software Foundation, incorporated herein by reference.
 */




/*
 * linux/mdio.h: definitions for MDIO (clause 45) transceivers
 * Copyright 2006-2009 Solarflare Communications Inc.
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License version 2 as published
 * by the Free Software Foundation, incorporated herein by reference.
 */






/*
 * linux/mii.h: definitions for MII-compatible transceivers
 * Originally drivers/net/sunhme.h.
 *
 * Copyright (C) 1996, 1999, 2001 David S. Miller (davem@redhat.com)
 */






/*
 * linux/mii.h: definitions for MII-compatible transceivers
 * Originally drivers/net/sunhme.h.
 *
 * Copyright (C) 1996, 1999, 2001 David S. Miller (davem@redhat.com)
 */







/* Generic MII registers. */
/* Basic mode control register. */
/* Basic mode status register. */
/* Advertisement control register. */
/* Link partner ability register. */
/* Expansion register for auto-negotiation. */
/* N-way test register. */




/* 1000BASE-T Control register */





/* 1000BASE-T Status register */





/* Flow control flags */



/* MMD Access Control register fields */






/* This structure is used in all SIOCxMIIxxx ioctl calls */
struct Model1_mii_ioctl_data {
 Model1___u16 Model1_phy_id;
 Model1___u16 Model1_reg_num;
 Model1___u16 Model1_val_in;
 Model1___u16 Model1_val_out;
};

struct Model1_ethtool_cmd;

struct Model1_mii_if_info {
 int Model1_phy_id;
 int Model1_advertising;
 int Model1_phy_id_mask;
 int Model1_reg_num_mask;

 unsigned int Model1_full_duplex : 1; /* is full duplex? */
 unsigned int Model1_force_media : 1; /* is autoneg. disabled? */
 unsigned int Model1_supports_gmii : 1; /* are GMII registers supported? */

 struct Model1_net_device *Model1_dev;
 int (*Model1_mdio_read) (struct Model1_net_device *Model1_dev, int Model1_phy_id, int Model1_location);
 void (*Model1_mdio_write) (struct Model1_net_device *Model1_dev, int Model1_phy_id, int Model1_location, int Model1_val);
};

extern int Model1_mii_link_ok (struct Model1_mii_if_info *Model1_mii);
extern int Model1_mii_nway_restart (struct Model1_mii_if_info *Model1_mii);
extern int Model1_mii_ethtool_gset(struct Model1_mii_if_info *Model1_mii, struct Model1_ethtool_cmd *Model1_ecmd);
extern int Model1_mii_ethtool_sset(struct Model1_mii_if_info *Model1_mii, struct Model1_ethtool_cmd *Model1_ecmd);
extern int Model1_mii_check_gmii_support(struct Model1_mii_if_info *Model1_mii);
extern void Model1_mii_check_link (struct Model1_mii_if_info *Model1_mii);
extern unsigned int Model1_mii_check_media (struct Model1_mii_if_info *Model1_mii,
         unsigned int Model1_ok_to_print,
         unsigned int Model1_init_media);
extern int Model1_generic_mii_ioctl(struct Model1_mii_if_info *Model1_mii_if,
        struct Model1_mii_ioctl_data *Model1_mii_data, int Model1_cmd,
        unsigned int *Model1_duplex_changed);


static inline __attribute__((no_instrument_function)) struct Model1_mii_ioctl_data *Model1_if_mii(struct Model1_ifreq *Model1_rq)
{
 return (struct Model1_mii_ioctl_data *) &Model1_rq->Model1_ifr_ifru;
}

/**
 * mii_nway_result
 * @negotiated: value of MII ANAR and'd with ANLPAR
 *
 * Given a set of MII abilities, check each bit and returns the
 * currently supported media, in the priority order defined by
 * IEEE 802.3u.  We use LPA_xxx constants but note this is not the
 * value of LPA solely, as described above.
 *
 * The one exception to IEEE 802.3u is that 100baseT4 is placed
 * between 100T-full and 100T-half.  If your phy does not support
 * 100T4 this is fine.  If your phy places 100T4 elsewhere in the
 * priority order, you will need to roll your own function.
 */
static inline __attribute__((no_instrument_function)) unsigned int Model1_mii_nway_result (unsigned int Model1_negotiated)
{
 unsigned int Model1_ret;

 if (Model1_negotiated & 0x0100)
  Model1_ret = 0x0100;
 else if (Model1_negotiated & 0x0200)
  Model1_ret = 0x0200;
 else if (Model1_negotiated & 0x0080)
  Model1_ret = 0x0080;
 else if (Model1_negotiated & 0x0040)
  Model1_ret = 0x0040;
 else
  Model1_ret = 0x0020;

 return Model1_ret;
}

/**
 * mii_duplex
 * @duplex_lock: Non-zero if duplex is locked at full
 * @negotiated: value of MII ANAR and'd with ANLPAR
 *
 * A small helper function for a common case.  Returns one
 * if the media is operating or locked at full duplex, and
 * returns zero otherwise.
 */
static inline __attribute__((no_instrument_function)) unsigned int Model1_mii_duplex (unsigned int Model1_duplex_lock,
           unsigned int Model1_negotiated)
{
 if (Model1_duplex_lock)
  return 1;
 if (Model1_mii_nway_result(Model1_negotiated) & (0x0040 | 0x0100))
  return 1;
 return 0;
}

/**
 * ethtool_adv_to_mii_adv_t
 * @ethadv: the ethtool advertisement settings
 *
 * A small helper function that translates ethtool advertisement
 * settings to phy autonegotiation advertisements for the
 * MII_ADVERTISE register.
 */
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_ethtool_adv_to_mii_adv_t(Model1_u32 Model1_ethadv)
{
 Model1_u32 Model1_result = 0;

 if (Model1_ethadv & (1UL << (Model1_ETHTOOL_LINK_MODE_10baseT_Half_BIT)))
  Model1_result |= 0x0020;
 if (Model1_ethadv & (1UL << (Model1_ETHTOOL_LINK_MODE_10baseT_Full_BIT)))
  Model1_result |= 0x0040;
 if (Model1_ethadv & (1UL << (Model1_ETHTOOL_LINK_MODE_100baseT_Half_BIT)))
  Model1_result |= 0x0080;
 if (Model1_ethadv & (1UL << (Model1_ETHTOOL_LINK_MODE_100baseT_Full_BIT)))
  Model1_result |= 0x0100;
 if (Model1_ethadv & (1UL << (Model1_ETHTOOL_LINK_MODE_Pause_BIT)))
  Model1_result |= 0x0400;
 if (Model1_ethadv & (1UL << (Model1_ETHTOOL_LINK_MODE_Asym_Pause_BIT)))
  Model1_result |= 0x0800;

 return Model1_result;
}

/**
 * mii_adv_to_ethtool_adv_t
 * @adv: value of the MII_ADVERTISE register
 *
 * A small helper function that translates MII_ADVERTISE bits
 * to ethtool advertisement settings.
 */
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_mii_adv_to_ethtool_adv_t(Model1_u32 Model1_adv)
{
 Model1_u32 Model1_result = 0;

 if (Model1_adv & 0x0020)
  Model1_result |= (1UL << (Model1_ETHTOOL_LINK_MODE_10baseT_Half_BIT));
 if (Model1_adv & 0x0040)
  Model1_result |= (1UL << (Model1_ETHTOOL_LINK_MODE_10baseT_Full_BIT));
 if (Model1_adv & 0x0080)
  Model1_result |= (1UL << (Model1_ETHTOOL_LINK_MODE_100baseT_Half_BIT));
 if (Model1_adv & 0x0100)
  Model1_result |= (1UL << (Model1_ETHTOOL_LINK_MODE_100baseT_Full_BIT));
 if (Model1_adv & 0x0400)
  Model1_result |= (1UL << (Model1_ETHTOOL_LINK_MODE_Pause_BIT));
 if (Model1_adv & 0x0800)
  Model1_result |= (1UL << (Model1_ETHTOOL_LINK_MODE_Asym_Pause_BIT));

 return Model1_result;
}

/**
 * ethtool_adv_to_mii_ctrl1000_t
 * @ethadv: the ethtool advertisement settings
 *
 * A small helper function that translates ethtool advertisement
 * settings to phy autonegotiation advertisements for the
 * MII_CTRL1000 register when in 1000T mode.
 */
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_ethtool_adv_to_mii_ctrl1000_t(Model1_u32 Model1_ethadv)
{
 Model1_u32 Model1_result = 0;

 if (Model1_ethadv & (1UL << (Model1_ETHTOOL_LINK_MODE_1000baseT_Half_BIT)))
  Model1_result |= 0x0100;
 if (Model1_ethadv & (1UL << (Model1_ETHTOOL_LINK_MODE_1000baseT_Full_BIT)))
  Model1_result |= 0x0200;

 return Model1_result;
}

/**
 * mii_ctrl1000_to_ethtool_adv_t
 * @adv: value of the MII_CTRL1000 register
 *
 * A small helper function that translates MII_CTRL1000
 * bits, when in 1000Base-T mode, to ethtool
 * advertisement settings.
 */
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_mii_ctrl1000_to_ethtool_adv_t(Model1_u32 Model1_adv)
{
 Model1_u32 Model1_result = 0;

 if (Model1_adv & 0x0100)
  Model1_result |= (1UL << (Model1_ETHTOOL_LINK_MODE_1000baseT_Half_BIT));
 if (Model1_adv & 0x0200)
  Model1_result |= (1UL << (Model1_ETHTOOL_LINK_MODE_1000baseT_Full_BIT));

 return Model1_result;
}

/**
 * mii_lpa_to_ethtool_lpa_t
 * @adv: value of the MII_LPA register
 *
 * A small helper function that translates MII_LPA
 * bits, when in 1000Base-T mode, to ethtool
 * LP advertisement settings.
 */
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_mii_lpa_to_ethtool_lpa_t(Model1_u32 Model1_lpa)
{
 Model1_u32 Model1_result = 0;

 if (Model1_lpa & 0x4000)
  Model1_result |= (1UL << (Model1_ETHTOOL_LINK_MODE_Autoneg_BIT));

 return Model1_result | Model1_mii_adv_to_ethtool_adv_t(Model1_lpa);
}

/**
 * mii_stat1000_to_ethtool_lpa_t
 * @adv: value of the MII_STAT1000 register
 *
 * A small helper function that translates MII_STAT1000
 * bits, when in 1000Base-T mode, to ethtool
 * advertisement settings.
 */
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_mii_stat1000_to_ethtool_lpa_t(Model1_u32 Model1_lpa)
{
 Model1_u32 Model1_result = 0;

 if (Model1_lpa & 0x0400)
  Model1_result |= (1UL << (Model1_ETHTOOL_LINK_MODE_1000baseT_Half_BIT));
 if (Model1_lpa & 0x0800)
  Model1_result |= (1UL << (Model1_ETHTOOL_LINK_MODE_1000baseT_Full_BIT));

 return Model1_result;
}

/**
 * ethtool_adv_to_mii_adv_x
 * @ethadv: the ethtool advertisement settings
 *
 * A small helper function that translates ethtool advertisement
 * settings to phy autonegotiation advertisements for the
 * MII_CTRL1000 register when in 1000Base-X mode.
 */
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_ethtool_adv_to_mii_adv_x(Model1_u32 Model1_ethadv)
{
 Model1_u32 Model1_result = 0;

 if (Model1_ethadv & (1UL << (Model1_ETHTOOL_LINK_MODE_1000baseT_Half_BIT)))
  Model1_result |= 0x0040;
 if (Model1_ethadv & (1UL << (Model1_ETHTOOL_LINK_MODE_1000baseT_Full_BIT)))
  Model1_result |= 0x0020;
 if (Model1_ethadv & (1UL << (Model1_ETHTOOL_LINK_MODE_Pause_BIT)))
  Model1_result |= 0x0080;
 if (Model1_ethadv & (1UL << (Model1_ETHTOOL_LINK_MODE_Asym_Pause_BIT)))
  Model1_result |= 0x0100;

 return Model1_result;
}

/**
 * mii_adv_to_ethtool_adv_x
 * @adv: value of the MII_CTRL1000 register
 *
 * A small helper function that translates MII_CTRL1000
 * bits, when in 1000Base-X mode, to ethtool
 * advertisement settings.
 */
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_mii_adv_to_ethtool_adv_x(Model1_u32 Model1_adv)
{
 Model1_u32 Model1_result = 0;

 if (Model1_adv & 0x0040)
  Model1_result |= (1UL << (Model1_ETHTOOL_LINK_MODE_1000baseT_Half_BIT));
 if (Model1_adv & 0x0020)
  Model1_result |= (1UL << (Model1_ETHTOOL_LINK_MODE_1000baseT_Full_BIT));
 if (Model1_adv & 0x0080)
  Model1_result |= (1UL << (Model1_ETHTOOL_LINK_MODE_Pause_BIT));
 if (Model1_adv & 0x0100)
  Model1_result |= (1UL << (Model1_ETHTOOL_LINK_MODE_Asym_Pause_BIT));

 return Model1_result;
}

/**
 * mii_lpa_to_ethtool_lpa_x
 * @adv: value of the MII_LPA register
 *
 * A small helper function that translates MII_LPA
 * bits, when in 1000Base-X mode, to ethtool
 * LP advertisement settings.
 */
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_mii_lpa_to_ethtool_lpa_x(Model1_u32 Model1_lpa)
{
 Model1_u32 Model1_result = 0;

 if (Model1_lpa & 0x4000)
  Model1_result |= (1UL << (Model1_ETHTOOL_LINK_MODE_Autoneg_BIT));

 return Model1_result | Model1_mii_adv_to_ethtool_adv_x(Model1_lpa);
}

/**
 * mii_advertise_flowctrl - get flow control advertisement flags
 * @cap: Flow control capabilities (FLOW_CTRL_RX, FLOW_CTRL_TX or both)
 */
static inline __attribute__((no_instrument_function)) Model1_u16 Model1_mii_advertise_flowctrl(int Model1_cap)
{
 Model1_u16 Model1_adv = 0;

 if (Model1_cap & 0x02)
  Model1_adv = 0x0400 | 0x0800;
 if (Model1_cap & 0x01)
  Model1_adv ^= 0x0800;

 return Model1_adv;
}

/**
 * mii_resolve_flowctrl_fdx
 * @lcladv: value of MII ADVERTISE register
 * @rmtadv: value of MII LPA register
 *
 * Resolve full duplex flow control as per IEEE 802.3-2005 table 28B-3
 */
static inline __attribute__((no_instrument_function)) Model1_u8 Model1_mii_resolve_flowctrl_fdx(Model1_u16 Model1_lcladv, Model1_u16 Model1_rmtadv)
{
 Model1_u8 Model1_cap = 0;

 if (Model1_lcladv & Model1_rmtadv & 0x0400) {
  Model1_cap = 0x01 | 0x02;
 } else if (Model1_lcladv & Model1_rmtadv & 0x0800) {
  if (Model1_lcladv & 0x0400)
   Model1_cap = 0x02;
  else if (Model1_rmtadv & 0x0400)
   Model1_cap = 0x01;
 }

 return Model1_cap;
}

/* MDIO Manageable Devices (MMDs). */
/* Generic MDIO registers. */
/* Media-dependent registers. */
/* LASI (Link Alarm Status Interrupt) registers, defined by XENPAK MSA. */







/* Control register 1. */
/* Enable extended speed selection */

/* All speed selection bits */
/* 10 Gb/s */

/* 10PASS-TS/2BASE-TL */


/* Status register 1. */
/* Speed register. */
/* Device present registers. */
/* Control register 2. */
/* Status register 2. */
/* Transmit disable register. */






/* Receive signal detect register. */






/* Extended abilities register. */
/* PHY XGXS lane state register. */






/* PMA 10GBASE-T pair swap & polarity */







/* PMA 10GBASE-T TX power register. */


/* PMA 10GBASE-T SNR registers. */
/* Value is SNR margin in dB, clamped to range [-127, 127], plus 0x8000. */



/* PMA 10GBASE-R FEC ability register. */



/* PCS 10GBASE-R/-T status register 1. */


/* PCS 10GBASE-R/-T status register 2. */



/* AN 10GBASE-T control register. */


/* AN 10GBASE-T status register. */
/* EEE Supported/Advertisement/LP Advertisement registers.
 *
 * EEE capability Register (3.20), Advertisement (7.60) and
 * Link partner ability (7.61) registers have and can use the same identical
 * bit masks.
 */


/* Note: the two defines above can be potentially used by the user-land
 * and cannot remove them now.
 * So, we define the new generic MDIO_EEE_100TX and MDIO_EEE_1000T macros
 * using the previous ones (that can be considered obsolete).
 */







/* LASI RX_ALARM control/status registers. */






/* LASI TX_ALARM control/status registers. */







/* LASI control/status registers. */




/* Mapping between MDIO PRTAD/DEVAD and mii_ioctl_data::phy_id */







static inline __attribute__((no_instrument_function)) Model1___u16 Model1_mdio_phy_id_c45(int Model1_prtad, int Model1_devad)
{
 return 0x8000 | (Model1_prtad << 5) | Model1_devad;
}

struct Model1_mii_bus;

/* Multiple levels of nesting are possible. However typically this is
 * limited to nested DSA like layer, a MUX layer, and the normal
 * user. Instead of trying to handle the general case, just define
 * these cases.
 */
enum Model1_mdio_mutex_lock_class {
 Model1_MDIO_MUTEX_NORMAL,
 Model1_MDIO_MUTEX_MUX,
 Model1_MDIO_MUTEX_NESTED,
};

struct Model1_mdio_device {
 struct Model1_device Model1_dev;

 const struct Model1_dev_pm_ops *Model1_pm_ops;
 struct Model1_mii_bus *Model1_bus;

 int (*Model1_bus_match)(struct Model1_device *Model1_dev, struct Model1_device_driver *Model1_drv);
 void (*Model1_device_free)(struct Model1_mdio_device *Model1_mdiodev);
 void (*Model1_device_remove)(struct Model1_mdio_device *Model1_mdiodev);

 /* Bus address of the MDIO device (0-31) */
 int Model1_addr;
 int Model1_flags;
};


/* struct mdio_driver_common: Common to all MDIO drivers */
struct Model1_mdio_driver_common {
 struct Model1_device_driver Model1_driver;
 int Model1_flags;
};




/* struct mdio_driver: Generic MDIO driver */
struct Model1_mdio_driver {
 struct Model1_mdio_driver_common Model1_mdiodrv;

 /*
	 * Called during discovery.  Used to set
	 * up device-specific structures, if any
	 */
 int (*Model1_probe)(struct Model1_mdio_device *Model1_mdiodev);

 /* Clears up any memory if needed */
 void (*Model1_remove)(struct Model1_mdio_device *Model1_mdiodev);
};



void Model1_mdio_device_free(struct Model1_mdio_device *Model1_mdiodev);
struct Model1_mdio_device *Model1_mdio_device_create(struct Model1_mii_bus *Model1_bus, int Model1_addr);
int Model1_mdio_device_register(struct Model1_mdio_device *Model1_mdiodev);
void Model1_mdio_device_remove(struct Model1_mdio_device *Model1_mdiodev);
int Model1_mdio_driver_register(struct Model1_mdio_driver *Model1_drv);
void Model1_mdio_driver_unregister(struct Model1_mdio_driver *Model1_drv);

static inline __attribute__((no_instrument_function)) bool Model1_mdio_phy_id_is_c45(int Model1_phy_id)
{
 return (Model1_phy_id & 0x8000) && !(Model1_phy_id & ~(0x8000 | 0x03e0 | 0x001f));
}

static inline __attribute__((no_instrument_function)) Model1___u16 Model1_mdio_phy_id_prtad(int Model1_phy_id)
{
 return (Model1_phy_id & 0x03e0) >> 5;
}

static inline __attribute__((no_instrument_function)) Model1___u16 Model1_mdio_phy_id_devad(int Model1_phy_id)
{
 return Model1_phy_id & 0x001f;
}

/**
 * struct mdio_if_info - Ethernet controller MDIO interface
 * @prtad: PRTAD of the PHY (%MDIO_PRTAD_NONE if not present/unknown)
 * @mmds: Mask of MMDs expected to be present in the PHY.  This must be
 *	non-zero unless @prtad = %MDIO_PRTAD_NONE.
 * @mode_support: MDIO modes supported.  If %MDIO_SUPPORTS_C22 is set then
 *	MII register access will be passed through with @devad =
 *	%MDIO_DEVAD_NONE.  If %MDIO_EMULATE_C22 is set then access to
 *	commonly used clause 22 registers will be translated into
 *	clause 45 registers.
 * @dev: Net device structure
 * @mdio_read: Register read function; returns value or negative error code
 * @mdio_write: Register write function; returns 0 or negative error code
 */
struct Model1_mdio_if_info {
 int Model1_prtad;
 Model1_u32 Model1_mmds;
 unsigned Model1_mode_support;

 struct Model1_net_device *Model1_dev;
 int (*Model1_mdio_read)(struct Model1_net_device *Model1_dev, int Model1_prtad, int Model1_devad,
    Model1_u16 Model1_addr);
 int (*Model1_mdio_write)(struct Model1_net_device *Model1_dev, int Model1_prtad, int Model1_devad,
     Model1_u16 Model1_addr, Model1_u16 Model1_val);
};







struct Model1_ethtool_cmd;
struct Model1_ethtool_pauseparam;
extern int Model1_mdio45_probe(struct Model1_mdio_if_info *Model1_mdio, int Model1_prtad);
extern int Model1_mdio_set_flag(const struct Model1_mdio_if_info *Model1_mdio,
    int Model1_prtad, int Model1_devad, Model1_u16 Model1_addr, int Model1_mask,
    bool Model1_sense);
extern int Model1_mdio45_links_ok(const struct Model1_mdio_if_info *Model1_mdio, Model1_u32 Model1_mmds);
extern int Model1_mdio45_nway_restart(const struct Model1_mdio_if_info *Model1_mdio);
extern void Model1_mdio45_ethtool_gset_npage(const struct Model1_mdio_if_info *Model1_mdio,
          struct Model1_ethtool_cmd *Model1_ecmd,
          Model1_u32 Model1_npage_adv, Model1_u32 Model1_npage_lpa);

/**
 * mdio45_ethtool_gset - get settings for ETHTOOL_GSET
 * @mdio: MDIO interface
 * @ecmd: Ethtool request structure
 *
 * Since the CSRs for auto-negotiation using next pages are not fully
 * standardised, this function does not attempt to decode them.  Use
 * mdio45_ethtool_gset_npage() to specify advertisement bits from next
 * pages.
 */
static inline __attribute__((no_instrument_function)) void Model1_mdio45_ethtool_gset(const struct Model1_mdio_if_info *Model1_mdio,
           struct Model1_ethtool_cmd *Model1_ecmd)
{
 Model1_mdio45_ethtool_gset_npage(Model1_mdio, Model1_ecmd, 0, 0);
}

extern int Model1_mdio_mii_ioctl(const struct Model1_mdio_if_info *Model1_mdio,
     struct Model1_mii_ioctl_data *Model1_mii_data, int Model1_cmd);

/**
 * mmd_eee_cap_to_ethtool_sup_t
 * @eee_cap: value of the MMD EEE Capability register
 *
 * A small helper function that translates MMD EEE Capability (3.20) bits
 * to ethtool supported settings.
 */
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_mmd_eee_cap_to_ethtool_sup_t(Model1_u16 Model1_eee_cap)
{
 Model1_u32 Model1_supported = 0;

 if (Model1_eee_cap & 0x0002)
  Model1_supported |= (1UL << (Model1_ETHTOOL_LINK_MODE_100baseT_Full_BIT));
 if (Model1_eee_cap & 0x0004)
  Model1_supported |= (1UL << (Model1_ETHTOOL_LINK_MODE_1000baseT_Full_BIT));
 if (Model1_eee_cap & 0x0008)
  Model1_supported |= (1UL << (Model1_ETHTOOL_LINK_MODE_10000baseT_Full_BIT));
 if (Model1_eee_cap & 0x0010)
  Model1_supported |= (1UL << (Model1_ETHTOOL_LINK_MODE_1000baseKX_Full_BIT));
 if (Model1_eee_cap & 0x0020)
  Model1_supported |= (1UL << (Model1_ETHTOOL_LINK_MODE_10000baseKX4_Full_BIT));
 if (Model1_eee_cap & 0x0040)
  Model1_supported |= (1UL << (Model1_ETHTOOL_LINK_MODE_10000baseKR_Full_BIT));

 return Model1_supported;
}

/**
 * mmd_eee_adv_to_ethtool_adv_t
 * @eee_adv: value of the MMD EEE Advertisement/Link Partner Ability registers
 *
 * A small helper function that translates the MMD EEE Advertisment (7.60)
 * and MMD EEE Link Partner Ability (7.61) bits to ethtool advertisement
 * settings.
 */
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_mmd_eee_adv_to_ethtool_adv_t(Model1_u16 Model1_eee_adv)
{
 Model1_u32 Model1_adv = 0;

 if (Model1_eee_adv & 0x0002)
  Model1_adv |= (1UL << (Model1_ETHTOOL_LINK_MODE_100baseT_Full_BIT));
 if (Model1_eee_adv & 0x0004)
  Model1_adv |= (1UL << (Model1_ETHTOOL_LINK_MODE_1000baseT_Full_BIT));
 if (Model1_eee_adv & 0x0008)
  Model1_adv |= (1UL << (Model1_ETHTOOL_LINK_MODE_10000baseT_Full_BIT));
 if (Model1_eee_adv & 0x0010)
  Model1_adv |= (1UL << (Model1_ETHTOOL_LINK_MODE_1000baseKX_Full_BIT));
 if (Model1_eee_adv & 0x0020)
  Model1_adv |= (1UL << (Model1_ETHTOOL_LINK_MODE_10000baseKX4_Full_BIT));
 if (Model1_eee_adv & 0x0040)
  Model1_adv |= (1UL << (Model1_ETHTOOL_LINK_MODE_10000baseKR_Full_BIT));

 return Model1_adv;
}

/**
 * ethtool_adv_to_mmd_eee_adv_t
 * @adv: the ethtool advertisement settings
 *
 * A small helper function that translates ethtool advertisement settings
 * to EEE advertisements for the MMD EEE Advertisement (7.60) and
 * MMD EEE Link Partner Ability (7.61) registers.
 */
static inline __attribute__((no_instrument_function)) Model1_u16 Model1_ethtool_adv_to_mmd_eee_adv_t(Model1_u32 Model1_adv)
{
 Model1_u16 Model1_reg = 0;

 if (Model1_adv & (1UL << (Model1_ETHTOOL_LINK_MODE_100baseT_Full_BIT)))
  Model1_reg |= 0x0002;
 if (Model1_adv & (1UL << (Model1_ETHTOOL_LINK_MODE_1000baseT_Full_BIT)))
  Model1_reg |= 0x0004;
 if (Model1_adv & (1UL << (Model1_ETHTOOL_LINK_MODE_10000baseT_Full_BIT)))
  Model1_reg |= 0x0008;
 if (Model1_adv & (1UL << (Model1_ETHTOOL_LINK_MODE_1000baseKX_Full_BIT)))
  Model1_reg |= 0x0010;
 if (Model1_adv & (1UL << (Model1_ETHTOOL_LINK_MODE_10000baseKX4_Full_BIT)))
  Model1_reg |= 0x0020;
 if (Model1_adv & (1UL << (Model1_ETHTOOL_LINK_MODE_10000baseKR_Full_BIT)))
  Model1_reg |= 0x0040;

 return Model1_reg;
}

int Model1_mdiobus_read(struct Model1_mii_bus *Model1_bus, int Model1_addr, Model1_u32 Model1_regnum);
int Model1_mdiobus_read_nested(struct Model1_mii_bus *Model1_bus, int Model1_addr, Model1_u32 Model1_regnum);
int Model1_mdiobus_write(struct Model1_mii_bus *Model1_bus, int Model1_addr, Model1_u32 Model1_regnum, Model1_u16 Model1_val);
int Model1_mdiobus_write_nested(struct Model1_mii_bus *Model1_bus, int Model1_addr, Model1_u32 Model1_regnum, Model1_u16 Model1_val);

int Model1_mdiobus_register_device(struct Model1_mdio_device *Model1_mdiodev);
int Model1_mdiobus_unregister_device(struct Model1_mdio_device *Model1_mdiodev);
bool Model1_mdiobus_is_registered_device(struct Model1_mii_bus *Model1_bus, int Model1_addr);
struct Model1_phy_device *Model1_mdiobus_get_phy(struct Model1_mii_bus *Model1_bus, int Model1_addr);

/**
 * module_mdio_driver() - Helper macro for registering mdio drivers
 *
 * Helper macro for MDIO drivers which do not do anything special in module
 * init/exit. Each module may only use this macro once, and calling it
 * replaces module_init() and module_exit().
 */
/*
 * Set phydev->irq to PHY_POLL if interrupts are not supported,
 * or not desired for this PHY.  Set to PHY_IGNORE_INTERRUPT if
 * the attached driver handles the interrupt
 */
/* Interface Mode definitions */
typedef enum {
 Model1_PHY_INTERFACE_MODE_NA,
 Model1_PHY_INTERFACE_MODE_MII,
 Model1_PHY_INTERFACE_MODE_GMII,
 Model1_PHY_INTERFACE_MODE_SGMII,
 Model1_PHY_INTERFACE_MODE_TBI,
 Model1_PHY_INTERFACE_MODE_REVMII,
 Model1_PHY_INTERFACE_MODE_RMII,
 Model1_PHY_INTERFACE_MODE_RGMII,
 Model1_PHY_INTERFACE_MODE_RGMII_ID,
 Model1_PHY_INTERFACE_MODE_RGMII_RXID,
 Model1_PHY_INTERFACE_MODE_RGMII_TXID,
 Model1_PHY_INTERFACE_MODE_RTBI,
 Model1_PHY_INTERFACE_MODE_SMII,
 Model1_PHY_INTERFACE_MODE_XGMII,
 Model1_PHY_INTERFACE_MODE_MOCA,
 Model1_PHY_INTERFACE_MODE_QSGMII,
 Model1_PHY_INTERFACE_MODE_MAX,
} Model1_phy_interface_t;

/**
 * It maps 'enum phy_interface_t' found in include/linux/phy.h
 * into the device tree binding of 'phy-mode', so that Ethernet
 * device driver can get phy interface from device tree.
 */
static inline __attribute__((no_instrument_function)) const char *Model1_phy_modes(Model1_phy_interface_t Model1_interface)
{
 switch (Model1_interface) {
 case Model1_PHY_INTERFACE_MODE_NA:
  return "";
 case Model1_PHY_INTERFACE_MODE_MII:
  return "mii";
 case Model1_PHY_INTERFACE_MODE_GMII:
  return "gmii";
 case Model1_PHY_INTERFACE_MODE_SGMII:
  return "sgmii";
 case Model1_PHY_INTERFACE_MODE_TBI:
  return "tbi";
 case Model1_PHY_INTERFACE_MODE_REVMII:
  return "rev-mii";
 case Model1_PHY_INTERFACE_MODE_RMII:
  return "rmii";
 case Model1_PHY_INTERFACE_MODE_RGMII:
  return "rgmii";
 case Model1_PHY_INTERFACE_MODE_RGMII_ID:
  return "rgmii-id";
 case Model1_PHY_INTERFACE_MODE_RGMII_RXID:
  return "rgmii-rxid";
 case Model1_PHY_INTERFACE_MODE_RGMII_TXID:
  return "rgmii-txid";
 case Model1_PHY_INTERFACE_MODE_RTBI:
  return "rtbi";
 case Model1_PHY_INTERFACE_MODE_SMII:
  return "smii";
 case Model1_PHY_INTERFACE_MODE_XGMII:
  return "xgmii";
 case Model1_PHY_INTERFACE_MODE_MOCA:
  return "moca";
 case Model1_PHY_INTERFACE_MODE_QSGMII:
  return "qsgmii";
 default:
  return "unknown";
 }
}
/* Used when trying to connect to a specific phy (mii bus id:phy device id) */


/*
 * Need to be a little smaller than phydev->dev.bus_id to leave room
 * for the ":%02x"
 */


/* Or MII_ADDR_C45 into regnum for read/write on mii_bus to enable the 21 bit
   IEEE 802.3ae clause 45 addressing mode used by 10GIGE phy chips. */


struct Model1_device;
struct Model1_sk_buff;

/*
 * The Bus class for PHYs.  Devices which provide access to
 * PHYs should register using this structure
 */
struct Model1_mii_bus {
 struct Model1_module *Model1_owner;
 const char *Model1_name;
 char Model1_id[(20 - 3)];
 void *Model1_priv;
 int (*Model1_read)(struct Model1_mii_bus *Model1_bus, int Model1_addr, int Model1_regnum);
 int (*Model1_write)(struct Model1_mii_bus *Model1_bus, int Model1_addr, int Model1_regnum, Model1_u16 Model1_val);
 int (*Model1_reset)(struct Model1_mii_bus *Model1_bus);

 /*
	 * A lock to ensure that only one thing can read/write
	 * the MDIO bus at a time
	 */
 struct Model1_mutex Model1_mdio_lock;

 struct Model1_device *Model1_parent;
 enum {
  Model1_MDIOBUS_ALLOCATED = 1,
  Model1_MDIOBUS_REGISTERED,
  Model1_MDIOBUS_UNREGISTERED,
  Model1_MDIOBUS_RELEASED,
 } Model1_state;
 struct Model1_device Model1_dev;

 /* list of all PHYs on bus */
 struct Model1_mdio_device *Model1_mdio_map[32];

 /* PHY addresses to be ignored when probing */
 Model1_u32 Model1_phy_mask;

 /* PHY addresses to ignore the TA/read failure */
 Model1_u32 Model1_phy_ignore_ta_mask;

 /*
	 * An array of interrupts, each PHY's interrupt at the index
	 * matching its address
	 */
 int Model1_irq[32];
};


struct Model1_mii_bus *Model1_mdiobus_alloc_size(Model1_size_t);
static inline __attribute__((no_instrument_function)) struct Model1_mii_bus *Model1_mdiobus_alloc(void)
{
 return Model1_mdiobus_alloc_size(0);
}

int Model1___mdiobus_register(struct Model1_mii_bus *Model1_bus, struct Model1_module *Model1_owner);

void Model1_mdiobus_unregister(struct Model1_mii_bus *Model1_bus);
void Model1_mdiobus_free(struct Model1_mii_bus *Model1_bus);
struct Model1_mii_bus *Model1_devm_mdiobus_alloc_size(struct Model1_device *Model1_dev, int Model1_sizeof_priv);
static inline __attribute__((no_instrument_function)) struct Model1_mii_bus *Model1_devm_mdiobus_alloc(struct Model1_device *Model1_dev)
{
 return Model1_devm_mdiobus_alloc_size(Model1_dev, 0);
}

void Model1_devm_mdiobus_free(struct Model1_device *Model1_dev, struct Model1_mii_bus *Model1_bus);
struct Model1_phy_device *Model1_mdiobus_scan(struct Model1_mii_bus *Model1_bus, int Model1_addr);




/* PHY state machine states:
 *
 * DOWN: PHY device and driver are not ready for anything.  probe
 * should be called if and only if the PHY is in this state,
 * given that the PHY device exists.
 * - PHY driver probe function will, depending on the PHY, set
 * the state to STARTING or READY
 *
 * STARTING:  PHY device is coming up, and the ethernet driver is
 * not ready.  PHY drivers may set this in the probe function.
 * If they do, they are responsible for making sure the state is
 * eventually set to indicate whether the PHY is UP or READY,
 * depending on the state when the PHY is done starting up.
 * - PHY driver will set the state to READY
 * - start will set the state to PENDING
 *
 * READY: PHY is ready to send and receive packets, but the
 * controller is not.  By default, PHYs which do not implement
 * probe will be set to this state by phy_probe().  If the PHY
 * driver knows the PHY is ready, and the PHY state is STARTING,
 * then it sets this STATE.
 * - start will set the state to UP
 *
 * PENDING: PHY device is coming up, but the ethernet driver is
 * ready.  phy_start will set this state if the PHY state is
 * STARTING.
 * - PHY driver will set the state to UP when the PHY is ready
 *
 * UP: The PHY and attached device are ready to do work.
 * Interrupts should be started here.
 * - timer moves to AN
 *
 * AN: The PHY is currently negotiating the link state.  Link is
 * therefore down for now.  phy_timer will set this state when it
 * detects the state is UP.  config_aneg will set this state
 * whenever called with phydev->autoneg set to AUTONEG_ENABLE.
 * - If autonegotiation finishes, but there's no link, it sets
 *   the state to NOLINK.
 * - If aneg finishes with link, it sets the state to RUNNING,
 *   and calls adjust_link
 * - If autonegotiation did not finish after an arbitrary amount
 *   of time, autonegotiation should be tried again if the PHY
 *   supports "magic" autonegotiation (back to AN)
 * - If it didn't finish, and no magic_aneg, move to FORCING.
 *
 * NOLINK: PHY is up, but not currently plugged in.
 * - If the timer notes that the link comes back, we move to RUNNING
 * - config_aneg moves to AN
 * - phy_stop moves to HALTED
 *
 * FORCING: PHY is being configured with forced settings
 * - if link is up, move to RUNNING
 * - If link is down, we drop to the next highest setting, and
 *   retry (FORCING) after a timeout
 * - phy_stop moves to HALTED
 *
 * RUNNING: PHY is currently up, running, and possibly sending
 * and/or receiving packets
 * - timer will set CHANGELINK if we're polling (this ensures the
 *   link state is polled every other cycle of this state machine,
 *   which makes it every other second)
 * - irq will set CHANGELINK
 * - config_aneg will set AN
 * - phy_stop moves to HALTED
 *
 * CHANGELINK: PHY experienced a change in link state
 * - timer moves to RUNNING if link
 * - timer moves to NOLINK if the link is down
 * - phy_stop moves to HALTED
 *
 * HALTED: PHY is up, but no polling or interrupts are done. Or
 * PHY is in an error state.
 *
 * - phy_start moves to RESUMING
 *
 * RESUMING: PHY was halted, but now wants to run again.
 * - If we are forcing, or aneg is done, timer moves to RUNNING
 * - If aneg is not done, timer moves to AN
 * - phy_stop moves to HALTED
 */
enum Model1_phy_state {
 Model1_PHY_DOWN = 0,
 Model1_PHY_STARTING,
 Model1_PHY_READY,
 Model1_PHY_PENDING,
 Model1_PHY_UP,
 Model1_PHY_AN,
 Model1_PHY_RUNNING,
 Model1_PHY_NOLINK,
 Model1_PHY_FORCING,
 Model1_PHY_CHANGELINK,
 Model1_PHY_HALTED,
 Model1_PHY_RESUMING
};

/**
 * struct phy_c45_device_ids - 802.3-c45 Device Identifiers
 * @devices_in_package: Bit vector of devices present.
 * @device_ids: The device identifer for each present device.
 */
struct Model1_phy_c45_device_ids {
 Model1_u32 Model1_devices_in_package;
 Model1_u32 Model1_device_ids[8];
};

/* phy_device: An instance of a PHY
 *
 * drv: Pointer to the driver for this PHY instance
 * phy_id: UID for this device found during discovery
 * c45_ids: 802.3-c45 Device Identifers if is_c45.
 * is_c45:  Set to true if this phy uses clause 45 addressing.
 * is_internal: Set to true if this phy is internal to a MAC.
 * is_pseudo_fixed_link: Set to true if this phy is an Ethernet switch, etc.
 * has_fixups: Set to true if this phy has fixups/quirks.
 * suspended: Set to true if this phy has been suspended successfully.
 * state: state of the PHY for management purposes
 * dev_flags: Device-specific flags used by the PHY driver.
 * link_timeout: The number of timer firings to wait before the
 * giving up on the current attempt at acquiring a link
 * irq: IRQ number of the PHY's interrupt (-1 if none)
 * phy_timer: The timer for handling the state machine
 * phy_queue: A work_queue for the interrupt
 * attached_dev: The attached enet driver's device instance ptr
 * adjust_link: Callback for the enet controller to respond to
 * changes in the link state.
 *
 * speed, duplex, pause, supported, advertising, lp_advertising,
 * and autoneg are used like in mii_if_info
 *
 * interrupts currently only supports enabled or disabled,
 * but could be changed in the future to support enabling
 * and disabling specific interrupts
 *
 * Contains some infrastructure for polling and interrupt
 * handling, as well as handling shifts in PHY hardware state
 */
struct Model1_phy_device {
 struct Model1_mdio_device Model1_mdio;

 /* Information about the PHY type */
 /* And management functions */
 struct Model1_phy_driver *Model1_drv;

 Model1_u32 Model1_phy_id;

 struct Model1_phy_c45_device_ids Model1_c45_ids;
 bool Model1_is_c45;
 bool Model1_is_internal;
 bool Model1_is_pseudo_fixed_link;
 bool Model1_has_fixups;
 bool Model1_suspended;

 enum Model1_phy_state Model1_state;

 Model1_u32 Model1_dev_flags;

 Model1_phy_interface_t Model1_interface;

 /*
	 * forced speed & duplex (no autoneg)
	 * partner speed & duplex & pause (autoneg)
	 */
 int Model1_speed;
 int Model1_duplex;
 int Model1_pause;
 int Model1_asym_pause;

 /* The most recently read link state */
 int Model1_link;

 /* Enabled Interrupts */
 Model1_u32 Model1_interrupts;

 /* Union of PHY and Attached devices' supported modes */
 /* See mii.h for more info */
 Model1_u32 Model1_supported;
 Model1_u32 Model1_advertising;
 Model1_u32 Model1_lp_advertising;

 int Model1_autoneg;

 int Model1_link_timeout;

 /*
	 * Interrupt number for this PHY
	 * -1 means no interrupt
	 */
 int Model1_irq;

 /* private data pointer */
 /* For use by PHYs to maintain extra state */
 void *Model1_priv;

 /* Interrupt and Polling infrastructure */
 struct Model1_work_struct Model1_phy_queue;
 struct Model1_delayed_work Model1_state_queue;
 Model1_atomic_t Model1_irq_disable;

 struct Model1_mutex Model1_lock;

 struct Model1_net_device *Model1_attached_dev;

 Model1_u8 Model1_mdix;

 void (*Model1_adjust_link)(struct Model1_net_device *Model1_dev);
};



/* struct phy_driver: Driver structure for a particular PHY type
 *
 * driver_data: static driver data
 * phy_id: The result of reading the UID registers of this PHY
 *   type, and ANDing them with the phy_id_mask.  This driver
 *   only works for PHYs with IDs which match this field
 * name: The friendly name of this PHY type
 * phy_id_mask: Defines the important bits of the phy_id
 * features: A list of features (speed, duplex, etc) supported
 *   by this PHY
 * flags: A bitfield defining certain other features this PHY
 *   supports (like interrupts)
 *
 * The drivers must implement config_aneg and read_status.  All
 * other functions are optional. Note that none of these
 * functions should be called from interrupt time.  The goal is
 * for the bus read/write functions to be able to block when the
 * bus transaction is happening, and be freed up by an interrupt
 * (The MPC85xx has this ability, though it is not currently
 * supported in the driver).
 */
struct Model1_phy_driver {
 struct Model1_mdio_driver_common Model1_mdiodrv;
 Model1_u32 Model1_phy_id;
 char *Model1_name;
 unsigned int Model1_phy_id_mask;
 Model1_u32 Model1_features;
 Model1_u32 Model1_flags;
 const void *Model1_driver_data;

 /*
	 * Called to issue a PHY software reset
	 */
 int (*Model1_soft_reset)(struct Model1_phy_device *Model1_phydev);

 /*
	 * Called to initialize the PHY,
	 * including after a reset
	 */
 int (*Model1_config_init)(struct Model1_phy_device *Model1_phydev);

 /*
	 * Called during discovery.  Used to set
	 * up device-specific structures, if any
	 */
 int (*Model1_probe)(struct Model1_phy_device *Model1_phydev);

 /* PHY Power Management */
 int (*Model1_suspend)(struct Model1_phy_device *Model1_phydev);
 int (*Model1_resume)(struct Model1_phy_device *Model1_phydev);

 /*
	 * Configures the advertisement and resets
	 * autonegotiation if phydev->autoneg is on,
	 * forces the speed to the current settings in phydev
	 * if phydev->autoneg is off
	 */
 int (*Model1_config_aneg)(struct Model1_phy_device *Model1_phydev);

 /* Determines the auto negotiation result */
 int (*Model1_aneg_done)(struct Model1_phy_device *Model1_phydev);

 /* Determines the negotiated speed and duplex */
 int (*Model1_read_status)(struct Model1_phy_device *Model1_phydev);

 /* Clears any pending interrupts */
 int (*Model1_ack_interrupt)(struct Model1_phy_device *Model1_phydev);

 /* Enables or disables interrupts */
 int (*Model1_config_intr)(struct Model1_phy_device *Model1_phydev);

 /*
	 * Checks if the PHY generated an interrupt.
	 * For multi-PHY devices with shared PHY interrupt pin
	 */
 int (*Model1_did_interrupt)(struct Model1_phy_device *Model1_phydev);

 /* Clears up any memory if needed */
 void (*Model1_remove)(struct Model1_phy_device *Model1_phydev);

 /* Returns true if this is a suitable driver for the given
	 * phydev.  If NULL, matching is based on phy_id and
	 * phy_id_mask.
	 */
 int (*Model1_match_phy_device)(struct Model1_phy_device *Model1_phydev);

 /* Handles ethtool queries for hardware time stamping. */
 int (*Model1_ts_info)(struct Model1_phy_device *Model1_phydev, struct Model1_ethtool_ts_info *Model1_ti);

 /* Handles SIOCSHWTSTAMP ioctl for hardware time stamping. */
 int (*Model1_hwtstamp)(struct Model1_phy_device *Model1_phydev, struct Model1_ifreq *Model1_ifr);

 /*
	 * Requests a Rx timestamp for 'skb'. If the skb is accepted,
	 * the phy driver promises to deliver it using netif_rx() as
	 * soon as a timestamp becomes available. One of the
	 * PTP_CLASS_ values is passed in 'type'. The function must
	 * return true if the skb is accepted for delivery.
	 */
 bool (*Model1_rxtstamp)(struct Model1_phy_device *Model1_dev, struct Model1_sk_buff *Model1_skb, int Model1_type);

 /*
	 * Requests a Tx timestamp for 'skb'. The phy driver promises
	 * to deliver it using skb_complete_tx_timestamp() as soon as a
	 * timestamp becomes available. One of the PTP_CLASS_ values
	 * is passed in 'type'.
	 */
 void (*Model1_txtstamp)(struct Model1_phy_device *Model1_dev, struct Model1_sk_buff *Model1_skb, int Model1_type);

 /* Some devices (e.g. qnap TS-119P II) require PHY register changes to
	 * enable Wake on LAN, so set_wol is provided to be called in the
	 * ethernet driver's set_wol function. */
 int (*Model1_set_wol)(struct Model1_phy_device *Model1_dev, struct Model1_ethtool_wolinfo *Model1_wol);

 /* See set_wol, but for checking whether Wake on LAN is enabled. */
 void (*Model1_get_wol)(struct Model1_phy_device *Model1_dev, struct Model1_ethtool_wolinfo *Model1_wol);

 /*
	 * Called to inform a PHY device driver when the core is about to
	 * change the link state. This callback is supposed to be used as
	 * fixup hook for drivers that need to take action when the link
	 * state changes. Drivers are by no means allowed to mess with the
	 * PHY device structure in their implementations.
	 */
 void (*Model1_link_change_notify)(struct Model1_phy_device *Model1_dev);

 /* A function provided by a phy specific driver to override the
	 * the PHY driver framework support for reading a MMD register
	 * from the PHY. If not supported, return -1. This function is
	 * optional for PHY specific drivers, if not provided then the
	 * default MMD read function is used by the PHY framework.
	 */
 int (*Model1_read_mmd_indirect)(struct Model1_phy_device *Model1_dev, int Model1_ptrad,
     int Model1_devnum, int Model1_regnum);

 /* A function provided by a phy specific driver to override the
	 * the PHY driver framework support for writing a MMD register
	 * from the PHY. This function is optional for PHY specific drivers,
	 * if not provided then the default MMD read function is used by
	 * the PHY framework.
	 */
 void (*Model1_write_mmd_indirect)(struct Model1_phy_device *Model1_dev, int Model1_ptrad,
       int Model1_devnum, int Model1_regnum, Model1_u32 Model1_val);

 /* Get the size and type of the eeprom contained within a plug-in
	 * module */
 int (*Model1_module_info)(struct Model1_phy_device *Model1_dev,
      struct Model1_ethtool_modinfo *Model1_modinfo);

 /* Get the eeprom information from the plug-in module */
 int (*Model1_module_eeprom)(struct Model1_phy_device *Model1_dev,
        struct Model1_ethtool_eeprom *Model1_ee, Model1_u8 *Model1_data);

 /* Get statistics from the phy using ethtool */
 int (*Model1_get_sset_count)(struct Model1_phy_device *Model1_dev);
 void (*Model1_get_strings)(struct Model1_phy_device *Model1_dev, Model1_u8 *Model1_data);
 void (*Model1_get_stats)(struct Model1_phy_device *Model1_dev,
     struct Model1_ethtool_stats *Model1_stats, Model1_u64 *Model1_data);
};






/* A Structure for boards to register fixups with the PHY Lib */
struct Model1_phy_fixup {
 struct Model1_list_head Model1_list;
 char Model1_bus_id[20];
 Model1_u32 Model1_phy_uid;
 Model1_u32 Model1_phy_uid_mask;
 int (*Model1_run)(struct Model1_phy_device *Model1_phydev);
};

/**
 * phy_read_mmd - Convenience function for reading a register
 * from an MMD on a given PHY.
 * @phydev: The phy_device struct
 * @devad: The MMD to read from
 * @regnum: The register on the MMD to read
 *
 * Same rules as for phy_read();
 */
static inline __attribute__((no_instrument_function)) int Model1_phy_read_mmd(struct Model1_phy_device *Model1_phydev, int Model1_devad, Model1_u32 Model1_regnum)
{
 if (!Model1_phydev->Model1_is_c45)
  return -95;

 return Model1_mdiobus_read(Model1_phydev->Model1_mdio.Model1_bus, Model1_phydev->Model1_mdio.Model1_addr,
       (1<<30) | (Model1_devad << 16) | (Model1_regnum & 0xffff));
}

/**
 * phy_read_mmd_indirect - reads data from the MMD registers
 * @phydev: The PHY device bus
 * @prtad: MMD Address
 * @addr: PHY address on the MII bus
 *
 * Description: it reads data from the MMD registers (clause 22 to access to
 * clause 45) of the specified phy address.
 */
int Model1_phy_read_mmd_indirect(struct Model1_phy_device *Model1_phydev, int Model1_prtad, int Model1_devad);

/**
 * phy_read - Convenience function for reading a given PHY register
 * @phydev: the phy_device struct
 * @regnum: register number to read
 *
 * NOTE: MUST NOT be called from interrupt context,
 * because the bus read/write functions may wait for an interrupt
 * to conclude the operation.
 */
static inline __attribute__((no_instrument_function)) int Model1_phy_read(struct Model1_phy_device *Model1_phydev, Model1_u32 Model1_regnum)
{
 return Model1_mdiobus_read(Model1_phydev->Model1_mdio.Model1_bus, Model1_phydev->Model1_mdio.Model1_addr, Model1_regnum);
}

/**
 * phy_write - Convenience function for writing a given PHY register
 * @phydev: the phy_device struct
 * @regnum: register number to write
 * @val: value to write to @regnum
 *
 * NOTE: MUST NOT be called from interrupt context,
 * because the bus read/write functions may wait for an interrupt
 * to conclude the operation.
 */
static inline __attribute__((no_instrument_function)) int Model1_phy_write(struct Model1_phy_device *Model1_phydev, Model1_u32 Model1_regnum, Model1_u16 Model1_val)
{
 return Model1_mdiobus_write(Model1_phydev->Model1_mdio.Model1_bus, Model1_phydev->Model1_mdio.Model1_addr, Model1_regnum, Model1_val);
}

/**
 * phy_interrupt_is_valid - Convenience function for testing a given PHY irq
 * @phydev: the phy_device struct
 *
 * NOTE: must be kept in sync with addition/removal of PHY_POLL and
 * PHY_IGNORE_INTERRUPT
 */
static inline __attribute__((no_instrument_function)) bool Model1_phy_interrupt_is_valid(struct Model1_phy_device *Model1_phydev)
{
 return Model1_phydev->Model1_irq != -1 && Model1_phydev->Model1_irq != -2;
}

/**
 * phy_is_internal - Convenience function for testing if a PHY is internal
 * @phydev: the phy_device struct
 */
static inline __attribute__((no_instrument_function)) bool Model1_phy_is_internal(struct Model1_phy_device *Model1_phydev)
{
 return Model1_phydev->Model1_is_internal;
}

/**
 * phy_interface_is_rgmii - Convenience function for testing if a PHY interface
 * is RGMII (all variants)
 * @phydev: the phy_device struct
 */
static inline __attribute__((no_instrument_function)) bool Model1_phy_interface_is_rgmii(struct Model1_phy_device *Model1_phydev)
{
 return Model1_phydev->Model1_interface >= Model1_PHY_INTERFACE_MODE_RGMII &&
  Model1_phydev->Model1_interface <= Model1_PHY_INTERFACE_MODE_RGMII_TXID;
};

/*
 * phy_is_pseudo_fixed_link - Convenience function for testing if this
 * PHY is the CPU port facing side of an Ethernet switch, or similar.
 * @phydev: the phy_device struct
 */
static inline __attribute__((no_instrument_function)) bool Model1_phy_is_pseudo_fixed_link(struct Model1_phy_device *Model1_phydev)
{
 return Model1_phydev->Model1_is_pseudo_fixed_link;
}

/**
 * phy_write_mmd - Convenience function for writing a register
 * on an MMD on a given PHY.
 * @phydev: The phy_device struct
 * @devad: The MMD to read from
 * @regnum: The register on the MMD to read
 * @val: value to write to @regnum
 *
 * Same rules as for phy_write();
 */
static inline __attribute__((no_instrument_function)) int Model1_phy_write_mmd(struct Model1_phy_device *Model1_phydev, int Model1_devad,
    Model1_u32 Model1_regnum, Model1_u16 Model1_val)
{
 if (!Model1_phydev->Model1_is_c45)
  return -95;

 Model1_regnum = (1<<30) | ((Model1_devad & 0x1f) << 16) | (Model1_regnum & 0xffff);

 return Model1_mdiobus_write(Model1_phydev->Model1_mdio.Model1_bus, Model1_phydev->Model1_mdio.Model1_addr, Model1_regnum, Model1_val);
}

/**
 * phy_write_mmd_indirect - writes data to the MMD registers
 * @phydev: The PHY device
 * @prtad: MMD Address
 * @devad: MMD DEVAD
 * @data: data to write in the MMD register
 *
 * Description: Write data from the MMD registers of the specified
 * phy address.
 */
void Model1_phy_write_mmd_indirect(struct Model1_phy_device *Model1_phydev, int Model1_prtad,
       int Model1_devad, Model1_u32 Model1_data);

struct Model1_phy_device *Model1_phy_device_create(struct Model1_mii_bus *Model1_bus, int Model1_addr, int Model1_phy_id,
         bool Model1_is_c45,
         struct Model1_phy_c45_device_ids *Model1_c45_ids);
struct Model1_phy_device *Model1_get_phy_device(struct Model1_mii_bus *Model1_bus, int Model1_addr, bool Model1_is_c45);
int Model1_phy_device_register(struct Model1_phy_device *Model1_phy);
void Model1_phy_device_remove(struct Model1_phy_device *Model1_phydev);
int Model1_phy_init_hw(struct Model1_phy_device *Model1_phydev);
int Model1_phy_suspend(struct Model1_phy_device *Model1_phydev);
int Model1_phy_resume(struct Model1_phy_device *Model1_phydev);
struct Model1_phy_device *Model1_phy_attach(struct Model1_net_device *Model1_dev, const char *Model1_bus_id,
         Model1_phy_interface_t Model1_interface);
struct Model1_phy_device *Model1_phy_find_first(struct Model1_mii_bus *Model1_bus);
int Model1_phy_attach_direct(struct Model1_net_device *Model1_dev, struct Model1_phy_device *Model1_phydev,
        Model1_u32 Model1_flags, Model1_phy_interface_t Model1_interface);
int Model1_phy_connect_direct(struct Model1_net_device *Model1_dev, struct Model1_phy_device *Model1_phydev,
         void (*Model1_handler)(struct Model1_net_device *),
         Model1_phy_interface_t Model1_interface);
struct Model1_phy_device *Model1_phy_connect(struct Model1_net_device *Model1_dev, const char *Model1_bus_id,
          void (*Model1_handler)(struct Model1_net_device *),
          Model1_phy_interface_t Model1_interface);
void Model1_phy_disconnect(struct Model1_phy_device *Model1_phydev);
void Model1_phy_detach(struct Model1_phy_device *Model1_phydev);
void Model1_phy_start(struct Model1_phy_device *Model1_phydev);
void Model1_phy_stop(struct Model1_phy_device *Model1_phydev);
int Model1_phy_start_aneg(struct Model1_phy_device *Model1_phydev);

int Model1_phy_stop_interrupts(struct Model1_phy_device *Model1_phydev);

static inline __attribute__((no_instrument_function)) int Model1_phy_read_status(struct Model1_phy_device *Model1_phydev)
{
 return Model1_phydev->Model1_drv->Model1_read_status(Model1_phydev);
}







static inline __attribute__((no_instrument_function)) const char *Model1_phydev_name(const struct Model1_phy_device *Model1_phydev)
{
 return Model1_dev_name(&Model1_phydev->Model1_mdio.Model1_dev);
}

void Model1_phy_attached_print(struct Model1_phy_device *Model1_phydev, const char *Model1_fmt, ...)
 __attribute__((format(printf, 2, 3)));
void Model1_phy_attached_info(struct Model1_phy_device *Model1_phydev);
int Model1_genphy_config_init(struct Model1_phy_device *Model1_phydev);
int Model1_genphy_setup_forced(struct Model1_phy_device *Model1_phydev);
int Model1_genphy_restart_aneg(struct Model1_phy_device *Model1_phydev);
int Model1_genphy_config_aneg(struct Model1_phy_device *Model1_phydev);
int Model1_genphy_aneg_done(struct Model1_phy_device *Model1_phydev);
int Model1_genphy_update_link(struct Model1_phy_device *Model1_phydev);
int Model1_genphy_read_status(struct Model1_phy_device *Model1_phydev);
int Model1_genphy_suspend(struct Model1_phy_device *Model1_phydev);
int Model1_genphy_resume(struct Model1_phy_device *Model1_phydev);
int Model1_genphy_soft_reset(struct Model1_phy_device *Model1_phydev);
void Model1_phy_driver_unregister(struct Model1_phy_driver *Model1_drv);
void Model1_phy_drivers_unregister(struct Model1_phy_driver *Model1_drv, int Model1_n);
int Model1_phy_driver_register(struct Model1_phy_driver *Model1_new_driver, struct Model1_module *Model1_owner);
int Model1_phy_drivers_register(struct Model1_phy_driver *Model1_new_driver, int Model1_n,
    struct Model1_module *Model1_owner);
void Model1_phy_state_machine(struct Model1_work_struct *Model1_work);
void Model1_phy_change(struct Model1_work_struct *Model1_work);
void Model1_phy_mac_interrupt(struct Model1_phy_device *Model1_phydev, int Model1_new_link);
void Model1_phy_start_machine(struct Model1_phy_device *Model1_phydev);
void Model1_phy_stop_machine(struct Model1_phy_device *Model1_phydev);
int Model1_phy_ethtool_sset(struct Model1_phy_device *Model1_phydev, struct Model1_ethtool_cmd *Model1_cmd);
int Model1_phy_ethtool_gset(struct Model1_phy_device *Model1_phydev, struct Model1_ethtool_cmd *Model1_cmd);
int Model1_phy_ethtool_ksettings_get(struct Model1_phy_device *Model1_phydev,
         struct Model1_ethtool_link_ksettings *Model1_cmd);
int Model1_phy_ethtool_ksettings_set(struct Model1_phy_device *Model1_phydev,
         const struct Model1_ethtool_link_ksettings *Model1_cmd);
int Model1_phy_mii_ioctl(struct Model1_phy_device *Model1_phydev, struct Model1_ifreq *Model1_ifr, int Model1_cmd);
int Model1_phy_start_interrupts(struct Model1_phy_device *Model1_phydev);
void Model1_phy_print_status(struct Model1_phy_device *Model1_phydev);
void Model1_phy_device_free(struct Model1_phy_device *Model1_phydev);
int Model1_phy_set_max_speed(struct Model1_phy_device *Model1_phydev, Model1_u32 Model1_max_speed);

int Model1_phy_register_fixup(const char *Model1_bus_id, Model1_u32 Model1_phy_uid, Model1_u32 Model1_phy_uid_mask,
         int (*Model1_run)(struct Model1_phy_device *));
int Model1_phy_register_fixup_for_id(const char *Model1_bus_id,
         int (*Model1_run)(struct Model1_phy_device *));
int Model1_phy_register_fixup_for_uid(Model1_u32 Model1_phy_uid, Model1_u32 Model1_phy_uid_mask,
          int (*Model1_run)(struct Model1_phy_device *));

int Model1_phy_init_eee(struct Model1_phy_device *Model1_phydev, bool Model1_clk_stop_enable);
int Model1_phy_get_eee_err(struct Model1_phy_device *Model1_phydev);
int Model1_phy_ethtool_set_eee(struct Model1_phy_device *Model1_phydev, struct Model1_ethtool_eee *Model1_data);
int Model1_phy_ethtool_get_eee(struct Model1_phy_device *Model1_phydev, struct Model1_ethtool_eee *Model1_data);
int Model1_phy_ethtool_set_wol(struct Model1_phy_device *Model1_phydev, struct Model1_ethtool_wolinfo *Model1_wol);
void Model1_phy_ethtool_get_wol(struct Model1_phy_device *Model1_phydev,
    struct Model1_ethtool_wolinfo *Model1_wol);
int Model1_phy_ethtool_get_link_ksettings(struct Model1_net_device *Model1_ndev,
       struct Model1_ethtool_link_ksettings *Model1_cmd);
int Model1_phy_ethtool_set_link_ksettings(struct Model1_net_device *Model1_ndev,
       const struct Model1_ethtool_link_ksettings *Model1_cmd);

int __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_mdio_bus_init(void);
void Model1_mdio_bus_exit(void);

extern struct Model1_bus_type Model1_mdio_bus_type;

/**
 * module_phy_driver() - Helper macro for registering PHY drivers
 * @__phy_drivers: array of PHY drivers to register
 *
 * Helper macro for PHY drivers which do not do anything special in module
 * init/exit. Each module may only use this macro once, and calling it
 * replaces module_init() and module_exit().
 */



struct Model1_fixed_phy_status {
 int Model1_link;
 int Model1_speed;
 int Model1_duplex;
 int Model1_pause;
 int Model1_asym_pause;
};

struct Model1_device_node;
static inline __attribute__((no_instrument_function)) int Model1_fixed_phy_add(unsigned int Model1_irq, int Model1_phy_id,
    struct Model1_fixed_phy_status *Model1_status,
    int Model1_link_gpio)
{
 return -19;
}
static inline __attribute__((no_instrument_function)) struct Model1_phy_device *Model1_fixed_phy_register(unsigned int Model1_irq,
      struct Model1_fixed_phy_status *Model1_status,
      int Model1_gpio_link,
      struct Model1_device_node *Model1_np)
{
 return Model1_ERR_PTR(-19);
}
static inline __attribute__((no_instrument_function)) void Model1_fixed_phy_unregister(struct Model1_phy_device *Model1_phydev)
{
}
static inline __attribute__((no_instrument_function)) int Model1_fixed_phy_set_link_update(struct Model1_phy_device *Model1_phydev,
   int (*Model1_link_update)(struct Model1_net_device *,
        struct Model1_fixed_phy_status *))
{
 return -19;
}
static inline __attribute__((no_instrument_function)) int Model1_fixed_phy_update_state(struct Model1_phy_device *Model1_phydev,
      const struct Model1_fixed_phy_status *Model1_status,
      const struct Model1_fixed_phy_status *Model1_changed)
{
 return -19;
}


enum Model1_dsa_tag_protocol {
 Model1_DSA_TAG_PROTO_NONE = 0,
 Model1_DSA_TAG_PROTO_DSA,
 Model1_DSA_TAG_PROTO_TRAILER,
 Model1_DSA_TAG_PROTO_EDSA,
 Model1_DSA_TAG_PROTO_BRCM,
 Model1_DSA_TAG_LAST, /* MUST BE LAST */
};






struct Model1_dsa_chip_data {
 /*
	 * How to access the switch configuration registers.
	 */
 struct Model1_device *Model1_host_dev;
 int Model1_sw_addr;

 /* set to size of eeprom if supported by the switch */
 int Model1_eeprom_len;

 /* Device tree node pointer for this specific switch chip
	 * used during switch setup in case additional properties
	 * and resources needs to be used
	 */
 struct Model1_device_node *Model1_of_node;

 /*
	 * The names of the switch's ports.  Use "cpu" to
	 * designate the switch port that the cpu is connected to,
	 * "dsa" to indicate that this port is a DSA link to
	 * another switch, NULL to indicate the port is unused,
	 * or any other string to indicate this is a physical port.
	 */
 char *Model1_port_names[12];
 struct Model1_device_node *Model1_port_dn[12];

 /*
	 * An array of which element [a] indicates which port on this
	 * switch should be used to send packets to that are destined
	 * for switch a. Can be NULL if there is only one switch chip.
	 */
 Model1_s8 Model1_rtable[4];
};

struct Model1_dsa_platform_data {
 /*
	 * Reference to a Linux network interface that connects
	 * to the root switch chip of the tree.
	 */
 struct Model1_device *Model1_netdev;
 struct Model1_net_device *Model1_of_netdev;

 /*
	 * Info structs describing each of the switch chips
	 * connected via this network interface.
	 */
 int Model1_nr_chips;
 struct Model1_dsa_chip_data *Model1_chip;
};

struct Model1_packet_type;

struct Model1_dsa_switch_tree {
 struct Model1_list_head Model1_list;

 /* Tree identifier */
 Model1_u32 Model1_tree;

 /* Number of switches attached to this tree */
 struct Model1_kref Model1_refcount;

 /* Has this tree been applied to the hardware? */
 bool Model1_applied;

 /*
	 * Configuration data for the platform device that owns
	 * this dsa switch tree instance.
	 */
 struct Model1_dsa_platform_data *Model1_pd;

 /*
	 * Reference to network device to use, and which tagging
	 * protocol to use.
	 */
 struct Model1_net_device *Model1_master_netdev;
 int (*Model1_rcv)(struct Model1_sk_buff *Model1_skb,
           struct Model1_net_device *Model1_dev,
           struct Model1_packet_type *Model1_pt,
           struct Model1_net_device *Model1_orig_dev);

 /*
	 * Original copy of the master netdev ethtool_ops
	 */
 struct Model1_ethtool_ops Model1_master_ethtool_ops;
 const struct Model1_ethtool_ops *Model1_master_orig_ethtool_ops;

 /*
	 * The switch and port to which the CPU is attached.
	 */
 Model1_s8 Model1_cpu_switch;
 Model1_s8 Model1_cpu_port;

 /*
	 * Data for the individual switch chips.
	 */
 struct Model1_dsa_switch *Model1_ds[4];

 /*
	 * Tagging protocol operations for adding and removing an
	 * encapsulation tag.
	 */
 const struct Model1_dsa_device_ops *Model1_tag_ops;
};

struct Model1_dsa_port {
 struct Model1_net_device *Model1_netdev;
 struct Model1_device_node *Model1_dn;
 unsigned int Model1_ageing_time;
};

struct Model1_dsa_switch {
 struct Model1_device *Model1_dev;

 /*
	 * Parent switch tree, and switch index.
	 */
 struct Model1_dsa_switch_tree *Model1_dst;
 int Model1_index;

 /*
	 * Give the switch driver somewhere to hang its private data
	 * structure.
	 */
 void *Model1_priv;

 /*
	 * Configuration data for this switch.
	 */
 struct Model1_dsa_chip_data *Model1_cd;

 /*
	 * The used switch driver.
	 */
 struct Model1_dsa_switch_driver *Model1_drv;

 /*
	 * An array of which element [a] indicates which port on this
	 * switch should be used to send packets to that are destined
	 * for switch a. Can be NULL if there is only one switch chip.
	 */
 Model1_s8 Model1_rtable[4];
 /*
	 * The lower device this switch uses to talk to the host
	 */
 struct Model1_net_device *Model1_master_netdev;

 /*
	 * Slave mii_bus and devices for the individual ports.
	 */
 Model1_u32 Model1_dsa_port_mask;
 Model1_u32 Model1_cpu_port_mask;
 Model1_u32 Model1_enabled_port_mask;
 Model1_u32 Model1_phys_mii_mask;
 struct Model1_dsa_port Model1_ports[12];
 struct Model1_mii_bus *Model1_slave_mii_bus;
};

static inline __attribute__((no_instrument_function)) bool Model1_dsa_is_cpu_port(struct Model1_dsa_switch *Model1_ds, int Model1_p)
{
 return !!(Model1_ds->Model1_index == Model1_ds->Model1_dst->Model1_cpu_switch && Model1_p == Model1_ds->Model1_dst->Model1_cpu_port);
}

static inline __attribute__((no_instrument_function)) bool Model1_dsa_is_dsa_port(struct Model1_dsa_switch *Model1_ds, int Model1_p)
{
 return !!((Model1_ds->Model1_dsa_port_mask) & (1 << Model1_p));
}

static inline __attribute__((no_instrument_function)) bool Model1_dsa_is_port_initialized(struct Model1_dsa_switch *Model1_ds, int Model1_p)
{
 return Model1_ds->Model1_enabled_port_mask & (1 << Model1_p) && Model1_ds->Model1_ports[Model1_p].Model1_netdev;
}

static inline __attribute__((no_instrument_function)) Model1_u8 Model1_dsa_upstream_port(struct Model1_dsa_switch *Model1_ds)
{
 struct Model1_dsa_switch_tree *Model1_dst = Model1_ds->Model1_dst;

 /*
	 * If this is the root switch (i.e. the switch that connects
	 * to the CPU), return the cpu port number on this switch.
	 * Else return the (DSA) port number that connects to the
	 * switch that is one hop closer to the cpu.
	 */
 if (Model1_dst->Model1_cpu_switch == Model1_ds->Model1_index)
  return Model1_dst->Model1_cpu_port;
 else
  return Model1_ds->Model1_rtable[Model1_dst->Model1_cpu_switch];
}

struct Model1_switchdev_trans;
struct Model1_switchdev_obj;
struct Model1_switchdev_obj_port_fdb;
struct Model1_switchdev_obj_port_vlan;

struct Model1_dsa_switch_driver {
 struct Model1_list_head Model1_list;

 enum Model1_dsa_tag_protocol Model1_tag_protocol;

 /*
	 * Probing and setup.
	 */
 const char *(*Model1_probe)(struct Model1_device *Model1_dsa_dev,
      struct Model1_device *Model1_host_dev, int Model1_sw_addr,
      void **Model1_priv);
 int (*Model1_setup)(struct Model1_dsa_switch *Model1_ds);
 int (*Model1_set_addr)(struct Model1_dsa_switch *Model1_ds, Model1_u8 *Model1_addr);
 Model1_u32 (*Model1_get_phy_flags)(struct Model1_dsa_switch *Model1_ds, int Model1_port);

 /*
	 * Access to the switch's PHY registers.
	 */
 int (*Model1_phy_read)(struct Model1_dsa_switch *Model1_ds, int Model1_port, int Model1_regnum);
 int (*Model1_phy_write)(struct Model1_dsa_switch *Model1_ds, int Model1_port,
        int Model1_regnum, Model1_u16 Model1_val);

 /*
	 * Link state adjustment (called from libphy)
	 */
 void (*Model1_adjust_link)(struct Model1_dsa_switch *Model1_ds, int Model1_port,
    struct Model1_phy_device *Model1_phydev);
 void (*Model1_fixed_link_update)(struct Model1_dsa_switch *Model1_ds, int Model1_port,
    struct Model1_fixed_phy_status *Model1_st);

 /*
	 * ethtool hardware statistics.
	 */
 void (*Model1_get_strings)(struct Model1_dsa_switch *Model1_ds, int Model1_port, Model1_uint8_t *Model1_data);
 void (*Model1_get_ethtool_stats)(struct Model1_dsa_switch *Model1_ds,
         int Model1_port, Model1_uint64_t *Model1_data);
 int (*Model1_get_sset_count)(struct Model1_dsa_switch *Model1_ds);

 /*
	 * ethtool Wake-on-LAN
	 */
 void (*Model1_get_wol)(struct Model1_dsa_switch *Model1_ds, int Model1_port,
      struct Model1_ethtool_wolinfo *Model1_w);
 int (*Model1_set_wol)(struct Model1_dsa_switch *Model1_ds, int Model1_port,
      struct Model1_ethtool_wolinfo *Model1_w);

 /*
	 * Suspend and resume
	 */
 int (*Model1_suspend)(struct Model1_dsa_switch *Model1_ds);
 int (*Model1_resume)(struct Model1_dsa_switch *Model1_ds);

 /*
	 * Port enable/disable
	 */
 int (*Model1_port_enable)(struct Model1_dsa_switch *Model1_ds, int Model1_port,
          struct Model1_phy_device *Model1_phy);
 void (*Model1_port_disable)(struct Model1_dsa_switch *Model1_ds, int Model1_port,
    struct Model1_phy_device *Model1_phy);

 /*
	 * EEE setttings
	 */
 int (*Model1_set_eee)(struct Model1_dsa_switch *Model1_ds, int Model1_port,
      struct Model1_phy_device *Model1_phydev,
      struct Model1_ethtool_eee *Model1_e);
 int (*Model1_get_eee)(struct Model1_dsa_switch *Model1_ds, int Model1_port,
      struct Model1_ethtool_eee *Model1_e);
 /* EEPROM access */
 int (*Model1_get_eeprom_len)(struct Model1_dsa_switch *Model1_ds);
 int (*Model1_get_eeprom)(struct Model1_dsa_switch *Model1_ds,
         struct Model1_ethtool_eeprom *Model1_eeprom, Model1_u8 *Model1_data);
 int (*Model1_set_eeprom)(struct Model1_dsa_switch *Model1_ds,
         struct Model1_ethtool_eeprom *Model1_eeprom, Model1_u8 *Model1_data);

 /*
	 * Register access.
	 */
 int (*Model1_get_regs_len)(struct Model1_dsa_switch *Model1_ds, int Model1_port);
 void (*Model1_get_regs)(struct Model1_dsa_switch *Model1_ds, int Model1_port,
       struct Model1_ethtool_regs *Model1_regs, void *Model1_p);

 /*
	 * Bridge integration
	 */
 int (*Model1_set_ageing_time)(struct Model1_dsa_switch *Model1_ds, unsigned int Model1_msecs);
 int (*Model1_port_bridge_join)(struct Model1_dsa_switch *Model1_ds, int Model1_port,
        struct Model1_net_device *Model1_bridge);
 void (*Model1_port_bridge_leave)(struct Model1_dsa_switch *Model1_ds, int Model1_port);
 void (*Model1_port_stp_state_set)(struct Model1_dsa_switch *Model1_ds, int Model1_port,
          Model1_u8 Model1_state);

 /*
	 * VLAN support
	 */
 int (*Model1_port_vlan_filtering)(struct Model1_dsa_switch *Model1_ds, int Model1_port,
           bool Model1_vlan_filtering);
 int (*Model1_port_vlan_prepare)(struct Model1_dsa_switch *Model1_ds, int Model1_port,
         const struct Model1_switchdev_obj_port_vlan *Model1_vlan,
         struct Model1_switchdev_trans *Model1_trans);
 void (*Model1_port_vlan_add)(struct Model1_dsa_switch *Model1_ds, int Model1_port,
     const struct Model1_switchdev_obj_port_vlan *Model1_vlan,
     struct Model1_switchdev_trans *Model1_trans);
 int (*Model1_port_vlan_del)(struct Model1_dsa_switch *Model1_ds, int Model1_port,
     const struct Model1_switchdev_obj_port_vlan *Model1_vlan);
 int (*Model1_port_vlan_dump)(struct Model1_dsa_switch *Model1_ds, int Model1_port,
      struct Model1_switchdev_obj_port_vlan *Model1_vlan,
      int (*Model1_cb)(struct Model1_switchdev_obj *Model1_obj));

 /*
	 * Forwarding database
	 */
 int (*Model1_port_fdb_prepare)(struct Model1_dsa_switch *Model1_ds, int Model1_port,
        const struct Model1_switchdev_obj_port_fdb *Model1_fdb,
        struct Model1_switchdev_trans *Model1_trans);
 void (*Model1_port_fdb_add)(struct Model1_dsa_switch *Model1_ds, int Model1_port,
    const struct Model1_switchdev_obj_port_fdb *Model1_fdb,
    struct Model1_switchdev_trans *Model1_trans);
 int (*Model1_port_fdb_del)(struct Model1_dsa_switch *Model1_ds, int Model1_port,
    const struct Model1_switchdev_obj_port_fdb *Model1_fdb);
 int (*Model1_port_fdb_dump)(struct Model1_dsa_switch *Model1_ds, int Model1_port,
     struct Model1_switchdev_obj_port_fdb *Model1_fdb,
     int (*Model1_cb)(struct Model1_switchdev_obj *Model1_obj));
};

void Model1_register_switch_driver(struct Model1_dsa_switch_driver *Model1_type);
void Model1_unregister_switch_driver(struct Model1_dsa_switch_driver *Model1_type);
struct Model1_mii_bus *Model1_dsa_host_dev_to_mii_bus(struct Model1_device *Model1_dev);

static inline __attribute__((no_instrument_function)) void *Model1_ds_to_priv(struct Model1_dsa_switch *Model1_ds)
{
 return Model1_ds->Model1_priv;
}

static inline __attribute__((no_instrument_function)) bool Model1_dsa_uses_tagged_protocol(struct Model1_dsa_switch_tree *Model1_dst)
{
 return Model1_dst->Model1_rcv != ((void *)0);
}

void Model1_dsa_unregister_switch(struct Model1_dsa_switch *Model1_ds);
int Model1_dsa_register_switch(struct Model1_dsa_switch *Model1_ds, struct Model1_device_node *Model1_np);



/*
 * netprio_cgroup.h			Control Group Priority set
 *
 *
 * Authors:	Neil Horman <nhorman@tuxdriver.com>
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the Free
 * Software Foundation; either version 2 of the License, or (at your option)
 * any later version.
 *
 */







/*
 *  cgroup interface
 *
 *  Copyright (C) 2003 BULL SA
 *  Copyright (C) 2004-2006 Silicon Graphics, Inc.
 *
 */






/* cgroupstats.h - exporting per-cgroup statistics
 *
 * Copyright IBM Corporation, 2007
 * Author Balbir Singh <balbir@linux.vnet.ibm.com>
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of version 2.1 of the GNU Lesser General Public License
 * as published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it would be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
 */






/* taskstats.h - exporting per-task statistics
 *
 * Copyright (C) Shailabh Nagar, IBM Corp. 2006
 *           (C) Balbir Singh,   IBM Corp. 2006
 *           (C) Jay Lan,        SGI, 2006
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of version 2.1 of the GNU Lesser General Public License
 * as published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it would be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
 */






/* Format for per-task data returned to userland when
 *	- a task exits
 *	- listener requests stats for a task
 *
 * The struct is versioned. Newer versions should only add fields to
 * the bottom of the struct to maintain backward compatibility.
 *
 *
 * To add new fields
 *	a) bump up TASKSTATS_VERSION
 *	b) add comment indicating new version number at end of struct
 *	c) add new fields after version comment; maintain 64-bit alignment
 */






struct Model1_taskstats {

 /* The version number of this struct. This field is always set to
	 * TAKSTATS_VERSION, which is defined in <linux/taskstats.h>.
	 * Each time the struct is changed, the value should be incremented.
	 */
 Model1___u16 Model1_version;
 __u32 Model1_ac_exitcode; /* Exit status */

 /* The accounting flags of a task as defined in <linux/acct.h>
	 * Defined values are AFORK, ASU, ACOMPAT, ACORE, and AXSIG.
	 */
 __u8 Model1_ac_flag; /* Record flags */
 __u8 Model1_ac_nice; /* task_nice */

 /* Delay accounting fields start
	 *
	 * All values, until comment "Delay accounting fields end" are
	 * available only if delay accounting is enabled, even though the last
	 * few fields are not delays
	 *
	 * xxx_count is the number of delay values recorded
	 * xxx_delay_total is the corresponding cumulative delay in nanoseconds
	 *
	 * xxx_delay_total wraps around to zero on overflow
	 * xxx_count incremented regardless of overflow
	 */

 /* Delay waiting for cpu, while runnable
	 * count, delay_total NOT updated atomically
	 */
 __u64 Model1_cpu_count __attribute__((aligned(8)));
 __u64 Model1_cpu_delay_total;

 /* Following four fields atomically updated using task->delays->lock */

 /* Delay waiting for synchronous block I/O to complete
	 * does not account for delays in I/O submission
	 */
 __u64 Model1_blkio_count;
 __u64 Model1_blkio_delay_total;

 /* Delay waiting for page fault I/O (swap in only) */
 __u64 Model1_swapin_count;
 __u64 Model1_swapin_delay_total;

 /* cpu "wall-clock" running time
	 * On some architectures, value will adjust for cpu time stolen
	 * from the kernel in involuntary waits due to virtualization.
	 * Value is cumulative, in nanoseconds, without a corresponding count
	 * and wraps around to zero silently on overflow
	 */
 __u64 Model1_cpu_run_real_total;

 /* cpu "virtual" running time
	 * Uses time intervals seen by the kernel i.e. no adjustment
	 * for kernel's involuntary waits due to virtualization.
	 * Value is cumulative, in nanoseconds, without a corresponding count
	 * and wraps around to zero silently on overflow
	 */
 __u64 Model1_cpu_run_virtual_total;
 /* Delay accounting fields end */
 /* version 1 ends here */

 /* Basic Accounting Fields start */
 char Model1_ac_comm[32]; /* Command name */
 __u8 Model1_ac_sched __attribute__((aligned(8)));
     /* Scheduling discipline */
 __u8 Model1_ac_pad[3];
 __u32 Model1_ac_uid __attribute__((aligned(8)));
     /* User ID */
 __u32 Model1_ac_gid; /* Group ID */
 __u32 Model1_ac_pid; /* Process ID */
 __u32 Model1_ac_ppid; /* Parent process ID */
 __u32 Model1_ac_btime; /* Begin time [sec since 1970] */
 __u64 Model1_ac_etime __attribute__((aligned(8)));
     /* Elapsed time [usec] */
 __u64 Model1_ac_utime; /* User CPU time [usec] */
 __u64 Model1_ac_stime; /* SYstem CPU time [usec] */
 __u64 Model1_ac_minflt; /* Minor Page Fault Count */
 __u64 Model1_ac_majflt; /* Major Page Fault Count */
 /* Basic Accounting Fields end */

 /* Extended accounting fields start */
 /* Accumulated RSS usage in duration of a task, in MBytes-usecs.
	 * The current rss usage is added to this counter every time
	 * a tick is charged to a task's system time. So, at the end we
	 * will have memory usage multiplied by system time. Thus an
	 * average usage per system time unit can be calculated.
	 */
 __u64 Model1_coremem; /* accumulated RSS usage in MB-usec */
 /* Accumulated virtual memory usage in duration of a task.
	 * Same as acct_rss_mem1 above except that we keep track of VM usage.
	 */
 __u64 Model1_virtmem; /* accumulated VM  usage in MB-usec */

 /* High watermark of RSS and virtual memory usage in duration of
	 * a task, in KBytes.
	 */
 __u64 Model1_hiwater_rss; /* High-watermark of RSS usage, in KB */
 __u64 Model1_hiwater_vm; /* High-water VM usage, in KB */

 /* The following four fields are I/O statistics of a task. */
 __u64 Model1_read_char; /* bytes read */
 __u64 Model1_write_char; /* bytes written */
 __u64 Model1_read_syscalls; /* read syscalls */
 __u64 Model1_write_syscalls; /* write syscalls */
 /* Extended accounting fields end */


 /* Per-task storage I/O accounting starts */
 __u64 Model1_read_bytes; /* bytes of read I/O */
 __u64 Model1_write_bytes; /* bytes of write I/O */
 __u64 Model1_cancelled_write_bytes; /* bytes of cancelled write I/O */

 __u64 Model1_nvcsw; /* voluntary_ctxt_switches */
 __u64 Model1_nivcsw; /* nonvoluntary_ctxt_switches */

 /* time accounting for SMT machines */
 __u64 Model1_ac_utimescaled; /* utime scaled on frequency etc */
 __u64 Model1_ac_stimescaled; /* stime scaled on frequency etc */
 __u64 Model1_cpu_scaled_run_real_total; /* scaled cpu_run_real_total */

 /* Delay waiting for memory reclaim */
 __u64 Model1_freepages_count;
 __u64 Model1_freepages_delay_total;
};


/*
 * Commands sent from userspace
 * Not versioned. New commands should only be inserted at the enum's end
 * prior to __TASKSTATS_CMD_MAX
 */

enum {
 Model1_TASKSTATS_CMD_UNSPEC = 0, /* Reserved */
 Model1_TASKSTATS_CMD_GET, /* user->kernel request/get-response */
 Model1_TASKSTATS_CMD_NEW, /* kernel->user event */
 Model1___TASKSTATS_CMD_MAX,
};



enum {
 Model1_TASKSTATS_TYPE_UNSPEC = 0, /* Reserved */
 Model1_TASKSTATS_TYPE_PID, /* Process id */
 Model1_TASKSTATS_TYPE_TGID, /* Thread group id */
 Model1_TASKSTATS_TYPE_STATS, /* taskstats structure */
 Model1_TASKSTATS_TYPE_AGGR_PID, /* contains pid + stats */
 Model1_TASKSTATS_TYPE_AGGR_TGID, /* contains tgid + stats */
 Model1_TASKSTATS_TYPE_NULL, /* contains nothing */
 Model1___TASKSTATS_TYPE_MAX,
};



enum {
 Model1_TASKSTATS_CMD_ATTR_UNSPEC = 0,
 Model1_TASKSTATS_CMD_ATTR_PID,
 Model1_TASKSTATS_CMD_ATTR_TGID,
 Model1_TASKSTATS_CMD_ATTR_REGISTER_CPUMASK,
 Model1_TASKSTATS_CMD_ATTR_DEREGISTER_CPUMASK,
 Model1___TASKSTATS_CMD_ATTR_MAX,
};



/* NETLINK_GENERIC related info */

/*
 * Data shared between user space and kernel space on a per cgroup
 * basis. This data is shared using taskstats.
 *
 * Most of these states are derived by looking at the task->state value
 * For the nr_io_wait state, a flag in the delay accounting structure
 * indicates that the task is waiting on IO
 *
 * Each member is aligned to a 8 byte boundary.
 */
struct Model1_cgroupstats {
 __u64 Model1_nr_sleeping; /* Number of tasks sleeping */
 __u64 Model1_nr_running; /* Number of tasks running */
 __u64 Model1_nr_stopped; /* Number of tasks in stopped state */
 __u64 Model1_nr_uninterruptible; /* Number of tasks in uninterruptible */
     /* state */
 __u64 Model1_nr_io_wait; /* Number of tasks waiting on IO */
};

/*
 * Commands sent from userspace
 * Not versioned. New commands should only be inserted at the enum's end
 * prior to __CGROUPSTATS_CMD_MAX
 */

enum {
 Model1_CGROUPSTATS_CMD_UNSPEC = Model1___TASKSTATS_CMD_MAX, /* Reserved */
 Model1_CGROUPSTATS_CMD_GET, /* user->kernel request/get-response */
 Model1_CGROUPSTATS_CMD_NEW, /* kernel->user event */
 Model1___CGROUPSTATS_CMD_MAX,
};



enum {
 Model1_CGROUPSTATS_TYPE_UNSPEC = 0, /* Reserved */
 Model1_CGROUPSTATS_TYPE_CGROUP_STATS, /* contains name + stats */
 Model1___CGROUPSTATS_TYPE_MAX,
};



enum {
 Model1_CGROUPSTATS_CMD_ATTR_UNSPEC = 0,
 Model1_CGROUPSTATS_CMD_ATTR_FD,
 Model1___CGROUPSTATS_CMD_ATTR_MAX,
};










struct Model1_mnt_namespace;
struct Model1_uts_namespace;
struct Model1_ipc_namespace;
struct Model1_pid_namespace;
struct Model1_cgroup_namespace;
struct Model1_fs_struct;

/*
 * A structure to contain pointers to all per-process
 * namespaces - fs (mount), uts, network, sysvipc, etc.
 *
 * The pid namespace is an exception -- it's accessed using
 * task_active_pid_ns.  The pid namespace here is the
 * namespace that children will use.
 *
 * 'count' is the number of tasks holding a reference.
 * The count for each namespace, then, will be the number
 * of nsproxies pointing to it, not the number of tasks.
 *
 * The nsproxy is shared by tasks which share all namespaces.
 * As soon as a single namespace is cloned or unshared, the
 * nsproxy is copied.
 */
struct Model1_nsproxy {
 Model1_atomic_t Model1_count;
 struct Model1_uts_namespace *Model1_uts_ns;
 struct Model1_ipc_namespace *Model1_ipc_ns;
 struct Model1_mnt_namespace *Model1_mnt_ns;
 struct Model1_pid_namespace *Model1_pid_ns_for_children;
 struct Model1_net *Model1_net_ns;
 struct Model1_cgroup_namespace *Model1_cgroup_ns;
};
extern struct Model1_nsproxy Model1_init_nsproxy;

/*
 * the namespaces access rules are:
 *
 *  1. only current task is allowed to change tsk->nsproxy pointer or
 *     any pointer on the nsproxy itself.  Current must hold the task_lock
 *     when changing tsk->nsproxy.
 *
 *  2. when accessing (i.e. reading) current task's namespaces - no
 *     precautions should be taken - just dereference the pointers
 *
 *  3. the access to other task namespaces is performed like this
 *     task_lock(task);
 *     nsproxy = task->nsproxy;
 *     if (nsproxy != NULL) {
 *             / *
 *               * work with the namespaces here
 *               * e.g. get the reference on one of them
 *               * /
 *     } / *
 *         * NULL task->nsproxy means that this task is
 *         * almost dead (zombie)
 *         * /
 *     task_unlock(task);
 *
 */

int Model1_copy_namespaces(unsigned long Model1_flags, struct Model1_task_struct *Model1_tsk);
void Model1_exit_task_namespaces(struct Model1_task_struct *Model1_tsk);
void Model1_switch_task_namespaces(struct Model1_task_struct *Model1_tsk, struct Model1_nsproxy *Model1_new);
void Model1_free_nsproxy(struct Model1_nsproxy *Model1_ns);
int Model1_unshare_nsproxy_namespaces(unsigned long, struct Model1_nsproxy **,
 struct Model1_cred *, struct Model1_fs_struct *);
int __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_nsproxy_cache_init(void);

static inline __attribute__((no_instrument_function)) void Model1_put_nsproxy(struct Model1_nsproxy *Model1_ns)
{
 if (Model1_atomic_dec_and_test(&Model1_ns->Model1_count)) {
  Model1_free_nsproxy(Model1_ns);
 }
}

static inline __attribute__((no_instrument_function)) void Model1_get_nsproxy(struct Model1_nsproxy *Model1_ns)
{
 Model1_atomic_inc(&Model1_ns->Model1_count);
}



struct Model1_uid_gid_map { /* 64 bytes -- 1 cache line */
 Model1_u32 Model1_nr_extents;
 struct Model1_uid_gid_extent {
  Model1_u32 Model1_first;
  Model1_u32 Model1_lower_first;
  Model1_u32 Model1_count;
 } Model1_extent[5];
};





struct Model1_user_namespace {
 struct Model1_uid_gid_map Model1_uid_map;
 struct Model1_uid_gid_map Model1_gid_map;
 struct Model1_uid_gid_map Model1_projid_map;
 Model1_atomic_t Model1_count;
 struct Model1_user_namespace *Model1_parent;
 int Model1_level;
 Model1_kuid_t Model1_owner;
 Model1_kgid_t Model1_group;
 struct Model1_ns_common Model1_ns;
 unsigned long Model1_flags;

 /* Register of per-UID persistent keyrings for this namespace */




};

extern struct Model1_user_namespace Model1_init_user_ns;
static inline __attribute__((no_instrument_function)) struct Model1_user_namespace *Model1_get_user_ns(struct Model1_user_namespace *Model1_ns)
{
 return &Model1_init_user_ns;
}

static inline __attribute__((no_instrument_function)) int Model1_create_user_ns(struct Model1_cred *Model1_new)
{
 return -22;
}

static inline __attribute__((no_instrument_function)) int Model1_unshare_userns(unsigned long Model1_unshare_flags,
     struct Model1_cred **Model1_new_cred)
{
 if (Model1_unshare_flags & 0x10000000)
  return -22;
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model1_put_user_ns(struct Model1_user_namespace *Model1_ns)
{
}

static inline __attribute__((no_instrument_function)) bool Model1_userns_may_setgroups(const struct Model1_user_namespace *Model1_ns)
{
 return true;
}

static inline __attribute__((no_instrument_function)) bool Model1_current_in_userns(const struct Model1_user_namespace *Model1_target_ns)
{
 return true;
}





/*
 * All weight knobs on the default hierarhcy should use the following min,
 * default and max values.  The default value is the logarithmic center of
 * MIN and MAX and allows 100x to be expressed in both directions.
 */




/* a css_task_iter should be treated as an opaque object */
struct Model1_css_task_iter {
 struct Model1_cgroup_subsys *Model1_ss;

 struct Model1_list_head *Model1_cset_pos;
 struct Model1_list_head *Model1_cset_head;

 struct Model1_list_head *Model1_task_pos;
 struct Model1_list_head *Model1_tasks_head;
 struct Model1_list_head *Model1_mg_tasks_head;

 struct Model1_css_set *Model1_cur_cset;
 struct Model1_task_struct *Model1_cur_task;
 struct Model1_list_head Model1_iters_node; /* css_set->task_iters */
};

extern struct Model1_cgroup_root Model1_cgrp_dfl_root;
extern struct Model1_css_set Model1_init_css_set;



/*
 * List of cgroup subsystems.
 *
 * DO NOT ADD ANY SUBSYSTEM WITHOUT EXPLICIT ACKS FROM CGROUP MAINTAINERS.
 */

/*
 * This file *must* be included with SUBSYS() defined.
 */


extern struct Model1_cgroup_subsys Model1_cpuset_cgrp_subsys;



extern struct Model1_cgroup_subsys Model1_cpu_cgrp_subsys;



extern struct Model1_cgroup_subsys Model1_cpuacct_cgrp_subsys;
extern struct Model1_cgroup_subsys Model1_freezer_cgrp_subsys;
/*
 * The following subsystems are not supported on the default hierarchy.
 */




/*
 * DO NOT ADD ANY SUBSYSTEM WITHOUT EXPLICIT ACKS FROM CGROUP MAINTAINERS.
 */





/*
 * List of cgroup subsystems.
 *
 * DO NOT ADD ANY SUBSYSTEM WITHOUT EXPLICIT ACKS FROM CGROUP MAINTAINERS.
 */

/*
 * This file *must* be included with SUBSYS() defined.
 */


extern struct Model1_static_key_true Model1_cpuset_cgrp_subsys_enabled_key; extern struct Model1_static_key_true Model1_cpuset_cgrp_subsys_on_dfl_key;



extern struct Model1_static_key_true Model1_cpu_cgrp_subsys_enabled_key; extern struct Model1_static_key_true Model1_cpu_cgrp_subsys_on_dfl_key;



extern struct Model1_static_key_true Model1_cpuacct_cgrp_subsys_enabled_key; extern struct Model1_static_key_true Model1_cpuacct_cgrp_subsys_on_dfl_key;
extern struct Model1_static_key_true Model1_freezer_cgrp_subsys_enabled_key; extern struct Model1_static_key_true Model1_freezer_cgrp_subsys_on_dfl_key;
/*
 * The following subsystems are not supported on the default hierarchy.
 */




/*
 * DO NOT ADD ANY SUBSYSTEM WITHOUT EXPLICIT ACKS FROM CGROUP MAINTAINERS.
 */


/**
 * cgroup_subsys_enabled - fast test on whether a subsys is enabled
 * @ss: subsystem in question
 */



/**
 * cgroup_subsys_on_dfl - fast test on whether a subsys is on default hierarchy
 * @ss: subsystem in question
 */



bool Model1_css_has_online_children(struct Model1_cgroup_subsys_state *Model1_css);
struct Model1_cgroup_subsys_state *Model1_css_from_id(int Model1_id, struct Model1_cgroup_subsys *Model1_ss);
struct Model1_cgroup_subsys_state *Model1_cgroup_get_e_css(struct Model1_cgroup *Model1_cgroup,
          struct Model1_cgroup_subsys *Model1_ss);
struct Model1_cgroup_subsys_state *Model1_css_tryget_online_from_dir(struct Model1_dentry *Model1_dentry,
             struct Model1_cgroup_subsys *Model1_ss);

struct Model1_cgroup *Model1_cgroup_get_from_path(const char *Model1_path);
struct Model1_cgroup *Model1_cgroup_get_from_fd(int Model1_fd);

int Model1_cgroup_attach_task_all(struct Model1_task_struct *Model1_from, struct Model1_task_struct *);
int Model1_cgroup_transfer_tasks(struct Model1_cgroup *Model1_to, struct Model1_cgroup *Model1_from);

int Model1_cgroup_add_dfl_cftypes(struct Model1_cgroup_subsys *Model1_ss, struct Model1_cftype *Model1_cfts);
int Model1_cgroup_add_legacy_cftypes(struct Model1_cgroup_subsys *Model1_ss, struct Model1_cftype *Model1_cfts);
int Model1_cgroup_rm_cftypes(struct Model1_cftype *Model1_cfts);
void Model1_cgroup_file_notify(struct Model1_cgroup_file *Model1_cfile);

char *Model1_task_cgroup_path(struct Model1_task_struct *Model1_task, char *Model1_buf, Model1_size_t Model1_buflen);
int Model1_cgroupstats_build(struct Model1_cgroupstats *Model1_stats, struct Model1_dentry *Model1_dentry);
int Model1_proc_cgroup_show(struct Model1_seq_file *Model1_m, struct Model1_pid_namespace *Model1_ns,
       struct Model1_pid *Model1_pid, struct Model1_task_struct *Model1_tsk);

void Model1_cgroup_fork(struct Model1_task_struct *Model1_p);
extern int Model1_cgroup_can_fork(struct Model1_task_struct *Model1_p);
extern void Model1_cgroup_cancel_fork(struct Model1_task_struct *Model1_p);
extern void Model1_cgroup_post_fork(struct Model1_task_struct *Model1_p);
void Model1_cgroup_exit(struct Model1_task_struct *Model1_p);
void Model1_cgroup_free(struct Model1_task_struct *Model1_p);

int Model1_cgroup_init_early(void);
int Model1_cgroup_init(void);

/*
 * Iteration helpers and macros.
 */

struct Model1_cgroup_subsys_state *Model1_css_next_child(struct Model1_cgroup_subsys_state *Model1_pos,
        struct Model1_cgroup_subsys_state *Model1_parent);
struct Model1_cgroup_subsys_state *Model1_css_next_descendant_pre(struct Model1_cgroup_subsys_state *Model1_pos,
          struct Model1_cgroup_subsys_state *Model1_css);
struct Model1_cgroup_subsys_state *Model1_css_rightmost_descendant(struct Model1_cgroup_subsys_state *Model1_pos);
struct Model1_cgroup_subsys_state *Model1_css_next_descendant_post(struct Model1_cgroup_subsys_state *Model1_pos,
           struct Model1_cgroup_subsys_state *Model1_css);

struct Model1_task_struct *Model1_cgroup_taskset_first(struct Model1_cgroup_taskset *Model1_tset,
      struct Model1_cgroup_subsys_state **Model1_dst_cssp);
struct Model1_task_struct *Model1_cgroup_taskset_next(struct Model1_cgroup_taskset *Model1_tset,
     struct Model1_cgroup_subsys_state **Model1_dst_cssp);

void Model1_css_task_iter_start(struct Model1_cgroup_subsys_state *Model1_css,
    struct Model1_css_task_iter *Model1_it);
struct Model1_task_struct *Model1_css_task_iter_next(struct Model1_css_task_iter *Model1_it);
void Model1_css_task_iter_end(struct Model1_css_task_iter *Model1_it);

/**
 * css_for_each_child - iterate through children of a css
 * @pos: the css * to use as the loop cursor
 * @parent: css whose children to walk
 *
 * Walk @parent's children.  Must be called under rcu_read_lock().
 *
 * If a subsystem synchronizes ->css_online() and the start of iteration, a
 * css which finished ->css_online() is guaranteed to be visible in the
 * future iterations and will stay visible until the last reference is put.
 * A css which hasn't finished ->css_online() or already finished
 * ->css_offline() may show up during traversal.  It's each subsystem's
 * responsibility to synchronize against on/offlining.
 *
 * It is allowed to temporarily drop RCU read lock during iteration.  The
 * caller is responsible for ensuring that @pos remains accessible until
 * the start of the next iteration by, for example, bumping the css refcnt.
 */




/**
 * css_for_each_descendant_pre - pre-order walk of a css's descendants
 * @pos: the css * to use as the loop cursor
 * @root: css whose descendants to walk
 *
 * Walk @root's descendants.  @root is included in the iteration and the
 * first node to be visited.  Must be called under rcu_read_lock().
 *
 * If a subsystem synchronizes ->css_online() and the start of iteration, a
 * css which finished ->css_online() is guaranteed to be visible in the
 * future iterations and will stay visible until the last reference is put.
 * A css which hasn't finished ->css_online() or already finished
 * ->css_offline() may show up during traversal.  It's each subsystem's
 * responsibility to synchronize against on/offlining.
 *
 * For example, the following guarantees that a descendant can't escape
 * state updates of its ancestors.
 *
 * my_online(@css)
 * {
 *	Lock @css's parent and @css;
 *	Inherit state from the parent;
 *	Unlock both.
 * }
 *
 * my_update_state(@css)
 * {
 *	css_for_each_descendant_pre(@pos, @css) {
 *		Lock @pos;
 *		if (@pos == @css)
 *			Update @css's state;
 *		else
 *			Verify @pos is alive and inherit state from its parent;
 *		Unlock @pos;
 *	}
 * }
 *
 * As long as the inheriting step, including checking the parent state, is
 * enclosed inside @pos locking, double-locking the parent isn't necessary
 * while inheriting.  The state update to the parent is guaranteed to be
 * visible by walking order and, as long as inheriting operations to the
 * same @pos are atomic to each other, multiple updates racing each other
 * still result in the correct state.  It's guaranateed that at least one
 * inheritance happens for any css after the latest update to its parent.
 *
 * If checking parent's state requires locking the parent, each inheriting
 * iteration should lock and unlock both @pos->parent and @pos.
 *
 * Alternatively, a subsystem may choose to use a single global lock to
 * synchronize ->css_online() and ->css_offline() against tree-walking
 * operations.
 *
 * It is allowed to temporarily drop RCU read lock during iteration.  The
 * caller is responsible for ensuring that @pos remains accessible until
 * the start of the next iteration by, for example, bumping the css refcnt.
 */




/**
 * css_for_each_descendant_post - post-order walk of a css's descendants
 * @pos: the css * to use as the loop cursor
 * @css: css whose descendants to walk
 *
 * Similar to css_for_each_descendant_pre() but performs post-order
 * traversal instead.  @root is included in the iteration and the last
 * node to be visited.
 *
 * If a subsystem synchronizes ->css_online() and the start of iteration, a
 * css which finished ->css_online() is guaranteed to be visible in the
 * future iterations and will stay visible until the last reference is put.
 * A css which hasn't finished ->css_online() or already finished
 * ->css_offline() may show up during traversal.  It's each subsystem's
 * responsibility to synchronize against on/offlining.
 *
 * Note that the walk visibility guarantee example described in pre-order
 * walk doesn't apply the same to post-order walks.
 */




/**
 * cgroup_taskset_for_each - iterate cgroup_taskset
 * @task: the loop cursor
 * @dst_css: the destination css
 * @tset: taskset to iterate
 *
 * @tset may contain multiple tasks and they may belong to multiple
 * processes.
 *
 * On the v2 hierarchy, there may be tasks from multiple processes and they
 * may not share the source or destination csses.
 *
 * On traditional hierarchies, when there are multiple tasks in @tset, if a
 * task of a process is in @tset, all tasks of the process are in @tset.
 * Also, all are guaranteed to share the same source and destination csses.
 *
 * Iteration is not in any specific order.
 */





/**
 * cgroup_taskset_for_each_leader - iterate group leaders in a cgroup_taskset
 * @leader: the loop cursor
 * @dst_css: the destination css
 * @tset: takset to iterate
 *
 * Iterate threadgroup leaders of @tset.  For single-task migrations, @tset
 * may not contain any.
 */
/*
 * Inline functions.
 */

/**
 * css_get - obtain a reference on the specified css
 * @css: target css
 *
 * The caller must already have a reference.
 */
static inline __attribute__((no_instrument_function)) void Model1_css_get(struct Model1_cgroup_subsys_state *Model1_css)
{
 if (!(Model1_css->Model1_flags & Model1_CSS_NO_REF))
  Model1_percpu_ref_get(&Model1_css->Model1_refcnt);
}

/**
 * css_get_many - obtain references on the specified css
 * @css: target css
 * @n: number of references to get
 *
 * The caller must already have a reference.
 */
static inline __attribute__((no_instrument_function)) void Model1_css_get_many(struct Model1_cgroup_subsys_state *Model1_css, unsigned int Model1_n)
{
 if (!(Model1_css->Model1_flags & Model1_CSS_NO_REF))
  Model1_percpu_ref_get_many(&Model1_css->Model1_refcnt, Model1_n);
}

/**
 * css_tryget - try to obtain a reference on the specified css
 * @css: target css
 *
 * Obtain a reference on @css unless it already has reached zero and is
 * being released.  This function doesn't care whether @css is on or
 * offline.  The caller naturally needs to ensure that @css is accessible
 * but doesn't have to be holding a reference on it - IOW, RCU protected
 * access is good enough for this function.  Returns %true if a reference
 * count was successfully obtained; %false otherwise.
 */
static inline __attribute__((no_instrument_function)) bool Model1_css_tryget(struct Model1_cgroup_subsys_state *Model1_css)
{
 if (!(Model1_css->Model1_flags & Model1_CSS_NO_REF))
  return Model1_percpu_ref_tryget(&Model1_css->Model1_refcnt);
 return true;
}

/**
 * css_tryget_online - try to obtain a reference on the specified css if online
 * @css: target css
 *
 * Obtain a reference on @css if it's online.  The caller naturally needs
 * to ensure that @css is accessible but doesn't have to be holding a
 * reference on it - IOW, RCU protected access is good enough for this
 * function.  Returns %true if a reference count was successfully obtained;
 * %false otherwise.
 */
static inline __attribute__((no_instrument_function)) bool Model1_css_tryget_online(struct Model1_cgroup_subsys_state *Model1_css)
{
 if (!(Model1_css->Model1_flags & Model1_CSS_NO_REF))
  return Model1_percpu_ref_tryget_live(&Model1_css->Model1_refcnt);
 return true;
}

/**
 * css_put - put a css reference
 * @css: target css
 *
 * Put a reference obtained via css_get() and css_tryget_online().
 */
static inline __attribute__((no_instrument_function)) void Model1_css_put(struct Model1_cgroup_subsys_state *Model1_css)
{
 if (!(Model1_css->Model1_flags & Model1_CSS_NO_REF))
  Model1_percpu_ref_put(&Model1_css->Model1_refcnt);
}

/**
 * css_put_many - put css references
 * @css: target css
 * @n: number of references to put
 *
 * Put references obtained via css_get() and css_tryget_online().
 */
static inline __attribute__((no_instrument_function)) void Model1_css_put_many(struct Model1_cgroup_subsys_state *Model1_css, unsigned int Model1_n)
{
 if (!(Model1_css->Model1_flags & Model1_CSS_NO_REF))
  Model1_percpu_ref_put_many(&Model1_css->Model1_refcnt, Model1_n);
}

static inline __attribute__((no_instrument_function)) void Model1_cgroup_put(struct Model1_cgroup *Model1_cgrp)
{
 Model1_css_put(&Model1_cgrp->Model1_self);
}

/**
 * task_css_set_check - obtain a task's css_set with extra access conditions
 * @task: the task to obtain css_set for
 * @__c: extra condition expression to be passed to rcu_dereference_check()
 *
 * A task's css_set is RCU protected, initialized and exited while holding
 * task_lock(), and can only be modified while holding both cgroup_mutex
 * and task_lock() while the task is alive.  This macro verifies that the
 * caller is inside proper critical section and returns @task's css_set.
 *
 * The caller can also specify additional allowed conditions via @__c, such
 * as locks used during the cgroup_subsys::attach() methods.
 */
/**
 * task_css_check - obtain css for (task, subsys) w/ extra access conds
 * @task: the target task
 * @subsys_id: the target subsystem ID
 * @__c: extra condition expression to be passed to rcu_dereference_check()
 *
 * Return the cgroup_subsys_state for the (@task, @subsys_id) pair.  The
 * synchronization rules are the same as task_css_set_check().
 */



/**
 * task_css_set - obtain a task's css_set
 * @task: the task to obtain css_set for
 *
 * See task_css_set_check().
 */
static inline __attribute__((no_instrument_function)) struct Model1_css_set *Model1_task_css_set(struct Model1_task_struct *Model1_task)
{
 return ({ typeof(*((Model1_task)->Model1_cgroups)) *Model1_________p1 = (typeof(*((Model1_task)->Model1_cgroups)) *)({ typeof(((Model1_task)->Model1_cgroups)) Model1__________p1 = ({ union { typeof(((Model1_task)->Model1_cgroups)) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&(((Model1_task)->Model1_cgroups)), Model1___u.Model1___c, sizeof(((Model1_task)->Model1_cgroups))); else Model1___read_once_size_nocheck(&(((Model1_task)->Model1_cgroups)), Model1___u.Model1___c, sizeof(((Model1_task)->Model1_cgroups))); Model1___u.Model1___val; }); typeof(*(((Model1_task)->Model1_cgroups))) *Model1____typecheck_p __attribute__((unused)); do { } while (0); (Model1__________p1); }); do { } while (0); ; ((typeof(*((Model1_task)->Model1_cgroups)) *)(Model1_________p1)); });
}

/**
 * task_css - obtain css for (task, subsys)
 * @task: the target task
 * @subsys_id: the target subsystem ID
 *
 * See task_css_check().
 */
static inline __attribute__((no_instrument_function)) struct Model1_cgroup_subsys_state *Model1_task_css(struct Model1_task_struct *Model1_task,
         int Model1_subsys_id)
{
 return ({ typeof(*(((Model1_task))->Model1_cgroups)) *Model1_________p1 = (typeof(*(((Model1_task))->Model1_cgroups)) *)({ typeof((((Model1_task))->Model1_cgroups)) Model1__________p1 = ({ union { typeof((((Model1_task))->Model1_cgroups)) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&((((Model1_task))->Model1_cgroups)), Model1___u.Model1___c, sizeof((((Model1_task))->Model1_cgroups))); else Model1___read_once_size_nocheck(&((((Model1_task))->Model1_cgroups)), Model1___u.Model1___c, sizeof((((Model1_task))->Model1_cgroups))); Model1___u.Model1___val; }); typeof(*((((Model1_task))->Model1_cgroups))) *Model1____typecheck_p __attribute__((unused)); do { } while (0); (Model1__________p1); }); do { } while (0); ; ((typeof(*(((Model1_task))->Model1_cgroups)) *)(Model1_________p1)); })->Model1_subsys[(Model1_subsys_id)];
}

/**
 * task_get_css - find and get the css for (task, subsys)
 * @task: the target task
 * @subsys_id: the target subsystem ID
 *
 * Find the css for the (@task, @subsys_id) combination, increment a
 * reference on and return it.  This function is guaranteed to return a
 * valid css.
 */
static inline __attribute__((no_instrument_function)) struct Model1_cgroup_subsys_state *
Model1_task_get_css(struct Model1_task_struct *Model1_task, int Model1_subsys_id)
{
 struct Model1_cgroup_subsys_state *Model1_css;

 Model1_rcu_read_lock();
 while (true) {
  Model1_css = Model1_task_css(Model1_task, Model1_subsys_id);
  if (__builtin_expect(!!(Model1_css_tryget_online(Model1_css)), 1))
   break;
  Model1_cpu_relax();
 }
 Model1_rcu_read_unlock();
 return Model1_css;
}

/**
 * task_css_is_root - test whether a task belongs to the root css
 * @task: the target task
 * @subsys_id: the target subsystem ID
 *
 * Test whether @task belongs to the root css on the specified subsystem.
 * May be invoked in any context.
 */
static inline __attribute__((no_instrument_function)) bool Model1_task_css_is_root(struct Model1_task_struct *Model1_task, int Model1_subsys_id)
{
 return ({ typeof(*(((Model1_task))->Model1_cgroups)) *Model1_________p1 = (typeof(*(((Model1_task))->Model1_cgroups)) *)({ typeof((((Model1_task))->Model1_cgroups)) Model1__________p1 = ({ union { typeof((((Model1_task))->Model1_cgroups)) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&((((Model1_task))->Model1_cgroups)), Model1___u.Model1___c, sizeof((((Model1_task))->Model1_cgroups))); else Model1___read_once_size_nocheck(&((((Model1_task))->Model1_cgroups)), Model1___u.Model1___c, sizeof((((Model1_task))->Model1_cgroups))); Model1___u.Model1___val; }); typeof(*((((Model1_task))->Model1_cgroups))) *Model1____typecheck_p __attribute__((unused)); do { } while (0); (Model1__________p1); }); do { } while (0); ; ((typeof(*(((Model1_task))->Model1_cgroups)) *)(Model1_________p1)); })->Model1_subsys[(Model1_subsys_id)] ==
  Model1_init_css_set.Model1_subsys[Model1_subsys_id];
}

static inline __attribute__((no_instrument_function)) struct Model1_cgroup *Model1_task_cgroup(struct Model1_task_struct *Model1_task,
      int Model1_subsys_id)
{
 return Model1_task_css(Model1_task, Model1_subsys_id)->Model1_cgroup;
}

/**
 * cgroup_is_descendant - test ancestry
 * @cgrp: the cgroup to be tested
 * @ancestor: possible ancestor of @cgrp
 *
 * Test whether @cgrp is a descendant of @ancestor.  It also returns %true
 * if @cgrp == @ancestor.  This function is safe to call as long as @cgrp
 * and @ancestor are accessible.
 */
static inline __attribute__((no_instrument_function)) bool Model1_cgroup_is_descendant(struct Model1_cgroup *Model1_cgrp,
     struct Model1_cgroup *Model1_ancestor)
{
 if (Model1_cgrp->Model1_root != Model1_ancestor->Model1_root || Model1_cgrp->Model1_level < Model1_ancestor->Model1_level)
  return false;
 return Model1_cgrp->Model1_ancestor_ids[Model1_ancestor->Model1_level] == Model1_ancestor->Model1_id;
}

/* no synchronization, the result can only be used as a hint */
static inline __attribute__((no_instrument_function)) bool Model1_cgroup_is_populated(struct Model1_cgroup *Model1_cgrp)
{
 return Model1_cgrp->Model1_populated_cnt;
}

/* returns ino associated with a cgroup */
static inline __attribute__((no_instrument_function)) Model1_ino_t Model1_cgroup_ino(struct Model1_cgroup *Model1_cgrp)
{
 return Model1_cgrp->Model1_kn->Model1_ino;
}

/* cft/css accessors for cftype->write() operation */
static inline __attribute__((no_instrument_function)) struct Model1_cftype *Model1_of_cft(struct Model1_kernfs_open_file *Model1_of)
{
 return Model1_of->Model1_kn->Model1_priv;
}

struct Model1_cgroup_subsys_state *Model1_of_css(struct Model1_kernfs_open_file *Model1_of);

/* cft/css accessors for cftype->seq_*() operations */
static inline __attribute__((no_instrument_function)) struct Model1_cftype *Model1_seq_cft(struct Model1_seq_file *Model1_seq)
{
 return Model1_of_cft(Model1_seq->Model1_private);
}

static inline __attribute__((no_instrument_function)) struct Model1_cgroup_subsys_state *Model1_seq_css(struct Model1_seq_file *Model1_seq)
{
 return Model1_of_css(Model1_seq->Model1_private);
}

/*
 * Name / path handling functions.  All are thin wrappers around the kernfs
 * counterparts and can be called under any context.
 */

static inline __attribute__((no_instrument_function)) int Model1_cgroup_name(struct Model1_cgroup *Model1_cgrp, char *Model1_buf, Model1_size_t Model1_buflen)
{
 return Model1_kernfs_name(Model1_cgrp->Model1_kn, Model1_buf, Model1_buflen);
}

static inline __attribute__((no_instrument_function)) char * __attribute__((warn_unused_result)) Model1_cgroup_path(struct Model1_cgroup *Model1_cgrp, char *Model1_buf,
           Model1_size_t Model1_buflen)
{
 return Model1_kernfs_path(Model1_cgrp->Model1_kn, Model1_buf, Model1_buflen);
}

static inline __attribute__((no_instrument_function)) void Model1_pr_cont_cgroup_name(struct Model1_cgroup *Model1_cgrp)
{
 Model1_pr_cont_kernfs_name(Model1_cgrp->Model1_kn);
}

static inline __attribute__((no_instrument_function)) void Model1_pr_cont_cgroup_path(struct Model1_cgroup *Model1_cgrp)
{
 Model1_pr_cont_kernfs_path(Model1_cgrp->Model1_kn);
}
/*
 * sock->sk_cgrp_data handling.  For more info, see sock_cgroup_data
 * definition in cgroup-defs.h.
 */
static inline __attribute__((no_instrument_function)) void Model1_cgroup_sk_alloc(struct Model1_sock_cgroup_data *Model1_skcd) {}
static inline __attribute__((no_instrument_function)) void Model1_cgroup_sk_free(struct Model1_sock_cgroup_data *Model1_skcd) {}



struct Model1_cgroup_namespace {
 Model1_atomic_t Model1_count;
 struct Model1_ns_common Model1_ns;
 struct Model1_user_namespace *Model1_user_ns;
 struct Model1_css_set *Model1_root_cset;
};

extern struct Model1_cgroup_namespace Model1_init_cgroup_ns;



void Model1_free_cgroup_ns(struct Model1_cgroup_namespace *Model1_ns);

struct Model1_cgroup_namespace *Model1_copy_cgroup_ns(unsigned long Model1_flags,
     struct Model1_user_namespace *Model1_user_ns,
     struct Model1_cgroup_namespace *Model1_old_ns);

char *Model1_cgroup_path_ns(struct Model1_cgroup *Model1_cgrp, char *Model1_buf, Model1_size_t Model1_buflen,
       struct Model1_cgroup_namespace *Model1_ns);
static inline __attribute__((no_instrument_function)) void Model1_get_cgroup_ns(struct Model1_cgroup_namespace *Model1_ns)
{
 if (Model1_ns)
  Model1_atomic_inc(&Model1_ns->Model1_count);
}

static inline __attribute__((no_instrument_function)) void Model1_put_cgroup_ns(struct Model1_cgroup_namespace *Model1_ns)
{
 if (Model1_ns && Model1_atomic_dec_and_test(&Model1_ns->Model1_count))
  Model1_free_cgroup_ns(Model1_ns);
}
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_task_netprioidx(struct Model1_task_struct *Model1_p)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model1_sock_update_netprioidx(struct Model1_sock_cgroup_data *Model1_skcd)
{
}


















/*
 * Linux Security plug
 *
 * Copyright (C) 2001 WireX Communications, Inc <chris@wirex.com>
 * Copyright (C) 2001 Greg Kroah-Hartman <greg@kroah.com>
 * Copyright (C) 2001 Networks Associates Technology, Inc <ssmalley@nai.com>
 * Copyright (C) 2001 James Morris <jmorris@intercode.com.au>
 * Copyright (C) 2001 Silicon Graphics, Inc. (Trust Technology Group)
 *
 *	This program is free software; you can redistribute it and/or modify
 *	it under the terms of the GNU General Public License as published by
 *	the Free Software Foundation; either version 2 of the License, or
 *	(at your option) any later version.
 *
 *	Due to this file being licensed under the GPL there is controversy over
 *	whether this permits you to write a module that #includes this file
 *	without placing your module under the GPL.  Please consult a lawyer for
 *	advice before doing this.
 *
 */
struct Model1_linux_binprm;
struct Model1_cred;
struct Model1_rlimit;
struct Model1_siginfo;
struct Model1_sem_array;
struct Model1_sembuf;
struct Model1_kern_ipc_perm;
struct Model1_audit_context;
struct Model1_super_block;
struct Model1_inode;
struct Model1_dentry;
struct Model1_file;
struct Model1_vfsmount;
struct Model1_path;
struct Model1_qstr;
struct Model1_iattr;
struct Model1_fown_struct;
struct Model1_file_operations;
struct Model1_shmid_kernel;
struct Model1_msg_msg;
struct Model1_msg_queue;
struct Model1_xattr;
struct Model1_xfrm_sec_ctx;
struct Model1_mm_struct;

/* If capable should audit the security request */



/* LSM Agnostic defines for sb_set_mnt_opts */


struct Model1_ctl_table;
struct Model1_audit_krule;
struct Model1_user_namespace;
struct Model1_timezone;

/* These functions are in security/commoncap.c */
extern int Model1_cap_capable(const struct Model1_cred *Model1_cred, struct Model1_user_namespace *Model1_ns,
         int Model1_cap, int Model1_audit);
extern int Model1_cap_settime(const struct Model1_timespec *Model1_ts, const struct Model1_timezone *Model1_tz);
extern int Model1_cap_ptrace_access_check(struct Model1_task_struct *Model1_child, unsigned int Model1_mode);
extern int Model1_cap_ptrace_traceme(struct Model1_task_struct *Model1_parent);
extern int Model1_cap_capget(struct Model1_task_struct *Model1_target, Model1_kernel_cap_t *Model1_effective, Model1_kernel_cap_t *Model1_inheritable, Model1_kernel_cap_t *Model1_permitted);
extern int Model1_cap_capset(struct Model1_cred *Model1_new, const struct Model1_cred *old,
        const Model1_kernel_cap_t *Model1_effective,
        const Model1_kernel_cap_t *Model1_inheritable,
        const Model1_kernel_cap_t *Model1_permitted);
extern int Model1_cap_bprm_set_creds(struct Model1_linux_binprm *Model1_bprm);
extern int Model1_cap_bprm_secureexec(struct Model1_linux_binprm *Model1_bprm);
extern int Model1_cap_inode_setxattr(struct Model1_dentry *Model1_dentry, const char *Model1_name,
         const void *Model1_value, Model1_size_t Model1_size, int Model1_flags);
extern int Model1_cap_inode_removexattr(struct Model1_dentry *Model1_dentry, const char *Model1_name);
extern int Model1_cap_inode_need_killpriv(struct Model1_dentry *Model1_dentry);
extern int Model1_cap_inode_killpriv(struct Model1_dentry *Model1_dentry);
extern int Model1_cap_mmap_addr(unsigned long Model1_addr);
extern int Model1_cap_mmap_file(struct Model1_file *Model1_file, unsigned long Model1_reqprot,
    unsigned long Model1_prot, unsigned long Model1_flags);
extern int Model1_cap_task_fix_setuid(struct Model1_cred *Model1_new, const struct Model1_cred *old, int Model1_flags);
extern int Model1_cap_task_prctl(int Model1_option, unsigned long Model1_arg2, unsigned long Model1_arg3,
     unsigned long Model1_arg4, unsigned long Model1_arg5);
extern int Model1_cap_task_setscheduler(struct Model1_task_struct *Model1_p);
extern int Model1_cap_task_setioprio(struct Model1_task_struct *Model1_p, int Model1_ioprio);
extern int Model1_cap_task_setnice(struct Model1_task_struct *Model1_p, int Model1_nice);
extern int Model1_cap_vm_enough_memory(struct Model1_mm_struct *Model1_mm, long Model1_pages);

struct Model1_msghdr;
struct Model1_sk_buff;
struct Model1_sock;
struct Model1_sockaddr;
struct Model1_socket;
struct Model1_flowi;
struct Model1_dst_entry;
struct Model1_xfrm_selector;
struct Model1_xfrm_policy;
struct Model1_xfrm_state;
struct Model1_xfrm_user_sec_ctx;
struct Model1_seq_file;


extern unsigned long Model1_mmap_min_addr;
extern unsigned long Model1_dac_mmap_min_addr;





/*
 * Values used in the task_security_ops calls
 */
/* setuid or setgid, id0 == uid or gid */


/* setreuid or setregid, id0 == real, id1 == eff */


/* setresuid or setresgid, id0 == real, id1 == eff, uid2 == saved */


/* setfsuid or setfsgid, id0 == fsuid or fsgid */


/* forward declares to avoid warnings */
struct Model1_sched_param;
struct Model1_request_sock;

/* bprm->unsafe reasons */






extern int Model1_mmap_min_addr_handler(struct Model1_ctl_table *Model1_table, int Model1_write,
     void *Model1_buffer, Model1_size_t *Model1_lenp, Model1_loff_t *Model1_ppos);


/* security_inode_init_security callback function to write xattrs */
typedef int (*Model1_initxattrs) (struct Model1_inode *Model1_inode,
      const struct Model1_xattr *Model1_xattr_array, void *Model1_fs_data);



struct Model1_security_mnt_opts {
 char **Model1_mnt_opts;
 int *Model1_mnt_opts_flags;
 int Model1_num_mnt_opts;
};

static inline __attribute__((no_instrument_function)) void Model1_security_init_mnt_opts(struct Model1_security_mnt_opts *Model1_opts)
{
 Model1_opts->Model1_mnt_opts = ((void *)0);
 Model1_opts->Model1_mnt_opts_flags = ((void *)0);
 Model1_opts->Model1_num_mnt_opts = 0;
}

static inline __attribute__((no_instrument_function)) void Model1_security_free_mnt_opts(struct Model1_security_mnt_opts *Model1_opts)
{
 int Model1_i;
 if (Model1_opts->Model1_mnt_opts)
  for (Model1_i = 0; Model1_i < Model1_opts->Model1_num_mnt_opts; Model1_i++)
   Model1_kfree(Model1_opts->Model1_mnt_opts[Model1_i]);
 Model1_kfree(Model1_opts->Model1_mnt_opts);
 Model1_opts->Model1_mnt_opts = ((void *)0);
 Model1_kfree(Model1_opts->Model1_mnt_opts_flags);
 Model1_opts->Model1_mnt_opts_flags = ((void *)0);
 Model1_opts->Model1_num_mnt_opts = 0;
}

/* prototypes */
extern int Model1_security_init(void);

/* Security operations */
int Model1_security_binder_set_context_mgr(struct Model1_task_struct *Model1_mgr);
int Model1_security_binder_transaction(struct Model1_task_struct *Model1_from,
    struct Model1_task_struct *Model1_to);
int Model1_security_binder_transfer_binder(struct Model1_task_struct *Model1_from,
        struct Model1_task_struct *Model1_to);
int Model1_security_binder_transfer_file(struct Model1_task_struct *Model1_from,
      struct Model1_task_struct *Model1_to, struct Model1_file *Model1_file);
int Model1_security_ptrace_access_check(struct Model1_task_struct *Model1_child, unsigned int Model1_mode);
int Model1_security_ptrace_traceme(struct Model1_task_struct *Model1_parent);
int Model1_security_capget(struct Model1_task_struct *Model1_target,
      Model1_kernel_cap_t *Model1_effective,
      Model1_kernel_cap_t *Model1_inheritable,
      Model1_kernel_cap_t *Model1_permitted);
int Model1_security_capset(struct Model1_cred *Model1_new, const struct Model1_cred *old,
      const Model1_kernel_cap_t *Model1_effective,
      const Model1_kernel_cap_t *Model1_inheritable,
      const Model1_kernel_cap_t *Model1_permitted);
int Model1_security_capable(const struct Model1_cred *Model1_cred, struct Model1_user_namespace *Model1_ns,
   int Model1_cap);
int Model1_security_capable_noaudit(const struct Model1_cred *Model1_cred, struct Model1_user_namespace *Model1_ns,
        int Model1_cap);
int Model1_security_quotactl(int Model1_cmds, int Model1_type, int Model1_id, struct Model1_super_block *Model1_sb);
int Model1_security_quota_on(struct Model1_dentry *Model1_dentry);
int Model1_security_syslog(int Model1_type);
int Model1_security_settime64(const struct Model1_timespec *Model1_ts, const struct Model1_timezone *Model1_tz);
static inline __attribute__((no_instrument_function)) int Model1_security_settime(const struct Model1_timespec *Model1_ts, const struct Model1_timezone *Model1_tz)
{
 struct Model1_timespec Model1_ts64 = Model1_timespec_to_timespec64(*Model1_ts);

 return Model1_security_settime64(&Model1_ts64, Model1_tz);
}
int Model1_security_vm_enough_memory_mm(struct Model1_mm_struct *Model1_mm, long Model1_pages);
int Model1_security_bprm_set_creds(struct Model1_linux_binprm *Model1_bprm);
int Model1_security_bprm_check(struct Model1_linux_binprm *Model1_bprm);
void Model1_security_bprm_committing_creds(struct Model1_linux_binprm *Model1_bprm);
void Model1_security_bprm_committed_creds(struct Model1_linux_binprm *Model1_bprm);
int Model1_security_bprm_secureexec(struct Model1_linux_binprm *Model1_bprm);
int Model1_security_sb_alloc(struct Model1_super_block *Model1_sb);
void Model1_security_sb_free(struct Model1_super_block *Model1_sb);
int Model1_security_sb_copy_data(char *Model1_orig, char *Model1_copy);
int Model1_security_sb_remount(struct Model1_super_block *Model1_sb, void *Model1_data);
int Model1_security_sb_kern_mount(struct Model1_super_block *Model1_sb, int Model1_flags, void *Model1_data);
int Model1_security_sb_show_options(struct Model1_seq_file *Model1_m, struct Model1_super_block *Model1_sb);
int Model1_security_sb_statfs(struct Model1_dentry *Model1_dentry);
int Model1_security_sb_mount(const char *Model1_dev_name, const struct Model1_path *Model1_path,
        const char *Model1_type, unsigned long Model1_flags, void *Model1_data);
int Model1_security_sb_umount(struct Model1_vfsmount *Model1_mnt, int Model1_flags);
int Model1_security_sb_pivotroot(const struct Model1_path *Model1_old_path, const struct Model1_path *Model1_new_path);
int Model1_security_sb_set_mnt_opts(struct Model1_super_block *Model1_sb,
    struct Model1_security_mnt_opts *Model1_opts,
    unsigned long Model1_kern_flags,
    unsigned long *Model1_set_kern_flags);
int Model1_security_sb_clone_mnt_opts(const struct Model1_super_block *Model1_oldsb,
    struct Model1_super_block *Model1_newsb);
int Model1_security_sb_parse_opts_str(char *Model1_options, struct Model1_security_mnt_opts *Model1_opts);
int Model1_security_dentry_init_security(struct Model1_dentry *Model1_dentry, int Model1_mode,
     const struct Model1_qstr *Model1_name, void **Model1_ctx,
     Model1_u32 *Model1_ctxlen);

int Model1_security_inode_alloc(struct Model1_inode *Model1_inode);
void Model1_security_inode_free(struct Model1_inode *Model1_inode);
int Model1_security_inode_init_security(struct Model1_inode *Model1_inode, struct Model1_inode *Model1_dir,
     const struct Model1_qstr *Model1_qstr,
     Model1_initxattrs Model1_initxattrs, void *Model1_fs_data);
int Model1_security_old_inode_init_security(struct Model1_inode *Model1_inode, struct Model1_inode *Model1_dir,
         const struct Model1_qstr *Model1_qstr, const char **Model1_name,
         void **Model1_value, Model1_size_t *Model1_len);
int Model1_security_inode_create(struct Model1_inode *Model1_dir, struct Model1_dentry *Model1_dentry, Model1_umode_t Model1_mode);
int Model1_security_inode_link(struct Model1_dentry *Model1_old_dentry, struct Model1_inode *Model1_dir,
    struct Model1_dentry *Model1_new_dentry);
int Model1_security_inode_unlink(struct Model1_inode *Model1_dir, struct Model1_dentry *Model1_dentry);
int Model1_security_inode_symlink(struct Model1_inode *Model1_dir, struct Model1_dentry *Model1_dentry,
      const char *Model1_old_name);
int Model1_security_inode_mkdir(struct Model1_inode *Model1_dir, struct Model1_dentry *Model1_dentry, Model1_umode_t Model1_mode);
int Model1_security_inode_rmdir(struct Model1_inode *Model1_dir, struct Model1_dentry *Model1_dentry);
int Model1_security_inode_mknod(struct Model1_inode *Model1_dir, struct Model1_dentry *Model1_dentry, Model1_umode_t Model1_mode, Model1_dev_t Model1_dev);
int Model1_security_inode_rename(struct Model1_inode *Model1_old_dir, struct Model1_dentry *Model1_old_dentry,
     struct Model1_inode *Model1_new_dir, struct Model1_dentry *Model1_new_dentry,
     unsigned int Model1_flags);
int Model1_security_inode_readlink(struct Model1_dentry *Model1_dentry);
int Model1_security_inode_follow_link(struct Model1_dentry *Model1_dentry, struct Model1_inode *Model1_inode,
          bool Model1_rcu);
int Model1_security_inode_permission(struct Model1_inode *Model1_inode, int Model1_mask);
int Model1_security_inode_setattr(struct Model1_dentry *Model1_dentry, struct Model1_iattr *Model1_attr);
int Model1_security_inode_getattr(const struct Model1_path *Model1_path);
int Model1_security_inode_setxattr(struct Model1_dentry *Model1_dentry, const char *Model1_name,
       const void *Model1_value, Model1_size_t Model1_size, int Model1_flags);
void Model1_security_inode_post_setxattr(struct Model1_dentry *Model1_dentry, const char *Model1_name,
      const void *Model1_value, Model1_size_t Model1_size, int Model1_flags);
int Model1_security_inode_getxattr(struct Model1_dentry *Model1_dentry, const char *Model1_name);
int Model1_security_inode_listxattr(struct Model1_dentry *Model1_dentry);
int Model1_security_inode_removexattr(struct Model1_dentry *Model1_dentry, const char *Model1_name);
int Model1_security_inode_need_killpriv(struct Model1_dentry *Model1_dentry);
int Model1_security_inode_killpriv(struct Model1_dentry *Model1_dentry);
int Model1_security_inode_getsecurity(struct Model1_inode *Model1_inode, const char *Model1_name, void **Model1_buffer, bool Model1_alloc);
int Model1_security_inode_setsecurity(struct Model1_inode *Model1_inode, const char *Model1_name, const void *Model1_value, Model1_size_t Model1_size, int Model1_flags);
int Model1_security_inode_listsecurity(struct Model1_inode *Model1_inode, char *Model1_buffer, Model1_size_t Model1_buffer_size);
void Model1_security_inode_getsecid(struct Model1_inode *Model1_inode, Model1_u32 *Model1_secid);
int Model1_security_file_permission(struct Model1_file *Model1_file, int Model1_mask);
int Model1_security_file_alloc(struct Model1_file *Model1_file);
void Model1_security_file_free(struct Model1_file *Model1_file);
int Model1_security_file_ioctl(struct Model1_file *Model1_file, unsigned int Model1_cmd, unsigned long Model1_arg);
int Model1_security_mmap_file(struct Model1_file *Model1_file, unsigned long Model1_prot,
   unsigned long Model1_flags);
int Model1_security_mmap_addr(unsigned long Model1_addr);
int Model1_security_file_mprotect(struct Model1_vm_area_struct *Model1_vma, unsigned long Model1_reqprot,
      unsigned long Model1_prot);
int Model1_security_file_lock(struct Model1_file *Model1_file, unsigned int Model1_cmd);
int Model1_security_file_fcntl(struct Model1_file *Model1_file, unsigned int Model1_cmd, unsigned long Model1_arg);
void Model1_security_file_set_fowner(struct Model1_file *Model1_file);
int Model1_security_file_send_sigiotask(struct Model1_task_struct *Model1_tsk,
     struct Model1_fown_struct *Model1_fown, int Model1_sig);
int Model1_security_file_receive(struct Model1_file *Model1_file);
int Model1_security_file_open(struct Model1_file *Model1_file, const struct Model1_cred *Model1_cred);
int Model1_security_task_create(unsigned long Model1_clone_flags);
void Model1_security_task_free(struct Model1_task_struct *Model1_task);
int Model1_security_cred_alloc_blank(struct Model1_cred *Model1_cred, Model1_gfp_t Model1_gfp);
void Model1_security_cred_free(struct Model1_cred *Model1_cred);
int Model1_security_prepare_creds(struct Model1_cred *Model1_new, const struct Model1_cred *old, Model1_gfp_t Model1_gfp);
void Model1_security_transfer_creds(struct Model1_cred *Model1_new, const struct Model1_cred *old);
int Model1_security_kernel_act_as(struct Model1_cred *Model1_new, Model1_u32 Model1_secid);
int Model1_security_kernel_create_files_as(struct Model1_cred *Model1_new, struct Model1_inode *Model1_inode);
int Model1_security_kernel_module_request(char *Model1_kmod_name);
int Model1_security_kernel_module_from_file(struct Model1_file *Model1_file);
int Model1_security_kernel_read_file(struct Model1_file *Model1_file, enum Model1_kernel_read_file_id Model1_id);
int Model1_security_kernel_post_read_file(struct Model1_file *Model1_file, char *Model1_buf, Model1_loff_t Model1_size,
       enum Model1_kernel_read_file_id Model1_id);
int Model1_security_task_fix_setuid(struct Model1_cred *Model1_new, const struct Model1_cred *old,
        int Model1_flags);
int Model1_security_task_setpgid(struct Model1_task_struct *Model1_p, Model1_pid_t Model1_pgid);
int Model1_security_task_getpgid(struct Model1_task_struct *Model1_p);
int Model1_security_task_getsid(struct Model1_task_struct *Model1_p);
void Model1_security_task_getsecid(struct Model1_task_struct *Model1_p, Model1_u32 *Model1_secid);
int Model1_security_task_setnice(struct Model1_task_struct *Model1_p, int Model1_nice);
int Model1_security_task_setioprio(struct Model1_task_struct *Model1_p, int Model1_ioprio);
int Model1_security_task_getioprio(struct Model1_task_struct *Model1_p);
int Model1_security_task_setrlimit(struct Model1_task_struct *Model1_p, unsigned int Model1_resource,
  struct Model1_rlimit *Model1_new_rlim);
int Model1_security_task_setscheduler(struct Model1_task_struct *Model1_p);
int Model1_security_task_getscheduler(struct Model1_task_struct *Model1_p);
int Model1_security_task_movememory(struct Model1_task_struct *Model1_p);
int Model1_security_task_kill(struct Model1_task_struct *Model1_p, struct Model1_siginfo *Model1_info,
   int Model1_sig, Model1_u32 Model1_secid);
int Model1_security_task_wait(struct Model1_task_struct *Model1_p);
int Model1_security_task_prctl(int Model1_option, unsigned long Model1_arg2, unsigned long Model1_arg3,
   unsigned long Model1_arg4, unsigned long Model1_arg5);
void Model1_security_task_to_inode(struct Model1_task_struct *Model1_p, struct Model1_inode *Model1_inode);
int Model1_security_ipc_permission(struct Model1_kern_ipc_perm *Model1_ipcp, short Model1_flag);
void Model1_security_ipc_getsecid(struct Model1_kern_ipc_perm *Model1_ipcp, Model1_u32 *Model1_secid);
int Model1_security_msg_msg_alloc(struct Model1_msg_msg *Model1_msg);
void Model1_security_msg_msg_free(struct Model1_msg_msg *Model1_msg);
int Model1_security_msg_queue_alloc(struct Model1_msg_queue *Model1_msq);
void Model1_security_msg_queue_free(struct Model1_msg_queue *Model1_msq);
int Model1_security_msg_queue_associate(struct Model1_msg_queue *Model1_msq, int Model1_msqflg);
int Model1_security_msg_queue_msgctl(struct Model1_msg_queue *Model1_msq, int Model1_cmd);
int Model1_security_msg_queue_msgsnd(struct Model1_msg_queue *Model1_msq,
         struct Model1_msg_msg *Model1_msg, int Model1_msqflg);
int Model1_security_msg_queue_msgrcv(struct Model1_msg_queue *Model1_msq, struct Model1_msg_msg *Model1_msg,
         struct Model1_task_struct *Model1_target, long Model1_type, int Model1_mode);
int Model1_security_shm_alloc(struct Model1_shmid_kernel *Model1_shp);
void Model1_security_shm_free(struct Model1_shmid_kernel *Model1_shp);
int Model1_security_shm_associate(struct Model1_shmid_kernel *Model1_shp, int Model1_shmflg);
int Model1_security_shm_shmctl(struct Model1_shmid_kernel *Model1_shp, int Model1_cmd);
int Model1_security_shm_shmat(struct Model1_shmid_kernel *Model1_shp, char *Model1_shmaddr, int Model1_shmflg);
int Model1_security_sem_alloc(struct Model1_sem_array *Model1_sma);
void Model1_security_sem_free(struct Model1_sem_array *Model1_sma);
int Model1_security_sem_associate(struct Model1_sem_array *Model1_sma, int Model1_semflg);
int Model1_security_sem_semctl(struct Model1_sem_array *Model1_sma, int Model1_cmd);
int Model1_security_sem_semop(struct Model1_sem_array *Model1_sma, struct Model1_sembuf *Model1_sops,
   unsigned Model1_nsops, int Model1_alter);
void Model1_security_d_instantiate(struct Model1_dentry *Model1_dentry, struct Model1_inode *Model1_inode);
int Model1_security_getprocattr(struct Model1_task_struct *Model1_p, char *Model1_name, char **Model1_value);
int Model1_security_setprocattr(struct Model1_task_struct *Model1_p, char *Model1_name, void *Model1_value, Model1_size_t Model1_size);
int Model1_security_netlink_send(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);
int Model1_security_ismaclabel(const char *Model1_name);
int Model1_security_secid_to_secctx(Model1_u32 Model1_secid, char **Model1_secdata, Model1_u32 *Model1_seclen);
int Model1_security_secctx_to_secid(const char *Model1_secdata, Model1_u32 Model1_seclen, Model1_u32 *Model1_secid);
void Model1_security_release_secctx(char *Model1_secdata, Model1_u32 Model1_seclen);

void Model1_security_inode_invalidate_secctx(struct Model1_inode *Model1_inode);
int Model1_security_inode_notifysecctx(struct Model1_inode *Model1_inode, void *Model1_ctx, Model1_u32 Model1_ctxlen);
int Model1_security_inode_setsecctx(struct Model1_dentry *Model1_dentry, void *Model1_ctx, Model1_u32 Model1_ctxlen);
int Model1_security_inode_getsecctx(struct Model1_inode *Model1_inode, void **Model1_ctx, Model1_u32 *Model1_ctxlen);
int Model1_security_unix_stream_connect(struct Model1_sock *Model1_sock, struct Model1_sock *Model1_other, struct Model1_sock *Model1_newsk);
int Model1_security_unix_may_send(struct Model1_socket *Model1_sock, struct Model1_socket *Model1_other);
int Model1_security_socket_create(int Model1_family, int Model1_type, int Model1_protocol, int Model1_kern);
int Model1_security_socket_post_create(struct Model1_socket *Model1_sock, int Model1_family,
    int Model1_type, int Model1_protocol, int Model1_kern);
int Model1_security_socket_bind(struct Model1_socket *Model1_sock, struct Model1_sockaddr *Model1_address, int Model1_addrlen);
int Model1_security_socket_connect(struct Model1_socket *Model1_sock, struct Model1_sockaddr *Model1_address, int Model1_addrlen);
int Model1_security_socket_listen(struct Model1_socket *Model1_sock, int Model1_backlog);
int Model1_security_socket_accept(struct Model1_socket *Model1_sock, struct Model1_socket *Model1_newsock);
int Model1_security_socket_sendmsg(struct Model1_socket *Model1_sock, struct Model1_msghdr *Model1_msg, int Model1_size);
int Model1_security_socket_recvmsg(struct Model1_socket *Model1_sock, struct Model1_msghdr *Model1_msg,
       int Model1_size, int Model1_flags);
int Model1_security_socket_getsockname(struct Model1_socket *Model1_sock);
int Model1_security_socket_getpeername(struct Model1_socket *Model1_sock);
int Model1_security_socket_getsockopt(struct Model1_socket *Model1_sock, int Model1_level, int Model1_optname);
int Model1_security_socket_setsockopt(struct Model1_socket *Model1_sock, int Model1_level, int Model1_optname);
int Model1_security_socket_shutdown(struct Model1_socket *Model1_sock, int Model1_how);
int Model1_security_sock_rcv_skb(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);
int Model1_security_socket_getpeersec_stream(struct Model1_socket *Model1_sock, char *Model1_optval,
          int *Model1_optlen, unsigned Model1_len);
int Model1_security_socket_getpeersec_dgram(struct Model1_socket *Model1_sock, struct Model1_sk_buff *Model1_skb, Model1_u32 *Model1_secid);
int Model1_security_sk_alloc(struct Model1_sock *Model1_sk, int Model1_family, Model1_gfp_t Model1_priority);
void Model1_security_sk_free(struct Model1_sock *Model1_sk);
void Model1_security_sk_clone(const struct Model1_sock *Model1_sk, struct Model1_sock *Model1_newsk);
void Model1_security_sk_classify_flow(struct Model1_sock *Model1_sk, struct Model1_flowi *Model1_fl);
void Model1_security_req_classify_flow(const struct Model1_request_sock *Model1_req, struct Model1_flowi *Model1_fl);
void Model1_security_sock_graft(struct Model1_sock*Model1_sk, struct Model1_socket *Model1_parent);
int Model1_security_inet_conn_request(struct Model1_sock *Model1_sk,
   struct Model1_sk_buff *Model1_skb, struct Model1_request_sock *Model1_req);
static void Model1_security_inet_csk_clone(struct Model1_sock *Model1_newsk,
   const struct Model1_request_sock *Model1_req);
void Model1_security_inet_conn_established(struct Model1_sock *Model1_sk,
   struct Model1_sk_buff *Model1_skb);
int Model1_security_secmark_relabel_packet(Model1_u32 Model1_secid);
void Model1_security_secmark_refcount_inc(void);
void Model1_security_secmark_refcount_dec(void);
int Model1_security_tun_dev_alloc_security(void **Model1_security);
void Model1_security_tun_dev_free_security(void *Model1_security);
int Model1_security_tun_dev_create(void);
int Model1_security_tun_dev_attach_queue(void *Model1_security);
int Model1_security_tun_dev_attach(struct Model1_sock *Model1_sk, void *Model1_security);
int Model1_security_tun_dev_open(void *Model1_security);
static inline __attribute__((no_instrument_function)) int Model1_security_xfrm_policy_alloc(struct Model1_xfrm_sec_ctx **Model1_ctxp,
          struct Model1_xfrm_user_sec_ctx *Model1_sec_ctx,
          Model1_gfp_t Model1_gfp)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_security_xfrm_policy_clone(struct Model1_xfrm_sec_ctx *old, struct Model1_xfrm_sec_ctx **Model1_new_ctxp)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model1_security_xfrm_policy_free(struct Model1_xfrm_sec_ctx *Model1_ctx)
{
}

static inline __attribute__((no_instrument_function)) int Model1_security_xfrm_policy_delete(struct Model1_xfrm_sec_ctx *Model1_ctx)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_security_xfrm_state_alloc(struct Model1_xfrm_state *Model1_x,
     struct Model1_xfrm_user_sec_ctx *Model1_sec_ctx)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_security_xfrm_state_alloc_acquire(struct Model1_xfrm_state *Model1_x,
     struct Model1_xfrm_sec_ctx *Model1_polsec, Model1_u32 Model1_secid)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model1_security_xfrm_state_free(struct Model1_xfrm_state *Model1_x)
{
}

static inline __attribute__((no_instrument_function)) int Model1_security_xfrm_state_delete(struct Model1_xfrm_state *Model1_x)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_security_xfrm_policy_lookup(struct Model1_xfrm_sec_ctx *Model1_ctx, Model1_u32 Model1_fl_secid, Model1_u8 Model1_dir)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_security_xfrm_state_pol_flow_match(struct Model1_xfrm_state *Model1_x,
   struct Model1_xfrm_policy *Model1_xp, const struct Model1_flowi *Model1_fl)
{
 return 1;
}

static inline __attribute__((no_instrument_function)) int Model1_security_xfrm_decode_session(struct Model1_sk_buff *Model1_skb, Model1_u32 *Model1_secid)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model1_security_skb_classify_flow(struct Model1_sk_buff *Model1_skb, struct Model1_flowi *Model1_fl)
{
}
static inline __attribute__((no_instrument_function)) int Model1_security_path_unlink(const struct Model1_path *Model1_dir, struct Model1_dentry *Model1_dentry)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_security_path_mkdir(const struct Model1_path *Model1_dir, struct Model1_dentry *Model1_dentry,
          Model1_umode_t Model1_mode)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_security_path_rmdir(const struct Model1_path *Model1_dir, struct Model1_dentry *Model1_dentry)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_security_path_mknod(const struct Model1_path *Model1_dir, struct Model1_dentry *Model1_dentry,
          Model1_umode_t Model1_mode, unsigned int Model1_dev)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_security_path_truncate(const struct Model1_path *Model1_path)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_security_path_symlink(const struct Model1_path *Model1_dir, struct Model1_dentry *Model1_dentry,
     const char *Model1_old_name)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_security_path_link(struct Model1_dentry *Model1_old_dentry,
         const struct Model1_path *Model1_new_dir,
         struct Model1_dentry *Model1_new_dentry)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_security_path_rename(const struct Model1_path *Model1_old_dir,
           struct Model1_dentry *Model1_old_dentry,
           const struct Model1_path *Model1_new_dir,
           struct Model1_dentry *Model1_new_dentry,
           unsigned int Model1_flags)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_security_path_chmod(const struct Model1_path *Model1_path, Model1_umode_t Model1_mode)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_security_path_chown(const struct Model1_path *Model1_path, Model1_kuid_t Model1_uid, Model1_kgid_t Model1_gid)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_security_path_chroot(const struct Model1_path *Model1_path)
{
 return 0;
}





int Model1_security_key_alloc(struct Model1_key *Model1_key, const struct Model1_cred *Model1_cred, unsigned long Model1_flags);
void Model1_security_key_free(struct Model1_key *Model1_key);
int Model1_security_key_permission(Model1_key_ref_t Model1_key_ref,
       const struct Model1_cred *Model1_cred, unsigned Model1_perm);
int Model1_security_key_getsecurity(struct Model1_key *Model1_key, char **Model1__buffer);
int Model1_security_audit_rule_init(Model1_u32 Model1_field, Model1_u32 Model1_op, char *Model1_rulestr, void **Model1_lsmrule);
int Model1_security_audit_rule_known(struct Model1_audit_krule *Model1_krule);
int Model1_security_audit_rule_match(Model1_u32 Model1_secid, Model1_u32 Model1_field, Model1_u32 Model1_op, void *Model1_lsmrule,
         struct Model1_audit_context *Model1_actx);
void Model1_security_audit_rule_free(void *Model1_lsmrule);
static inline __attribute__((no_instrument_function)) struct Model1_dentry *Model1_securityfs_create_dir(const char *Model1_name,
         struct Model1_dentry *Model1_parent)
{
 return Model1_ERR_PTR(-19);
}

static inline __attribute__((no_instrument_function)) struct Model1_dentry *Model1_securityfs_create_file(const char *Model1_name,
          Model1_umode_t Model1_mode,
          struct Model1_dentry *Model1_parent,
          void *Model1_data,
          const struct Model1_file_operations *Model1_fops)
{
 return Model1_ERR_PTR(-19);
}

static inline __attribute__((no_instrument_function)) void Model1_securityfs_remove(struct Model1_dentry *Model1_dentry)
{}





static inline __attribute__((no_instrument_function)) char *Model1_alloc_secdata(void)
{
 return (char *)Model1_get_zeroed_page(((( Model1_gfp_t)(0x400000u|0x2000000u)) | (( Model1_gfp_t)0x40u) | (( Model1_gfp_t)0x80u)));
}

static inline __attribute__((no_instrument_function)) void Model1_free_secdata(void *Model1_secdata)
{
 Model1_free_pages(((unsigned long)Model1_secdata), 0);
}



/* Well, we should have at least one descriptor open
 * to accept passed FDs 8)
 */


struct Model1_scm_creds {
 Model1_u32 Model1_pid;
 Model1_kuid_t Model1_uid;
 Model1_kgid_t Model1_gid;
};

struct Model1_scm_fp_list {
 short Model1_count;
 short Model1_max;
 struct Model1_user_struct *Model1_user;
 struct Model1_file *Model1_fp[253];
};

struct Model1_scm_cookie {
 struct Model1_pid *Model1_pid; /* Skb credentials */
 struct Model1_scm_fp_list *Model1_fp; /* Passed files		*/
 struct Model1_scm_creds Model1_creds; /* Skb credentials	*/

 Model1_u32 Model1_secid; /* Passed security ID 	*/

};

void Model1_scm_detach_fds(struct Model1_msghdr *Model1_msg, struct Model1_scm_cookie *Model1_scm);
void Model1_scm_detach_fds_compat(struct Model1_msghdr *Model1_msg, struct Model1_scm_cookie *Model1_scm);
int Model1___scm_send(struct Model1_socket *Model1_sock, struct Model1_msghdr *Model1_msg, struct Model1_scm_cookie *Model1_scm);
void Model1___scm_destroy(struct Model1_scm_cookie *Model1_scm);
struct Model1_scm_fp_list *Model1_scm_fp_dup(struct Model1_scm_fp_list *Model1_fpl);


static __inline__ __attribute__((no_instrument_function)) void Model1_unix_get_peersec_dgram(struct Model1_socket *Model1_sock, struct Model1_scm_cookie *Model1_scm)
{
 Model1_security_socket_getpeersec_dgram(Model1_sock, ((void *)0), &Model1_scm->Model1_secid);
}





static __inline__ __attribute__((no_instrument_function)) void Model1_scm_set_cred(struct Model1_scm_cookie *Model1_scm,
        struct Model1_pid *Model1_pid, Model1_kuid_t Model1_uid, Model1_kgid_t Model1_gid)
{
 Model1_scm->Model1_pid = Model1_get_pid(Model1_pid);
 Model1_scm->Model1_creds.Model1_pid = Model1_pid_vnr(Model1_pid);
 Model1_scm->Model1_creds.Model1_uid = Model1_uid;
 Model1_scm->Model1_creds.Model1_gid = Model1_gid;
}

static __inline__ __attribute__((no_instrument_function)) void Model1_scm_destroy_cred(struct Model1_scm_cookie *Model1_scm)
{
 Model1_put_pid(Model1_scm->Model1_pid);
 Model1_scm->Model1_pid = ((void *)0);
}

static __inline__ __attribute__((no_instrument_function)) void Model1_scm_destroy(struct Model1_scm_cookie *Model1_scm)
{
 Model1_scm_destroy_cred(Model1_scm);
 if (Model1_scm->Model1_fp)
  Model1___scm_destroy(Model1_scm);
}

static __inline__ __attribute__((no_instrument_function)) int Model1_scm_send(struct Model1_socket *Model1_sock, struct Model1_msghdr *Model1_msg,
          struct Model1_scm_cookie *Model1_scm, bool Model1_forcecreds)
{
 memset(Model1_scm, 0, sizeof(*Model1_scm));
 Model1_scm->Model1_creds.Model1_uid = (Model1_kuid_t){ -1 };
 Model1_scm->Model1_creds.Model1_gid = (Model1_kgid_t){ -1 };
 if (Model1_forcecreds)
  Model1_scm_set_cred(Model1_scm, Model1_task_tgid(Model1_get_current()), (({ ({ do { } while (0); ; ((typeof(*(Model1_get_current()->Model1_cred)) *)((Model1_get_current()->Model1_cred))); })->Model1_uid; })), (({ ({ do { } while (0); ; ((typeof(*(Model1_get_current()->Model1_cred)) *)((Model1_get_current()->Model1_cred))); })->Model1_gid; })));
 Model1_unix_get_peersec_dgram(Model1_sock, Model1_scm);
 if (Model1_msg->Model1_msg_controllen <= 0)
  return 0;
 return Model1___scm_send(Model1_sock, Model1_msg, Model1_scm);
}


static inline __attribute__((no_instrument_function)) void Model1_scm_passec(struct Model1_socket *Model1_sock, struct Model1_msghdr *Model1_msg, struct Model1_scm_cookie *Model1_scm)
{
 char *Model1_secdata;
 Model1_u32 Model1_seclen;
 int err;

 if ((__builtin_constant_p((4)) ? Model1_constant_test_bit((4), (&Model1_sock->Model1_flags)) : Model1_variable_test_bit((4), (&Model1_sock->Model1_flags)))) {
  err = Model1_security_secid_to_secctx(Model1_scm->Model1_secid, &Model1_secdata, &Model1_seclen);

  if (!err) {
   Model1_put_cmsg(Model1_msg, 1, 0x03, Model1_seclen, Model1_secdata);
   Model1_security_release_secctx(Model1_secdata, Model1_seclen);
  }
 }
}





static __inline__ __attribute__((no_instrument_function)) void Model1_scm_recv(struct Model1_socket *Model1_sock, struct Model1_msghdr *Model1_msg,
    struct Model1_scm_cookie *Model1_scm, int Model1_flags)
{
 if (!Model1_msg->Model1_msg_control) {
  if ((__builtin_constant_p((3)) ? Model1_constant_test_bit((3), (&Model1_sock->Model1_flags)) : Model1_variable_test_bit((3), (&Model1_sock->Model1_flags))) || Model1_scm->Model1_fp)
   Model1_msg->Model1_msg_flags |= 8;
  Model1_scm_destroy(Model1_scm);
  return;
 }

 if ((__builtin_constant_p((3)) ? Model1_constant_test_bit((3), (&Model1_sock->Model1_flags)) : Model1_variable_test_bit((3), (&Model1_sock->Model1_flags)))) {
  struct Model1_user_namespace *Model1_current_ns = Model1_current_user_ns();
  struct Model1_ucred Model1_ucreds = {
   .Model1_pid = Model1_scm->Model1_creds.Model1_pid,
   .Model1_uid = Model1_from_kuid_munged(Model1_current_ns, Model1_scm->Model1_creds.Model1_uid),
   .Model1_gid = Model1_from_kgid_munged(Model1_current_ns, Model1_scm->Model1_creds.Model1_gid),
  };
  Model1_put_cmsg(Model1_msg, 1, 0x02, sizeof(Model1_ucreds), &Model1_ucreds);
 }

 Model1_scm_destroy_cred(Model1_scm);

 Model1_scm_passec(Model1_sock, Model1_msg, Model1_scm);

 if (!Model1_scm->Model1_fp)
  return;

 Model1_scm_detach_fds(Model1_msg, Model1_scm);
}
/* leave room for NETLINK_DM (DM Events) */
struct Model1_sockaddr_nl {
 Model1___kernel_sa_family_t Model1_nl_family; /* AF_NETLINK	*/
 unsigned short Model1_nl_pad; /* zero		*/
 __u32 Model1_nl_pid; /* port ID	*/
        __u32 Model1_nl_groups; /* multicast groups mask */
};

struct Model1_nlmsghdr {
 __u32 Model1_nlmsg_len; /* Length of message including header */
 Model1___u16 Model1_nlmsg_type; /* Message content */
 Model1___u16 Model1_nlmsg_flags; /* Additional flags */
 __u32 Model1_nlmsg_seq; /* Sequence number */
 __u32 Model1_nlmsg_pid; /* Sending process port ID */
};

/* Flags values */
/* Modifiers to GET request */





/* Modifiers to NEW request */





/*
   4.4BSD ADD		NLM_F_CREATE|NLM_F_EXCL
   4.4BSD CHANGE	NLM_F_REPLACE

   True CHANGE		NLM_F_CREATE|NLM_F_REPLACE
   Append		NLM_F_CREATE
   Check		NLM_F_EXCL
 */
struct Model1_nlmsgerr {
 int error;
 struct Model1_nlmsghdr Model1_msg;
};
struct Model1_nl_pktinfo {
 __u32 Model1_group;
};

struct Model1_nl_mmap_req {
 unsigned int Model1_nm_block_size;
 unsigned int Model1_nm_block_nr;
 unsigned int Model1_nm_frame_size;
 unsigned int Model1_nm_frame_nr;
};

struct Model1_nl_mmap_hdr {
 unsigned int Model1_nm_status;
 unsigned int Model1_nm_len;
 __u32 Model1_nm_group;
 /* credentials */
 __u32 Model1_nm_pid;
 __u32 Model1_nm_uid;
 __u32 Model1_nm_gid;
};
enum {
 Model1_NETLINK_UNCONNECTED = 0,
 Model1_NETLINK_CONNECTED,
};

/*
 *  <------- NLA_HDRLEN ------> <-- NLA_ALIGN(payload)-->
 * +---------------------+- - -+- - - - - - - - - -+- - -+
 * |        Header       | Pad |     Payload       | Pad |
 * |   (struct nlattr)   | ing |                   | ing |
 * +---------------------+- - -+- - - - - - - - - -+- - -+
 *  <-------------- nlattr->nla_len -------------->
 */

struct Model1_nlattr {
 Model1___u16 Model1_nla_len;
 Model1___u16 Model1_nla_type;
};

/*
 * nla_type (16 bits)
 * +---+---+-------------------------------+
 * | N | O | Attribute Type                |
 * +---+---+-------------------------------+
 * N := Carries nested attributes
 * O := Payload stored in network byte order
 *
 * Note: The N and O flag are mutually exclusive.
 */

struct Model1_net;

static inline __attribute__((no_instrument_function)) struct Model1_nlmsghdr *Model1_nlmsg_hdr(const struct Model1_sk_buff *Model1_skb)
{
 return (struct Model1_nlmsghdr *)Model1_skb->Model1_data;
}

enum Model1_netlink_skb_flags {
 Model1_NETLINK_SKB_MMAPED = 0x1, /* Packet data is mmaped */
 Model1_NETLINK_SKB_TX = 0x2, /* Packet was sent by userspace */
 Model1_NETLINK_SKB_DELIVERED = 0x4, /* Packet was delivered */
 Model1_NETLINK_SKB_DST = 0x8, /* Dst set in sendto or sendmsg */
};

struct Model1_netlink_skb_parms {
 struct Model1_scm_creds Model1_creds; /* Skb credentials	*/
 __u32 Model1_portid;
 __u32 Model1_dst_group;
 __u32 Model1_flags;
 struct Model1_sock *Model1_sk;
 bool Model1_nsid_is_set;
 int Model1_nsid;
};





extern void Model1_netlink_table_grab(void);
extern void Model1_netlink_table_ungrab(void);




/* optional Netlink kernel configuration parameters */
struct Model1_netlink_kernel_cfg {
 unsigned int Model1_groups;
 unsigned int Model1_flags;
 void (*Model1_input)(struct Model1_sk_buff *Model1_skb);
 struct Model1_mutex *Model1_cb_mutex;
 int (*Model1_bind)(struct Model1_net *Model1_net, int Model1_group);
 void (*Model1_unbind)(struct Model1_net *Model1_net, int Model1_group);
 bool (*Model1_compare)(struct Model1_net *Model1_net, struct Model1_sock *Model1_sk);
};

extern struct Model1_sock *Model1___netlink_kernel_create(struct Model1_net *Model1_net, int Model1_unit,
         struct Model1_module *Model1_module,
         struct Model1_netlink_kernel_cfg *Model1_cfg);
static inline __attribute__((no_instrument_function)) struct Model1_sock *
Model1_netlink_kernel_create(struct Model1_net *Model1_net, int Model1_unit, struct Model1_netlink_kernel_cfg *Model1_cfg)
{
 return Model1___netlink_kernel_create(Model1_net, Model1_unit, ((struct Model1_module *)0), Model1_cfg);
}

extern void Model1_netlink_kernel_release(struct Model1_sock *Model1_sk);
extern int Model1___netlink_change_ngroups(struct Model1_sock *Model1_sk, unsigned int Model1_groups);
extern int Model1_netlink_change_ngroups(struct Model1_sock *Model1_sk, unsigned int Model1_groups);
extern void Model1___netlink_clear_multicast_users(struct Model1_sock *Model1_sk, unsigned int Model1_group);
extern void Model1_netlink_ack(struct Model1_sk_buff *Model1_in_skb, struct Model1_nlmsghdr *Model1_nlh, int err);
extern int Model1_netlink_has_listeners(struct Model1_sock *Model1_sk, unsigned int Model1_group);

extern int Model1_netlink_unicast(struct Model1_sock *Model1_ssk, struct Model1_sk_buff *Model1_skb, __u32 Model1_portid, int Model1_nonblock);
extern int Model1_netlink_broadcast(struct Model1_sock *Model1_ssk, struct Model1_sk_buff *Model1_skb, __u32 Model1_portid,
        __u32 Model1_group, Model1_gfp_t Model1_allocation);
extern int Model1_netlink_broadcast_filtered(struct Model1_sock *Model1_ssk, struct Model1_sk_buff *Model1_skb,
 __u32 Model1_portid, __u32 Model1_group, Model1_gfp_t Model1_allocation,
 int (*Model1_filter)(struct Model1_sock *Model1_dsk, struct Model1_sk_buff *Model1_skb, void *Model1_data),
 void *Model1_filter_data);
extern int Model1_netlink_set_err(struct Model1_sock *Model1_ssk, __u32 Model1_portid, __u32 Model1_group, int Model1_code);
extern int Model1_netlink_register_notifier(struct Model1_notifier_block *Model1_nb);
extern int Model1_netlink_unregister_notifier(struct Model1_notifier_block *Model1_nb);

/* finegrained unicast helpers: */
struct Model1_sock *Model1_netlink_getsockbyfilp(struct Model1_file *Model1_filp);
int Model1_netlink_attachskb(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb,
        long *Model1_timeo, struct Model1_sock *Model1_ssk);
void Model1_netlink_detachskb(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);
int Model1_netlink_sendskb(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);

static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *
Model1_netlink_skb_clone(struct Model1_sk_buff *Model1_skb, Model1_gfp_t Model1_gfp_mask)
{
 struct Model1_sk_buff *Model1_nskb;

 Model1_nskb = Model1_skb_clone(Model1_skb, Model1_gfp_mask);
 if (!Model1_nskb)
  return ((void *)0);

 /* This is a large skb, set destructor callback to release head */
 if (Model1_is_vmalloc_addr(Model1_skb->Model1_head))
  Model1_nskb->Model1_destructor = Model1_skb->Model1_destructor;

 return Model1_nskb;
}

/*
 *	skb should fit one page. This choice is good for headerless malloc.
 *	But we should limit to 8K so that userspace does not have to
 *	use enormous buffer sizes on recvmsg() calls just to avoid
 *	MSG_TRUNC when PAGE_SIZE is very large.
 */
struct Model1_netlink_callback {
 struct Model1_sk_buff *Model1_skb;
 const struct Model1_nlmsghdr *Model1_nlh;
 int (*Model1_start)(struct Model1_netlink_callback *);
 int (*Model1_dump)(struct Model1_sk_buff * Model1_skb,
     struct Model1_netlink_callback *Model1_cb);
 int (*Model1_done)(struct Model1_netlink_callback *Model1_cb);
 void *Model1_data;
 /* the module that dump function belong to */
 struct Model1_module *Model1_module;
 Model1_u16 Model1_family;
 Model1_u16 Model1_min_dump_alloc;
 unsigned int Model1_prev_seq, Model1_seq;
 long Model1_args[6];
};

struct Model1_netlink_notify {
 struct Model1_net *Model1_net;
 Model1_u32 Model1_portid;
 int Model1_protocol;
};

struct Model1_nlmsghdr *
Model1___nlmsg_put(struct Model1_sk_buff *Model1_skb, Model1_u32 Model1_portid, Model1_u32 Model1_seq, int Model1_type, int Model1_len, int Model1_flags);

struct Model1_netlink_dump_control {
 int (*Model1_start)(struct Model1_netlink_callback *);
 int (*Model1_dump)(struct Model1_sk_buff *Model1_skb, struct Model1_netlink_callback *);
 int (*Model1_done)(struct Model1_netlink_callback *);
 void *Model1_data;
 struct Model1_module *Model1_module;
 Model1_u16 Model1_min_dump_alloc;
};

extern int Model1___netlink_dump_start(struct Model1_sock *Model1_ssk, struct Model1_sk_buff *Model1_skb,
    const struct Model1_nlmsghdr *Model1_nlh,
    struct Model1_netlink_dump_control *Model1_control);
static inline __attribute__((no_instrument_function)) int Model1_netlink_dump_start(struct Model1_sock *Model1_ssk, struct Model1_sk_buff *Model1_skb,
         const struct Model1_nlmsghdr *Model1_nlh,
         struct Model1_netlink_dump_control *Model1_control)
{
 if (!Model1_control->Model1_module)
  Model1_control->Model1_module = ((struct Model1_module *)0);

 return Model1___netlink_dump_start(Model1_ssk, Model1_skb, Model1_nlh, Model1_control);
}

struct Model1_netlink_tap {
 struct Model1_net_device *Model1_dev;
 struct Model1_module *Model1_module;
 struct Model1_list_head Model1_list;
};

extern int Model1_netlink_add_tap(struct Model1_netlink_tap *Model1_nt);
extern int Model1_netlink_remove_tap(struct Model1_netlink_tap *Model1_nt);

bool Model1___netlink_ns_capable(const struct Model1_netlink_skb_parms *Model1_nsp,
     struct Model1_user_namespace *Model1_ns, int Model1_cap);
bool Model1_netlink_ns_capable(const struct Model1_sk_buff *Model1_skb,
   struct Model1_user_namespace *Model1_ns, int Model1_cap);
bool Model1_netlink_capable(const struct Model1_sk_buff *Model1_skb, int Model1_cap);
bool Model1_netlink_net_capable(const struct Model1_sk_buff *Model1_skb, int Model1_cap);

struct Model1_ndmsg {
 __u8 Model1_ndm_family;
 __u8 Model1_ndm_pad1;
 Model1___u16 Model1_ndm_pad2;
 Model1___s32 Model1_ndm_ifindex;
 Model1___u16 Model1_ndm_state;
 __u8 Model1_ndm_flags;
 __u8 Model1_ndm_type;
};

enum {
 Model1_NDA_UNSPEC,
 Model1_NDA_DST,
 Model1_NDA_LLADDR,
 Model1_NDA_CACHEINFO,
 Model1_NDA_PROBES,
 Model1_NDA_VLAN,
 Model1_NDA_PORT,
 Model1_NDA_VNI,
 Model1_NDA_IFINDEX,
 Model1_NDA_MASTER,
 Model1_NDA_LINK_NETNSID,
 Model1___NDA_MAX
};



/*
 *	Neighbor Cache Entry Flags
 */
/*
 *	Neighbor Cache Entry States.
 */
/* Dummy states */




/* NUD_NOARP & NUD_PERMANENT are pseudostates, they never change
   and make no address resolution or NUD.
   NUD_PERMANENT also cannot be deleted by garbage collectors.
 */

struct Model1_nda_cacheinfo {
 __u32 Model1_ndm_confirmed;
 __u32 Model1_ndm_used;
 __u32 Model1_ndm_updated;
 __u32 Model1_ndm_refcnt;
};

/*****************************************************************
 *		Neighbour tables specific messages.
 *
 * To retrieve the neighbour tables send RTM_GETNEIGHTBL with the
 * NLM_F_DUMP flag set. Every neighbour table configuration is
 * spread over multiple messages to avoid running into message
 * size limits on systems with many interfaces. The first message
 * in the sequence transports all not device specific data such as
 * statistics, configuration, and the default parameter set.
 * This message is followed by 0..n messages carrying device
 * specific parameter sets.
 * Although the ordering should be sufficient, NDTA_NAME can be
 * used to identify sequences. The initial message can be identified
 * by checking for NDTA_CONFIG. The device specific messages do
 * not contain this TLV but have NDTPA_IFINDEX set to the
 * corresponding interface index.
 *
 * To change neighbour table attributes, send RTM_SETNEIGHTBL
 * with NDTA_NAME set. Changeable attribute include NDTA_THRESH[1-3],
 * NDTA_GC_INTERVAL, and all TLVs in NDTA_PARMS unless marked
 * otherwise. Device specific parameter sets can be changed by
 * setting NDTPA_IFINDEX to the interface index of the corresponding
 * device.
 ****/

struct Model1_ndt_stats {
 __u64 Model1_ndts_allocs;
 __u64 Model1_ndts_destroys;
 __u64 Model1_ndts_hash_grows;
 __u64 Model1_ndts_res_failed;
 __u64 Model1_ndts_lookups;
 __u64 Model1_ndts_hits;
 __u64 Model1_ndts_rcv_probes_mcast;
 __u64 Model1_ndts_rcv_probes_ucast;
 __u64 Model1_ndts_periodic_gc_runs;
 __u64 Model1_ndts_forced_gc_runs;
 __u64 Model1_ndts_table_fulls;
};

enum {
 Model1_NDTPA_UNSPEC,
 Model1_NDTPA_IFINDEX, /* u32, unchangeable */
 Model1_NDTPA_REFCNT, /* u32, read-only */
 Model1_NDTPA_REACHABLE_TIME, /* u64, read-only, msecs */
 Model1_NDTPA_BASE_REACHABLE_TIME, /* u64, msecs */
 Model1_NDTPA_RETRANS_TIME, /* u64, msecs */
 Model1_NDTPA_GC_STALETIME, /* u64, msecs */
 Model1_NDTPA_DELAY_PROBE_TIME, /* u64, msecs */
 Model1_NDTPA_QUEUE_LEN, /* u32 */
 Model1_NDTPA_APP_PROBES, /* u32 */
 Model1_NDTPA_UCAST_PROBES, /* u32 */
 Model1_NDTPA_MCAST_PROBES, /* u32 */
 Model1_NDTPA_ANYCAST_DELAY, /* u64, msecs */
 Model1_NDTPA_PROXY_DELAY, /* u64, msecs */
 Model1_NDTPA_PROXY_QLEN, /* u32 */
 Model1_NDTPA_LOCKTIME, /* u64, msecs */
 Model1_NDTPA_QUEUE_LENBYTES, /* u32 */
 Model1_NDTPA_MCAST_REPROBES, /* u32 */
 Model1_NDTPA_PAD,
 Model1___NDTPA_MAX
};


struct Model1_ndtmsg {
 __u8 Model1_ndtm_family;
 __u8 Model1_ndtm_pad1;
 Model1___u16 Model1_ndtm_pad2;
};

struct Model1_ndt_config {
 Model1___u16 Model1_ndtc_key_len;
 Model1___u16 Model1_ndtc_entry_size;
 __u32 Model1_ndtc_entries;
 __u32 Model1_ndtc_last_flush; /* delta to now in msecs */
 __u32 Model1_ndtc_last_rand; /* delta to now in msecs */
 __u32 Model1_ndtc_hash_rnd;
 __u32 Model1_ndtc_hash_mask;
 __u32 Model1_ndtc_hash_chain_gc;
 __u32 Model1_ndtc_proxy_qlen;
};

enum {
 Model1_NDTA_UNSPEC,
 Model1_NDTA_NAME, /* char *, unchangeable */
 Model1_NDTA_THRESH1, /* u32 */
 Model1_NDTA_THRESH2, /* u32 */
 Model1_NDTA_THRESH3, /* u32 */
 Model1_NDTA_CONFIG, /* struct ndt_config, read-only */
 Model1_NDTA_PARMS, /* nested TLV NDTPA_* */
 Model1_NDTA_STATS, /* struct ndt_stats, read-only */
 Model1_NDTA_GC_INTERVAL, /* u64, msecs */
 Model1_NDTA_PAD,
 Model1___NDTA_MAX
};
/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for the Interfaces handler.
 *
 * Version:	@(#)dev.h	1.0.10	08/12/93
 *
 * Authors:	Ross Biro
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *		Corey Minyard <wf-rch!minyard@relay.EU.net>
 *		Donald J. Becker, <becker@cesdis.gsfc.nasa.gov>
 *		Alan Cox, <alan@lxorguk.ukuu.org.uk>
 *		Bjorn Ekwall. <bj0rn@blox.se>
 *              Pekka Riikonen <priikone@poseidon.pspt.fi>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 *
 *		Moved to /usr/include/linux for NET3
 */
















/* This struct should be in sync with struct rtnl_link_stats64 */
struct Model1_rtnl_link_stats {
 __u32 Model1_rx_packets; /* total packets received	*/
 __u32 Model1_tx_packets; /* total packets transmitted	*/
 __u32 Model1_rx_bytes; /* total bytes received 	*/
 __u32 Model1_tx_bytes; /* total bytes transmitted	*/
 __u32 Model1_rx_errors; /* bad packets received		*/
 __u32 Model1_tx_errors; /* packet transmit problems	*/
 __u32 Model1_rx_dropped; /* no space in linux buffers	*/
 __u32 Model1_tx_dropped; /* no space available in linux	*/
 __u32 Model1_multicast; /* multicast packets received	*/
 __u32 Model1_collisions;

 /* detailed rx_errors: */
 __u32 Model1_rx_length_errors;
 __u32 Model1_rx_over_errors; /* receiver ring buff overflow	*/
 __u32 Model1_rx_crc_errors; /* recved pkt with crc error	*/
 __u32 Model1_rx_frame_errors; /* recv'd frame alignment error */
 __u32 Model1_rx_fifo_errors; /* recv'r fifo overrun		*/
 __u32 Model1_rx_missed_errors; /* receiver missed packet	*/

 /* detailed tx_errors */
 __u32 Model1_tx_aborted_errors;
 __u32 Model1_tx_carrier_errors;
 __u32 Model1_tx_fifo_errors;
 __u32 Model1_tx_heartbeat_errors;
 __u32 Model1_tx_window_errors;

 /* for cslip etc */
 __u32 Model1_rx_compressed;
 __u32 Model1_tx_compressed;

 __u32 Model1_rx_nohandler; /* dropped, no handler found	*/
};

/* The main device statistics structure */
struct Model1_rtnl_link_stats64 {
 __u64 Model1_rx_packets; /* total packets received	*/
 __u64 Model1_tx_packets; /* total packets transmitted	*/
 __u64 Model1_rx_bytes; /* total bytes received 	*/
 __u64 Model1_tx_bytes; /* total bytes transmitted	*/
 __u64 Model1_rx_errors; /* bad packets received		*/
 __u64 Model1_tx_errors; /* packet transmit problems	*/
 __u64 Model1_rx_dropped; /* no space in linux buffers	*/
 __u64 Model1_tx_dropped; /* no space available in linux	*/
 __u64 Model1_multicast; /* multicast packets received	*/
 __u64 Model1_collisions;

 /* detailed rx_errors: */
 __u64 Model1_rx_length_errors;
 __u64 Model1_rx_over_errors; /* receiver ring buff overflow	*/
 __u64 Model1_rx_crc_errors; /* recved pkt with crc error	*/
 __u64 Model1_rx_frame_errors; /* recv'd frame alignment error */
 __u64 Model1_rx_fifo_errors; /* recv'r fifo overrun		*/
 __u64 Model1_rx_missed_errors; /* receiver missed packet	*/

 /* detailed tx_errors */
 __u64 Model1_tx_aborted_errors;
 __u64 Model1_tx_carrier_errors;
 __u64 Model1_tx_fifo_errors;
 __u64 Model1_tx_heartbeat_errors;
 __u64 Model1_tx_window_errors;

 /* for cslip etc */
 __u64 Model1_rx_compressed;
 __u64 Model1_tx_compressed;

 __u64 Model1_rx_nohandler; /* dropped, no handler found	*/
};

/* The struct should be in sync with struct ifmap */
struct Model1_rtnl_link_ifmap {
 __u64 Model1_mem_start;
 __u64 Model1_mem_end;
 __u64 Model1_base_addr;
 Model1___u16 Model1_irq;
 __u8 Model1_dma;
 __u8 Model1_port;
};

/*
 * IFLA_AF_SPEC
 *   Contains nested attributes for address family specific attributes.
 *   Each address family may create a attribute with the address family
 *   number as type and create its own attribute structure in it.
 *
 *   Example:
 *   [IFLA_AF_SPEC] = {
 *       [AF_INET] = {
 *           [IFLA_INET_CONF] = ...,
 *       },
 *       [AF_INET6] = {
 *           [IFLA_INET6_FLAGS] = ...,
 *           [IFLA_INET6_CONF] = ...,
 *       }
 *   }
 */

enum {
 Model1_IFLA_UNSPEC,
 Model1_IFLA_ADDRESS,
 Model1_IFLA_BROADCAST,
 Model1_IFLA_IFNAME,
 Model1_IFLA_MTU,
 Model1_IFLA_LINK,
 Model1_IFLA_QDISC,
 Model1_IFLA_STATS,
 Model1_IFLA_COST,

 Model1_IFLA_PRIORITY,

 Model1_IFLA_MASTER,

 Model1_IFLA_WIRELESS, /* Wireless Extension event - see wireless.h */

 Model1_IFLA_PROTINFO, /* Protocol specific information for a link */

 Model1_IFLA_TXQLEN,

 Model1_IFLA_MAP,

 Model1_IFLA_WEIGHT,

 Model1_IFLA_OPERSTATE,
 Model1_IFLA_LINKMODE,
 Model1_IFLA_LINKINFO,

 Model1_IFLA_NET_NS_PID,
 Model1_IFLA_IFALIAS,
 Model1_IFLA_NUM_VF, /* Number of VFs if device is SR-IOV PF */
 Model1_IFLA_VFINFO_LIST,
 Model1_IFLA_STATS64,
 Model1_IFLA_VF_PORTS,
 Model1_IFLA_PORT_SELF,
 Model1_IFLA_AF_SPEC,
 Model1_IFLA_GROUP, /* Group the device belongs to */
 Model1_IFLA_NET_NS_FD,
 Model1_IFLA_EXT_MASK, /* Extended info mask, VFs, etc */
 Model1_IFLA_PROMISCUITY, /* Promiscuity count: > 0 means acts PROMISC */

 Model1_IFLA_NUM_TX_QUEUES,
 Model1_IFLA_NUM_RX_QUEUES,
 Model1_IFLA_CARRIER,
 Model1_IFLA_PHYS_PORT_ID,
 Model1_IFLA_CARRIER_CHANGES,
 Model1_IFLA_PHYS_SWITCH_ID,
 Model1_IFLA_LINK_NETNSID,
 Model1_IFLA_PHYS_PORT_NAME,
 Model1_IFLA_PROTO_DOWN,
 Model1_IFLA_GSO_MAX_SEGS,
 Model1_IFLA_GSO_MAX_SIZE,
 Model1_IFLA_PAD,
 Model1_IFLA_XDP,
 Model1___IFLA_MAX
};




/* backwards compatibility for userspace */





enum {
 Model1_IFLA_INET_UNSPEC,
 Model1_IFLA_INET_CONF,
 Model1___IFLA_INET_MAX,
};



/* ifi_flags.

   IFF_* flags.

   The only change is:
   IFF_LOOPBACK, IFF_BROADCAST and IFF_POINTOPOINT are
   more not changeable by user. They describe link media
   characteristics and set by device driver.

   Comments:
   - Combination IFF_BROADCAST|IFF_POINTOPOINT is invalid
   - If neither of these three flags are set;
     the interface is NBMA.

   - IFF_MULTICAST does not mean anything special:
   multicasts can be used on all not-NBMA links.
   IFF_MULTICAST means that this media uses special encapsulation
   for multicast frames. Apparently, all IFF_POINTOPOINT and
   IFF_BROADCAST devices are able to use multicasts too.
 */

/* IFLA_LINK.
   For usual devices it is equal ifi_index.
   If it is a "virtual interface" (f.e. tunnel), ifi_link
   can point to real physical interface (f.e. for bandwidth calculations),
   or maybe 0, what means, that real media is unknown (usual
   for IPIP tunnels, when route to endpoint is allowed to change)
 */

/* Subtype attributes for IFLA_PROTINFO */
enum {
 Model1_IFLA_INET6_UNSPEC,
 Model1_IFLA_INET6_FLAGS, /* link flags			*/
 Model1_IFLA_INET6_CONF, /* sysctl parameters		*/
 Model1_IFLA_INET6_STATS, /* statistics			*/
 Model1_IFLA_INET6_MCAST, /* MC things. What of them?	*/
 Model1_IFLA_INET6_CACHEINFO, /* time values and max reasm size */
 Model1_IFLA_INET6_ICMP6STATS, /* statistics (icmpv6)		*/
 Model1_IFLA_INET6_TOKEN, /* device token			*/
 Model1_IFLA_INET6_ADDR_GEN_MODE, /* implicit address generator mode */
 Model1___IFLA_INET6_MAX
};



enum Model1_in6_addr_gen_mode {
 Model1_IN6_ADDR_GEN_MODE_EUI64,
 Model1_IN6_ADDR_GEN_MODE_NONE,
 Model1_IN6_ADDR_GEN_MODE_STABLE_PRIVACY,
 Model1_IN6_ADDR_GEN_MODE_RANDOM,
};

/* Bridge section */

enum {
 Model1_IFLA_BR_UNSPEC,
 Model1_IFLA_BR_FORWARD_DELAY,
 Model1_IFLA_BR_HELLO_TIME,
 Model1_IFLA_BR_MAX_AGE,
 Model1_IFLA_BR_AGEING_TIME,
 Model1_IFLA_BR_STP_STATE,
 Model1_IFLA_BR_PRIORITY,
 Model1_IFLA_BR_VLAN_FILTERING,
 Model1_IFLA_BR_VLAN_PROTOCOL,
 Model1_IFLA_BR_GROUP_FWD_MASK,
 Model1_IFLA_BR_ROOT_ID,
 Model1_IFLA_BR_BRIDGE_ID,
 Model1_IFLA_BR_ROOT_PORT,
 Model1_IFLA_BR_ROOT_PATH_COST,
 Model1_IFLA_BR_TOPOLOGY_CHANGE,
 Model1_IFLA_BR_TOPOLOGY_CHANGE_DETECTED,
 Model1_IFLA_BR_HELLO_TIMER,
 Model1_IFLA_BR_TCN_TIMER,
 Model1_IFLA_BR_TOPOLOGY_CHANGE_TIMER,
 Model1_IFLA_BR_GC_TIMER,
 Model1_IFLA_BR_GROUP_ADDR,
 Model1_IFLA_BR_FDB_FLUSH,
 Model1_IFLA_BR_MCAST_ROUTER,
 Model1_IFLA_BR_MCAST_SNOOPING,
 Model1_IFLA_BR_MCAST_QUERY_USE_IFADDR,
 Model1_IFLA_BR_MCAST_QUERIER,
 Model1_IFLA_BR_MCAST_HASH_ELASTICITY,
 Model1_IFLA_BR_MCAST_HASH_MAX,
 Model1_IFLA_BR_MCAST_LAST_MEMBER_CNT,
 Model1_IFLA_BR_MCAST_STARTUP_QUERY_CNT,
 Model1_IFLA_BR_MCAST_LAST_MEMBER_INTVL,
 Model1_IFLA_BR_MCAST_MEMBERSHIP_INTVL,
 Model1_IFLA_BR_MCAST_QUERIER_INTVL,
 Model1_IFLA_BR_MCAST_QUERY_INTVL,
 Model1_IFLA_BR_MCAST_QUERY_RESPONSE_INTVL,
 Model1_IFLA_BR_MCAST_STARTUP_QUERY_INTVL,
 Model1_IFLA_BR_NF_CALL_IPTABLES,
 Model1_IFLA_BR_NF_CALL_IP6TABLES,
 Model1_IFLA_BR_NF_CALL_ARPTABLES,
 Model1_IFLA_BR_VLAN_DEFAULT_PVID,
 Model1_IFLA_BR_PAD,
 Model1_IFLA_BR_VLAN_STATS_ENABLED,
 Model1_IFLA_BR_MCAST_STATS_ENABLED,
 Model1___IFLA_BR_MAX,
};



struct Model1_ifla_bridge_id {
 __u8 Model1_prio[2];
 __u8 Model1_addr[6]; /* ETH_ALEN */
};

enum {
 Model1_BRIDGE_MODE_UNSPEC,
 Model1_BRIDGE_MODE_HAIRPIN,
};

enum {
 Model1_IFLA_BRPORT_UNSPEC,
 Model1_IFLA_BRPORT_STATE, /* Spanning tree state     */
 Model1_IFLA_BRPORT_PRIORITY, /* "             priority  */
 Model1_IFLA_BRPORT_COST, /* "             cost      */
 Model1_IFLA_BRPORT_MODE, /* mode (hairpin)          */
 Model1_IFLA_BRPORT_GUARD, /* bpdu guard              */
 Model1_IFLA_BRPORT_PROTECT, /* root port protection    */
 Model1_IFLA_BRPORT_FAST_LEAVE, /* multicast fast leave    */
 Model1_IFLA_BRPORT_LEARNING, /* mac learning */
 Model1_IFLA_BRPORT_UNICAST_FLOOD, /* flood unicast traffic */
 Model1_IFLA_BRPORT_PROXYARP, /* proxy ARP */
 Model1_IFLA_BRPORT_LEARNING_SYNC, /* mac learning sync from device */
 Model1_IFLA_BRPORT_PROXYARP_WIFI, /* proxy ARP for Wi-Fi */
 Model1_IFLA_BRPORT_ROOT_ID, /* designated root */
 Model1_IFLA_BRPORT_BRIDGE_ID, /* designated bridge */
 Model1_IFLA_BRPORT_DESIGNATED_PORT,
 Model1_IFLA_BRPORT_DESIGNATED_COST,
 Model1_IFLA_BRPORT_ID,
 Model1_IFLA_BRPORT_NO,
 Model1_IFLA_BRPORT_TOPOLOGY_CHANGE_ACK,
 Model1_IFLA_BRPORT_CONFIG_PENDING,
 Model1_IFLA_BRPORT_MESSAGE_AGE_TIMER,
 Model1_IFLA_BRPORT_FORWARD_DELAY_TIMER,
 Model1_IFLA_BRPORT_HOLD_TIMER,
 Model1_IFLA_BRPORT_FLUSH,
 Model1_IFLA_BRPORT_MULTICAST_ROUTER,
 Model1_IFLA_BRPORT_PAD,
 Model1___IFLA_BRPORT_MAX
};


struct Model1_ifla_cacheinfo {
 __u32 Model1_max_reasm_len;
 __u32 Model1_tstamp; /* ipv6InterfaceTable updated timestamp */
 __u32 Model1_reachable_time;
 __u32 Model1_retrans_time;
};

enum {
 Model1_IFLA_INFO_UNSPEC,
 Model1_IFLA_INFO_KIND,
 Model1_IFLA_INFO_DATA,
 Model1_IFLA_INFO_XSTATS,
 Model1_IFLA_INFO_SLAVE_KIND,
 Model1_IFLA_INFO_SLAVE_DATA,
 Model1___IFLA_INFO_MAX,
};



/* VLAN section */

enum {
 Model1_IFLA_VLAN_UNSPEC,
 Model1_IFLA_VLAN_ID,
 Model1_IFLA_VLAN_FLAGS,
 Model1_IFLA_VLAN_EGRESS_QOS,
 Model1_IFLA_VLAN_INGRESS_QOS,
 Model1_IFLA_VLAN_PROTOCOL,
 Model1___IFLA_VLAN_MAX,
};



struct Model1_ifla_vlan_flags {
 __u32 Model1_flags;
 __u32 Model1_mask;
};

enum {
 Model1_IFLA_VLAN_QOS_UNSPEC,
 Model1_IFLA_VLAN_QOS_MAPPING,
 Model1___IFLA_VLAN_QOS_MAX
};



struct Model1_ifla_vlan_qos_mapping {
 __u32 Model1_from;
 __u32 Model1_to;
};

/* MACVLAN section */
enum {
 Model1_IFLA_MACVLAN_UNSPEC,
 Model1_IFLA_MACVLAN_MODE,
 Model1_IFLA_MACVLAN_FLAGS,
 Model1_IFLA_MACVLAN_MACADDR_MODE,
 Model1_IFLA_MACVLAN_MACADDR,
 Model1_IFLA_MACVLAN_MACADDR_DATA,
 Model1_IFLA_MACVLAN_MACADDR_COUNT,
 Model1___IFLA_MACVLAN_MAX,
};



enum Model1_macvlan_mode {
 Model1_MACVLAN_MODE_PRIVATE = 1, /* don't talk to other macvlans */
 Model1_MACVLAN_MODE_VEPA = 2, /* talk to other ports through ext bridge */
 Model1_MACVLAN_MODE_BRIDGE = 4, /* talk to bridge ports directly */
 Model1_MACVLAN_MODE_PASSTHRU = 8,/* take over the underlying device */
 Model1_MACVLAN_MODE_SOURCE = 16,/* use source MAC address list to assign */
};

enum Model1_macvlan_macaddr_mode {
 Model1_MACVLAN_MACADDR_ADD,
 Model1_MACVLAN_MACADDR_DEL,
 Model1_MACVLAN_MACADDR_FLUSH,
 Model1_MACVLAN_MACADDR_SET,
};



/* VRF section */
enum {
 Model1_IFLA_VRF_UNSPEC,
 Model1_IFLA_VRF_TABLE,
 Model1___IFLA_VRF_MAX
};



enum {
 Model1_IFLA_VRF_PORT_UNSPEC,
 Model1_IFLA_VRF_PORT_TABLE,
 Model1___IFLA_VRF_PORT_MAX
};



/* MACSEC section */
enum {
 Model1_IFLA_MACSEC_UNSPEC,
 Model1_IFLA_MACSEC_SCI,
 Model1_IFLA_MACSEC_PORT,
 Model1_IFLA_MACSEC_ICV_LEN,
 Model1_IFLA_MACSEC_CIPHER_SUITE,
 Model1_IFLA_MACSEC_WINDOW,
 Model1_IFLA_MACSEC_ENCODING_SA,
 Model1_IFLA_MACSEC_ENCRYPT,
 Model1_IFLA_MACSEC_PROTECT,
 Model1_IFLA_MACSEC_INC_SCI,
 Model1_IFLA_MACSEC_ES,
 Model1_IFLA_MACSEC_SCB,
 Model1_IFLA_MACSEC_REPLAY_PROTECT,
 Model1_IFLA_MACSEC_VALIDATION,
 Model1_IFLA_MACSEC_PAD,
 Model1___IFLA_MACSEC_MAX,
};



enum Model1_macsec_validation_type {
 Model1_MACSEC_VALIDATE_DISABLED = 0,
 Model1_MACSEC_VALIDATE_CHECK = 1,
 Model1_MACSEC_VALIDATE_STRICT = 2,
 Model1___MACSEC_VALIDATE_END,
 Model1_MACSEC_VALIDATE_MAX = Model1___MACSEC_VALIDATE_END - 1,
};

/* IPVLAN section */
enum {
 Model1_IFLA_IPVLAN_UNSPEC,
 Model1_IFLA_IPVLAN_MODE,
 Model1___IFLA_IPVLAN_MAX
};



enum Model1_ipvlan_mode {
 Model1_IPVLAN_MODE_L2 = 0,
 Model1_IPVLAN_MODE_L3,
 Model1_IPVLAN_MODE_MAX
};

/* VXLAN section */
enum {
 Model1_IFLA_VXLAN_UNSPEC,
 Model1_IFLA_VXLAN_ID,
 Model1_IFLA_VXLAN_GROUP, /* group or remote address */
 Model1_IFLA_VXLAN_LINK,
 Model1_IFLA_VXLAN_LOCAL,
 Model1_IFLA_VXLAN_TTL,
 Model1_IFLA_VXLAN_TOS,
 Model1_IFLA_VXLAN_LEARNING,
 Model1_IFLA_VXLAN_AGEING,
 Model1_IFLA_VXLAN_LIMIT,
 Model1_IFLA_VXLAN_PORT_RANGE, /* source port */
 Model1_IFLA_VXLAN_PROXY,
 Model1_IFLA_VXLAN_RSC,
 Model1_IFLA_VXLAN_L2MISS,
 Model1_IFLA_VXLAN_L3MISS,
 Model1_IFLA_VXLAN_PORT, /* destination port */
 Model1_IFLA_VXLAN_GROUP6,
 Model1_IFLA_VXLAN_LOCAL6,
 Model1_IFLA_VXLAN_UDP_CSUM,
 Model1_IFLA_VXLAN_UDP_ZERO_CSUM6_TX,
 Model1_IFLA_VXLAN_UDP_ZERO_CSUM6_RX,
 Model1_IFLA_VXLAN_REMCSUM_TX,
 Model1_IFLA_VXLAN_REMCSUM_RX,
 Model1_IFLA_VXLAN_GBP,
 Model1_IFLA_VXLAN_REMCSUM_NOPARTIAL,
 Model1_IFLA_VXLAN_COLLECT_METADATA,
 Model1_IFLA_VXLAN_LABEL,
 Model1_IFLA_VXLAN_GPE,
 Model1___IFLA_VXLAN_MAX
};


struct Model1_ifla_vxlan_port_range {
 Model1___be16 Model1_low;
 Model1___be16 Model1_high;
};

/* GENEVE section */
enum {
 Model1_IFLA_GENEVE_UNSPEC,
 Model1_IFLA_GENEVE_ID,
 Model1_IFLA_GENEVE_REMOTE,
 Model1_IFLA_GENEVE_TTL,
 Model1_IFLA_GENEVE_TOS,
 Model1_IFLA_GENEVE_PORT, /* destination port */
 Model1_IFLA_GENEVE_COLLECT_METADATA,
 Model1_IFLA_GENEVE_REMOTE6,
 Model1_IFLA_GENEVE_UDP_CSUM,
 Model1_IFLA_GENEVE_UDP_ZERO_CSUM6_TX,
 Model1_IFLA_GENEVE_UDP_ZERO_CSUM6_RX,
 Model1_IFLA_GENEVE_LABEL,
 Model1___IFLA_GENEVE_MAX
};


/* PPP section */
enum {
 Model1_IFLA_PPP_UNSPEC,
 Model1_IFLA_PPP_DEV_FD,
 Model1___IFLA_PPP_MAX
};


/* GTP section */
enum {
 Model1_IFLA_GTP_UNSPEC,
 Model1_IFLA_GTP_FD0,
 Model1_IFLA_GTP_FD1,
 Model1_IFLA_GTP_PDP_HASHSIZE,
 Model1___IFLA_GTP_MAX,
};


/* Bonding section */

enum {
 Model1_IFLA_BOND_UNSPEC,
 Model1_IFLA_BOND_MODE,
 Model1_IFLA_BOND_ACTIVE_SLAVE,
 Model1_IFLA_BOND_MIIMON,
 Model1_IFLA_BOND_UPDELAY,
 Model1_IFLA_BOND_DOWNDELAY,
 Model1_IFLA_BOND_USE_CARRIER,
 Model1_IFLA_BOND_ARP_INTERVAL,
 Model1_IFLA_BOND_ARP_IP_TARGET,
 Model1_IFLA_BOND_ARP_VALIDATE,
 Model1_IFLA_BOND_ARP_ALL_TARGETS,
 Model1_IFLA_BOND_PRIMARY,
 Model1_IFLA_BOND_PRIMARY_RESELECT,
 Model1_IFLA_BOND_FAIL_OVER_MAC,
 Model1_IFLA_BOND_XMIT_HASH_POLICY,
 Model1_IFLA_BOND_RESEND_IGMP,
 Model1_IFLA_BOND_NUM_PEER_NOTIF,
 Model1_IFLA_BOND_ALL_SLAVES_ACTIVE,
 Model1_IFLA_BOND_MIN_LINKS,
 Model1_IFLA_BOND_LP_INTERVAL,
 Model1_IFLA_BOND_PACKETS_PER_SLAVE,
 Model1_IFLA_BOND_AD_LACP_RATE,
 Model1_IFLA_BOND_AD_SELECT,
 Model1_IFLA_BOND_AD_INFO,
 Model1_IFLA_BOND_AD_ACTOR_SYS_PRIO,
 Model1_IFLA_BOND_AD_USER_PORT_KEY,
 Model1_IFLA_BOND_AD_ACTOR_SYSTEM,
 Model1_IFLA_BOND_TLB_DYNAMIC_LB,
 Model1___IFLA_BOND_MAX,
};



enum {
 Model1_IFLA_BOND_AD_INFO_UNSPEC,
 Model1_IFLA_BOND_AD_INFO_AGGREGATOR,
 Model1_IFLA_BOND_AD_INFO_NUM_PORTS,
 Model1_IFLA_BOND_AD_INFO_ACTOR_KEY,
 Model1_IFLA_BOND_AD_INFO_PARTNER_KEY,
 Model1_IFLA_BOND_AD_INFO_PARTNER_MAC,
 Model1___IFLA_BOND_AD_INFO_MAX,
};



enum {
 Model1_IFLA_BOND_SLAVE_UNSPEC,
 Model1_IFLA_BOND_SLAVE_STATE,
 Model1_IFLA_BOND_SLAVE_MII_STATUS,
 Model1_IFLA_BOND_SLAVE_LINK_FAILURE_COUNT,
 Model1_IFLA_BOND_SLAVE_PERM_HWADDR,
 Model1_IFLA_BOND_SLAVE_QUEUE_ID,
 Model1_IFLA_BOND_SLAVE_AD_AGGREGATOR_ID,
 Model1_IFLA_BOND_SLAVE_AD_ACTOR_OPER_PORT_STATE,
 Model1_IFLA_BOND_SLAVE_AD_PARTNER_OPER_PORT_STATE,
 Model1___IFLA_BOND_SLAVE_MAX,
};



/* SR-IOV virtual function management section */

enum {
 Model1_IFLA_VF_INFO_UNSPEC,
 Model1_IFLA_VF_INFO,
 Model1___IFLA_VF_INFO_MAX,
};



enum {
 Model1_IFLA_VF_UNSPEC,
 Model1_IFLA_VF_MAC, /* Hardware queue specific attributes */
 Model1_IFLA_VF_VLAN,
 Model1_IFLA_VF_TX_RATE, /* Max TX Bandwidth Allocation */
 Model1_IFLA_VF_SPOOFCHK, /* Spoof Checking on/off switch */
 Model1_IFLA_VF_LINK_STATE, /* link state enable/disable/auto switch */
 Model1_IFLA_VF_RATE, /* Min and Max TX Bandwidth Allocation */
 Model1_IFLA_VF_RSS_QUERY_EN, /* RSS Redirection Table and Hash Key query
				 * on/off switch
				 */
 Model1_IFLA_VF_STATS, /* network device statistics */
 Model1_IFLA_VF_TRUST, /* Trust VF */
 Model1_IFLA_VF_IB_NODE_GUID, /* VF Infiniband node GUID */
 Model1_IFLA_VF_IB_PORT_GUID, /* VF Infiniband port GUID */
 Model1___IFLA_VF_MAX,
};



struct Model1_ifla_vf_mac {
 __u32 Model1_vf;
 __u8 Model1_mac[32]; /* MAX_ADDR_LEN */
};

struct Model1_ifla_vf_vlan {
 __u32 Model1_vf;
 __u32 Model1_vlan; /* 0 - 4095, 0 disables VLAN filter */
 __u32 Model1_qos;
};

struct Model1_ifla_vf_tx_rate {
 __u32 Model1_vf;
 __u32 Model1_rate; /* Max TX bandwidth in Mbps, 0 disables throttling */
};

struct Model1_ifla_vf_rate {
 __u32 Model1_vf;
 __u32 Model1_min_tx_rate; /* Min Bandwidth in Mbps */
 __u32 Model1_max_tx_rate; /* Max Bandwidth in Mbps */
};

struct Model1_ifla_vf_spoofchk {
 __u32 Model1_vf;
 __u32 Model1_setting;
};

struct Model1_ifla_vf_guid {
 __u32 Model1_vf;
 __u64 Model1_guid;
};

enum {
 Model1_IFLA_VF_LINK_STATE_AUTO, /* link state of the uplink */
 Model1_IFLA_VF_LINK_STATE_ENABLE, /* link always up */
 Model1_IFLA_VF_LINK_STATE_DISABLE, /* link always down */
 Model1___IFLA_VF_LINK_STATE_MAX,
};

struct Model1_ifla_vf_link_state {
 __u32 Model1_vf;
 __u32 Model1_link_state;
};

struct Model1_ifla_vf_rss_query_en {
 __u32 Model1_vf;
 __u32 Model1_setting;
};

enum {
 Model1_IFLA_VF_STATS_RX_PACKETS,
 Model1_IFLA_VF_STATS_TX_PACKETS,
 Model1_IFLA_VF_STATS_RX_BYTES,
 Model1_IFLA_VF_STATS_TX_BYTES,
 Model1_IFLA_VF_STATS_BROADCAST,
 Model1_IFLA_VF_STATS_MULTICAST,
 Model1_IFLA_VF_STATS_PAD,
 Model1___IFLA_VF_STATS_MAX,
};



struct Model1_ifla_vf_trust {
 __u32 Model1_vf;
 __u32 Model1_setting;
};

/* VF ports management section
 *
 *	Nested layout of set/get msg is:
 *
 *		[IFLA_NUM_VF]
 *		[IFLA_VF_PORTS]
 *			[IFLA_VF_PORT]
 *				[IFLA_PORT_*], ...
 *			[IFLA_VF_PORT]
 *				[IFLA_PORT_*], ...
 *			...
 *		[IFLA_PORT_SELF]
 *			[IFLA_PORT_*], ...
 */

enum {
 Model1_IFLA_VF_PORT_UNSPEC,
 Model1_IFLA_VF_PORT, /* nest */
 Model1___IFLA_VF_PORT_MAX,
};



enum {
 Model1_IFLA_PORT_UNSPEC,
 Model1_IFLA_PORT_VF, /* __u32 */
 Model1_IFLA_PORT_PROFILE, /* string */
 Model1_IFLA_PORT_VSI_TYPE, /* 802.1Qbg (pre-)standard VDP */
 Model1_IFLA_PORT_INSTANCE_UUID, /* binary UUID */
 Model1_IFLA_PORT_HOST_UUID, /* binary UUID */
 Model1_IFLA_PORT_REQUEST, /* __u8 */
 Model1_IFLA_PORT_RESPONSE, /* __u16, output only */
 Model1___IFLA_PORT_MAX,
};







enum {
 Model1_PORT_REQUEST_PREASSOCIATE = 0,
 Model1_PORT_REQUEST_PREASSOCIATE_RR,
 Model1_PORT_REQUEST_ASSOCIATE,
 Model1_PORT_REQUEST_DISASSOCIATE,
};

enum {
 Model1_PORT_VDP_RESPONSE_SUCCESS = 0,
 Model1_PORT_VDP_RESPONSE_INVALID_FORMAT,
 Model1_PORT_VDP_RESPONSE_INSUFFICIENT_RESOURCES,
 Model1_PORT_VDP_RESPONSE_UNUSED_VTID,
 Model1_PORT_VDP_RESPONSE_VTID_VIOLATION,
 Model1_PORT_VDP_RESPONSE_VTID_VERSION_VIOALTION,
 Model1_PORT_VDP_RESPONSE_OUT_OF_SYNC,
 /* 0x08-0xFF reserved for future VDP use */
 Model1_PORT_PROFILE_RESPONSE_SUCCESS = 0x100,
 Model1_PORT_PROFILE_RESPONSE_INPROGRESS,
 Model1_PORT_PROFILE_RESPONSE_INVALID,
 Model1_PORT_PROFILE_RESPONSE_BADSTATE,
 Model1_PORT_PROFILE_RESPONSE_INSUFFICIENT_RESOURCES,
 Model1_PORT_PROFILE_RESPONSE_ERROR,
};

struct Model1_ifla_port_vsi {
 __u8 Model1_vsi_mgr_id;
 __u8 Model1_vsi_type_id[3];
 __u8 Model1_vsi_type_version;
 __u8 Model1_pad[3];
};


/* IPoIB section */

enum {
 Model1_IFLA_IPOIB_UNSPEC,
 Model1_IFLA_IPOIB_PKEY,
 Model1_IFLA_IPOIB_MODE,
 Model1_IFLA_IPOIB_UMCAST,
 Model1___IFLA_IPOIB_MAX
};

enum {
 Model1_IPOIB_MODE_DATAGRAM = 0, /* using unreliable datagram QPs */
 Model1_IPOIB_MODE_CONNECTED = 1, /* using connected QPs */
};




/* HSR section */

enum {
 Model1_IFLA_HSR_UNSPEC,
 Model1_IFLA_HSR_SLAVE1,
 Model1_IFLA_HSR_SLAVE2,
 Model1_IFLA_HSR_MULTICAST_SPEC, /* Last byte of supervision addr */
 Model1_IFLA_HSR_SUPERVISION_ADDR, /* Supervision frame multicast addr */
 Model1_IFLA_HSR_SEQ_NR,
 Model1_IFLA_HSR_VERSION, /* HSR version */
 Model1___IFLA_HSR_MAX,
};



/* STATS section */

struct Model1_if_stats_msg {
 __u8 Model1_family;
 __u8 Model1_pad1;
 Model1___u16 Model1_pad2;
 __u32 Model1_ifindex;
 __u32 Model1_filter_mask;
};

/* A stats attribute can be netdev specific or a global stat.
 * For netdev stats, lets use the prefix IFLA_STATS_LINK_*
 */
enum {
 Model1_IFLA_STATS_UNSPEC, /* also used as 64bit pad attribute */
 Model1_IFLA_STATS_LINK_64,
 Model1_IFLA_STATS_LINK_XSTATS,
 Model1_IFLA_STATS_LINK_XSTATS_SLAVE,
 Model1___IFLA_STATS_MAX,
};





/* These are embedded into IFLA_STATS_LINK_XSTATS:
 * [IFLA_STATS_LINK_XSTATS]
 * -> [LINK_XSTATS_TYPE_xxx]
 *    -> [rtnl link type specific attributes]
 */
enum {
 Model1_LINK_XSTATS_TYPE_UNSPEC,
 Model1_LINK_XSTATS_TYPE_BRIDGE,
 Model1___LINK_XSTATS_TYPE_MAX
};


/* XDP section */

enum {
 Model1_IFLA_XDP_UNSPEC,
 Model1_IFLA_XDP_FD,
 Model1_IFLA_XDP_ATTACHED,
 Model1___IFLA_XDP_MAX,
};


/* We don't want this structure exposed to user space */
struct Model1_ifla_vf_stats {
 __u64 Model1_rx_packets;
 __u64 Model1_tx_packets;
 __u64 Model1_rx_bytes;
 __u64 Model1_tx_bytes;
 __u64 Model1_broadcast;
 __u64 Model1_multicast;
};

struct Model1_ifla_vf_info {
 __u32 Model1_vf;
 __u8 Model1_mac[32];
 __u32 Model1_vlan;
 __u32 Model1_qos;
 __u32 Model1_spoofchk;
 __u32 Model1_linkstate;
 __u32 Model1_min_tx_rate;
 __u32 Model1_max_tx_rate;
 __u32 Model1_rss_query_en;
 __u32 Model1_trusted;
};




/* Initial net device group. All devices belong to group 0 by default. */



/* interface name assignment types (sysfs name_assign_type attribute) */






/* Media selection options. */
enum {
        Model1_IF_PORT_UNKNOWN = 0,
        Model1_IF_PORT_10BASE2,
        Model1_IF_PORT_10BASET,
        Model1_IF_PORT_AUI,
        Model1_IF_PORT_100BASET,
        Model1_IF_PORT_100BASETX,
        Model1_IF_PORT_100BASEFX
};

/* hardware address assignment types */
/*
 * Bond several ethernet interfaces into a Cisco, running 'Etherchannel'.
 *
 *
 * Portions are (c) Copyright 1995 Simon "Guru Aleph-Null" Janes
 * NCM: Network and Communications Management, Inc.
 *
 * BUT, I'm the one who modified it for ethernet, so:
 * (c) Copyright 1999, Thomas Davis, tadavis@lbl.gov
 *
 *	This software may be used and distributed according to the terms
 *	of the GNU Public License, incorporated herein by reference.
 *
 * 2003/03/18 - Amir Noam <amir.noam at intel dot com>
 *	- Added support for getting slave's speed and duplex via ethtool.
 *	  Needed for 802.3ad and other future modes.
 *
 * 2003/03/18 - Tsippy Mendelson <tsippy.mendelson at intel dot com> and
 *		Shmulik Hen <shmulik.hen at intel dot com>
 *	- Enable support of modes that need to use the unique mac address of
 *	  each slave.
 *
 * 2003/03/18 - Tsippy Mendelson <tsippy.mendelson at intel dot com> and
 *		Amir Noam <amir.noam at intel dot com>
 *	- Moved driver's private data types to bonding.h
 *
 * 2003/03/18 - Amir Noam <amir.noam at intel dot com>,
 *		Tsippy Mendelson <tsippy.mendelson at intel dot com> and
 *		Shmulik Hen <shmulik.hen at intel dot com>
 *	- Added support for IEEE 802.3ad Dynamic link aggregation mode.
 *
 * 2003/05/01 - Amir Noam <amir.noam at intel dot com>
 *	- Added ABI version control to restore compatibility between
 *	  new/old ifenslave and new/old bonding.
 *
 * 2003/12/01 - Shmulik Hen <shmulik.hen at intel dot com>
 *	- Code cleanup and style changes
 *
 * 2005/05/05 - Jason Gabler <jygabler at lbl dot gov>
 *      - added definitions for various XOR hashing policies
 */
/* userland - kernel ABI version (2003/05/08) */


/*
 * We can remove these ioctl definitions in 2.5.  People should use the
 * SIOC*** versions of them instead
 */
/* each slave's link has 4 states */





/* each slave has several states */
/* hashing types */






typedef struct Model1_ifbond {
 Model1___s32 Model1_bond_mode;
 Model1___s32 Model1_num_slaves;
 Model1___s32 Model1_miimon;
} Model1_ifbond;

typedef struct Model1_ifslave {
 Model1___s32 Model1_slave_id; /* Used as an IN param to the BOND_SLAVE_INFO_QUERY ioctl */
 char Model1_slave_name[16];
 Model1___s8 Model1_link;
 Model1___s8 Model1_state;
 __u32 Model1_link_failure_count;
} Model1_ifslave;

struct Model1_ad_info {
 Model1___u16 Model1_aggregator_id;
 Model1___u16 Model1_ports;
 Model1___u16 Model1_actor_key;
 Model1___u16 Model1_partner_key;
 __u8 Model1_partner_system[6];
};



/*
 * Local variables:
 *  version-control: t
 *  kept-new-versions: 5
 *  c-indent-level: 8
 *  c-basic-offset: 8
 *  tab-width: 8
 * End:
 */









/* Logical priority bands not depending on specific packet scheduler.
   Every scheduler will map them to real traffic classes, if it has
   no more precise mechanism to classify packets.

   These numbers have no special meaning, though their coincidence
   with obsolete IPv6 values is not occasional :-). New IPv6 drafts
   preferred full anarchy inspired by diffserv group.

   Note: TC_PRIO_BESTEFFORT does not mean that it is the most unhappy
   class, actually, as rule it will be handled with more care than
   filler or even bulk.
 */
/* Generic queue statistics, available for all the elements.
   Particular schedulers may have also their private records.
 */

struct Model1_tc_stats {
 __u64 Model1_bytes; /* Number of enqueued bytes */
 __u32 Model1_packets; /* Number of enqueued packets	*/
 __u32 Model1_drops; /* Packets dropped because of lack of resources */
 __u32 Model1_overlimits; /* Number of throttle events when this
					 * flow goes out of allocated bandwidth */
 __u32 Model1_bps; /* Current flow byte rate */
 __u32 Model1_pps; /* Current flow packet rate */
 __u32 Model1_qlen;
 __u32 Model1_backlog;
};

struct Model1_tc_estimator {
 signed char Model1_interval;
 unsigned char Model1_ewma_log;
};

/* "Handles"
   ---------

    All the traffic control objects have 32bit identifiers, or "handles".

    They can be considered as opaque numbers from user API viewpoint,
    but actually they always consist of two fields: major and
    minor numbers, which are interpreted by kernel specially,
    that may be used by applications, though not recommended.

    F.e. qdisc handles always have minor number equal to zero,
    classes (or flows) have major equal to parent qdisc major, and
    minor uniquely identifying class inside qdisc.

    Macros to manipulate handles:
 */
/* Need to corrospond to iproute2 tc/tc_core.h "enum link_layer" */
enum Model1_tc_link_layer {
 Model1_TC_LINKLAYER_UNAWARE, /* Indicate unaware old iproute2 util */
 Model1_TC_LINKLAYER_ETHERNET,
 Model1_TC_LINKLAYER_ATM,
};


struct Model1_tc_ratespec {
 unsigned char Model1_cell_log;
 __u8 Model1_linklayer; /* lower 4 bits */
 unsigned short Model1_overhead;
 short Model1_cell_align;
 unsigned short Model1_mpu;
 __u32 Model1_rate;
};



struct Model1_tc_sizespec {
 unsigned char Model1_cell_log;
 unsigned char Model1_size_log;
 short Model1_cell_align;
 int Model1_overhead;
 unsigned int Model1_linklayer;
 unsigned int Model1_mpu;
 unsigned int Model1_mtu;
 unsigned int Model1_tsize;
};

enum {
 Model1_TCA_STAB_UNSPEC,
 Model1_TCA_STAB_BASE,
 Model1_TCA_STAB_DATA,
 Model1___TCA_STAB_MAX
};



/* FIFO section */

struct Model1_tc_fifo_qopt {
 __u32 Model1_limit; /* Queue length: bytes for bfifo, packets for pfifo */
};

/* PRIO section */




struct Model1_tc_prio_qopt {
 int Model1_bands; /* Number of bands */
 __u8 Model1_priomap[15 +1]; /* Map: logical priority -> PRIO band */
};

/* MULTIQ section */

struct Model1_tc_multiq_qopt {
 Model1___u16 Model1_bands; /* Number of bands */
 Model1___u16 Model1_max_bands; /* Maximum number of queues */
};

/* PLUG section */






struct Model1_tc_plug_qopt {
 /* TCQ_PLUG_BUFFER: Inset a plug into the queue and
	 *  buffer any incoming packets
	 * TCQ_PLUG_RELEASE_ONE: Dequeue packets from queue head
	 *   to beginning of the next plug.
	 * TCQ_PLUG_RELEASE_INDEFINITE: Dequeue all packets from queue.
	 *   Stop buffering packets until the next TCQ_PLUG_BUFFER
	 *   command is received (just act as a pass-thru queue).
	 * TCQ_PLUG_LIMIT: Increase/decrease queue size
	 */
 int Model1_action;
 __u32 Model1_limit;
};

/* TBF section */

struct Model1_tc_tbf_qopt {
 struct Model1_tc_ratespec Model1_rate;
 struct Model1_tc_ratespec Model1_peakrate;
 __u32 Model1_limit;
 __u32 Model1_buffer;
 __u32 Model1_mtu;
};

enum {
 Model1_TCA_TBF_UNSPEC,
 Model1_TCA_TBF_PARMS,
 Model1_TCA_TBF_RTAB,
 Model1_TCA_TBF_PTAB,
 Model1_TCA_TBF_RATE64,
 Model1_TCA_TBF_PRATE64,
 Model1_TCA_TBF_BURST,
 Model1_TCA_TBF_PBURST,
 Model1_TCA_TBF_PAD,
 Model1___TCA_TBF_MAX,
};




/* TEQL section */

/* TEQL does not require any parameters */

/* SFQ section */

struct Model1_tc_sfq_qopt {
 unsigned Model1_quantum; /* Bytes per round allocated to flow */
 int Model1_perturb_period; /* Period of hash perturbation */
 __u32 Model1_limit; /* Maximal packets in queue */
 unsigned Model1_divisor; /* Hash divisor  */
 unsigned Model1_flows; /* Maximal number of flows  */
};

struct Model1_tc_sfqred_stats {
 __u32 Model1_prob_drop; /* Early drops, below max threshold */
 __u32 Model1_forced_drop; /* Early drops, after max threshold */
 __u32 Model1_prob_mark; /* Marked packets, below max threshold */
 __u32 Model1_forced_mark; /* Marked packets, after max threshold */
 __u32 Model1_prob_mark_head; /* Marked packets, below max threshold */
 __u32 Model1_forced_mark_head;/* Marked packets, after max threshold */
};

struct Model1_tc_sfq_qopt_v1 {
 struct Model1_tc_sfq_qopt Model1_v0;
 unsigned int Model1_depth; /* max number of packets per flow */
 unsigned int Model1_headdrop;
/* SFQRED parameters */
 __u32 Model1_limit; /* HARD maximal flow queue length (bytes) */
 __u32 Model1_qth_min; /* Min average length threshold (bytes) */
 __u32 Model1_qth_max; /* Max average length threshold (bytes) */
 unsigned char Model1_Wlog; /* log(W)		*/
 unsigned char Model1_Plog; /* log(P_max/(qth_max-qth_min))	*/
 unsigned char Model1_Scell_log; /* cell size for idle damping */
 unsigned char Model1_flags;
 __u32 Model1_max_P; /* probability, high resolution */
/* SFQRED stats */
 struct Model1_tc_sfqred_stats Model1_stats;
};


struct Model1_tc_sfq_xstats {
 Model1___s32 Model1_allot;
};

/* RED section */

enum {
 Model1_TCA_RED_UNSPEC,
 Model1_TCA_RED_PARMS,
 Model1_TCA_RED_STAB,
 Model1_TCA_RED_MAX_P,
 Model1___TCA_RED_MAX,
};



struct Model1_tc_red_qopt {
 __u32 Model1_limit; /* HARD maximal queue length (bytes)	*/
 __u32 Model1_qth_min; /* Min average length threshold (bytes) */
 __u32 Model1_qth_max; /* Max average length threshold (bytes) */
 unsigned char Model1_Wlog; /* log(W)		*/
 unsigned char Model1_Plog; /* log(P_max/(qth_max-qth_min))	*/
 unsigned char Model1_Scell_log; /* cell size for idle damping */
 unsigned char Model1_flags;



};

struct Model1_tc_red_xstats {
 __u32 Model1_early; /* Early drops */
 __u32 Model1_pdrop; /* Drops due to queue limits */
 __u32 Model1_other; /* Drops due to drop() calls */
 __u32 Model1_marked; /* Marked packets */
};

/* GRED section */



enum {
       Model1_TCA_GRED_UNSPEC,
       Model1_TCA_GRED_PARMS,
       Model1_TCA_GRED_STAB,
       Model1_TCA_GRED_DPS,
       Model1_TCA_GRED_MAX_P,
       Model1_TCA_GRED_LIMIT,
       Model1___TCA_GRED_MAX,
};



struct Model1_tc_gred_qopt {
 __u32 Model1_limit; /* HARD maximal queue length (bytes)    */
 __u32 Model1_qth_min; /* Min average length threshold (bytes) */
 __u32 Model1_qth_max; /* Max average length threshold (bytes) */
 __u32 Model1_DP; /* up to 2^32 DPs */
 __u32 Model1_backlog;
 __u32 Model1_qave;
 __u32 Model1_forced;
 __u32 Model1_early;
 __u32 Model1_other;
 __u32 Model1_pdrop;
 __u8 Model1_Wlog; /* log(W)               */
 __u8 Model1_Plog; /* log(P_max/(qth_max-qth_min)) */
 __u8 Model1_Scell_log; /* cell size for idle damping */
 __u8 Model1_prio; /* prio of this VQ */
 __u32 Model1_packets;
 __u32 Model1_bytesin;
};

/* gred setup */
struct Model1_tc_gred_sopt {
 __u32 Model1_DPs;
 __u32 Model1_def_DP;
 __u8 Model1_grio;
 __u8 Model1_flags;
 Model1___u16 Model1_pad1;
};

/* CHOKe section */

enum {
 Model1_TCA_CHOKE_UNSPEC,
 Model1_TCA_CHOKE_PARMS,
 Model1_TCA_CHOKE_STAB,
 Model1_TCA_CHOKE_MAX_P,
 Model1___TCA_CHOKE_MAX,
};



struct Model1_tc_choke_qopt {
 __u32 Model1_limit; /* Hard queue length (packets)	*/
 __u32 Model1_qth_min; /* Min average threshold (packets) */
 __u32 Model1_qth_max; /* Max average threshold (packets) */
 unsigned char Model1_Wlog; /* log(W)		*/
 unsigned char Model1_Plog; /* log(P_max/(qth_max-qth_min))	*/
 unsigned char Model1_Scell_log; /* cell size for idle damping */
 unsigned char Model1_flags; /* see RED flags */
};

struct Model1_tc_choke_xstats {
 __u32 Model1_early; /* Early drops */
 __u32 Model1_pdrop; /* Drops due to queue limits */
 __u32 Model1_other; /* Drops due to drop() calls */
 __u32 Model1_marked; /* Marked packets */
 __u32 Model1_matched; /* Drops due to flow match */
};

/* HTB section */




struct Model1_tc_htb_opt {
 struct Model1_tc_ratespec Model1_rate;
 struct Model1_tc_ratespec Model1_ceil;
 __u32 Model1_buffer;
 __u32 Model1_cbuffer;
 __u32 Model1_quantum;
 __u32 Model1_level; /* out only */
 __u32 Model1_prio;
};
struct Model1_tc_htb_glob {
 __u32 Model1_version; /* to match HTB/TC */
     __u32 Model1_rate2quantum; /* bps->quantum divisor */
     __u32 Model1_defcls; /* default class number */
 __u32 Model1_debug; /* debug flags */

 /* stats */
 __u32 Model1_direct_pkts; /* count of non shaped packets */
};
enum {
 Model1_TCA_HTB_UNSPEC,
 Model1_TCA_HTB_PARMS,
 Model1_TCA_HTB_INIT,
 Model1_TCA_HTB_CTAB,
 Model1_TCA_HTB_RTAB,
 Model1_TCA_HTB_DIRECT_QLEN,
 Model1_TCA_HTB_RATE64,
 Model1_TCA_HTB_CEIL64,
 Model1_TCA_HTB_PAD,
 Model1___TCA_HTB_MAX,
};



struct Model1_tc_htb_xstats {
 __u32 Model1_lends;
 __u32 Model1_borrows;
 __u32 Model1_giants; /* too big packets (rate will not be accurate) */
 __u32 Model1_tokens;
 __u32 Model1_ctokens;
};

/* HFSC section */

struct Model1_tc_hfsc_qopt {
 Model1___u16 Model1_defcls; /* default class */
};

struct Model1_tc_service_curve {
 __u32 Model1_m1; /* slope of the first segment in bps */
 __u32 Model1_d; /* x-projection of the first segment in us */
 __u32 Model1_m2; /* slope of the second segment in bps */
};

struct Model1_tc_hfsc_stats {
 __u64 Model1_work; /* total work done */
 __u64 Model1_rtwork; /* work done by real-time criteria */
 __u32 Model1_period; /* current period */
 __u32 Model1_level; /* class level in hierarchy */
};

enum {
 Model1_TCA_HFSC_UNSPEC,
 Model1_TCA_HFSC_RSC,
 Model1_TCA_HFSC_FSC,
 Model1_TCA_HFSC_USC,
 Model1___TCA_HFSC_MAX,
};




/* CBQ section */





struct Model1_tc_cbq_lssopt {
 unsigned char Model1_change;
 unsigned char Model1_flags;


 unsigned char Model1_ewma_log;
 unsigned char Model1_level;






 __u32 Model1_maxidle;
 __u32 Model1_minidle;
 __u32 Model1_offtime;
 __u32 Model1_avpkt;
};

struct Model1_tc_cbq_wrropt {
 unsigned char Model1_flags;
 unsigned char Model1_priority;
 unsigned char Model1_cpriority;
 unsigned char Model1___reserved;
 __u32 Model1_allot;
 __u32 Model1_weight;
};

struct Model1_tc_cbq_ovl {
 unsigned char Model1_strategy;





 unsigned char Model1_priority2;
 Model1___u16 Model1_pad;
 __u32 Model1_penalty;
};

struct Model1_tc_cbq_police {
 unsigned char Model1_police;
 unsigned char Model1___res1;
 unsigned short Model1___res2;
};

struct Model1_tc_cbq_fopt {
 __u32 Model1_split;
 __u32 Model1_defmap;
 __u32 Model1_defchange;
};

struct Model1_tc_cbq_xstats {
 __u32 Model1_borrows;
 __u32 Model1_overactions;
 Model1___s32 Model1_avgidle;
 Model1___s32 Model1_undertime;
};

enum {
 Model1_TCA_CBQ_UNSPEC,
 Model1_TCA_CBQ_LSSOPT,
 Model1_TCA_CBQ_WRROPT,
 Model1_TCA_CBQ_FOPT,
 Model1_TCA_CBQ_OVL_STRATEGY,
 Model1_TCA_CBQ_RATE,
 Model1_TCA_CBQ_RTAB,
 Model1_TCA_CBQ_POLICE,
 Model1___TCA_CBQ_MAX,
};



/* dsmark section */

enum {
 Model1_TCA_DSMARK_UNSPEC,
 Model1_TCA_DSMARK_INDICES,
 Model1_TCA_DSMARK_DEFAULT_INDEX,
 Model1_TCA_DSMARK_SET_TC_INDEX,
 Model1_TCA_DSMARK_MASK,
 Model1_TCA_DSMARK_VALUE,
 Model1___TCA_DSMARK_MAX,
};



/* ATM  section */

enum {
 Model1_TCA_ATM_UNSPEC,
 Model1_TCA_ATM_FD, /* file/socket descriptor */
 Model1_TCA_ATM_PTR, /* pointer to descriptor - later */
 Model1_TCA_ATM_HDR, /* LL header */
 Model1_TCA_ATM_EXCESS, /* excess traffic class (0 for CLP)  */
 Model1_TCA_ATM_ADDR, /* PVC address (for output only) */
 Model1_TCA_ATM_STATE, /* VC state (ATM_VS_*; for output only) */
 Model1___TCA_ATM_MAX,
};



/* Network emulator */

enum {
 Model1_TCA_NETEM_UNSPEC,
 Model1_TCA_NETEM_CORR,
 Model1_TCA_NETEM_DELAY_DIST,
 Model1_TCA_NETEM_REORDER,
 Model1_TCA_NETEM_CORRUPT,
 Model1_TCA_NETEM_LOSS,
 Model1_TCA_NETEM_RATE,
 Model1_TCA_NETEM_ECN,
 Model1_TCA_NETEM_RATE64,
 Model1_TCA_NETEM_PAD,
 Model1___TCA_NETEM_MAX,
};



struct Model1_tc_netem_qopt {
 __u32 Model1_latency; /* added delay (us) */
 __u32 Model1_limit; /* fifo limit (packets) */
 __u32 Model1_loss; /* random packet loss (0=none ~0=100%) */
 __u32 Model1_gap; /* re-ordering gap (0 for none) */
 __u32 Model1_duplicate; /* random packet dup  (0=none ~0=100%) */
 __u32 Model1_jitter; /* random jitter in latency (us) */
};

struct Model1_tc_netem_corr {
 __u32 Model1_delay_corr; /* delay correlation */
 __u32 Model1_loss_corr; /* packet loss correlation */
 __u32 Model1_dup_corr; /* duplicate correlation  */
};

struct Model1_tc_netem_reorder {
 __u32 Model1_probability;
 __u32 Model1_correlation;
};

struct Model1_tc_netem_corrupt {
 __u32 Model1_probability;
 __u32 Model1_correlation;
};

struct Model1_tc_netem_rate {
 __u32 Model1_rate; /* byte/s */
 Model1___s32 Model1_packet_overhead;
 __u32 Model1_cell_size;
 Model1___s32 Model1_cell_overhead;
};

enum {
 Model1_NETEM_LOSS_UNSPEC,
 Model1_NETEM_LOSS_GI, /* General Intuitive - 4 state model */
 Model1_NETEM_LOSS_GE, /* Gilbert Elliot models */
 Model1___NETEM_LOSS_MAX
};


/* State transition probabilities for 4 state model */
struct Model1_tc_netem_gimodel {
 __u32 Model1_p13;
 __u32 Model1_p31;
 __u32 Model1_p32;
 __u32 Model1_p14;
 __u32 Model1_p23;
};

/* Gilbert-Elliot models */
struct Model1_tc_netem_gemodel {
 __u32 Model1_p;
 __u32 Model1_r;
 __u32 Model1_h;
 __u32 Model1_k1;
};




/* DRR */

enum {
 Model1_TCA_DRR_UNSPEC,
 Model1_TCA_DRR_QUANTUM,
 Model1___TCA_DRR_MAX
};



struct Model1_tc_drr_stats {
 __u32 Model1_deficit;
};

/* MQPRIO */



struct Model1_tc_mqprio_qopt {
 __u8 Model1_num_tc;
 __u8 Model1_prio_tc_map[15 + 1];
 __u8 Model1_hw;
 Model1___u16 Model1_count[16];
 Model1___u16 Model1_offset[16];
};

/* SFB */

enum {
 Model1_TCA_SFB_UNSPEC,
 Model1_TCA_SFB_PARMS,
 Model1___TCA_SFB_MAX,
};



/*
 * Note: increment, decrement are Q0.16 fixed-point values.
 */
struct Model1_tc_sfb_qopt {
 __u32 Model1_rehash_interval; /* delay between hash move, in ms */
 __u32 Model1_warmup_time; /* double buffering warmup time in ms (warmup_time < rehash_interval) */
 __u32 Model1_max; /* max len of qlen_min */
 __u32 Model1_bin_size; /* maximum queue length per bin */
 __u32 Model1_increment; /* probability increment, (d1 in Blue) */
 __u32 Model1_decrement; /* probability decrement, (d2 in Blue) */
 __u32 Model1_limit; /* max SFB queue length */
 __u32 Model1_penalty_rate; /* inelastic flows are rate limited to 'rate' pps */
 __u32 Model1_penalty_burst;
};

struct Model1_tc_sfb_xstats {
 __u32 Model1_earlydrop;
 __u32 Model1_penaltydrop;
 __u32 Model1_bucketdrop;
 __u32 Model1_queuedrop;
 __u32 Model1_childdrop; /* drops in child qdisc */
 __u32 Model1_marked;
 __u32 Model1_maxqlen;
 __u32 Model1_maxprob;
 __u32 Model1_avgprob;
};



/* QFQ */
enum {
 Model1_TCA_QFQ_UNSPEC,
 Model1_TCA_QFQ_WEIGHT,
 Model1_TCA_QFQ_LMAX,
 Model1___TCA_QFQ_MAX
};



struct Model1_tc_qfq_stats {
 __u32 Model1_weight;
 __u32 Model1_lmax;
};

/* CODEL */

enum {
 Model1_TCA_CODEL_UNSPEC,
 Model1_TCA_CODEL_TARGET,
 Model1_TCA_CODEL_LIMIT,
 Model1_TCA_CODEL_INTERVAL,
 Model1_TCA_CODEL_ECN,
 Model1_TCA_CODEL_CE_THRESHOLD,
 Model1___TCA_CODEL_MAX
};



struct Model1_tc_codel_xstats {
 __u32 Model1_maxpacket; /* largest packet we've seen so far */
 __u32 Model1_count; /* how many drops we've done since the last time we
			    * entered dropping state
			    */
 __u32 Model1_lastcount; /* count at entry to dropping state */
 __u32 Model1_ldelay; /* in-queue delay seen by most recently dequeued packet */
 Model1___s32 Model1_drop_next; /* time to drop next packet */
 __u32 Model1_drop_overlimit; /* number of time max qdisc packet limit was hit */
 __u32 Model1_ecn_mark; /* number of packets we ECN marked instead of dropped */
 __u32 Model1_dropping; /* are we in dropping state ? */
 __u32 Model1_ce_mark; /* number of CE marked packets because of ce_threshold */
};

/* FQ_CODEL */

enum {
 Model1_TCA_FQ_CODEL_UNSPEC,
 Model1_TCA_FQ_CODEL_TARGET,
 Model1_TCA_FQ_CODEL_LIMIT,
 Model1_TCA_FQ_CODEL_INTERVAL,
 Model1_TCA_FQ_CODEL_ECN,
 Model1_TCA_FQ_CODEL_FLOWS,
 Model1_TCA_FQ_CODEL_QUANTUM,
 Model1_TCA_FQ_CODEL_CE_THRESHOLD,
 Model1_TCA_FQ_CODEL_DROP_BATCH_SIZE,
 Model1_TCA_FQ_CODEL_MEMORY_LIMIT,
 Model1___TCA_FQ_CODEL_MAX
};



enum {
 Model1_TCA_FQ_CODEL_XSTATS_QDISC,
 Model1_TCA_FQ_CODEL_XSTATS_CLASS,
};

struct Model1_tc_fq_codel_qd_stats {
 __u32 Model1_maxpacket; /* largest packet we've seen so far */
 __u32 Model1_drop_overlimit; /* number of time max qdisc
				 * packet limit was hit
				 */
 __u32 Model1_ecn_mark; /* number of packets we ECN marked
				 * instead of being dropped
				 */
 __u32 Model1_new_flow_count; /* number of time packets
				 * created a 'new flow'
				 */
 __u32 Model1_new_flows_len; /* count of flows in new list */
 __u32 Model1_old_flows_len; /* count of flows in old list */
 __u32 Model1_ce_mark; /* packets above ce_threshold */
 __u32 Model1_memory_usage; /* in bytes */
 __u32 Model1_drop_overmemory;
};

struct Model1_tc_fq_codel_cl_stats {
 Model1___s32 Model1_deficit;
 __u32 Model1_ldelay; /* in-queue delay seen by most recently
				 * dequeued packet
				 */
 __u32 Model1_count;
 __u32 Model1_lastcount;
 __u32 Model1_dropping;
 Model1___s32 Model1_drop_next;
};

struct Model1_tc_fq_codel_xstats {
 __u32 Model1_type;
 union {
  struct Model1_tc_fq_codel_qd_stats Model1_qdisc_stats;
  struct Model1_tc_fq_codel_cl_stats Model1_class_stats;
 };
};

/* FQ */

enum {
 Model1_TCA_FQ_UNSPEC,

 Model1_TCA_FQ_PLIMIT, /* limit of total number of packets in queue */

 Model1_TCA_FQ_FLOW_PLIMIT, /* limit of packets per flow */

 Model1_TCA_FQ_QUANTUM, /* RR quantum */

 Model1_TCA_FQ_INITIAL_QUANTUM, /* RR quantum for new flow */

 Model1_TCA_FQ_RATE_ENABLE, /* enable/disable rate limiting */

 Model1_TCA_FQ_FLOW_DEFAULT_RATE,/* obsolete, do not use */

 Model1_TCA_FQ_FLOW_MAX_RATE, /* per flow max rate */

 Model1_TCA_FQ_BUCKETS_LOG, /* log2(number of buckets) */

 Model1_TCA_FQ_FLOW_REFILL_DELAY, /* flow credit refill delay in usec */

 Model1_TCA_FQ_ORPHAN_MASK, /* mask applied to orphaned skb hashes */

 Model1___TCA_FQ_MAX
};



struct Model1_tc_fq_qd_stats {
 __u64 Model1_gc_flows;
 __u64 Model1_highprio_packets;
 __u64 Model1_tcp_retrans;
 __u64 Model1_throttled;
 __u64 Model1_flows_plimit;
 __u64 Model1_pkts_too_long;
 __u64 Model1_allocation_errors;
 Model1___s64 Model1_time_next_delayed_flow;
 __u32 Model1_flows;
 __u32 Model1_inactive_flows;
 __u32 Model1_throttled_flows;
 __u32 Model1_pad;
};

/* Heavy-Hitter Filter */

enum {
 Model1_TCA_HHF_UNSPEC,
 Model1_TCA_HHF_BACKLOG_LIMIT,
 Model1_TCA_HHF_QUANTUM,
 Model1_TCA_HHF_HH_FLOWS_LIMIT,
 Model1_TCA_HHF_RESET_TIMEOUT,
 Model1_TCA_HHF_ADMIT_BYTES,
 Model1_TCA_HHF_EVICT_TIMEOUT,
 Model1_TCA_HHF_NON_HH_WEIGHT,
 Model1___TCA_HHF_MAX
};



struct Model1_tc_hhf_xstats {
 __u32 Model1_drop_overlimit; /* number of times max qdisc packet limit
				 * was hit
				 */
 __u32 Model1_hh_overlimit; /* number of times max heavy-hitters was hit */
 __u32 Model1_hh_tot_count; /* number of captured heavy-hitters so far */
 __u32 Model1_hh_cur_count; /* number of current heavy-hitters */
};

/* PIE */
enum {
 Model1_TCA_PIE_UNSPEC,
 Model1_TCA_PIE_TARGET,
 Model1_TCA_PIE_LIMIT,
 Model1_TCA_PIE_TUPDATE,
 Model1_TCA_PIE_ALPHA,
 Model1_TCA_PIE_BETA,
 Model1_TCA_PIE_ECN,
 Model1_TCA_PIE_BYTEMODE,
 Model1___TCA_PIE_MAX
};


struct Model1_tc_pie_xstats {
 __u32 Model1_prob; /* current probability */
 __u32 Model1_delay; /* current delay in ms */
 __u32 Model1_avg_dq_rate; /* current average dq_rate in bits/pie_time */
 __u32 Model1_packets_in; /* total number of packets enqueued */
 __u32 Model1_dropped; /* packets dropped due to pie_action */
 __u32 Model1_overlimit; /* dropped due to lack of space in queue */
 __u32 Model1_maxq; /* maximum queue size */
 __u32 Model1_ecn_mark; /* packets marked with ecn*/
};


/* I think i could have done better macros ; for now this is stolen from
 * some arch/mips code - jhs
*/







/* verdict bit breakdown 
 *
bit 0: when set -> this packet has been munged already

bit 1: when set -> It is ok to munge this packet

bit 2,3,4,5: Reclassify counter - sort of reverse TTL - if exceeded
assume loop

bit 6,7: Where this packet was last seen 
0: Above the transmit example at the socket level
1: on the Ingress
2: on the Egress

bit 8: when set --> Request not to classify on ingress. 

bits 9,10,11: redirect counter -  redirect TTL. Loop avoidance

 *
 * */
/* Action attributes */
enum {
 Model1_TCA_ACT_UNSPEC,
 Model1_TCA_ACT_KIND,
 Model1_TCA_ACT_OPTIONS,
 Model1_TCA_ACT_INDEX,
 Model1_TCA_ACT_STATS,
 Model1_TCA_ACT_PAD,
 Model1___TCA_ACT_MAX
};
/* Action type identifiers*/
enum {
 Model1_TCA_ID_UNSPEC=0,
 Model1_TCA_ID_POLICE=1,
 /* other actions go here */
 Model1___TCA_ID_MAX=255
};



struct Model1_tc_police {
 __u32 Model1_index;
 int Model1_action;






 __u32 Model1_limit;
 __u32 Model1_burst;
 __u32 Model1_mtu;
 struct Model1_tc_ratespec Model1_rate;
 struct Model1_tc_ratespec Model1_peakrate;
 int Model1_refcnt;
 int Model1_bindcnt;
 __u32 Model1_capab;
};

struct Model1_tcf_t {
 __u64 Model1_install;
 __u64 Model1_lastuse;
 __u64 Model1_expires;
 __u64 Model1_firstuse;
};

struct Model1_tc_cnt {
 int Model1_refcnt;
 int Model1_bindcnt;
};
enum {
 Model1_TCA_POLICE_UNSPEC,
 Model1_TCA_POLICE_TBF,
 Model1_TCA_POLICE_RATE,
 Model1_TCA_POLICE_PEAKRATE,
 Model1_TCA_POLICE_AVRATE,
 Model1_TCA_POLICE_RESULT,
 Model1_TCA_POLICE_TM,
 Model1_TCA_POLICE_PAD,
 Model1___TCA_POLICE_MAX

};



/* tca flags definitions */



/* U32 filters */
enum {
 Model1_TCA_U32_UNSPEC,
 Model1_TCA_U32_CLASSID,
 Model1_TCA_U32_HASH,
 Model1_TCA_U32_LINK,
 Model1_TCA_U32_DIVISOR,
 Model1_TCA_U32_SEL,
 Model1_TCA_U32_POLICE,
 Model1_TCA_U32_ACT,
 Model1_TCA_U32_INDEV,
 Model1_TCA_U32_PCNT,
 Model1_TCA_U32_MARK,
 Model1_TCA_U32_FLAGS,
 Model1_TCA_U32_PAD,
 Model1___TCA_U32_MAX
};



struct Model1_tc_u32_key {
 Model1___be32 Model1_mask;
 Model1___be32 Model1_val;
 int Model1_off;
 int Model1_offmask;
};

struct Model1_tc_u32_sel {
 unsigned char Model1_flags;
 unsigned char Model1_offshift;
 unsigned char Model1_nkeys;

 Model1___be16 Model1_offmask;
 Model1___u16 Model1_off;
 short Model1_offoff;

 short Model1_hoff;
 Model1___be32 Model1_hmask;
 struct Model1_tc_u32_key Model1_keys[0];
};

struct Model1_tc_u32_mark {
 __u32 Model1_val;
 __u32 Model1_mask;
 __u32 Model1_success;
};

struct Model1_tc_u32_pcnt {
 __u64 Model1_rcnt;
 __u64 Model1_rhit;
 __u64 Model1_kcnts[0];
};

/* Flags */
/* RSVP filter */

enum {
 Model1_TCA_RSVP_UNSPEC,
 Model1_TCA_RSVP_CLASSID,
 Model1_TCA_RSVP_DST,
 Model1_TCA_RSVP_SRC,
 Model1_TCA_RSVP_PINFO,
 Model1_TCA_RSVP_POLICE,
 Model1_TCA_RSVP_ACT,
 Model1___TCA_RSVP_MAX
};



struct Model1_tc_rsvp_gpi {
 __u32 Model1_key;
 __u32 Model1_mask;
 int Model1_offset;
};

struct Model1_tc_rsvp_pinfo {
 struct Model1_tc_rsvp_gpi Model1_dpi;
 struct Model1_tc_rsvp_gpi Model1_spi;
 __u8 Model1_protocol;
 __u8 Model1_tunnelid;
 __u8 Model1_tunnelhdr;
 __u8 Model1_pad;
};

/* ROUTE filter */

enum {
 Model1_TCA_ROUTE4_UNSPEC,
 Model1_TCA_ROUTE4_CLASSID,
 Model1_TCA_ROUTE4_TO,
 Model1_TCA_ROUTE4_FROM,
 Model1_TCA_ROUTE4_IIF,
 Model1_TCA_ROUTE4_POLICE,
 Model1_TCA_ROUTE4_ACT,
 Model1___TCA_ROUTE4_MAX
};




/* FW filter */

enum {
 Model1_TCA_FW_UNSPEC,
 Model1_TCA_FW_CLASSID,
 Model1_TCA_FW_POLICE,
 Model1_TCA_FW_INDEV, /*  used by CONFIG_NET_CLS_IND */
 Model1_TCA_FW_ACT, /* used by CONFIG_NET_CLS_ACT */
 Model1_TCA_FW_MASK,
 Model1___TCA_FW_MAX
};



/* TC index filter */

enum {
 Model1_TCA_TCINDEX_UNSPEC,
 Model1_TCA_TCINDEX_HASH,
 Model1_TCA_TCINDEX_MASK,
 Model1_TCA_TCINDEX_SHIFT,
 Model1_TCA_TCINDEX_FALL_THROUGH,
 Model1_TCA_TCINDEX_CLASSID,
 Model1_TCA_TCINDEX_POLICE,
 Model1_TCA_TCINDEX_ACT,
 Model1___TCA_TCINDEX_MAX
};



/* Flow filter */

enum {
 Model1_FLOW_KEY_SRC,
 Model1_FLOW_KEY_DST,
 Model1_FLOW_KEY_PROTO,
 Model1_FLOW_KEY_PROTO_SRC,
 Model1_FLOW_KEY_PROTO_DST,
 Model1_FLOW_KEY_IIF,
 Model1_FLOW_KEY_PRIORITY,
 Model1_FLOW_KEY_MARK,
 Model1_FLOW_KEY_NFCT,
 Model1_FLOW_KEY_NFCT_SRC,
 Model1_FLOW_KEY_NFCT_DST,
 Model1_FLOW_KEY_NFCT_PROTO_SRC,
 Model1_FLOW_KEY_NFCT_PROTO_DST,
 Model1_FLOW_KEY_RTCLASSID,
 Model1_FLOW_KEY_SKUID,
 Model1_FLOW_KEY_SKGID,
 Model1_FLOW_KEY_VLAN_TAG,
 Model1_FLOW_KEY_RXHASH,
 Model1___FLOW_KEY_MAX,
};



enum {
 Model1_FLOW_MODE_MAP,
 Model1_FLOW_MODE_HASH,
};

enum {
 Model1_TCA_FLOW_UNSPEC,
 Model1_TCA_FLOW_KEYS,
 Model1_TCA_FLOW_MODE,
 Model1_TCA_FLOW_BASECLASS,
 Model1_TCA_FLOW_RSHIFT,
 Model1_TCA_FLOW_ADDEND,
 Model1_TCA_FLOW_MASK,
 Model1_TCA_FLOW_XOR,
 Model1_TCA_FLOW_DIVISOR,
 Model1_TCA_FLOW_ACT,
 Model1_TCA_FLOW_POLICE,
 Model1_TCA_FLOW_EMATCHES,
 Model1_TCA_FLOW_PERTURB,
 Model1___TCA_FLOW_MAX
};



/* Basic filter */

enum {
 Model1_TCA_BASIC_UNSPEC,
 Model1_TCA_BASIC_CLASSID,
 Model1_TCA_BASIC_EMATCHES,
 Model1_TCA_BASIC_ACT,
 Model1_TCA_BASIC_POLICE,
 Model1___TCA_BASIC_MAX
};




/* Cgroup classifier */

enum {
 Model1_TCA_CGROUP_UNSPEC,
 Model1_TCA_CGROUP_ACT,
 Model1_TCA_CGROUP_POLICE,
 Model1_TCA_CGROUP_EMATCHES,
 Model1___TCA_CGROUP_MAX,
};



/* BPF classifier */



enum {
 Model1_TCA_BPF_UNSPEC,
 Model1_TCA_BPF_ACT,
 Model1_TCA_BPF_POLICE,
 Model1_TCA_BPF_CLASSID,
 Model1_TCA_BPF_OPS_LEN,
 Model1_TCA_BPF_OPS,
 Model1_TCA_BPF_FD,
 Model1_TCA_BPF_NAME,
 Model1_TCA_BPF_FLAGS,
 Model1___TCA_BPF_MAX,
};



/* Flower classifier */

enum {
 Model1_TCA_FLOWER_UNSPEC,
 Model1_TCA_FLOWER_CLASSID,
 Model1_TCA_FLOWER_INDEV,
 Model1_TCA_FLOWER_ACT,
 Model1_TCA_FLOWER_KEY_ETH_DST, /* ETH_ALEN */
 Model1_TCA_FLOWER_KEY_ETH_DST_MASK, /* ETH_ALEN */
 Model1_TCA_FLOWER_KEY_ETH_SRC, /* ETH_ALEN */
 Model1_TCA_FLOWER_KEY_ETH_SRC_MASK, /* ETH_ALEN */
 Model1_TCA_FLOWER_KEY_ETH_TYPE, /* be16 */
 Model1_TCA_FLOWER_KEY_IP_PROTO, /* u8 */
 Model1_TCA_FLOWER_KEY_IPV4_SRC, /* be32 */
 Model1_TCA_FLOWER_KEY_IPV4_SRC_MASK, /* be32 */
 Model1_TCA_FLOWER_KEY_IPV4_DST, /* be32 */
 Model1_TCA_FLOWER_KEY_IPV4_DST_MASK, /* be32 */
 Model1_TCA_FLOWER_KEY_IPV6_SRC, /* struct in6_addr */
 Model1_TCA_FLOWER_KEY_IPV6_SRC_MASK, /* struct in6_addr */
 Model1_TCA_FLOWER_KEY_IPV6_DST, /* struct in6_addr */
 Model1_TCA_FLOWER_KEY_IPV6_DST_MASK, /* struct in6_addr */
 Model1_TCA_FLOWER_KEY_TCP_SRC, /* be16 */
 Model1_TCA_FLOWER_KEY_TCP_DST, /* be16 */
 Model1_TCA_FLOWER_KEY_UDP_SRC, /* be16 */
 Model1_TCA_FLOWER_KEY_UDP_DST, /* be16 */

 Model1_TCA_FLOWER_FLAGS,
 Model1___TCA_FLOWER_MAX,
};



/* Match-all classifier */

enum {
 Model1_TCA_MATCHALL_UNSPEC,
 Model1_TCA_MATCHALL_CLASSID,
 Model1_TCA_MATCHALL_ACT,
 Model1_TCA_MATCHALL_FLAGS,
 Model1___TCA_MATCHALL_MAX,
};



/* Extended Matches */

struct Model1_tcf_ematch_tree_hdr {
 Model1___u16 Model1_nmatches;
 Model1___u16 Model1_progid;
};

enum {
 Model1_TCA_EMATCH_TREE_UNSPEC,
 Model1_TCA_EMATCH_TREE_HDR,
 Model1_TCA_EMATCH_TREE_LIST,
 Model1___TCA_EMATCH_TREE_MAX
};


struct Model1_tcf_ematch_hdr {
 Model1___u16 Model1_matchid;
 Model1___u16 Model1_kind;
 Model1___u16 Model1_flags;
 Model1___u16 Model1_pad; /* currently unused */
};

/*  0                   1
 *  0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 
 * +-----------------------+-+-+---+
 * |         Unused        |S|I| R |
 * +-----------------------+-+-+---+
 *
 * R(2) ::= relation to next ematch
 *          where: 0 0 END (last ematch)
 *                 0 1 AND
 *                 1 0 OR
 *                 1 1 Unused (invalid)
 * I(1) ::= invert result
 * S(1) ::= simple payload
 */
enum {
 Model1_TCF_LAYER_LINK,
 Model1_TCF_LAYER_NETWORK,
 Model1_TCF_LAYER_TRANSPORT,
 Model1___TCF_LAYER_MAX
};


/* Ematch type assignments
 *   1..32767		Reserved for ematches inside kernel tree
 *   32768..65535	Free to use, not reliable
 */
enum {
 Model1_TCF_EM_PROG_TC
};

enum {
 Model1_TCF_EM_OPND_EQ,
 Model1_TCF_EM_OPND_GT,
 Model1_TCF_EM_OPND_LT
};

struct Model1_netpoll_info;
struct Model1_device;
struct Model1_phy_device;
/* 802.11 specific */
struct Model1_wireless_dev;
/* 802.15.4 specific */
struct Model1_wpan_dev;
struct Model1_mpls_dev;
/* UDP Tunnel offloads */
struct Model1_udp_tunnel_info;
struct Model1_bpf_prog;

void Model1_netdev_set_default_ethtool_ops(struct Model1_net_device *Model1_dev,
        const struct Model1_ethtool_ops *Model1_ops);

/* Backlog congestion levels */



/*
 * Transmit return codes: transmit return codes originate from three different
 * namespaces:
 *
 * - qdisc return codes
 * - driver transmit return codes
 * - errno values
 *
 * Drivers are allowed to return any one of those in their hard_start_xmit()
 * function. Real network devices commonly used with qdiscs should only return
 * the driver transmit return codes though - when qdiscs are used, the actual
 * transmission happens asynchronously, so the value is not propagated to
 * higher layers. Virtual network devices transmit synchronously; in this case
 * the driver transmit return codes are consumed by dev_queue_xmit(), and all
 * others are propagated to higher layers.
 */

/* qdisc ->enqueue() return codes. */





/* NET_XMIT_CN is special. It does not guarantee that this packet is lost. It
 * indicates that the device will soon be dropping packets, or already drops
 * some packets of the same priority; prompting us to send less aggressively. */



/* Driver transmit return codes */


enum Model1_netdev_tx {
 Model1___NETDEV_TX_MIN = (-((int)(~0U>>1)) - 1), /* make sure enum is signed */
 Model1_NETDEV_TX_OK = 0x00, /* driver took care of packet */
 Model1_NETDEV_TX_BUSY = 0x10, /* driver tx path was busy*/
};
typedef enum Model1_netdev_tx Model1_netdev_tx_t;

/*
 * Current order: NETDEV_TX_MASK > NET_XMIT_MASK >= 0 is significant;
 * hard_start_xmit() return < NET_XMIT_MASK means skb was consumed.
 */
static inline __attribute__((no_instrument_function)) bool Model1_dev_xmit_complete(int Model1_rc)
{
 /*
	 * Positive cases with an skb consumed by a driver:
	 * - successful transmission (rc == NETDEV_TX_OK)
	 * - error while transmitting (rc < 0)
	 * - error while queueing to a different device (rc & NET_XMIT_MASK)
	 */
 if (__builtin_expect(!!(Model1_rc < 0x0f), 1))
  return true;

 return false;
}

/*
 *	Compute the worst-case header length according to the protocols
 *	used.
 */
/*
 *	Old network device statistics. Fields are native words
 *	(unsigned long) so they can be read and written atomically.
 */

struct Model1_net_device_stats {
 unsigned long Model1_rx_packets;
 unsigned long Model1_tx_packets;
 unsigned long Model1_rx_bytes;
 unsigned long Model1_tx_bytes;
 unsigned long Model1_rx_errors;
 unsigned long Model1_tx_errors;
 unsigned long Model1_rx_dropped;
 unsigned long Model1_tx_dropped;
 unsigned long Model1_multicast;
 unsigned long Model1_collisions;
 unsigned long Model1_rx_length_errors;
 unsigned long Model1_rx_over_errors;
 unsigned long Model1_rx_crc_errors;
 unsigned long Model1_rx_frame_errors;
 unsigned long Model1_rx_fifo_errors;
 unsigned long Model1_rx_missed_errors;
 unsigned long Model1_tx_aborted_errors;
 unsigned long Model1_tx_carrier_errors;
 unsigned long Model1_tx_fifo_errors;
 unsigned long Model1_tx_heartbeat_errors;
 unsigned long Model1_tx_window_errors;
 unsigned long Model1_rx_compressed;
 unsigned long Model1_tx_compressed;
};







extern struct Model1_static_key Model1_rps_needed;


struct Model1_neighbour;
struct Model1_neigh_parms;
struct Model1_sk_buff;

struct Model1_netdev_hw_addr {
 struct Model1_list_head Model1_list;
 unsigned char Model1_addr[32];
 unsigned char Model1_type;





 bool Model1_global_use;
 int Model1_sync_cnt;
 int Model1_refcount;
 int Model1_synced;
 struct Model1_callback_head Model1_callback_head;
};

struct Model1_netdev_hw_addr_list {
 struct Model1_list_head Model1_list;
 int Model1_count;
};
struct Model1_hh_cache {
 Model1_u16 Model1_hh_len;
 Model1_u16 Model1___pad;
 Model1_seqlock_t Model1_hh_lock;

 /* cached hardware header; allow for machine alignment needs.        */





 unsigned long Model1_hh_data[(((96)+(16 -1))&~(16 - 1)) / sizeof(long)];
};

/* Reserve HH_DATA_MOD byte-aligned hard_header_len, but at least that much.
 * Alternative is:
 *   dev->hard_header_len ? (dev->hard_header_len +
 *                           (HH_DATA_MOD - 1)) & ~(HH_DATA_MOD - 1) : 0
 *
 * We could use other alignment values, but we must maintain the
 * relationship HH alignment <= LL alignment.
 */





struct Model1_header_ops {
 int (*Model1_create) (struct Model1_sk_buff *Model1_skb, struct Model1_net_device *Model1_dev,
      unsigned short Model1_type, const void *Model1_daddr,
      const void *Model1_saddr, unsigned int Model1_len);
 int (*Model1_parse)(const struct Model1_sk_buff *Model1_skb, unsigned char *Model1_haddr);
 int (*Model1_cache)(const struct Model1_neighbour *Model1_neigh, struct Model1_hh_cache *Model1_hh, Model1___be16 Model1_type);
 void (*Model1_cache_update)(struct Model1_hh_cache *Model1_hh,
    const struct Model1_net_device *Model1_dev,
    const unsigned char *Model1_haddr);
 bool (*Model1_validate)(const char *Model1_ll_header, unsigned int Model1_len);
};

/* These flag bits are private to the generic network queueing
 * layer; they may not be explicitly referenced by any other
 * code.
 */

enum Model1_netdev_state_t {
 Model1___LINK_STATE_START,
 Model1___LINK_STATE_PRESENT,
 Model1___LINK_STATE_NOCARRIER,
 Model1___LINK_STATE_LINKWATCH_PENDING,
 Model1___LINK_STATE_DORMANT,
};


/*
 * This structure holds boot-time configured netdevice settings. They
 * are then used in the device probing.
 */
struct Model1_netdev_boot_setup {
 char Model1_name[16];
 struct Model1_ifmap Model1_map;
};


int __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_netdev_boot_setup(char *Model1_str);

/*
 * Structure for NAPI scheduling similar to tasklet but with weighting
 */
struct Model1_napi_struct {
 /* The poll_list must only be managed by the entity which
	 * changes the state of the NAPI_STATE_SCHED bit.  This means
	 * whoever atomically sets that bit can add this napi_struct
	 * to the per-CPU poll_list, and whoever clears that bit
	 * can remove from the list right before clearing the bit.
	 */
 struct Model1_list_head Model1_poll_list;

 unsigned long Model1_state;
 int Model1_weight;
 unsigned int Model1_gro_count;
 int (*Model1_poll)(struct Model1_napi_struct *, int);

 Model1_spinlock_t Model1_poll_lock;
 int Model1_poll_owner;

 struct Model1_net_device *Model1_dev;
 struct Model1_sk_buff *Model1_gro_list;
 struct Model1_sk_buff *Model1_skb;
 struct Model1_hrtimer Model1_timer;
 struct Model1_list_head Model1_dev_list;
 struct Model1_hlist_node Model1_napi_hash_node;
 unsigned int Model1_napi_id;
};

enum {
 Model1_NAPI_STATE_SCHED, /* Poll is scheduled */
 Model1_NAPI_STATE_DISABLE, /* Disable pending */
 Model1_NAPI_STATE_NPSVC, /* Netpoll - don't dequeue from poll_list */
 Model1_NAPI_STATE_HASHED, /* In NAPI hash (busy polling possible) */
 Model1_NAPI_STATE_NO_BUSY_POLL,/* Do not add in napi_hash, no busy polling */
};

enum Model1_gro_result {
 Model1_GRO_MERGED,
 Model1_GRO_MERGED_FREE,
 Model1_GRO_HELD,
 Model1_GRO_NORMAL,
 Model1_GRO_DROP,
};
typedef enum Model1_gro_result Model1_gro_result_t;

/*
 * enum rx_handler_result - Possible return values for rx_handlers.
 * @RX_HANDLER_CONSUMED: skb was consumed by rx_handler, do not process it
 * further.
 * @RX_HANDLER_ANOTHER: Do another round in receive path. This is indicated in
 * case skb->dev was changed by rx_handler.
 * @RX_HANDLER_EXACT: Force exact delivery, no wildcard.
 * @RX_HANDLER_PASS: Do nothing, pass the skb as if no rx_handler was called.
 *
 * rx_handlers are functions called from inside __netif_receive_skb(), to do
 * special processing of the skb, prior to delivery to protocol handlers.
 *
 * Currently, a net_device can only have a single rx_handler registered. Trying
 * to register a second rx_handler will return -EBUSY.
 *
 * To register a rx_handler on a net_device, use netdev_rx_handler_register().
 * To unregister a rx_handler on a net_device, use
 * netdev_rx_handler_unregister().
 *
 * Upon return, rx_handler is expected to tell __netif_receive_skb() what to
 * do with the skb.
 *
 * If the rx_handler consumed the skb in some way, it should return
 * RX_HANDLER_CONSUMED. This is appropriate when the rx_handler arranged for
 * the skb to be delivered in some other way.
 *
 * If the rx_handler changed skb->dev, to divert the skb to another
 * net_device, it should return RX_HANDLER_ANOTHER. The rx_handler for the
 * new device will be called if it exists.
 *
 * If the rx_handler decides the skb should be ignored, it should return
 * RX_HANDLER_EXACT. The skb will only be delivered to protocol handlers that
 * are registered on exact device (ptype->dev == skb->dev).
 *
 * If the rx_handler didn't change skb->dev, but wants the skb to be normally
 * delivered, it should return RX_HANDLER_PASS.
 *
 * A device without a registered rx_handler will behave as if rx_handler
 * returned RX_HANDLER_PASS.
 */

enum Model1_rx_handler_result {
 Model1_RX_HANDLER_CONSUMED,
 Model1_RX_HANDLER_ANOTHER,
 Model1_RX_HANDLER_EXACT,
 Model1_RX_HANDLER_PASS,
};
typedef enum Model1_rx_handler_result Model1_rx_handler_result_t;
typedef Model1_rx_handler_result_t Model1_rx_handler_func_t(struct Model1_sk_buff **Model1_pskb);

void Model1___napi_schedule(struct Model1_napi_struct *Model1_n);
void Model1___napi_schedule_irqoff(struct Model1_napi_struct *Model1_n);

static inline __attribute__((no_instrument_function)) bool Model1_napi_disable_pending(struct Model1_napi_struct *Model1_n)
{
 return (__builtin_constant_p((Model1_NAPI_STATE_DISABLE)) ? Model1_constant_test_bit((Model1_NAPI_STATE_DISABLE), (&Model1_n->Model1_state)) : Model1_variable_test_bit((Model1_NAPI_STATE_DISABLE), (&Model1_n->Model1_state)));
}

/**
 *	napi_schedule_prep - check if NAPI can be scheduled
 *	@n: NAPI context
 *
 * Test if NAPI routine is already running, and if not mark
 * it as running.  This is used as a condition variable to
 * insure only one NAPI poll instance runs.  We also make
 * sure there is no pending NAPI disable.
 */
static inline __attribute__((no_instrument_function)) bool Model1_napi_schedule_prep(struct Model1_napi_struct *Model1_n)
{
 return !Model1_napi_disable_pending(Model1_n) &&
  !Model1_test_and_set_bit(Model1_NAPI_STATE_SCHED, &Model1_n->Model1_state);
}

/**
 *	napi_schedule - schedule NAPI poll
 *	@n: NAPI context
 *
 * Schedule NAPI poll routine to be called if it is not already
 * running.
 */
static inline __attribute__((no_instrument_function)) void Model1_napi_schedule(struct Model1_napi_struct *Model1_n)
{
 if (Model1_napi_schedule_prep(Model1_n))
  Model1___napi_schedule(Model1_n);
}

/**
 *	napi_schedule_irqoff - schedule NAPI poll
 *	@n: NAPI context
 *
 * Variant of napi_schedule(), assuming hard irqs are masked.
 */
static inline __attribute__((no_instrument_function)) void Model1_napi_schedule_irqoff(struct Model1_napi_struct *Model1_n)
{
 if (Model1_napi_schedule_prep(Model1_n))
  Model1___napi_schedule_irqoff(Model1_n);
}

/* Try to reschedule poll. Called by dev->poll() after napi_complete().  */
static inline __attribute__((no_instrument_function)) bool Model1_napi_reschedule(struct Model1_napi_struct *Model1_napi)
{
 if (Model1_napi_schedule_prep(Model1_napi)) {
  Model1___napi_schedule(Model1_napi);
  return true;
 }
 return false;
}

void Model1___napi_complete(struct Model1_napi_struct *Model1_n);
void Model1_napi_complete_done(struct Model1_napi_struct *Model1_n, int Model1_work_done);
/**
 *	napi_complete - NAPI processing complete
 *	@n: NAPI context
 *
 * Mark NAPI processing as complete.
 * Consider using napi_complete_done() instead.
 */
static inline __attribute__((no_instrument_function)) void Model1_napi_complete(struct Model1_napi_struct *Model1_n)
{
 return Model1_napi_complete_done(Model1_n, 0);
}

/**
 *	napi_hash_add - add a NAPI to global hashtable
 *	@napi: NAPI context
 *
 * Generate a new napi_id and store a @napi under it in napi_hash.
 * Used for busy polling (CONFIG_NET_RX_BUSY_POLL).
 * Note: This is normally automatically done from netif_napi_add(),
 * so might disappear in a future Linux version.
 */
void Model1_napi_hash_add(struct Model1_napi_struct *Model1_napi);

/**
 *	napi_hash_del - remove a NAPI from global table
 *	@napi: NAPI context
 *
 * Warning: caller must observe RCU grace period
 * before freeing memory containing @napi, if
 * this function returns true.
 * Note: core networking stack automatically calls it
 * from netif_napi_del().
 * Drivers might want to call this helper to combine all
 * the needed RCU grace periods into a single one.
 */
bool Model1_napi_hash_del(struct Model1_napi_struct *Model1_napi);

/**
 *	napi_disable - prevent NAPI from scheduling
 *	@n: NAPI context
 *
 * Stop NAPI from being scheduled on this context.
 * Waits till any outstanding processing completes.
 */
void Model1_napi_disable(struct Model1_napi_struct *Model1_n);

/**
 *	napi_enable - enable NAPI scheduling
 *	@n: NAPI context
 *
 * Resume NAPI from being scheduled on this context.
 * Must be paired with napi_disable.
 */
static inline __attribute__((no_instrument_function)) void Model1_napi_enable(struct Model1_napi_struct *Model1_n)
{
 do { if (__builtin_expect(!!(!(__builtin_constant_p((Model1_NAPI_STATE_SCHED)) ? Model1_constant_test_bit((Model1_NAPI_STATE_SCHED), (&Model1_n->Model1_state)) : Model1_variable_test_bit((Model1_NAPI_STATE_SCHED), (&Model1_n->Model1_state)))), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/netdevice.h"), "i" (512), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);
 __asm__ __volatile__("": : :"memory");
 Model1_clear_bit(Model1_NAPI_STATE_SCHED, &Model1_n->Model1_state);
 Model1_clear_bit(Model1_NAPI_STATE_NPSVC, &Model1_n->Model1_state);
}

/**
 *	napi_synchronize - wait until NAPI is not running
 *	@n: NAPI context
 *
 * Wait until NAPI is done being scheduled on this context.
 * Waits till any outstanding processing completes but
 * does not disable future activations.
 */
static inline __attribute__((no_instrument_function)) void Model1_napi_synchronize(const struct Model1_napi_struct *Model1_n)
{
 if (1)
  while ((__builtin_constant_p((Model1_NAPI_STATE_SCHED)) ? Model1_constant_test_bit((Model1_NAPI_STATE_SCHED), (&Model1_n->Model1_state)) : Model1_variable_test_bit((Model1_NAPI_STATE_SCHED), (&Model1_n->Model1_state))))
   Model1_msleep(1);
 else
  __asm__ __volatile__("": : :"memory");
}

enum Model1_netdev_queue_state_t {
 Model1___QUEUE_STATE_DRV_XOFF,
 Model1___QUEUE_STATE_STACK_XOFF,
 Model1___QUEUE_STATE_FROZEN,
};
/*
 * __QUEUE_STATE_DRV_XOFF is used by drivers to stop the transmit queue.  The
 * netif_tx_* functions below are used to manipulate this flag.  The
 * __QUEUE_STATE_STACK_XOFF flag is used by the stack to stop the transmit
 * queue independently.  The netif_xmit_*stopped functions below are called
 * to check if the queue has been stopped by the driver or stack (either
 * of the XOFF bits are set in the state).  Drivers should not need to call
 * netif_xmit*stopped functions, they should only be using netif_tx_*.
 */

struct Model1_netdev_queue {
/*
 * read-mostly part
 */
 struct Model1_net_device *Model1_dev;
 struct Model1_Qdisc *Model1_qdisc;
 struct Model1_Qdisc *Model1_qdisc_sleeping;

 struct Model1_kobject Model1_kobj;


 int Model1_numa_node;

 unsigned long Model1_tx_maxrate;
 /*
	 * Number of TX timeouts for this queue
	 * (/sys/class/net/DEV/Q/trans_timeout)
	 */
 unsigned long Model1_trans_timeout;
/*
 * write-mostly part
 */
 Model1_spinlock_t Model1__xmit_lock __attribute__((__aligned__((1 << (6)))));
 int Model1_xmit_lock_owner;
 /*
	 * Time (in jiffies) of last Tx
	 */
 unsigned long Model1_trans_start;

 unsigned long Model1_state;


 struct Model1_dql Model1_dql;

} __attribute__((__aligned__((1 << (6)))));

static inline __attribute__((no_instrument_function)) int Model1_netdev_queue_numa_node_read(const struct Model1_netdev_queue *Model1_q)
{

 return Model1_q->Model1_numa_node;



}

static inline __attribute__((no_instrument_function)) void Model1_netdev_queue_numa_node_write(struct Model1_netdev_queue *Model1_q, int Model1_node)
{

 Model1_q->Model1_numa_node = Model1_node;

}


/*
 * This structure holds an RPS map which can be of variable length.  The
 * map is an array of CPUs.
 */
struct Model1_rps_map {
 unsigned int Model1_len;
 struct Model1_callback_head Model1_rcu;
 Model1_u16 Model1_cpus[0];
};


/*
 * The rps_dev_flow structure contains the mapping of a flow to a CPU, the
 * tail pointer for that CPU's input queue at the time of last enqueue, and
 * a hardware filter index.
 */
struct Model1_rps_dev_flow {
 Model1_u16 Model1_cpu;
 Model1_u16 Model1_filter;
 unsigned int Model1_last_qtail;
};


/*
 * The rps_dev_flow_table structure contains a table of flow mappings.
 */
struct Model1_rps_dev_flow_table {
 unsigned int Model1_mask;
 struct Model1_callback_head Model1_rcu;
 struct Model1_rps_dev_flow Model1_flows[0];
};



/*
 * The rps_sock_flow_table contains mappings of flows to the last CPU
 * on which they were processed by the application (set in recvmsg).
 * Each entry is a 32bit value. Upper part is the high-order bits
 * of flow hash, lower part is CPU number.
 * rps_cpu_mask is used to partition the space, depending on number of
 * possible CPUs : rps_cpu_mask = roundup_pow_of_two(nr_cpu_ids) - 1
 * For example, if 64 CPUs are possible, rps_cpu_mask = 0x3f,
 * meaning we use 32-6=26 bits for the hash.
 */
struct Model1_rps_sock_flow_table {
 Model1_u32 Model1_mask;

 Model1_u32 Model1_ents[0] __attribute__((__aligned__((1 << (6)))));
};




extern Model1_u32 Model1_rps_cpu_mask;
extern struct Model1_rps_sock_flow_table *Model1_rps_sock_flow_table;

static inline __attribute__((no_instrument_function)) void Model1_rps_record_sock_flow(struct Model1_rps_sock_flow_table *Model1_table,
     Model1_u32 Model1_hash)
{
 if (Model1_table && Model1_hash) {
  unsigned int Model1_index = Model1_hash & Model1_table->Model1_mask;
  Model1_u32 Model1_val = Model1_hash & ~Model1_rps_cpu_mask;

  /* We only give a hint, preemption can change CPU under us */
  Model1_val |= (({ typeof(Model1_cpu_number) Model1_pscr_ret__; do { const void *Model1___vpp_verify = (typeof((&(Model1_cpu_number)) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); switch(sizeof(Model1_cpu_number)) { case 1: Model1_pscr_ret__ = ({ typeof(Model1_cpu_number) Model1_pfo_ret__; switch (sizeof(Model1_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 2: Model1_pscr_ret__ = ({ typeof(Model1_cpu_number) Model1_pfo_ret__; switch (sizeof(Model1_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 4: Model1_pscr_ret__ = ({ typeof(Model1_cpu_number) Model1_pfo_ret__; switch (sizeof(Model1_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 8: Model1_pscr_ret__ = ({ typeof(Model1_cpu_number) Model1_pfo_ret__; switch (sizeof(Model1_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; default: Model1___bad_size_call_parameter(); break; } Model1_pscr_ret__; }));

  if (Model1_table->Model1_ents[Model1_index] != Model1_val)
   Model1_table->Model1_ents[Model1_index] = Model1_val;
 }
}


bool Model1_rps_may_expire_flow(struct Model1_net_device *Model1_dev, Model1_u16 Model1_rxq_index, Model1_u32 Model1_flow_id,
    Model1_u16 Model1_filter_id);



/* This structure contains an instance of an RX queue. */
struct Model1_netdev_rx_queue {

 struct Model1_rps_map *Model1_rps_map;
 struct Model1_rps_dev_flow_table *Model1_rps_flow_table;

 struct Model1_kobject Model1_kobj;
 struct Model1_net_device *Model1_dev;
} __attribute__((__aligned__((1 << (6)))));

/*
 * RX queue sysfs structures and functions.
 */
struct Model1_rx_queue_attribute {
 struct Model1_attribute Model1_attr;
 Model1_ssize_t (*Model1_show)(struct Model1_netdev_rx_queue *Model1_queue,
     struct Model1_rx_queue_attribute *Model1_attr, char *Model1_buf);
 Model1_ssize_t (*Model1_store)(struct Model1_netdev_rx_queue *Model1_queue,
     struct Model1_rx_queue_attribute *Model1_attr, const char *Model1_buf, Model1_size_t Model1_len);
};


/*
 * This structure holds an XPS map which can be of variable length.  The
 * map is an array of queues.
 */
struct Model1_xps_map {
 unsigned int Model1_len;
 unsigned int Model1_alloc_len;
 struct Model1_callback_head Model1_rcu;
 Model1_u16 Model1_queues[0];
};




/*
 * This structure holds all XPS maps for device.  Maps are indexed by CPU.
 */
struct Model1_xps_dev_maps {
 struct Model1_callback_head Model1_rcu;
 struct Model1_xps_map *Model1_cpu_map[0];
};






/* HW offloaded queuing disciplines txq count and offset maps */
struct Model1_netdev_tc_txq {
 Model1_u16 Model1_count;
 Model1_u16 Model1_offset;
};
/* This structure holds a unique identifier to identify some
 * physical item (port for example) used by a netdevice.
 */
struct Model1_netdev_phys_item_id {
 unsigned char Model1_id[32];
 unsigned char Model1_id_len;
};

static inline __attribute__((no_instrument_function)) bool Model1_netdev_phys_item_id_same(struct Model1_netdev_phys_item_id *Model1_a,
         struct Model1_netdev_phys_item_id *Model1_b)
{
 return Model1_a->Model1_id_len == Model1_b->Model1_id_len &&
        Model1_memcmp(Model1_a->Model1_id, Model1_b->Model1_id, Model1_a->Model1_id_len) == 0;
}

typedef Model1_u16 (*Model1_select_queue_fallback_t)(struct Model1_net_device *Model1_dev,
           struct Model1_sk_buff *Model1_skb);

/* These structures hold the attributes of qdisc and classifiers
 * that are being passed to the netdevice through the setup_tc op.
 */
enum {
 Model1_TC_SETUP_MQPRIO,
 Model1_TC_SETUP_CLSU32,
 Model1_TC_SETUP_CLSFLOWER,
 Model1_TC_SETUP_MATCHALL,
};

struct Model1_tc_cls_u32_offload;

struct Model1_tc_to_netdev {
 unsigned int Model1_type;
 union {
  Model1_u8 Model1_tc;
  struct Model1_tc_cls_u32_offload *Model1_cls_u32;
  struct Model1_tc_cls_flower_offload *Model1_cls_flower;
  struct Model1_tc_cls_matchall_offload *Model1_cls_mall;
 };
};

/* These structures hold the attributes of xdp state that are being passed
 * to the netdevice through the xdp op.
 */
enum Model1_xdp_netdev_command {
 /* Set or clear a bpf program used in the earliest stages of packet
	 * rx. The prog will have been loaded as BPF_PROG_TYPE_XDP. The callee
	 * is responsible for calling bpf_prog_put on any old progs that are
	 * stored. In case of error, the callee need not release the new prog
	 * reference, but on success it takes ownership and must bpf_prog_put
	 * when it is no longer used.
	 */
 Model1_XDP_SETUP_PROG,
 /* Check if a bpf program is set on the device.  The callee should
	 * return true if a program is currently attached and running.
	 */
 Model1_XDP_QUERY_PROG,
};

struct Model1_netdev_xdp {
 enum Model1_xdp_netdev_command Model1_command;
 union {
  /* XDP_SETUP_PROG */
  struct Model1_bpf_prog *Model1_prog;
  /* XDP_QUERY_PROG */
  bool Model1_prog_attached;
 };
};

/*
 * This structure defines the management hooks for network devices.
 * The following hooks can be defined; unless noted otherwise, they are
 * optional and can be filled with a null pointer.
 *
 * int (*ndo_init)(struct net_device *dev);
 *     This function is called once when a network device is registered.
 *     The network device can use this for any late stage initialization
 *     or semantic validation. It can fail with an error code which will
 *     be propagated back to register_netdev.
 *
 * void (*ndo_uninit)(struct net_device *dev);
 *     This function is called when device is unregistered or when registration
 *     fails. It is not called if init fails.
 *
 * int (*ndo_open)(struct net_device *dev);
 *     This function is called when a network device transitions to the up
 *     state.
 *
 * int (*ndo_stop)(struct net_device *dev);
 *     This function is called when a network device transitions to the down
 *     state.
 *
 * netdev_tx_t (*ndo_start_xmit)(struct sk_buff *skb,
 *                               struct net_device *dev);
 *	Called when a packet needs to be transmitted.
 *	Returns NETDEV_TX_OK.  Can return NETDEV_TX_BUSY, but you should stop
 *	the queue before that can happen; it's for obsolete devices and weird
 *	corner cases, but the stack really does a non-trivial amount
 *	of useless work if you return NETDEV_TX_BUSY.
 *	Required; cannot be NULL.
 *
 * netdev_features_t (*ndo_fix_features)(struct net_device *dev,
 *		netdev_features_t features);
 *	Adjusts the requested feature flags according to device-specific
 *	constraints, and returns the resulting flags. Must not modify
 *	the device state.
 *
 * u16 (*ndo_select_queue)(struct net_device *dev, struct sk_buff *skb,
 *                         void *accel_priv, select_queue_fallback_t fallback);
 *	Called to decide which queue to use when device supports multiple
 *	transmit queues.
 *
 * void (*ndo_change_rx_flags)(struct net_device *dev, int flags);
 *	This function is called to allow device receiver to make
 *	changes to configuration when multicast or promiscuous is enabled.
 *
 * void (*ndo_set_rx_mode)(struct net_device *dev);
 *	This function is called device changes address list filtering.
 *	If driver handles unicast address filtering, it should set
 *	IFF_UNICAST_FLT in its priv_flags.
 *
 * int (*ndo_set_mac_address)(struct net_device *dev, void *addr);
 *	This function  is called when the Media Access Control address
 *	needs to be changed. If this interface is not defined, the
 *	MAC address can not be changed.
 *
 * int (*ndo_validate_addr)(struct net_device *dev);
 *	Test if Media Access Control address is valid for the device.
 *
 * int (*ndo_do_ioctl)(struct net_device *dev, struct ifreq *ifr, int cmd);
 *	Called when a user requests an ioctl which can't be handled by
 *	the generic interface code. If not defined ioctls return
 *	not supported error code.
 *
 * int (*ndo_set_config)(struct net_device *dev, struct ifmap *map);
 *	Used to set network devices bus interface parameters. This interface
 *	is retained for legacy reasons; new devices should use the bus
 *	interface (PCI) for low level management.
 *
 * int (*ndo_change_mtu)(struct net_device *dev, int new_mtu);
 *	Called when a user wants to change the Maximum Transfer Unit
 *	of a device. If not defined, any request to change MTU will
 *	will return an error.
 *
 * void (*ndo_tx_timeout)(struct net_device *dev);
 *	Callback used when the transmitter has not made any progress
 *	for dev->watchdog ticks.
 *
 * struct rtnl_link_stats64* (*ndo_get_stats64)(struct net_device *dev,
 *                      struct rtnl_link_stats64 *storage);
 * struct net_device_stats* (*ndo_get_stats)(struct net_device *dev);
 *	Called when a user wants to get the network device usage
 *	statistics. Drivers must do one of the following:
 *	1. Define @ndo_get_stats64 to fill in a zero-initialised
 *	   rtnl_link_stats64 structure passed by the caller.
 *	2. Define @ndo_get_stats to update a net_device_stats structure
 *	   (which should normally be dev->stats) and return a pointer to
 *	   it. The structure may be changed asynchronously only if each
 *	   field is written atomically.
 *	3. Update dev->stats asynchronously and atomically, and define
 *	   neither operation.
 *
 * int (*ndo_vlan_rx_add_vid)(struct net_device *dev, __be16 proto, u16 vid);
 *	If device supports VLAN filtering this function is called when a
 *	VLAN id is registered.
 *
 * int (*ndo_vlan_rx_kill_vid)(struct net_device *dev, __be16 proto, u16 vid);
 *	If device supports VLAN filtering this function is called when a
 *	VLAN id is unregistered.
 *
 * void (*ndo_poll_controller)(struct net_device *dev);
 *
 *	SR-IOV management functions.
 * int (*ndo_set_vf_mac)(struct net_device *dev, int vf, u8* mac);
 * int (*ndo_set_vf_vlan)(struct net_device *dev, int vf, u16 vlan, u8 qos);
 * int (*ndo_set_vf_rate)(struct net_device *dev, int vf, int min_tx_rate,
 *			  int max_tx_rate);
 * int (*ndo_set_vf_spoofchk)(struct net_device *dev, int vf, bool setting);
 * int (*ndo_set_vf_trust)(struct net_device *dev, int vf, bool setting);
 * int (*ndo_get_vf_config)(struct net_device *dev,
 *			    int vf, struct ifla_vf_info *ivf);
 * int (*ndo_set_vf_link_state)(struct net_device *dev, int vf, int link_state);
 * int (*ndo_set_vf_port)(struct net_device *dev, int vf,
 *			  struct nlattr *port[]);
 *
 *      Enable or disable the VF ability to query its RSS Redirection Table and
 *      Hash Key. This is needed since on some devices VF share this information
 *      with PF and querying it may introduce a theoretical security risk.
 * int (*ndo_set_vf_rss_query_en)(struct net_device *dev, int vf, bool setting);
 * int (*ndo_get_vf_port)(struct net_device *dev, int vf, struct sk_buff *skb);
 * int (*ndo_setup_tc)(struct net_device *dev, u8 tc)
 * 	Called to setup 'tc' number of traffic classes in the net device. This
 * 	is always called from the stack with the rtnl lock held and netif tx
 * 	queues stopped. This allows the netdevice to perform queue management
 * 	safely.
 *
 *	Fiber Channel over Ethernet (FCoE) offload functions.
 * int (*ndo_fcoe_enable)(struct net_device *dev);
 *	Called when the FCoE protocol stack wants to start using LLD for FCoE
 *	so the underlying device can perform whatever needed configuration or
 *	initialization to support acceleration of FCoE traffic.
 *
 * int (*ndo_fcoe_disable)(struct net_device *dev);
 *	Called when the FCoE protocol stack wants to stop using LLD for FCoE
 *	so the underlying device can perform whatever needed clean-ups to
 *	stop supporting acceleration of FCoE traffic.
 *
 * int (*ndo_fcoe_ddp_setup)(struct net_device *dev, u16 xid,
 *			     struct scatterlist *sgl, unsigned int sgc);
 *	Called when the FCoE Initiator wants to initialize an I/O that
 *	is a possible candidate for Direct Data Placement (DDP). The LLD can
 *	perform necessary setup and returns 1 to indicate the device is set up
 *	successfully to perform DDP on this I/O, otherwise this returns 0.
 *
 * int (*ndo_fcoe_ddp_done)(struct net_device *dev,  u16 xid);
 *	Called when the FCoE Initiator/Target is done with the DDPed I/O as
 *	indicated by the FC exchange id 'xid', so the underlying device can
 *	clean up and reuse resources for later DDP requests.
 *
 * int (*ndo_fcoe_ddp_target)(struct net_device *dev, u16 xid,
 *			      struct scatterlist *sgl, unsigned int sgc);
 *	Called when the FCoE Target wants to initialize an I/O that
 *	is a possible candidate for Direct Data Placement (DDP). The LLD can
 *	perform necessary setup and returns 1 to indicate the device is set up
 *	successfully to perform DDP on this I/O, otherwise this returns 0.
 *
 * int (*ndo_fcoe_get_hbainfo)(struct net_device *dev,
 *			       struct netdev_fcoe_hbainfo *hbainfo);
 *	Called when the FCoE Protocol stack wants information on the underlying
 *	device. This information is utilized by the FCoE protocol stack to
 *	register attributes with Fiber Channel management service as per the
 *	FC-GS Fabric Device Management Information(FDMI) specification.
 *
 * int (*ndo_fcoe_get_wwn)(struct net_device *dev, u64 *wwn, int type);
 *	Called when the underlying device wants to override default World Wide
 *	Name (WWN) generation mechanism in FCoE protocol stack to pass its own
 *	World Wide Port Name (WWPN) or World Wide Node Name (WWNN) to the FCoE
 *	protocol stack to use.
 *
 *	RFS acceleration.
 * int (*ndo_rx_flow_steer)(struct net_device *dev, const struct sk_buff *skb,
 *			    u16 rxq_index, u32 flow_id);
 *	Set hardware filter for RFS.  rxq_index is the target queue index;
 *	flow_id is a flow ID to be passed to rps_may_expire_flow() later.
 *	Return the filter ID on success, or a negative error code.
 *
 *	Slave management functions (for bridge, bonding, etc).
 * int (*ndo_add_slave)(struct net_device *dev, struct net_device *slave_dev);
 *	Called to make another netdev an underling.
 *
 * int (*ndo_del_slave)(struct net_device *dev, struct net_device *slave_dev);
 *	Called to release previously enslaved netdev.
 *
 *      Feature/offload setting functions.
 * int (*ndo_set_features)(struct net_device *dev, netdev_features_t features);
 *	Called to update device configuration to new features. Passed
 *	feature set might be less than what was returned by ndo_fix_features()).
 *	Must return >0 or -errno if it changed dev->features itself.
 *
 * int (*ndo_fdb_add)(struct ndmsg *ndm, struct nlattr *tb[],
 *		      struct net_device *dev,
 *		      const unsigned char *addr, u16 vid, u16 flags)
 *	Adds an FDB entry to dev for addr.
 * int (*ndo_fdb_del)(struct ndmsg *ndm, struct nlattr *tb[],
 *		      struct net_device *dev,
 *		      const unsigned char *addr, u16 vid)
 *	Deletes the FDB entry from dev coresponding to addr.
 * int (*ndo_fdb_dump)(struct sk_buff *skb, struct netlink_callback *cb,
 *		       struct net_device *dev, struct net_device *filter_dev,
 *		       int idx)
 *	Used to add FDB entries to dump requests. Implementers should add
 *	entries to skb and update idx with the number of entries.
 *
 * int (*ndo_bridge_setlink)(struct net_device *dev, struct nlmsghdr *nlh,
 *			     u16 flags)
 * int (*ndo_bridge_getlink)(struct sk_buff *skb, u32 pid, u32 seq,
 *			     struct net_device *dev, u32 filter_mask,
 *			     int nlflags)
 * int (*ndo_bridge_dellink)(struct net_device *dev, struct nlmsghdr *nlh,
 *			     u16 flags);
 *
 * int (*ndo_change_carrier)(struct net_device *dev, bool new_carrier);
 *	Called to change device carrier. Soft-devices (like dummy, team, etc)
 *	which do not represent real hardware may define this to allow their
 *	userspace components to manage their virtual carrier state. Devices
 *	that determine carrier state from physical hardware properties (eg
 *	network cables) or protocol-dependent mechanisms (eg
 *	USB_CDC_NOTIFY_NETWORK_CONNECTION) should NOT implement this function.
 *
 * int (*ndo_get_phys_port_id)(struct net_device *dev,
 *			       struct netdev_phys_item_id *ppid);
 *	Called to get ID of physical port of this device. If driver does
 *	not implement this, it is assumed that the hw is not able to have
 *	multiple net devices on single physical port.
 *
 * void (*ndo_udp_tunnel_add)(struct net_device *dev,
 *			      struct udp_tunnel_info *ti);
 *	Called by UDP tunnel to notify a driver about the UDP port and socket
 *	address family that a UDP tunnel is listnening to. It is called only
 *	when a new port starts listening. The operation is protected by the
 *	RTNL.
 *
 * void (*ndo_udp_tunnel_del)(struct net_device *dev,
 *			      struct udp_tunnel_info *ti);
 *	Called by UDP tunnel to notify the driver about a UDP port and socket
 *	address family that the UDP tunnel is not listening to anymore. The
 *	operation is protected by the RTNL.
 *
 * void* (*ndo_dfwd_add_station)(struct net_device *pdev,
 *				 struct net_device *dev)
 *	Called by upper layer devices to accelerate switching or other
 *	station functionality into hardware. 'pdev is the lowerdev
 *	to use for the offload and 'dev' is the net device that will
 *	back the offload. Returns a pointer to the private structure
 *	the upper layer will maintain.
 * void (*ndo_dfwd_del_station)(struct net_device *pdev, void *priv)
 *	Called by upper layer device to delete the station created
 *	by 'ndo_dfwd_add_station'. 'pdev' is the net device backing
 *	the station and priv is the structure returned by the add
 *	operation.
 * netdev_tx_t (*ndo_dfwd_start_xmit)(struct sk_buff *skb,
 *				      struct net_device *dev,
 *				      void *priv);
 *	Callback to use for xmit over the accelerated station. This
 *	is used in place of ndo_start_xmit on accelerated net
 *	devices.
 * netdev_features_t (*ndo_features_check)(struct sk_buff *skb,
 *					   struct net_device *dev
 *					   netdev_features_t features);
 *	Called by core transmit path to determine if device is capable of
 *	performing offload operations on a given packet. This is to give
 *	the device an opportunity to implement any restrictions that cannot
 *	be otherwise expressed by feature flags. The check is called with
 *	the set of features that the stack has calculated and it returns
 *	those the driver believes to be appropriate.
 * int (*ndo_set_tx_maxrate)(struct net_device *dev,
 *			     int queue_index, u32 maxrate);
 *	Called when a user wants to set a max-rate limitation of specific
 *	TX queue.
 * int (*ndo_get_iflink)(const struct net_device *dev);
 *	Called to get the iflink value of this device.
 * void (*ndo_change_proto_down)(struct net_device *dev,
 *				 bool proto_down);
 *	This function is used to pass protocol port error state information
 *	to the switch driver. The switch driver can react to the proto_down
 *      by doing a phys down on the associated switch port.
 * int (*ndo_fill_metadata_dst)(struct net_device *dev, struct sk_buff *skb);
 *	This function is used to get egress tunnel information for given skb.
 *	This is useful for retrieving outer tunnel header parameters while
 *	sampling packet.
 * void (*ndo_set_rx_headroom)(struct net_device *dev, int needed_headroom);
 *	This function is used to specify the headroom that the skb must
 *	consider when allocation skb during packet reception. Setting
 *	appropriate rx headroom value allows avoiding skb head copy on
 *	forward. Setting a negative value resets the rx headroom to the
 *	default value.
 * int (*ndo_xdp)(struct net_device *dev, struct netdev_xdp *xdp);
 *	This function is used to set or query state related to XDP on the
 *	netdevice. See definition of enum xdp_netdev_command for details.
 *
 */
struct Model1_net_device_ops {
 int (*Model1_ndo_init)(struct Model1_net_device *Model1_dev);
 void (*Model1_ndo_uninit)(struct Model1_net_device *Model1_dev);
 int (*Model1_ndo_open)(struct Model1_net_device *Model1_dev);
 int (*Model1_ndo_stop)(struct Model1_net_device *Model1_dev);
 Model1_netdev_tx_t (*Model1_ndo_start_xmit)(struct Model1_sk_buff *Model1_skb,
        struct Model1_net_device *Model1_dev);
 Model1_netdev_features_t (*Model1_ndo_features_check)(struct Model1_sk_buff *Model1_skb,
            struct Model1_net_device *Model1_dev,
            Model1_netdev_features_t Model1_features);
 Model1_u16 (*Model1_ndo_select_queue)(struct Model1_net_device *Model1_dev,
          struct Model1_sk_buff *Model1_skb,
          void *Model1_accel_priv,
          Model1_select_queue_fallback_t Model1_fallback);
 void (*Model1_ndo_change_rx_flags)(struct Model1_net_device *Model1_dev,
             int Model1_flags);
 void (*Model1_ndo_set_rx_mode)(struct Model1_net_device *Model1_dev);
 int (*Model1_ndo_set_mac_address)(struct Model1_net_device *Model1_dev,
             void *Model1_addr);
 int (*Model1_ndo_validate_addr)(struct Model1_net_device *Model1_dev);
 int (*Model1_ndo_do_ioctl)(struct Model1_net_device *Model1_dev,
             struct Model1_ifreq *Model1_ifr, int Model1_cmd);
 int (*Model1_ndo_set_config)(struct Model1_net_device *Model1_dev,
               struct Model1_ifmap *Model1_map);
 int (*Model1_ndo_change_mtu)(struct Model1_net_device *Model1_dev,
        int Model1_new_mtu);
 int (*Model1_ndo_neigh_setup)(struct Model1_net_device *Model1_dev,
         struct Model1_neigh_parms *);
 void (*Model1_ndo_tx_timeout) (struct Model1_net_device *Model1_dev);

 struct Model1_rtnl_link_stats64* (*Model1_ndo_get_stats64)(struct Model1_net_device *Model1_dev,
           struct Model1_rtnl_link_stats64 *Model1_storage);
 struct Model1_net_device_stats* (*Model1_ndo_get_stats)(struct Model1_net_device *Model1_dev);

 int (*Model1_ndo_vlan_rx_add_vid)(struct Model1_net_device *Model1_dev,
             Model1___be16 Model1_proto, Model1_u16 Model1_vid);
 int (*Model1_ndo_vlan_rx_kill_vid)(struct Model1_net_device *Model1_dev,
              Model1___be16 Model1_proto, Model1_u16 Model1_vid);

 void (*Model1_ndo_poll_controller)(struct Model1_net_device *Model1_dev);
 int (*Model1_ndo_netpoll_setup)(struct Model1_net_device *Model1_dev,
           struct Model1_netpoll_info *Model1_info);
 void (*Model1_ndo_netpoll_cleanup)(struct Model1_net_device *Model1_dev);


 int (*Model1_ndo_busy_poll)(struct Model1_napi_struct *Model1_dev);

 int (*Model1_ndo_set_vf_mac)(struct Model1_net_device *Model1_dev,
        int Model1_queue, Model1_u8 *Model1_mac);
 int (*Model1_ndo_set_vf_vlan)(struct Model1_net_device *Model1_dev,
         int Model1_queue, Model1_u16 Model1_vlan, Model1_u8 Model1_qos);
 int (*Model1_ndo_set_vf_rate)(struct Model1_net_device *Model1_dev,
         int Model1_vf, int Model1_min_tx_rate,
         int Model1_max_tx_rate);
 int (*Model1_ndo_set_vf_spoofchk)(struct Model1_net_device *Model1_dev,
             int Model1_vf, bool Model1_setting);
 int (*Model1_ndo_set_vf_trust)(struct Model1_net_device *Model1_dev,
          int Model1_vf, bool Model1_setting);
 int (*Model1_ndo_get_vf_config)(struct Model1_net_device *Model1_dev,
           int Model1_vf,
           struct Model1_ifla_vf_info *Model1_ivf);
 int (*Model1_ndo_set_vf_link_state)(struct Model1_net_device *Model1_dev,
        int Model1_vf, int Model1_link_state);
 int (*Model1_ndo_get_vf_stats)(struct Model1_net_device *Model1_dev,
          int Model1_vf,
          struct Model1_ifla_vf_stats
          *Model1_vf_stats);
 int (*Model1_ndo_set_vf_port)(struct Model1_net_device *Model1_dev,
         int Model1_vf,
         struct Model1_nlattr *Model1_port[]);
 int (*Model1_ndo_get_vf_port)(struct Model1_net_device *Model1_dev,
         int Model1_vf, struct Model1_sk_buff *Model1_skb);
 int (*Model1_ndo_set_vf_guid)(struct Model1_net_device *Model1_dev,
         int Model1_vf, Model1_u64 Model1_guid,
         int Model1_guid_type);
 int (*Model1_ndo_set_vf_rss_query_en)(
         struct Model1_net_device *Model1_dev,
         int Model1_vf, bool Model1_setting);
 int (*Model1_ndo_setup_tc)(struct Model1_net_device *Model1_dev,
      Model1_u32 Model1_handle,
      Model1___be16 Model1_protocol,
      struct Model1_tc_to_netdev *Model1_tc);
 int (*Model1_ndo_rx_flow_steer)(struct Model1_net_device *Model1_dev,
           const struct Model1_sk_buff *Model1_skb,
           Model1_u16 Model1_rxq_index,
           Model1_u32 Model1_flow_id);

 int (*Model1_ndo_add_slave)(struct Model1_net_device *Model1_dev,
       struct Model1_net_device *Model1_slave_dev);
 int (*Model1_ndo_del_slave)(struct Model1_net_device *Model1_dev,
       struct Model1_net_device *Model1_slave_dev);
 Model1_netdev_features_t (*Model1_ndo_fix_features)(struct Model1_net_device *Model1_dev,
          Model1_netdev_features_t Model1_features);
 int (*Model1_ndo_set_features)(struct Model1_net_device *Model1_dev,
          Model1_netdev_features_t Model1_features);
 int (*Model1_ndo_neigh_construct)(struct Model1_net_device *Model1_dev,
             struct Model1_neighbour *Model1_n);
 void (*Model1_ndo_neigh_destroy)(struct Model1_net_device *Model1_dev,
           struct Model1_neighbour *Model1_n);

 int (*Model1_ndo_fdb_add)(struct Model1_ndmsg *Model1_ndm,
            struct Model1_nlattr *Model1_tb[],
            struct Model1_net_device *Model1_dev,
            const unsigned char *Model1_addr,
            Model1_u16 Model1_vid,
            Model1_u16 Model1_flags);
 int (*Model1_ndo_fdb_del)(struct Model1_ndmsg *Model1_ndm,
            struct Model1_nlattr *Model1_tb[],
            struct Model1_net_device *Model1_dev,
            const unsigned char *Model1_addr,
            Model1_u16 Model1_vid);
 int (*Model1_ndo_fdb_dump)(struct Model1_sk_buff *Model1_skb,
      struct Model1_netlink_callback *Model1_cb,
      struct Model1_net_device *Model1_dev,
      struct Model1_net_device *Model1_filter_dev,
      int Model1_idx);

 int (*Model1_ndo_bridge_setlink)(struct Model1_net_device *Model1_dev,
            struct Model1_nlmsghdr *Model1_nlh,
            Model1_u16 Model1_flags);
 int (*Model1_ndo_bridge_getlink)(struct Model1_sk_buff *Model1_skb,
            Model1_u32 Model1_pid, Model1_u32 Model1_seq,
            struct Model1_net_device *Model1_dev,
            Model1_u32 Model1_filter_mask,
            int Model1_nlflags);
 int (*Model1_ndo_bridge_dellink)(struct Model1_net_device *Model1_dev,
            struct Model1_nlmsghdr *Model1_nlh,
            Model1_u16 Model1_flags);
 int (*Model1_ndo_change_carrier)(struct Model1_net_device *Model1_dev,
            bool Model1_new_carrier);
 int (*Model1_ndo_get_phys_port_id)(struct Model1_net_device *Model1_dev,
       struct Model1_netdev_phys_item_id *Model1_ppid);
 int (*Model1_ndo_get_phys_port_name)(struct Model1_net_device *Model1_dev,
         char *Model1_name, Model1_size_t Model1_len);
 void (*Model1_ndo_udp_tunnel_add)(struct Model1_net_device *Model1_dev,
            struct Model1_udp_tunnel_info *Model1_ti);
 void (*Model1_ndo_udp_tunnel_del)(struct Model1_net_device *Model1_dev,
            struct Model1_udp_tunnel_info *Model1_ti);
 void* (*Model1_ndo_dfwd_add_station)(struct Model1_net_device *Model1_pdev,
       struct Model1_net_device *Model1_dev);
 void (*Model1_ndo_dfwd_del_station)(struct Model1_net_device *Model1_pdev,
       void *Model1_priv);

 Model1_netdev_tx_t (*Model1_ndo_dfwd_start_xmit) (struct Model1_sk_buff *Model1_skb,
       struct Model1_net_device *Model1_dev,
       void *Model1_priv);
 int (*Model1_ndo_get_lock_subclass)(struct Model1_net_device *Model1_dev);
 int (*Model1_ndo_set_tx_maxrate)(struct Model1_net_device *Model1_dev,
            int Model1_queue_index,
            Model1_u32 Model1_maxrate);
 int (*Model1_ndo_get_iflink)(const struct Model1_net_device *Model1_dev);
 int (*Model1_ndo_change_proto_down)(struct Model1_net_device *Model1_dev,
        bool Model1_proto_down);
 int (*Model1_ndo_fill_metadata_dst)(struct Model1_net_device *Model1_dev,
             struct Model1_sk_buff *Model1_skb);
 void (*Model1_ndo_set_rx_headroom)(struct Model1_net_device *Model1_dev,
             int Model1_needed_headroom);
 int (*Model1_ndo_xdp)(struct Model1_net_device *Model1_dev,
        struct Model1_netdev_xdp *Model1_xdp);
};

/**
 * enum net_device_priv_flags - &struct net_device priv_flags
 *
 * These are the &struct net_device, they are only set internally
 * by drivers and used in the kernel. These flags are invisible to
 * userspace; this means that the order of these flags can change
 * during any kernel release.
 *
 * You should have a pretty good reason to be extending these flags.
 *
 * @IFF_802_1Q_VLAN: 802.1Q VLAN device
 * @IFF_EBRIDGE: Ethernet bridging device
 * @IFF_BONDING: bonding master or slave
 * @IFF_ISATAP: ISATAP interface (RFC4214)
 * @IFF_WAN_HDLC: WAN HDLC device
 * @IFF_XMIT_DST_RELEASE: dev_hard_start_xmit() is allowed to
 *	release skb->dst
 * @IFF_DONT_BRIDGE: disallow bridging this ether dev
 * @IFF_DISABLE_NETPOLL: disable netpoll at run-time
 * @IFF_MACVLAN_PORT: device used as macvlan port
 * @IFF_BRIDGE_PORT: device used as bridge port
 * @IFF_OVS_DATAPATH: device used as Open vSwitch datapath port
 * @IFF_TX_SKB_SHARING: The interface supports sharing skbs on transmit
 * @IFF_UNICAST_FLT: Supports unicast filtering
 * @IFF_TEAM_PORT: device used as team port
 * @IFF_SUPP_NOFCS: device supports sending custom FCS
 * @IFF_LIVE_ADDR_CHANGE: device supports hardware address
 *	change when it's running
 * @IFF_MACVLAN: Macvlan device
 * @IFF_XMIT_DST_RELEASE_PERM: IFF_XMIT_DST_RELEASE not taking into account
 *	underlying stacked devices
 * @IFF_IPVLAN_MASTER: IPvlan master device
 * @IFF_IPVLAN_SLAVE: IPvlan slave device
 * @IFF_L3MDEV_MASTER: device is an L3 master device
 * @IFF_NO_QUEUE: device can run without qdisc attached
 * @IFF_OPENVSWITCH: device is a Open vSwitch master
 * @IFF_L3MDEV_SLAVE: device is enslaved to an L3 master device
 * @IFF_TEAM: device is a team device
 * @IFF_RXFH_CONFIGURED: device has had Rx Flow indirection table configured
 * @IFF_PHONY_HEADROOM: the headroom value is controlled by an external
 *	entity (i.e. the master device for bridged veth)
 * @IFF_MACSEC: device is a MACsec device
 */
enum Model1_netdev_priv_flags {
 Model1_IFF_802_1Q_VLAN = 1<<0,
 Model1_IFF_EBRIDGE = 1<<1,
 Model1_IFF_BONDING = 1<<2,
 Model1_IFF_ISATAP = 1<<3,
 Model1_IFF_WAN_HDLC = 1<<4,
 Model1_IFF_XMIT_DST_RELEASE = 1<<5,
 Model1_IFF_DONT_BRIDGE = 1<<6,
 Model1_IFF_DISABLE_NETPOLL = 1<<7,
 Model1_IFF_MACVLAN_PORT = 1<<8,
 Model1_IFF_BRIDGE_PORT = 1<<9,
 Model1_IFF_OVS_DATAPATH = 1<<10,
 Model1_IFF_TX_SKB_SHARING = 1<<11,
 Model1_IFF_UNICAST_FLT = 1<<12,
 Model1_IFF_TEAM_PORT = 1<<13,
 Model1_IFF_SUPP_NOFCS = 1<<14,
 Model1_IFF_LIVE_ADDR_CHANGE = 1<<15,
 Model1_IFF_MACVLAN = 1<<16,
 Model1_IFF_XMIT_DST_RELEASE_PERM = 1<<17,
 Model1_IFF_IPVLAN_MASTER = 1<<18,
 Model1_IFF_IPVLAN_SLAVE = 1<<19,
 Model1_IFF_L3MDEV_MASTER = 1<<20,
 Model1_IFF_NO_QUEUE = 1<<21,
 Model1_IFF_OPENVSWITCH = 1<<22,
 Model1_IFF_L3MDEV_SLAVE = 1<<23,
 Model1_IFF_TEAM = 1<<24,
 Model1_IFF_RXFH_CONFIGURED = 1<<25,
 Model1_IFF_PHONY_HEADROOM = 1<<26,
 Model1_IFF_MACSEC = 1<<27,
};
/**
 *	struct net_device - The DEVICE structure.
 *		Actually, this whole structure is a big mistake.  It mixes I/O
 *		data with strictly "high-level" data, and it has to know about
 *		almost every data structure used in the INET module.
 *
 *	@name:	This is the first field of the "visible" part of this structure
 *		(i.e. as seen by users in the "Space.c" file).  It is the name
 *	 	of the interface.
 *
 *	@name_hlist: 	Device name hash chain, please keep it close to name[]
 *	@ifalias:	SNMP alias
 *	@mem_end:	Shared memory end
 *	@mem_start:	Shared memory start
 *	@base_addr:	Device I/O address
 *	@irq:		Device IRQ number
 *
 *	@carrier_changes:	Stats to monitor carrier on<->off transitions
 *
 *	@state:		Generic network queuing layer state, see netdev_state_t
 *	@dev_list:	The global list of network devices
 *	@napi_list:	List entry used for polling NAPI devices
 *	@unreg_list:	List entry  when we are unregistering the
 *			device; see the function unregister_netdev
 *	@close_list:	List entry used when we are closing the device
 *	@ptype_all:     Device-specific packet handlers for all protocols
 *	@ptype_specific: Device-specific, protocol-specific packet handlers
 *
 *	@adj_list:	Directly linked devices, like slaves for bonding
 *	@all_adj_list:	All linked devices, *including* neighbours
 *	@features:	Currently active device features
 *	@hw_features:	User-changeable features
 *
 *	@wanted_features:	User-requested features
 *	@vlan_features:		Mask of features inheritable by VLAN devices
 *
 *	@hw_enc_features:	Mask of features inherited by encapsulating devices
 *				This field indicates what encapsulation
 *				offloads the hardware is capable of doing,
 *				and drivers will need to set them appropriately.
 *
 *	@mpls_features:	Mask of features inheritable by MPLS
 *
 *	@ifindex:	interface index
 *	@group:		The group the device belongs to
 *
 *	@stats:		Statistics struct, which was left as a legacy, use
 *			rtnl_link_stats64 instead
 *
 *	@rx_dropped:	Dropped packets by core network,
 *			do not use this in drivers
 *	@tx_dropped:	Dropped packets by core network,
 *			do not use this in drivers
 *	@rx_nohandler:	nohandler dropped packets by core network on
 *			inactive devices, do not use this in drivers
 *
 *	@wireless_handlers:	List of functions to handle Wireless Extensions,
 *				instead of ioctl,
 *				see <net/iw_handler.h> for details.
 *	@wireless_data:	Instance data managed by the core of wireless extensions
 *
 *	@netdev_ops:	Includes several pointers to callbacks,
 *			if one wants to override the ndo_*() functions
 *	@ethtool_ops:	Management operations
 *	@ndisc_ops:	Includes callbacks for different IPv6 neighbour
 *			discovery handling. Necessary for e.g. 6LoWPAN.
 *	@header_ops:	Includes callbacks for creating,parsing,caching,etc
 *			of Layer 2 headers.
 *
 *	@flags:		Interface flags (a la BSD)
 *	@priv_flags:	Like 'flags' but invisible to userspace,
 *			see if.h for the definitions
 *	@gflags:	Global flags ( kept as legacy )
 *	@padded:	How much padding added by alloc_netdev()
 *	@operstate:	RFC2863 operstate
 *	@link_mode:	Mapping policy to operstate
 *	@if_port:	Selectable AUI, TP, ...
 *	@dma:		DMA channel
 *	@mtu:		Interface MTU value
 *	@type:		Interface hardware type
 *	@hard_header_len: Maximum hardware header length.
 *
 *	@needed_headroom: Extra headroom the hardware may need, but not in all
 *			  cases can this be guaranteed
 *	@needed_tailroom: Extra tailroom the hardware may need, but not in all
 *			  cases can this be guaranteed. Some cases also use
 *			  LL_MAX_HEADER instead to allocate the skb
 *
 *	interface address info:
 *
 * 	@perm_addr:		Permanent hw address
 * 	@addr_assign_type:	Hw address assignment type
 * 	@addr_len:		Hardware address length
 *	@neigh_priv_len:	Used in neigh_alloc()
 * 	@dev_id:		Used to differentiate devices that share
 * 				the same link layer address
 * 	@dev_port:		Used to differentiate devices that share
 * 				the same function
 *	@addr_list_lock:	XXX: need comments on this one
 *	@uc_promisc:		Counter that indicates promiscuous mode
 *				has been enabled due to the need to listen to
 *				additional unicast addresses in a device that
 *				does not implement ndo_set_rx_mode()
 *	@uc:			unicast mac addresses
 *	@mc:			multicast mac addresses
 *	@dev_addrs:		list of device hw addresses
 *	@queues_kset:		Group of all Kobjects in the Tx and RX queues
 *	@promiscuity:		Number of times the NIC is told to work in
 *				promiscuous mode; if it becomes 0 the NIC will
 *				exit promiscuous mode
 *	@allmulti:		Counter, enables or disables allmulticast mode
 *
 *	@vlan_info:	VLAN info
 *	@dsa_ptr:	dsa specific data
 *	@tipc_ptr:	TIPC specific data
 *	@atalk_ptr:	AppleTalk link
 *	@ip_ptr:	IPv4 specific data
 *	@dn_ptr:	DECnet specific data
 *	@ip6_ptr:	IPv6 specific data
 *	@ax25_ptr:	AX.25 specific data
 *	@ieee80211_ptr:	IEEE 802.11 specific data, assign before registering
 *
 *	@last_rx:	Time of last Rx
 *	@dev_addr:	Hw address (before bcast,
 *			because most packets are unicast)
 *
 *	@_rx:			Array of RX queues
 *	@num_rx_queues:		Number of RX queues
 *				allocated at register_netdev() time
 *	@real_num_rx_queues: 	Number of RX queues currently active in device
 *
 *	@rx_handler:		handler for received packets
 *	@rx_handler_data: 	XXX: need comments on this one
 *	@ingress_queue:		XXX: need comments on this one
 *	@broadcast:		hw bcast address
 *
 *	@rx_cpu_rmap:	CPU reverse-mapping for RX completion interrupts,
 *			indexed by RX queue number. Assigned by driver.
 *			This must only be set if the ndo_rx_flow_steer
 *			operation is defined
 *	@index_hlist:		Device index hash chain
 *
 *	@_tx:			Array of TX queues
 *	@num_tx_queues:		Number of TX queues allocated at alloc_netdev_mq() time
 *	@real_num_tx_queues: 	Number of TX queues currently active in device
 *	@qdisc:			Root qdisc from userspace point of view
 *	@tx_queue_len:		Max frames per queue allowed
 *	@tx_global_lock: 	XXX: need comments on this one
 *
 *	@xps_maps:	XXX: need comments on this one
 *
 *	@offload_fwd_mark:	Offload device fwding mark
 *
 *	@watchdog_timeo:	Represents the timeout that is used by
 *				the watchdog (see dev_watchdog())
 *	@watchdog_timer:	List of timers
 *
 *	@pcpu_refcnt:		Number of references to this device
 *	@todo_list:		Delayed register/unregister
 *	@link_watch_list:	XXX: need comments on this one
 *
 *	@reg_state:		Register/unregister state machine
 *	@dismantle:		Device is going to be freed
 *	@rtnl_link_state:	This enum represents the phases of creating
 *				a new link
 *
 *	@destructor:		Called from unregister,
 *				can be used to call free_netdev
 *	@npinfo:		XXX: need comments on this one
 * 	@nd_net:		Network namespace this network device is inside
 *
 * 	@ml_priv:	Mid-layer private
 * 	@lstats:	Loopback statistics
 * 	@tstats:	Tunnel statistics
 * 	@dstats:	Dummy statistics
 * 	@vstats:	Virtual ethernet statistics
 *
 *	@garp_port:	GARP
 *	@mrp_port:	MRP
 *
 *	@dev:		Class/net/name entry
 *	@sysfs_groups:	Space for optional device, statistics and wireless
 *			sysfs groups
 *
 *	@sysfs_rx_queue_group:	Space for optional per-rx queue attributes
 *	@rtnl_link_ops:	Rtnl_link_ops
 *
 *	@gso_max_size:	Maximum size of generic segmentation offload
 *	@gso_max_segs:	Maximum number of segments that can be passed to the
 *			NIC for GSO
 *
 *	@dcbnl_ops:	Data Center Bridging netlink ops
 *	@num_tc:	Number of traffic classes in the net device
 *	@tc_to_txq:	XXX: need comments on this one
 *	@prio_tc_map	XXX: need comments on this one
 *
 *	@fcoe_ddp_xid:	Max exchange id for FCoE LRO by ddp
 *
 *	@priomap:	XXX: need comments on this one
 *	@phydev:	Physical device may attach itself
 *			for hardware timestamping
 *
 *	@qdisc_tx_busylock: lockdep class annotating Qdisc->busylock spinlock
 *	@qdisc_running_key: lockdep class annotating Qdisc->running seqcount
 *
 *	@proto_down:	protocol port state information can be sent to the
 *			switch driver and used to set the phys state of the
 *			switch port.
 *
 *	FIXME: cleanup struct net_device such that network protocol info
 *	moves out.
 */

struct Model1_net_device {
 char Model1_name[16];
 struct Model1_hlist_node Model1_name_hlist;
 char *Model1_ifalias;
 /*
	 *	I/O specific fields
	 *	FIXME: Merge these and struct ifmap into one
	 */
 unsigned long Model1_mem_end;
 unsigned long Model1_mem_start;
 unsigned long Model1_base_addr;
 int Model1_irq;

 Model1_atomic_t Model1_carrier_changes;

 /*
	 *	Some hardware also needs these fields (state,dev_list,
	 *	napi_list,unreg_list,close_list) but they are not
	 *	part of the usual set specified in Space.c.
	 */

 unsigned long Model1_state;

 struct Model1_list_head Model1_dev_list;
 struct Model1_list_head Model1_napi_list;
 struct Model1_list_head Model1_unreg_list;
 struct Model1_list_head Model1_close_list;
 struct Model1_list_head Model1_ptype_all;
 struct Model1_list_head Model1_ptype_specific;

 struct {
  struct Model1_list_head Model1_upper;
  struct Model1_list_head Model1_lower;
 } Model1_adj_list;

 struct {
  struct Model1_list_head Model1_upper;
  struct Model1_list_head Model1_lower;
 } Model1_all_adj_list;

 Model1_netdev_features_t Model1_features;
 Model1_netdev_features_t Model1_hw_features;
 Model1_netdev_features_t Model1_wanted_features;
 Model1_netdev_features_t Model1_vlan_features;
 Model1_netdev_features_t Model1_hw_enc_features;
 Model1_netdev_features_t Model1_mpls_features;
 Model1_netdev_features_t Model1_gso_partial_features;

 int Model1_ifindex;
 int Model1_group;

 struct Model1_net_device_stats Model1_stats;

 Model1_atomic_long_t Model1_rx_dropped;
 Model1_atomic_long_t Model1_tx_dropped;
 Model1_atomic_long_t Model1_rx_nohandler;





 const struct Model1_net_device_ops *Model1_netdev_ops;
 const struct Model1_ethtool_ops *Model1_ethtool_ops;







 const struct Model1_ndisc_ops *Model1_ndisc_ops;


 const struct Model1_header_ops *Model1_header_ops;

 unsigned int Model1_flags;
 unsigned int Model1_priv_flags;

 unsigned short Model1_gflags;
 unsigned short Model1_padded;

 unsigned char Model1_operstate;
 unsigned char Model1_link_mode;

 unsigned char Model1_if_port;
 unsigned char Model1_dma;

 unsigned int Model1_mtu;
 unsigned short Model1_type;
 unsigned short Model1_hard_header_len;

 unsigned short Model1_needed_headroom;
 unsigned short Model1_needed_tailroom;

 /* Interface address info. */
 unsigned char Model1_perm_addr[32];
 unsigned char Model1_addr_assign_type;
 unsigned char Model1_addr_len;
 unsigned short Model1_neigh_priv_len;
 unsigned short Model1_dev_id;
 unsigned short Model1_dev_port;
 Model1_spinlock_t Model1_addr_list_lock;
 unsigned char Model1_name_assign_type;
 bool Model1_uc_promisc;
 struct Model1_netdev_hw_addr_list Model1_uc;
 struct Model1_netdev_hw_addr_list Model1_mc;
 struct Model1_netdev_hw_addr_list Model1_dev_addrs;


 struct Model1_kset *Model1_queues_kset;

 unsigned int Model1_promiscuity;
 unsigned int Model1_allmulti;


 /* Protocol-specific pointers */
 void *Model1_atalk_ptr;
 struct Model1_in_device *Model1_ip_ptr;
 struct Model1_dn_dev *Model1_dn_ptr;
 struct Model1_inet6_dev *Model1_ip6_ptr;
 void *Model1_ax25_ptr;
 struct Model1_wireless_dev *Model1_ieee80211_ptr;
 struct Model1_wpan_dev *Model1_ieee802154_ptr;




/*
 * Cache lines mostly used on receive path (including eth_type_trans())
 */
 unsigned long Model1_last_rx;

 /* Interface address info used in eth_type_trans() */
 unsigned char *Model1_dev_addr;


 struct Model1_netdev_rx_queue *Model1__rx;

 unsigned int Model1_num_rx_queues;
 unsigned int Model1_real_num_rx_queues;


 unsigned long Model1_gro_flush_timeout;
 Model1_rx_handler_func_t *Model1_rx_handler;
 void *Model1_rx_handler_data;


 struct Model1_tcf_proto *Model1_ingress_cl_list;

 struct Model1_netdev_queue *Model1_ingress_queue;

 struct Model1_list_head Model1_nf_hooks_ingress;


 unsigned char Model1_broadcast[32];

 struct Model1_cpu_rmap *Model1_rx_cpu_rmap;

 struct Model1_hlist_node Model1_index_hlist;

/*
 * Cache lines mostly used on transmit path
 */
 struct Model1_netdev_queue *Model1__tx __attribute__((__aligned__((1 << (6)))));
 unsigned int Model1_num_tx_queues;
 unsigned int Model1_real_num_tx_queues;
 struct Model1_Qdisc *Model1_qdisc;
 unsigned long Model1_tx_queue_len;
 Model1_spinlock_t Model1_tx_global_lock;
 int Model1_watchdog_timeo;


 struct Model1_xps_dev_maps *Model1_xps_maps;


 struct Model1_tcf_proto *Model1_egress_cl_list;





 /* These may be needed for future network-power-down code. */
 struct Model1_timer_list Model1_watchdog_timer;

 int *Model1_pcpu_refcnt;
 struct Model1_list_head Model1_todo_list;

 struct Model1_list_head Model1_link_watch_list;

 enum { Model1_NETREG_UNINITIALIZED=0,
        Model1_NETREG_REGISTERED, /* completed register_netdevice */
        Model1_NETREG_UNREGISTERING, /* called unregister_netdevice */
        Model1_NETREG_UNREGISTERED, /* completed unregister todo */
        Model1_NETREG_RELEASED, /* called free_netdev */
        Model1_NETREG_DUMMY, /* dummy device for NAPI poll */
 } Model1_reg_state:8;

 bool Model1_dismantle;

 enum {
  Model1_RTNL_LINK_INITIALIZED,
  Model1_RTNL_LINK_INITIALIZING,
 } Model1_rtnl_link_state:16;

 void (*Model1_destructor)(struct Model1_net_device *Model1_dev);


 struct Model1_netpoll_info *Model1_npinfo;


 Model1_possible_net_t Model1_nd_net;

 /* mid-layer private */
 union {
  void *Model1_ml_priv;
  struct Model1_pcpu_lstats *Model1_lstats;
  struct Model1_pcpu_sw_netstats *Model1_tstats;
  struct Model1_pcpu_dstats *Model1_dstats;
  struct Model1_pcpu_vstats *Model1_vstats;
 };

 struct Model1_garp_port *Model1_garp_port;
 struct Model1_mrp_port *Model1_mrp_port;

 struct Model1_device Model1_dev;
 const struct Model1_attribute_group *Model1_sysfs_groups[4];
 const struct Model1_attribute_group *Model1_sysfs_rx_queue_group;

 const struct Model1_rtnl_link_ops *Model1_rtnl_link_ops;

 /* for setting kernel sock attribute on TCP connection setup */

 unsigned int Model1_gso_max_size;

 Model1_u16 Model1_gso_max_segs;




 Model1_u8 Model1_num_tc;
 struct Model1_netdev_tc_txq Model1_tc_to_txq[16];
 Model1_u8 Model1_prio_tc_map[15 + 1];







 struct Model1_phy_device *Model1_phydev;
 struct Model1_lock_class_key *Model1_qdisc_tx_busylock;
 struct Model1_lock_class_key *Model1_qdisc_running_key;
 bool Model1_proto_down;
};




static inline __attribute__((no_instrument_function))
int Model1_netdev_get_prio_tc_map(const struct Model1_net_device *Model1_dev, Model1_u32 Model1_prio)
{
 return Model1_dev->Model1_prio_tc_map[Model1_prio & 15];
}

static inline __attribute__((no_instrument_function))
int Model1_netdev_set_prio_tc_map(struct Model1_net_device *Model1_dev, Model1_u8 Model1_prio, Model1_u8 Model1_tc)
{
 if (Model1_tc >= Model1_dev->Model1_num_tc)
  return -22;

 Model1_dev->Model1_prio_tc_map[Model1_prio & 15] = Model1_tc & 15;
 return 0;
}

static inline __attribute__((no_instrument_function))
void Model1_netdev_reset_tc(struct Model1_net_device *Model1_dev)
{
 Model1_dev->Model1_num_tc = 0;
 memset(Model1_dev->Model1_tc_to_txq, 0, sizeof(Model1_dev->Model1_tc_to_txq));
 memset(Model1_dev->Model1_prio_tc_map, 0, sizeof(Model1_dev->Model1_prio_tc_map));
}

static inline __attribute__((no_instrument_function))
int Model1_netdev_set_tc_queue(struct Model1_net_device *Model1_dev, Model1_u8 Model1_tc, Model1_u16 Model1_count, Model1_u16 Model1_offset)
{
 if (Model1_tc >= Model1_dev->Model1_num_tc)
  return -22;

 Model1_dev->Model1_tc_to_txq[Model1_tc].Model1_count = Model1_count;
 Model1_dev->Model1_tc_to_txq[Model1_tc].Model1_offset = Model1_offset;
 return 0;
}

static inline __attribute__((no_instrument_function))
int Model1_netdev_set_num_tc(struct Model1_net_device *Model1_dev, Model1_u8 Model1_num_tc)
{
 if (Model1_num_tc > 16)
  return -22;

 Model1_dev->Model1_num_tc = Model1_num_tc;
 return 0;
}

static inline __attribute__((no_instrument_function))
int Model1_netdev_get_num_tc(struct Model1_net_device *Model1_dev)
{
 return Model1_dev->Model1_num_tc;
}

static inline __attribute__((no_instrument_function))
struct Model1_netdev_queue *Model1_netdev_get_tx_queue(const struct Model1_net_device *Model1_dev,
      unsigned int Model1_index)
{
 return &Model1_dev->Model1__tx[Model1_index];
}

static inline __attribute__((no_instrument_function)) struct Model1_netdev_queue *Model1_skb_get_tx_queue(const struct Model1_net_device *Model1_dev,
          const struct Model1_sk_buff *Model1_skb)
{
 return Model1_netdev_get_tx_queue(Model1_dev, Model1_skb_get_queue_mapping(Model1_skb));
}

static inline __attribute__((no_instrument_function)) void Model1_netdev_for_each_tx_queue(struct Model1_net_device *Model1_dev,
         void (*Model1_f)(struct Model1_net_device *,
            struct Model1_netdev_queue *,
            void *),
         void *Model1_arg)
{
 unsigned int Model1_i;

 for (Model1_i = 0; Model1_i < Model1_dev->Model1_num_tx_queues; Model1_i++)
  Model1_f(Model1_dev, &Model1_dev->Model1__tx[Model1_i], Model1_arg);
}
struct Model1_netdev_queue *Model1_netdev_pick_tx(struct Model1_net_device *Model1_dev,
        struct Model1_sk_buff *Model1_skb,
        void *Model1_accel_priv);

/* returns the headroom that the master device needs to take in account
 * when forwarding to this dev
 */
static inline __attribute__((no_instrument_function)) unsigned Model1_netdev_get_fwd_headroom(struct Model1_net_device *Model1_dev)
{
 return Model1_dev->Model1_priv_flags & Model1_IFF_PHONY_HEADROOM ? 0 : Model1_dev->Model1_needed_headroom;
}

static inline __attribute__((no_instrument_function)) void Model1_netdev_set_rx_headroom(struct Model1_net_device *Model1_dev, int Model1_new_hr)
{
 if (Model1_dev->Model1_netdev_ops->Model1_ndo_set_rx_headroom)
  Model1_dev->Model1_netdev_ops->Model1_ndo_set_rx_headroom(Model1_dev, Model1_new_hr);
}

/* set the device rx headroom to the dev's default */
static inline __attribute__((no_instrument_function)) void Model1_netdev_reset_rx_headroom(struct Model1_net_device *Model1_dev)
{
 Model1_netdev_set_rx_headroom(Model1_dev, -1);
}

/*
 * Net namespace inlines
 */
static inline __attribute__((no_instrument_function))
struct Model1_net *Model1_dev_net(const struct Model1_net_device *Model1_dev)
{
 return Model1_read_pnet(&Model1_dev->Model1_nd_net);
}

static inline __attribute__((no_instrument_function))
void Model1_dev_net_set(struct Model1_net_device *Model1_dev, struct Model1_net *Model1_net)
{
 Model1_write_pnet(&Model1_dev->Model1_nd_net, Model1_net);
}

static inline __attribute__((no_instrument_function)) bool Model1_netdev_uses_dsa(struct Model1_net_device *Model1_dev)
{




 return false;
}

/**
 *	netdev_priv - access network device private data
 *	@dev: network device
 *
 * Get network device private data
 */
static inline __attribute__((no_instrument_function)) void *Model1_netdev_priv(const struct Model1_net_device *Model1_dev)
{
 return (char *)Model1_dev + ((((sizeof(struct Model1_net_device))) + ((typeof((sizeof(struct Model1_net_device))))((32)) - 1)) & ~((typeof((sizeof(struct Model1_net_device))))((32)) - 1));
}

/* Set the sysfs physical device reference for the network logical device
 * if set prior to registration will cause a symlink during initialization.
 */


/* Set the sysfs device type for the network logical device to allow
 * fine-grained identification of different network device types. For
 * example Ethernet, Wireless LAN, Bluetooth, WiMAX etc.
 */


/* Default NAPI poll() weight
 * Device drivers are strongly advised to not use bigger value
 */


/**
 *	netif_napi_add - initialize a NAPI context
 *	@dev:  network device
 *	@napi: NAPI context
 *	@poll: polling function
 *	@weight: default weight
 *
 * netif_napi_add() must be used to initialize a NAPI context prior to calling
 * *any* of the other NAPI-related functions.
 */
void Model1_netif_napi_add(struct Model1_net_device *Model1_dev, struct Model1_napi_struct *Model1_napi,
      int (*Model1_poll)(struct Model1_napi_struct *, int), int Model1_weight);

/**
 *	netif_tx_napi_add - initialize a NAPI context
 *	@dev:  network device
 *	@napi: NAPI context
 *	@poll: polling function
 *	@weight: default weight
 *
 * This variant of netif_napi_add() should be used from drivers using NAPI
 * to exclusively poll a TX queue.
 * This will avoid we add it into napi_hash[], thus polluting this hash table.
 */
static inline __attribute__((no_instrument_function)) void Model1_netif_tx_napi_add(struct Model1_net_device *Model1_dev,
         struct Model1_napi_struct *Model1_napi,
         int (*Model1_poll)(struct Model1_napi_struct *, int),
         int Model1_weight)
{
 Model1_set_bit(Model1_NAPI_STATE_NO_BUSY_POLL, &Model1_napi->Model1_state);
 Model1_netif_napi_add(Model1_dev, Model1_napi, Model1_poll, Model1_weight);
}

/**
 *  netif_napi_del - remove a NAPI context
 *  @napi: NAPI context
 *
 *  netif_napi_del() removes a NAPI context from the network device NAPI list
 */
void Model1_netif_napi_del(struct Model1_napi_struct *Model1_napi);

struct Model1_napi_gro_cb {
 /* Virtual address of skb_shinfo(skb)->frags[0].page + offset. */
 void *Model1_frag0;

 /* Length of frag0. */
 unsigned int Model1_frag0_len;

 /* This indicates where we are processing relative to skb->data. */
 int Model1_data_offset;

 /* This is non-zero if the packet cannot be merged with the new skb. */
 Model1_u16 Model1_flush;

 /* Save the IP ID here and check when we get to the transport layer */
 Model1_u16 Model1_flush_id;

 /* Number of segments aggregated. */
 Model1_u16 Model1_count;

 /* Start offset for remote checksum offload */
 Model1_u16 Model1_gro_remcsum_start;

 /* jiffies when first packet was created/queued */
 unsigned long Model1_age;

 /* Used in ipv6_gro_receive() and foo-over-udp */
 Model1_u16 Model1_proto;

 /* This is non-zero if the packet may be of the same flow. */
 Model1_u8 Model1_same_flow:1;

 /* Used in tunnel GRO receive */
 Model1_u8 Model1_encap_mark:1;

 /* GRO checksum is valid */
 Model1_u8 Model1_csum_valid:1;

 /* Number of checksums via CHECKSUM_UNNECESSARY */
 Model1_u8 Model1_csum_cnt:3;

 /* Free the skb? */
 Model1_u8 Model1_free:2;



 /* Used in foo-over-udp, set in udp[46]_gro_receive */
 Model1_u8 Model1_is_ipv6:1;

 /* Used in GRE, set in fou/gue_gro_receive */
 Model1_u8 Model1_is_fou:1;

 /* Used to determine if flush_id can be ignored */
 Model1_u8 Model1_is_atomic:1;

 /* 5 bit hole */

 /* used to support CHECKSUM_COMPLETE for tunneling protocols */
 Model1___wsum Model1_csum;

 /* used in skb_gro_receive() slow path */
 struct Model1_sk_buff *Model1_last;
};



struct Model1_packet_type {
 Model1___be16 Model1_type; /* This is really htons(ether_type). */
 struct Model1_net_device *Model1_dev; /* NULL is wildcarded here	     */
 int (*func) (struct Model1_sk_buff *,
      struct Model1_net_device *,
      struct Model1_packet_type *,
      struct Model1_net_device *);
 bool (*Model1_id_match)(struct Model1_packet_type *Model1_ptype,
         struct Model1_sock *Model1_sk);
 void *Model1_af_packet_priv;
 struct Model1_list_head Model1_list;
};

struct Model1_offload_callbacks {
 struct Model1_sk_buff *(*Model1_gso_segment)(struct Model1_sk_buff *Model1_skb,
      Model1_netdev_features_t Model1_features);
 struct Model1_sk_buff **(*Model1_gro_receive)(struct Model1_sk_buff **Model1_head,
       struct Model1_sk_buff *Model1_skb);
 int (*Model1_gro_complete)(struct Model1_sk_buff *Model1_skb, int Model1_nhoff);
};

struct Model1_packet_offload {
 Model1___be16 Model1_type; /* This is really htons(ether_type). */
 Model1_u16 Model1_priority;
 struct Model1_offload_callbacks Model1_callbacks;
 struct Model1_list_head Model1_list;
};

/* often modified stats are per-CPU, other are shared (netdev->stats) */
struct Model1_pcpu_sw_netstats {
 Model1_u64 Model1_rx_packets;
 Model1_u64 Model1_rx_bytes;
 Model1_u64 Model1_tx_packets;
 Model1_u64 Model1_tx_bytes;
 struct Model1_u64_stats_sync Model1_syncp;
};
enum Model1_netdev_lag_tx_type {
 Model1_NETDEV_LAG_TX_TYPE_UNKNOWN,
 Model1_NETDEV_LAG_TX_TYPE_RANDOM,
 Model1_NETDEV_LAG_TX_TYPE_BROADCAST,
 Model1_NETDEV_LAG_TX_TYPE_ROUNDROBIN,
 Model1_NETDEV_LAG_TX_TYPE_ACTIVEBACKUP,
 Model1_NETDEV_LAG_TX_TYPE_HASH,
};

struct Model1_netdev_lag_upper_info {
 enum Model1_netdev_lag_tx_type Model1_tx_type;
};

struct Model1_netdev_lag_lower_state_info {
 Model1_u8 Model1_link_up : 1,
    Model1_tx_enabled : 1;
};



/* netdevice notifier chain. Please remember to update the rtnetlink
 * notification exclusion list in rtnetlink_event() when adding new
 * types.
 */
int Model1_register_netdevice_notifier(struct Model1_notifier_block *Model1_nb);
int Model1_unregister_netdevice_notifier(struct Model1_notifier_block *Model1_nb);

struct Model1_netdev_notifier_info {
 struct Model1_net_device *Model1_dev;
};

struct Model1_netdev_notifier_change_info {
 struct Model1_netdev_notifier_info Model1_info; /* must be first */
 unsigned int Model1_flags_changed;
};

struct Model1_netdev_notifier_changeupper_info {
 struct Model1_netdev_notifier_info Model1_info; /* must be first */
 struct Model1_net_device *Model1_upper_dev; /* new upper dev */
 bool Model1_master; /* is upper dev master */
 bool Model1_linking; /* is the notification for link or unlink */
 void *Model1_upper_info; /* upper dev info */
};

struct Model1_netdev_notifier_changelowerstate_info {
 struct Model1_netdev_notifier_info Model1_info; /* must be first */
 void *Model1_lower_state_info; /* is lower dev state */
};

static inline __attribute__((no_instrument_function)) void Model1_netdev_notifier_info_init(struct Model1_netdev_notifier_info *Model1_info,
          struct Model1_net_device *Model1_dev)
{
 Model1_info->Model1_dev = Model1_dev;
}

static inline __attribute__((no_instrument_function)) struct Model1_net_device *
Model1_netdev_notifier_info_to_dev(const struct Model1_netdev_notifier_info *Model1_info)
{
 return Model1_info->Model1_dev;
}

int Model1_call_netdevice_notifiers(unsigned long Model1_val, struct Model1_net_device *Model1_dev);


extern Model1_rwlock_t Model1_dev_base_lock; /* Device list lock */
static inline __attribute__((no_instrument_function)) struct Model1_net_device *Model1_next_net_device(struct Model1_net_device *Model1_dev)
{
 struct Model1_list_head *Model1_lh;
 struct Model1_net *Model1_net;

 Model1_net = Model1_dev_net(Model1_dev);
 Model1_lh = Model1_dev->Model1_dev_list.Model1_next;
 return Model1_lh == &Model1_net->Model1_dev_base_head ? ((void *)0) : ({ const typeof( ((struct Model1_net_device *)0)->Model1_dev_list ) *Model1___mptr = (Model1_lh); (struct Model1_net_device *)( (char *)Model1___mptr - __builtin_offsetof(struct Model1_net_device, Model1_dev_list) );});
}

static inline __attribute__((no_instrument_function)) struct Model1_net_device *Model1_next_net_device_rcu(struct Model1_net_device *Model1_dev)
{
 struct Model1_list_head *Model1_lh;
 struct Model1_net *Model1_net;

 Model1_net = Model1_dev_net(Model1_dev);
 Model1_lh = ({ typeof(*((*((struct Model1_list_head **)(&(&Model1_dev->Model1_dev_list)->Model1_next))))) *Model1_________p1 = (typeof(*((*((struct Model1_list_head **)(&(&Model1_dev->Model1_dev_list)->Model1_next))))) *)({ typeof(((*((struct Model1_list_head **)(&(&Model1_dev->Model1_dev_list)->Model1_next))))) Model1__________p1 = ({ union { typeof(((*((struct Model1_list_head **)(&(&Model1_dev->Model1_dev_list)->Model1_next))))) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&(((*((struct Model1_list_head **)(&(&Model1_dev->Model1_dev_list)->Model1_next))))), Model1___u.Model1___c, sizeof(((*((struct Model1_list_head **)(&(&Model1_dev->Model1_dev_list)->Model1_next)))))); else Model1___read_once_size_nocheck(&(((*((struct Model1_list_head **)(&(&Model1_dev->Model1_dev_list)->Model1_next))))), Model1___u.Model1___c, sizeof(((*((struct Model1_list_head **)(&(&Model1_dev->Model1_dev_list)->Model1_next)))))); Model1___u.Model1___val; }); typeof(*(((*((struct Model1_list_head **)(&(&Model1_dev->Model1_dev_list)->Model1_next)))))) *Model1____typecheck_p __attribute__((unused)); do { } while (0); (Model1__________p1); }); do { } while (0); ; ((typeof(*((*((struct Model1_list_head **)(&(&Model1_dev->Model1_dev_list)->Model1_next))))) *)(Model1_________p1)); });
 return Model1_lh == &Model1_net->Model1_dev_base_head ? ((void *)0) : ({ const typeof( ((struct Model1_net_device *)0)->Model1_dev_list ) *Model1___mptr = (Model1_lh); (struct Model1_net_device *)( (char *)Model1___mptr - __builtin_offsetof(struct Model1_net_device, Model1_dev_list) );});
}

static inline __attribute__((no_instrument_function)) struct Model1_net_device *Model1_first_net_device(struct Model1_net *Model1_net)
{
 return Model1_list_empty(&Model1_net->Model1_dev_base_head) ? ((void *)0) :
  ({ const typeof( ((struct Model1_net_device *)0)->Model1_dev_list ) *Model1___mptr = (Model1_net->Model1_dev_base_head.Model1_next); (struct Model1_net_device *)( (char *)Model1___mptr - __builtin_offsetof(struct Model1_net_device, Model1_dev_list) );});
}

static inline __attribute__((no_instrument_function)) struct Model1_net_device *Model1_first_net_device_rcu(struct Model1_net *Model1_net)
{
 struct Model1_list_head *Model1_lh = ({ typeof(*((*((struct Model1_list_head **)(&(&Model1_net->Model1_dev_base_head)->Model1_next))))) *Model1_________p1 = (typeof(*((*((struct Model1_list_head **)(&(&Model1_net->Model1_dev_base_head)->Model1_next))))) *)({ typeof(((*((struct Model1_list_head **)(&(&Model1_net->Model1_dev_base_head)->Model1_next))))) Model1__________p1 = ({ union { typeof(((*((struct Model1_list_head **)(&(&Model1_net->Model1_dev_base_head)->Model1_next))))) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&(((*((struct Model1_list_head **)(&(&Model1_net->Model1_dev_base_head)->Model1_next))))), Model1___u.Model1___c, sizeof(((*((struct Model1_list_head **)(&(&Model1_net->Model1_dev_base_head)->Model1_next)))))); else Model1___read_once_size_nocheck(&(((*((struct Model1_list_head **)(&(&Model1_net->Model1_dev_base_head)->Model1_next))))), Model1___u.Model1___c, sizeof(((*((struct Model1_list_head **)(&(&Model1_net->Model1_dev_base_head)->Model1_next)))))); Model1___u.Model1___val; }); typeof(*(((*((struct Model1_list_head **)(&(&Model1_net->Model1_dev_base_head)->Model1_next)))))) *Model1____typecheck_p __attribute__((unused)); do { } while (0); (Model1__________p1); }); do { } while (0); ; ((typeof(*((*((struct Model1_list_head **)(&(&Model1_net->Model1_dev_base_head)->Model1_next))))) *)(Model1_________p1)); });

 return Model1_lh == &Model1_net->Model1_dev_base_head ? ((void *)0) : ({ const typeof( ((struct Model1_net_device *)0)->Model1_dev_list ) *Model1___mptr = (Model1_lh); (struct Model1_net_device *)( (char *)Model1___mptr - __builtin_offsetof(struct Model1_net_device, Model1_dev_list) );});
}

int Model1_netdev_boot_setup_check(struct Model1_net_device *Model1_dev);
unsigned long Model1_netdev_boot_base(const char *Model1_prefix, int Model1_unit);
struct Model1_net_device *Model1_dev_getbyhwaddr_rcu(struct Model1_net *Model1_net, unsigned short Model1_type,
           const char *Model1_hwaddr);
struct Model1_net_device *Model1_dev_getfirstbyhwtype(struct Model1_net *Model1_net, unsigned short Model1_type);
struct Model1_net_device *Model1___dev_getfirstbyhwtype(struct Model1_net *Model1_net, unsigned short Model1_type);
void Model1_dev_add_pack(struct Model1_packet_type *Model1_pt);
void Model1_dev_remove_pack(struct Model1_packet_type *Model1_pt);
void Model1___dev_remove_pack(struct Model1_packet_type *Model1_pt);
void Model1_dev_add_offload(struct Model1_packet_offload *Model1_po);
void Model1_dev_remove_offload(struct Model1_packet_offload *Model1_po);

int Model1_dev_get_iflink(const struct Model1_net_device *Model1_dev);
int Model1_dev_fill_metadata_dst(struct Model1_net_device *Model1_dev, struct Model1_sk_buff *Model1_skb);
struct Model1_net_device *Model1___dev_get_by_flags(struct Model1_net *Model1_net, unsigned short Model1_flags,
          unsigned short Model1_mask);
struct Model1_net_device *Model1_dev_get_by_name(struct Model1_net *Model1_net, const char *Model1_name);
struct Model1_net_device *Model1_dev_get_by_name_rcu(struct Model1_net *Model1_net, const char *Model1_name);
struct Model1_net_device *Model1___dev_get_by_name(struct Model1_net *Model1_net, const char *Model1_name);
int Model1_dev_alloc_name(struct Model1_net_device *Model1_dev, const char *Model1_name);
int Model1_dev_open(struct Model1_net_device *Model1_dev);
int Model1_dev_close(struct Model1_net_device *Model1_dev);
int Model1_dev_close_many(struct Model1_list_head *Model1_head, bool Model1_unlink);
void Model1_dev_disable_lro(struct Model1_net_device *Model1_dev);
int Model1_dev_loopback_xmit(struct Model1_net *Model1_net, struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_newskb);
int Model1_dev_queue_xmit(struct Model1_sk_buff *Model1_skb);
int Model1_dev_queue_xmit_accel(struct Model1_sk_buff *Model1_skb, void *Model1_accel_priv);
int Model1_register_netdevice(struct Model1_net_device *Model1_dev);
void Model1_unregister_netdevice_queue(struct Model1_net_device *Model1_dev, struct Model1_list_head *Model1_head);
void Model1_unregister_netdevice_many(struct Model1_list_head *Model1_head);
static inline __attribute__((no_instrument_function)) void Model1_unregister_netdevice(struct Model1_net_device *Model1_dev)
{
 Model1_unregister_netdevice_queue(Model1_dev, ((void *)0));
}

int Model1_netdev_refcnt_read(const struct Model1_net_device *Model1_dev);
void Model1_free_netdev(struct Model1_net_device *Model1_dev);
void Model1_netdev_freemem(struct Model1_net_device *Model1_dev);
void Model1_synchronize_net(void);
int Model1_init_dummy_netdev(struct Model1_net_device *Model1_dev);

extern __attribute__((section(".data..percpu" ""))) __typeof__(int) Model1_xmit_recursion;


static inline __attribute__((no_instrument_function)) int Model1_dev_recursion_level(void)
{
 return ({ typeof(Model1_xmit_recursion) Model1_pscr_ret__; do { const void *Model1___vpp_verify = (typeof((&(Model1_xmit_recursion)) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); switch(sizeof(Model1_xmit_recursion)) { case 1: Model1_pscr_ret__ = ({ typeof(Model1_xmit_recursion) Model1_pfo_ret__; switch (sizeof(Model1_xmit_recursion)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_xmit_recursion)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_xmit_recursion)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_xmit_recursion)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_xmit_recursion)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 2: Model1_pscr_ret__ = ({ typeof(Model1_xmit_recursion) Model1_pfo_ret__; switch (sizeof(Model1_xmit_recursion)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_xmit_recursion)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_xmit_recursion)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_xmit_recursion)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_xmit_recursion)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 4: Model1_pscr_ret__ = ({ typeof(Model1_xmit_recursion) Model1_pfo_ret__; switch (sizeof(Model1_xmit_recursion)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_xmit_recursion)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_xmit_recursion)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_xmit_recursion)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_xmit_recursion)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 8: Model1_pscr_ret__ = ({ typeof(Model1_xmit_recursion) Model1_pfo_ret__; switch (sizeof(Model1_xmit_recursion)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_xmit_recursion)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_xmit_recursion)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_xmit_recursion)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_xmit_recursion)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; default: Model1___bad_size_call_parameter(); break; } Model1_pscr_ret__; });
}

struct Model1_net_device *Model1_dev_get_by_index(struct Model1_net *Model1_net, int Model1_ifindex);
struct Model1_net_device *Model1___dev_get_by_index(struct Model1_net *Model1_net, int Model1_ifindex);
struct Model1_net_device *Model1_dev_get_by_index_rcu(struct Model1_net *Model1_net, int Model1_ifindex);
int Model1_netdev_get_name(struct Model1_net *Model1_net, char *Model1_name, int Model1_ifindex);
int Model1_dev_restart(struct Model1_net_device *Model1_dev);
int Model1_skb_gro_receive(struct Model1_sk_buff **Model1_head, struct Model1_sk_buff *Model1_skb);

static inline __attribute__((no_instrument_function)) unsigned int Model1_skb_gro_offset(const struct Model1_sk_buff *Model1_skb)
{
 return ((struct Model1_napi_gro_cb *)(Model1_skb)->Model1_cb)->Model1_data_offset;
}

static inline __attribute__((no_instrument_function)) unsigned int Model1_skb_gro_len(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb->Model1_len - ((struct Model1_napi_gro_cb *)(Model1_skb)->Model1_cb)->Model1_data_offset;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_gro_pull(struct Model1_sk_buff *Model1_skb, unsigned int Model1_len)
{
 ((struct Model1_napi_gro_cb *)(Model1_skb)->Model1_cb)->Model1_data_offset += Model1_len;
}

static inline __attribute__((no_instrument_function)) void *Model1_skb_gro_header_fast(struct Model1_sk_buff *Model1_skb,
     unsigned int Model1_offset)
{
 return ((struct Model1_napi_gro_cb *)(Model1_skb)->Model1_cb)->Model1_frag0 + Model1_offset;
}

static inline __attribute__((no_instrument_function)) int Model1_skb_gro_header_hard(struct Model1_sk_buff *Model1_skb, unsigned int Model1_hlen)
{
 return ((struct Model1_napi_gro_cb *)(Model1_skb)->Model1_cb)->Model1_frag0_len < Model1_hlen;
}

static inline __attribute__((no_instrument_function)) void *Model1_skb_gro_header_slow(struct Model1_sk_buff *Model1_skb, unsigned int Model1_hlen,
     unsigned int Model1_offset)
{
 if (!Model1_pskb_may_pull(Model1_skb, Model1_hlen))
  return ((void *)0);

 ((struct Model1_napi_gro_cb *)(Model1_skb)->Model1_cb)->Model1_frag0 = ((void *)0);
 ((struct Model1_napi_gro_cb *)(Model1_skb)->Model1_cb)->Model1_frag0_len = 0;
 return Model1_skb->Model1_data + Model1_offset;
}

static inline __attribute__((no_instrument_function)) void *Model1_skb_gro_network_header(struct Model1_sk_buff *Model1_skb)
{
 return (((struct Model1_napi_gro_cb *)(Model1_skb)->Model1_cb)->Model1_frag0 ?: Model1_skb->Model1_data) +
        Model1_skb_network_offset(Model1_skb);
}

static inline __attribute__((no_instrument_function)) void Model1_skb_gro_postpull_rcsum(struct Model1_sk_buff *Model1_skb,
     const void *Model1_start, unsigned int Model1_len)
{
 if (((struct Model1_napi_gro_cb *)(Model1_skb)->Model1_cb)->Model1_csum_valid)
  ((struct Model1_napi_gro_cb *)(Model1_skb)->Model1_cb)->Model1_csum = Model1_csum_sub(((struct Model1_napi_gro_cb *)(Model1_skb)->Model1_cb)->Model1_csum,
        Model1_csum_partial(Model1_start, Model1_len, 0));
}

/* GRO checksum functions. These are logical equivalents of the normal
 * checksum functions (in skbuff.h) except that they operate on the GRO
 * offsets and fields in sk_buff.
 */

Model1___sum16 Model1___skb_gro_checksum_complete(struct Model1_sk_buff *Model1_skb);

static inline __attribute__((no_instrument_function)) bool Model1_skb_at_gro_remcsum_start(struct Model1_sk_buff *Model1_skb)
{
 return (((struct Model1_napi_gro_cb *)(Model1_skb)->Model1_cb)->Model1_gro_remcsum_start == Model1_skb_gro_offset(Model1_skb));
}

static inline __attribute__((no_instrument_function)) bool Model1___skb_gro_checksum_validate_needed(struct Model1_sk_buff *Model1_skb,
            bool Model1_zero_okay,
            Model1___sum16 Model1_check)
{
 return ((Model1_skb->Model1_ip_summed != 3 ||
  Model1_skb_checksum_start_offset(Model1_skb) <
   Model1_skb_gro_offset(Model1_skb)) &&
  !Model1_skb_at_gro_remcsum_start(Model1_skb) &&
  ((struct Model1_napi_gro_cb *)(Model1_skb)->Model1_cb)->Model1_csum_cnt == 0 &&
  (!Model1_zero_okay || Model1_check));
}

static inline __attribute__((no_instrument_function)) Model1___sum16 Model1___skb_gro_checksum_validate_complete(struct Model1_sk_buff *Model1_skb,
          Model1___wsum Model1_psum)
{
 if (((struct Model1_napi_gro_cb *)(Model1_skb)->Model1_cb)->Model1_csum_valid &&
     !Model1_csum_fold(Model1_csum_add(Model1_psum, ((struct Model1_napi_gro_cb *)(Model1_skb)->Model1_cb)->Model1_csum)))
  return 0;

 ((struct Model1_napi_gro_cb *)(Model1_skb)->Model1_cb)->Model1_csum = Model1_psum;

 return Model1___skb_gro_checksum_complete(Model1_skb);
}

static inline __attribute__((no_instrument_function)) void Model1_skb_gro_incr_csum_unnecessary(struct Model1_sk_buff *Model1_skb)
{
 if (((struct Model1_napi_gro_cb *)(Model1_skb)->Model1_cb)->Model1_csum_cnt > 0) {
  /* Consume a checksum from CHECKSUM_UNNECESSARY */
  ((struct Model1_napi_gro_cb *)(Model1_skb)->Model1_cb)->Model1_csum_cnt--;
 } else {
  /* Update skb for CHECKSUM_UNNECESSARY and csum_level when we
		 * verified a new top level checksum or an encapsulated one
		 * during GRO. This saves work if we fallback to normal path.
		 */
  Model1___skb_incr_checksum_unnecessary(Model1_skb);
 }
}
static inline __attribute__((no_instrument_function)) bool Model1___skb_gro_checksum_convert_check(struct Model1_sk_buff *Model1_skb)
{
 return (((struct Model1_napi_gro_cb *)(Model1_skb)->Model1_cb)->Model1_csum_cnt == 0 &&
  !((struct Model1_napi_gro_cb *)(Model1_skb)->Model1_cb)->Model1_csum_valid);
}

static inline __attribute__((no_instrument_function)) void Model1___skb_gro_checksum_convert(struct Model1_sk_buff *Model1_skb,
           Model1___sum16 Model1_check, Model1___wsum Model1_pseudo)
{
 ((struct Model1_napi_gro_cb *)(Model1_skb)->Model1_cb)->Model1_csum = ~Model1_pseudo;
 ((struct Model1_napi_gro_cb *)(Model1_skb)->Model1_cb)->Model1_csum_valid = 1;
}
struct Model1_gro_remcsum {
 int Model1_offset;
 Model1___wsum Model1_delta;
};

static inline __attribute__((no_instrument_function)) void Model1_skb_gro_remcsum_init(struct Model1_gro_remcsum *Model1_grc)
{
 Model1_grc->Model1_offset = 0;
 Model1_grc->Model1_delta = 0;
}

static inline __attribute__((no_instrument_function)) void *Model1_skb_gro_remcsum_process(struct Model1_sk_buff *Model1_skb, void *Model1_ptr,
         unsigned int Model1_off, Model1_size_t Model1_hdrlen,
         int Model1_start, int Model1_offset,
         struct Model1_gro_remcsum *Model1_grc,
         bool Model1_nopartial)
{
 Model1___wsum Model1_delta;
 Model1_size_t Model1_plen = Model1_hdrlen + ({ Model1_size_t Model1___max1 = (Model1_offset + sizeof(Model1_u16)); Model1_size_t Model1___max2 = (Model1_start); Model1___max1 > Model1___max2 ? Model1___max1: Model1___max2; });

 do { if (__builtin_expect(!!(!((struct Model1_napi_gro_cb *)(Model1_skb)->Model1_cb)->Model1_csum_valid), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/netdevice.h"), "i" (2592), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);

 if (!Model1_nopartial) {
  ((struct Model1_napi_gro_cb *)(Model1_skb)->Model1_cb)->Model1_gro_remcsum_start = Model1_off + Model1_hdrlen + Model1_start;
  return Model1_ptr;
 }

 Model1_ptr = Model1_skb_gro_header_fast(Model1_skb, Model1_off);
 if (Model1_skb_gro_header_hard(Model1_skb, Model1_off + Model1_plen)) {
  Model1_ptr = Model1_skb_gro_header_slow(Model1_skb, Model1_off + Model1_plen, Model1_off);
  if (!Model1_ptr)
   return ((void *)0);
 }

 Model1_delta = Model1_remcsum_adjust(Model1_ptr + Model1_hdrlen, ((struct Model1_napi_gro_cb *)(Model1_skb)->Model1_cb)->Model1_csum,
          Model1_start, Model1_offset);

 /* Adjust skb->csum since we changed the packet */
 ((struct Model1_napi_gro_cb *)(Model1_skb)->Model1_cb)->Model1_csum = Model1_csum_add(((struct Model1_napi_gro_cb *)(Model1_skb)->Model1_cb)->Model1_csum, Model1_delta);

 Model1_grc->Model1_offset = Model1_off + Model1_hdrlen + Model1_offset;
 Model1_grc->Model1_delta = Model1_delta;

 return Model1_ptr;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_gro_remcsum_cleanup(struct Model1_sk_buff *Model1_skb,
        struct Model1_gro_remcsum *Model1_grc)
{
 void *Model1_ptr;
 Model1_size_t Model1_plen = Model1_grc->Model1_offset + sizeof(Model1_u16);

 if (!Model1_grc->Model1_delta)
  return;

 Model1_ptr = Model1_skb_gro_header_fast(Model1_skb, Model1_grc->Model1_offset);
 if (Model1_skb_gro_header_hard(Model1_skb, Model1_grc->Model1_offset + sizeof(Model1_u16))) {
  Model1_ptr = Model1_skb_gro_header_slow(Model1_skb, Model1_plen, Model1_grc->Model1_offset);
  if (!Model1_ptr)
   return;
 }

 Model1_remcsum_unadjust((Model1___sum16 *)Model1_ptr, Model1_grc->Model1_delta);
}

struct Model1_skb_csum_offl_spec {
 Model1___u16 Model1_ipv4_okay:1,
   Model1_ipv6_okay:1,
   Model1_encap_okay:1,
   Model1_ip_options_okay:1,
   Model1_ext_hdrs_okay:1,
   Model1_tcp_okay:1,
   Model1_udp_okay:1,
   Model1_sctp_okay:1,
   Model1_vlan_okay:1,
   Model1_no_encapped_ipv6:1,
   Model1_no_not_encapped:1;
};

bool Model1___skb_csum_offload_chk(struct Model1_sk_buff *Model1_skb,
       const struct Model1_skb_csum_offl_spec *Model1_spec,
       bool *Model1_csum_encapped,
       bool Model1_csum_help);

static inline __attribute__((no_instrument_function)) bool Model1_skb_csum_offload_chk(struct Model1_sk_buff *Model1_skb,
     const struct Model1_skb_csum_offl_spec *Model1_spec,
     bool *Model1_csum_encapped,
     bool Model1_csum_help)
{
 if (Model1_skb->Model1_ip_summed != 3)
  return false;

 return Model1___skb_csum_offload_chk(Model1_skb, Model1_spec, Model1_csum_encapped, Model1_csum_help);
}

static inline __attribute__((no_instrument_function)) bool Model1_skb_csum_offload_chk_help(struct Model1_sk_buff *Model1_skb,
          const struct Model1_skb_csum_offl_spec *Model1_spec)
{
 bool Model1_csum_encapped;

 return Model1_skb_csum_offload_chk(Model1_skb, Model1_spec, &Model1_csum_encapped, true);
}

static inline __attribute__((no_instrument_function)) bool Model1_skb_csum_off_chk_help_cmn(struct Model1_sk_buff *Model1_skb)
{
 static const struct Model1_skb_csum_offl_spec Model1_csum_offl_spec = {
  .Model1_ipv4_okay = 1,
  .Model1_ip_options_okay = 1,
  .Model1_ipv6_okay = 1,
  .Model1_vlan_okay = 1,
  .Model1_tcp_okay = 1,
  .Model1_udp_okay = 1,
 };

 return Model1_skb_csum_offload_chk_help(Model1_skb, &Model1_csum_offl_spec);
}

static inline __attribute__((no_instrument_function)) bool Model1_skb_csum_off_chk_help_cmn_v4_only(struct Model1_sk_buff *Model1_skb)
{
 static const struct Model1_skb_csum_offl_spec Model1_csum_offl_spec = {
  .Model1_ipv4_okay = 1,
  .Model1_ip_options_okay = 1,
  .Model1_tcp_okay = 1,
  .Model1_udp_okay = 1,
  .Model1_vlan_okay = 1,
 };

 return Model1_skb_csum_offload_chk_help(Model1_skb, &Model1_csum_offl_spec);
}

static inline __attribute__((no_instrument_function)) int Model1_dev_hard_header(struct Model1_sk_buff *Model1_skb, struct Model1_net_device *Model1_dev,
      unsigned short Model1_type,
      const void *Model1_daddr, const void *Model1_saddr,
      unsigned int Model1_len)
{
 if (!Model1_dev->Model1_header_ops || !Model1_dev->Model1_header_ops->Model1_create)
  return 0;

 return Model1_dev->Model1_header_ops->Model1_create(Model1_skb, Model1_dev, Model1_type, Model1_daddr, Model1_saddr, Model1_len);
}

static inline __attribute__((no_instrument_function)) int Model1_dev_parse_header(const struct Model1_sk_buff *Model1_skb,
       unsigned char *Model1_haddr)
{
 const struct Model1_net_device *Model1_dev = Model1_skb->Model1_dev;

 if (!Model1_dev->Model1_header_ops || !Model1_dev->Model1_header_ops->Model1_parse)
  return 0;
 return Model1_dev->Model1_header_ops->Model1_parse(Model1_skb, Model1_haddr);
}

/* ll_header must have at least hard_header_len allocated */
static inline __attribute__((no_instrument_function)) bool Model1_dev_validate_header(const struct Model1_net_device *Model1_dev,
           char *Model1_ll_header, int Model1_len)
{
 if (__builtin_expect(!!(Model1_len >= Model1_dev->Model1_hard_header_len), 1))
  return true;

 if (Model1_capable(17)) {
  memset(Model1_ll_header + Model1_len, 0, Model1_dev->Model1_hard_header_len - Model1_len);
  return true;
 }

 if (Model1_dev->Model1_header_ops && Model1_dev->Model1_header_ops->Model1_validate)
  return Model1_dev->Model1_header_ops->Model1_validate(Model1_ll_header, Model1_len);

 return false;
}

typedef int Model1_gifconf_func_t(struct Model1_net_device * Model1_dev, char * Model1_bufptr, int Model1_len);
int Model1_register_gifconf(unsigned int Model1_family, Model1_gifconf_func_t *Model1_gifconf);
static inline __attribute__((no_instrument_function)) int Model1_unregister_gifconf(unsigned int Model1_family)
{
 return Model1_register_gifconf(Model1_family, ((void *)0));
}



struct Model1_sd_flow_limit {
 Model1_u64 Model1_count;
 unsigned int Model1_num_buckets;
 unsigned int Model1_history_head;
 Model1_u16 Model1_history[(1 << 7)];
 Model1_u8 Model1_buckets[];
};

extern int Model1_netdev_flow_limit_table_len;


/*
 * Incoming packets are placed on per-CPU queues
 */
struct Model1_softnet_data {
 struct Model1_list_head Model1_poll_list;
 struct Model1_sk_buff_head Model1_process_queue;

 /* stats */
 unsigned int Model1_processed;
 unsigned int Model1_time_squeeze;
 unsigned int Model1_received_rps;

 struct Model1_softnet_data *Model1_rps_ipi_list;


 struct Model1_sd_flow_limit *Model1_flow_limit;

 struct Model1_Qdisc *Model1_output_queue;
 struct Model1_Qdisc **Model1_output_queue_tailp;
 struct Model1_sk_buff *Model1_completion_queue;


 /* input_queue_head should be written by cpu owning this struct,
	 * and only read by other cpus. Worth using a cache line.
	 */
 unsigned int Model1_input_queue_head __attribute__((__aligned__((1 << (6)))));

 /* Elements below can be accessed between CPUs for RPS/RFS */
 struct Model1_call_single_data Model1_csd __attribute__((__aligned__((1 << (6)))));
 struct Model1_softnet_data *Model1_rps_ipi_next;
 unsigned int Model1_cpu;
 unsigned int Model1_input_queue_tail;

 unsigned int Model1_dropped;
 struct Model1_sk_buff_head Model1_input_pkt_queue;
 struct Model1_napi_struct Model1_backlog;

};

static inline __attribute__((no_instrument_function)) void Model1_input_queue_head_incr(struct Model1_softnet_data *Model1_sd)
{

 Model1_sd->Model1_input_queue_head++;

}

static inline __attribute__((no_instrument_function)) void Model1_input_queue_tail_incr_save(struct Model1_softnet_data *Model1_sd,
           unsigned int *Model1_qtail)
{

 *Model1_qtail = ++Model1_sd->Model1_input_queue_tail;

}

extern __attribute__((section(".data..percpu" "..shared_aligned"))) __typeof__(struct Model1_softnet_data) Model1_softnet_data __attribute__((__aligned__((1 << (6)))));

void Model1___netif_schedule(struct Model1_Qdisc *Model1_q);
void Model1_netif_schedule_queue(struct Model1_netdev_queue *Model1_txq);

static inline __attribute__((no_instrument_function)) void Model1_netif_tx_schedule_all(struct Model1_net_device *Model1_dev)
{
 unsigned int Model1_i;

 for (Model1_i = 0; Model1_i < Model1_dev->Model1_num_tx_queues; Model1_i++)
  Model1_netif_schedule_queue(Model1_netdev_get_tx_queue(Model1_dev, Model1_i));
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_netif_tx_start_queue(struct Model1_netdev_queue *Model1_dev_queue)
{
 Model1_clear_bit(Model1___QUEUE_STATE_DRV_XOFF, &Model1_dev_queue->Model1_state);
}

/**
 *	netif_start_queue - allow transmit
 *	@dev: network device
 *
 *	Allow upper layers to call the device hard_start_xmit routine.
 */
static inline __attribute__((no_instrument_function)) void Model1_netif_start_queue(struct Model1_net_device *Model1_dev)
{
 Model1_netif_tx_start_queue(Model1_netdev_get_tx_queue(Model1_dev, 0));
}

static inline __attribute__((no_instrument_function)) void Model1_netif_tx_start_all_queues(struct Model1_net_device *Model1_dev)
{
 unsigned int Model1_i;

 for (Model1_i = 0; Model1_i < Model1_dev->Model1_num_tx_queues; Model1_i++) {
  struct Model1_netdev_queue *Model1_txq = Model1_netdev_get_tx_queue(Model1_dev, Model1_i);
  Model1_netif_tx_start_queue(Model1_txq);
 }
}

void Model1_netif_tx_wake_queue(struct Model1_netdev_queue *Model1_dev_queue);

/**
 *	netif_wake_queue - restart transmit
 *	@dev: network device
 *
 *	Allow upper layers to call the device hard_start_xmit routine.
 *	Used for flow control when transmit resources are available.
 */
static inline __attribute__((no_instrument_function)) void Model1_netif_wake_queue(struct Model1_net_device *Model1_dev)
{
 Model1_netif_tx_wake_queue(Model1_netdev_get_tx_queue(Model1_dev, 0));
}

static inline __attribute__((no_instrument_function)) void Model1_netif_tx_wake_all_queues(struct Model1_net_device *Model1_dev)
{
 unsigned int Model1_i;

 for (Model1_i = 0; Model1_i < Model1_dev->Model1_num_tx_queues; Model1_i++) {
  struct Model1_netdev_queue *Model1_txq = Model1_netdev_get_tx_queue(Model1_dev, Model1_i);
  Model1_netif_tx_wake_queue(Model1_txq);
 }
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_netif_tx_stop_queue(struct Model1_netdev_queue *Model1_dev_queue)
{
 Model1_set_bit(Model1___QUEUE_STATE_DRV_XOFF, &Model1_dev_queue->Model1_state);
}

/**
 *	netif_stop_queue - stop transmitted packets
 *	@dev: network device
 *
 *	Stop upper layers calling the device hard_start_xmit routine.
 *	Used for flow control when transmit resources are unavailable.
 */
static inline __attribute__((no_instrument_function)) void Model1_netif_stop_queue(struct Model1_net_device *Model1_dev)
{
 Model1_netif_tx_stop_queue(Model1_netdev_get_tx_queue(Model1_dev, 0));
}

void Model1_netif_tx_stop_all_queues(struct Model1_net_device *Model1_dev);

static inline __attribute__((no_instrument_function)) bool Model1_netif_tx_queue_stopped(const struct Model1_netdev_queue *Model1_dev_queue)
{
 return (__builtin_constant_p((Model1___QUEUE_STATE_DRV_XOFF)) ? Model1_constant_test_bit((Model1___QUEUE_STATE_DRV_XOFF), (&Model1_dev_queue->Model1_state)) : Model1_variable_test_bit((Model1___QUEUE_STATE_DRV_XOFF), (&Model1_dev_queue->Model1_state)));
}

/**
 *	netif_queue_stopped - test if transmit queue is flowblocked
 *	@dev: network device
 *
 *	Test if transmit queue on device is currently unable to send.
 */
static inline __attribute__((no_instrument_function)) bool Model1_netif_queue_stopped(const struct Model1_net_device *Model1_dev)
{
 return Model1_netif_tx_queue_stopped(Model1_netdev_get_tx_queue(Model1_dev, 0));
}

static inline __attribute__((no_instrument_function)) bool Model1_netif_xmit_stopped(const struct Model1_netdev_queue *Model1_dev_queue)
{
 return Model1_dev_queue->Model1_state & ((1 << Model1___QUEUE_STATE_DRV_XOFF) | (1 << Model1___QUEUE_STATE_STACK_XOFF));
}

static inline __attribute__((no_instrument_function)) bool
Model1_netif_xmit_frozen_or_stopped(const struct Model1_netdev_queue *Model1_dev_queue)
{
 return Model1_dev_queue->Model1_state & (((1 << Model1___QUEUE_STATE_DRV_XOFF) | (1 << Model1___QUEUE_STATE_STACK_XOFF)) | (1 << Model1___QUEUE_STATE_FROZEN));
}

static inline __attribute__((no_instrument_function)) bool
Model1_netif_xmit_frozen_or_drv_stopped(const struct Model1_netdev_queue *Model1_dev_queue)
{
 return Model1_dev_queue->Model1_state & ((1 << Model1___QUEUE_STATE_DRV_XOFF) | (1 << Model1___QUEUE_STATE_FROZEN));
}

/**
 *	netdev_txq_bql_enqueue_prefetchw - prefetch bql data for write
 *	@dev_queue: pointer to transmit queue
 *
 * BQL enabled drivers might use this helper in their ndo_start_xmit(),
 * to give appropriate hint to the CPU.
 */
static inline __attribute__((no_instrument_function)) void Model1_netdev_txq_bql_enqueue_prefetchw(struct Model1_netdev_queue *Model1_dev_queue)
{

 Model1_prefetchw(&Model1_dev_queue->Model1_dql.Model1_num_queued);

}

/**
 *	netdev_txq_bql_complete_prefetchw - prefetch bql data for write
 *	@dev_queue: pointer to transmit queue
 *
 * BQL enabled drivers might use this helper in their TX completion path,
 * to give appropriate hint to the CPU.
 */
static inline __attribute__((no_instrument_function)) void Model1_netdev_txq_bql_complete_prefetchw(struct Model1_netdev_queue *Model1_dev_queue)
{

 Model1_prefetchw(&Model1_dev_queue->Model1_dql.Model1_limit);

}

static inline __attribute__((no_instrument_function)) void Model1_netdev_tx_sent_queue(struct Model1_netdev_queue *Model1_dev_queue,
     unsigned int Model1_bytes)
{

 Model1_dql_queued(&Model1_dev_queue->Model1_dql, Model1_bytes);

 if (__builtin_expect(!!(Model1_dql_avail(&Model1_dev_queue->Model1_dql) >= 0), 1))
  return;

 Model1_set_bit(Model1___QUEUE_STATE_STACK_XOFF, &Model1_dev_queue->Model1_state);

 /*
	 * The XOFF flag must be set before checking the dql_avail below,
	 * because in netdev_tx_completed_queue we update the dql_completed
	 * before checking the XOFF flag.
	 */
 asm volatile("mfence":::"memory");

 /* check again in case another CPU has just made room avail */
 if (__builtin_expect(!!(Model1_dql_avail(&Model1_dev_queue->Model1_dql) >= 0), 0))
  Model1_clear_bit(Model1___QUEUE_STATE_STACK_XOFF, &Model1_dev_queue->Model1_state);

}

/**
 * 	netdev_sent_queue - report the number of bytes queued to hardware
 * 	@dev: network device
 * 	@bytes: number of bytes queued to the hardware device queue
 *
 * 	Report the number of bytes queued for sending/completion to the network
 * 	device hardware queue. @bytes should be a good approximation and should
 * 	exactly match netdev_completed_queue() @bytes
 */
static inline __attribute__((no_instrument_function)) void Model1_netdev_sent_queue(struct Model1_net_device *Model1_dev, unsigned int Model1_bytes)
{
 Model1_netdev_tx_sent_queue(Model1_netdev_get_tx_queue(Model1_dev, 0), Model1_bytes);
}

static inline __attribute__((no_instrument_function)) void Model1_netdev_tx_completed_queue(struct Model1_netdev_queue *Model1_dev_queue,
          unsigned int Model1_pkts, unsigned int Model1_bytes)
{

 if (__builtin_expect(!!(!Model1_bytes), 0))
  return;

 Model1_dql_completed(&Model1_dev_queue->Model1_dql, Model1_bytes);

 /*
	 * Without the memory barrier there is a small possiblity that
	 * netdev_tx_sent_queue will miss the update and cause the queue to
	 * be stopped forever
	 */
 asm volatile("mfence":::"memory");

 if (Model1_dql_avail(&Model1_dev_queue->Model1_dql) < 0)
  return;

 if (Model1_test_and_clear_bit(Model1___QUEUE_STATE_STACK_XOFF, &Model1_dev_queue->Model1_state))
  Model1_netif_schedule_queue(Model1_dev_queue);

}

/**
 * 	netdev_completed_queue - report bytes and packets completed by device
 * 	@dev: network device
 * 	@pkts: actual number of packets sent over the medium
 * 	@bytes: actual number of bytes sent over the medium
 *
 * 	Report the number of bytes and packets transmitted by the network device
 * 	hardware queue over the physical medium, @bytes must exactly match the
 * 	@bytes amount passed to netdev_sent_queue()
 */
static inline __attribute__((no_instrument_function)) void Model1_netdev_completed_queue(struct Model1_net_device *Model1_dev,
       unsigned int Model1_pkts, unsigned int Model1_bytes)
{
 Model1_netdev_tx_completed_queue(Model1_netdev_get_tx_queue(Model1_dev, 0), Model1_pkts, Model1_bytes);
}

static inline __attribute__((no_instrument_function)) void Model1_netdev_tx_reset_queue(struct Model1_netdev_queue *Model1_q)
{

 Model1_clear_bit(Model1___QUEUE_STATE_STACK_XOFF, &Model1_q->Model1_state);
 Model1_dql_reset(&Model1_q->Model1_dql);

}

/**
 * 	netdev_reset_queue - reset the packets and bytes count of a network device
 * 	@dev_queue: network device
 *
 * 	Reset the bytes and packet count of a network device and clear the
 * 	software flow control OFF bit for this network device
 */
static inline __attribute__((no_instrument_function)) void Model1_netdev_reset_queue(struct Model1_net_device *Model1_dev_queue)
{
 Model1_netdev_tx_reset_queue(Model1_netdev_get_tx_queue(Model1_dev_queue, 0));
}

/**
 * 	netdev_cap_txqueue - check if selected tx queue exceeds device queues
 * 	@dev: network device
 * 	@queue_index: given tx queue index
 *
 * 	Returns 0 if given tx queue index >= number of device tx queues,
 * 	otherwise returns the originally passed tx queue index.
 */
static inline __attribute__((no_instrument_function)) Model1_u16 Model1_netdev_cap_txqueue(struct Model1_net_device *Model1_dev, Model1_u16 Model1_queue_index)
{
 if (__builtin_expect(!!(Model1_queue_index >= Model1_dev->Model1_real_num_tx_queues), 0)) {
  do { if (Model1_net_ratelimit()) Model1_printk("\001" "4" "TCP: " "%s selects TX queue %d, but real number of TX queues is %d\n", Model1_dev->Model1_name, Model1_queue_index, Model1_dev->Model1_real_num_tx_queues); } while (0);


  return 0;
 }

 return Model1_queue_index;
}

/**
 *	netif_running - test if up
 *	@dev: network device
 *
 *	Test if the device has been brought up.
 */
static inline __attribute__((no_instrument_function)) bool Model1_netif_running(const struct Model1_net_device *Model1_dev)
{
 return (__builtin_constant_p((Model1___LINK_STATE_START)) ? Model1_constant_test_bit((Model1___LINK_STATE_START), (&Model1_dev->Model1_state)) : Model1_variable_test_bit((Model1___LINK_STATE_START), (&Model1_dev->Model1_state)));
}

/*
 * Routines to manage the subqueues on a device.  We only need start,
 * stop, and a check if it's stopped.  All other device management is
 * done at the overall netdevice level.
 * Also test the device if we're multiqueue.
 */

/**
 *	netif_start_subqueue - allow sending packets on subqueue
 *	@dev: network device
 *	@queue_index: sub queue index
 *
 * Start individual transmit queue of a device with multiple transmit queues.
 */
static inline __attribute__((no_instrument_function)) void Model1_netif_start_subqueue(struct Model1_net_device *Model1_dev, Model1_u16 Model1_queue_index)
{
 struct Model1_netdev_queue *Model1_txq = Model1_netdev_get_tx_queue(Model1_dev, Model1_queue_index);

 Model1_netif_tx_start_queue(Model1_txq);
}

/**
 *	netif_stop_subqueue - stop sending packets on subqueue
 *	@dev: network device
 *	@queue_index: sub queue index
 *
 * Stop individual transmit queue of a device with multiple transmit queues.
 */
static inline __attribute__((no_instrument_function)) void Model1_netif_stop_subqueue(struct Model1_net_device *Model1_dev, Model1_u16 Model1_queue_index)
{
 struct Model1_netdev_queue *Model1_txq = Model1_netdev_get_tx_queue(Model1_dev, Model1_queue_index);
 Model1_netif_tx_stop_queue(Model1_txq);
}

/**
 *	netif_subqueue_stopped - test status of subqueue
 *	@dev: network device
 *	@queue_index: sub queue index
 *
 * Check individual transmit queue of a device with multiple transmit queues.
 */
static inline __attribute__((no_instrument_function)) bool Model1___netif_subqueue_stopped(const struct Model1_net_device *Model1_dev,
         Model1_u16 Model1_queue_index)
{
 struct Model1_netdev_queue *Model1_txq = Model1_netdev_get_tx_queue(Model1_dev, Model1_queue_index);

 return Model1_netif_tx_queue_stopped(Model1_txq);
}

static inline __attribute__((no_instrument_function)) bool Model1_netif_subqueue_stopped(const struct Model1_net_device *Model1_dev,
       struct Model1_sk_buff *Model1_skb)
{
 return Model1___netif_subqueue_stopped(Model1_dev, Model1_skb_get_queue_mapping(Model1_skb));
}

void Model1_netif_wake_subqueue(struct Model1_net_device *Model1_dev, Model1_u16 Model1_queue_index);


int Model1_netif_set_xps_queue(struct Model1_net_device *Model1_dev, const struct Model1_cpumask *Model1_mask,
   Model1_u16 Model1_index);
Model1_u16 Model1___skb_tx_hash(const struct Model1_net_device *Model1_dev, struct Model1_sk_buff *Model1_skb,
    unsigned int Model1_num_tx_queues);

/*
 * Returns a Tx hash for the given packet when dev->real_num_tx_queues is used
 * as a distribution range limit for the returned value.
 */
static inline __attribute__((no_instrument_function)) Model1_u16 Model1_skb_tx_hash(const struct Model1_net_device *Model1_dev,
         struct Model1_sk_buff *Model1_skb)
{
 return Model1___skb_tx_hash(Model1_dev, Model1_skb, Model1_dev->Model1_real_num_tx_queues);
}

/**
 *	netif_is_multiqueue - test if device has multiple transmit queues
 *	@dev: network device
 *
 * Check if device has multiple transmit queues
 */
static inline __attribute__((no_instrument_function)) bool Model1_netif_is_multiqueue(const struct Model1_net_device *Model1_dev)
{
 return Model1_dev->Model1_num_tx_queues > 1;
}

int Model1_netif_set_real_num_tx_queues(struct Model1_net_device *Model1_dev, unsigned int Model1_txq);


int Model1_netif_set_real_num_rx_queues(struct Model1_net_device *Model1_dev, unsigned int Model1_rxq);
static inline __attribute__((no_instrument_function)) unsigned int Model1_get_netdev_rx_queue_index(
  struct Model1_netdev_rx_queue *Model1_queue)
{
 struct Model1_net_device *Model1_dev = Model1_queue->Model1_dev;
 int Model1_index = Model1_queue - Model1_dev->Model1__rx;

 do { if (__builtin_expect(!!(Model1_index >= Model1_dev->Model1_num_rx_queues), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/netdevice.h"), "i" (3199), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);
 return Model1_index;
}



int Model1_netif_get_num_default_rss_queues(void);

enum Model1_skb_free_reason {
 Model1_SKB_REASON_CONSUMED,
 Model1_SKB_REASON_DROPPED,
};

void Model1___dev_kfree_skb_irq(struct Model1_sk_buff *Model1_skb, enum Model1_skb_free_reason Model1_reason);
void Model1___dev_kfree_skb_any(struct Model1_sk_buff *Model1_skb, enum Model1_skb_free_reason Model1_reason);

/*
 * It is not allowed to call kfree_skb() or consume_skb() from hardware
 * interrupt context or with hardware interrupts being disabled.
 * (in_irq() || irqs_disabled())
 *
 * We provide four helpers that can be used in following contexts :
 *
 * dev_kfree_skb_irq(skb) when caller drops a packet from irq context,
 *  replacing kfree_skb(skb)
 *
 * dev_consume_skb_irq(skb) when caller consumes a packet from irq context.
 *  Typically used in place of consume_skb(skb) in TX completion path
 *
 * dev_kfree_skb_any(skb) when caller doesn't know its current irq context,
 *  replacing kfree_skb(skb)
 *
 * dev_consume_skb_any(skb) when caller doesn't know its current irq context,
 *  and consumed a packet. Used in place of consume_skb(skb)
 */
static inline __attribute__((no_instrument_function)) void Model1_dev_kfree_skb_irq(struct Model1_sk_buff *Model1_skb)
{
 Model1___dev_kfree_skb_irq(Model1_skb, Model1_SKB_REASON_DROPPED);
}

static inline __attribute__((no_instrument_function)) void Model1_dev_consume_skb_irq(struct Model1_sk_buff *Model1_skb)
{
 Model1___dev_kfree_skb_irq(Model1_skb, Model1_SKB_REASON_CONSUMED);
}

static inline __attribute__((no_instrument_function)) void Model1_dev_kfree_skb_any(struct Model1_sk_buff *Model1_skb)
{
 Model1___dev_kfree_skb_any(Model1_skb, Model1_SKB_REASON_DROPPED);
}

static inline __attribute__((no_instrument_function)) void Model1_dev_consume_skb_any(struct Model1_sk_buff *Model1_skb)
{
 Model1___dev_kfree_skb_any(Model1_skb, Model1_SKB_REASON_CONSUMED);
}

int Model1_netif_rx(struct Model1_sk_buff *Model1_skb);
int Model1_netif_rx_ni(struct Model1_sk_buff *Model1_skb);
int Model1_netif_receive_skb(struct Model1_sk_buff *Model1_skb);
Model1_gro_result_t Model1_napi_gro_receive(struct Model1_napi_struct *Model1_napi, struct Model1_sk_buff *Model1_skb);
void Model1_napi_gro_flush(struct Model1_napi_struct *Model1_napi, bool Model1_flush_old);
struct Model1_sk_buff *Model1_napi_get_frags(struct Model1_napi_struct *Model1_napi);
Model1_gro_result_t Model1_napi_gro_frags(struct Model1_napi_struct *Model1_napi);
struct Model1_packet_offload *Model1_gro_find_receive_by_type(Model1___be16 Model1_type);
struct Model1_packet_offload *Model1_gro_find_complete_by_type(Model1___be16 Model1_type);

static inline __attribute__((no_instrument_function)) void Model1_napi_free_frags(struct Model1_napi_struct *Model1_napi)
{
 Model1_kfree_skb(Model1_napi->Model1_skb);
 Model1_napi->Model1_skb = ((void *)0);
}

bool Model1_netdev_is_rx_handler_busy(struct Model1_net_device *Model1_dev);
int Model1_netdev_rx_handler_register(struct Model1_net_device *Model1_dev,
          Model1_rx_handler_func_t *Model1_rx_handler,
          void *Model1_rx_handler_data);
void Model1_netdev_rx_handler_unregister(struct Model1_net_device *Model1_dev);

bool Model1_dev_valid_name(const char *Model1_name);
int Model1_dev_ioctl(struct Model1_net *Model1_net, unsigned int Model1_cmd, void *);
int Model1_dev_ethtool(struct Model1_net *Model1_net, struct Model1_ifreq *);
unsigned int Model1_dev_get_flags(const struct Model1_net_device *);
int Model1___dev_change_flags(struct Model1_net_device *, unsigned int Model1_flags);
int Model1_dev_change_flags(struct Model1_net_device *, unsigned int);
void Model1___dev_notify_flags(struct Model1_net_device *, unsigned int Model1_old_flags,
   unsigned int Model1_gchanges);
int Model1_dev_change_name(struct Model1_net_device *, const char *);
int Model1_dev_set_alias(struct Model1_net_device *, const char *, Model1_size_t);
int Model1_dev_change_net_namespace(struct Model1_net_device *, struct Model1_net *, const char *);
int Model1_dev_set_mtu(struct Model1_net_device *, int);
void Model1_dev_set_group(struct Model1_net_device *, int);
int Model1_dev_set_mac_address(struct Model1_net_device *, struct Model1_sockaddr *);
int Model1_dev_change_carrier(struct Model1_net_device *, bool Model1_new_carrier);
int Model1_dev_get_phys_port_id(struct Model1_net_device *Model1_dev,
    struct Model1_netdev_phys_item_id *Model1_ppid);
int Model1_dev_get_phys_port_name(struct Model1_net_device *Model1_dev,
      char *Model1_name, Model1_size_t Model1_len);
int Model1_dev_change_proto_down(struct Model1_net_device *Model1_dev, bool Model1_proto_down);
int Model1_dev_change_xdp_fd(struct Model1_net_device *Model1_dev, int Model1_fd);
struct Model1_sk_buff *Model1_validate_xmit_skb_list(struct Model1_sk_buff *Model1_skb, struct Model1_net_device *Model1_dev);
struct Model1_sk_buff *Model1_dev_hard_start_xmit(struct Model1_sk_buff *Model1_skb, struct Model1_net_device *Model1_dev,
        struct Model1_netdev_queue *Model1_txq, int *Model1_ret);
int Model1___dev_forward_skb(struct Model1_net_device *Model1_dev, struct Model1_sk_buff *Model1_skb);
int Model1_dev_forward_skb(struct Model1_net_device *Model1_dev, struct Model1_sk_buff *Model1_skb);
bool Model1_is_skb_forwardable(const struct Model1_net_device *Model1_dev,
   const struct Model1_sk_buff *Model1_skb);

void Model1_dev_queue_xmit_nit(struct Model1_sk_buff *Model1_skb, struct Model1_net_device *Model1_dev);

extern int Model1_netdev_budget;

/* Called by rtnetlink.c:rtnl_unlock() */
void Model1_netdev_run_todo(void);

/**
 *	dev_put - release reference to device
 *	@dev: network device
 *
 * Release reference to device to allow it to be freed.
 */
static inline __attribute__((no_instrument_function)) void Model1_dev_put(struct Model1_net_device *Model1_dev)
{
 do { do { const void *Model1___vpp_verify = (typeof((&(*Model1_dev->Model1_pcpu_refcnt)) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); switch(sizeof(*Model1_dev->Model1_pcpu_refcnt)) { case 1: do { typedef typeof((*Model1_dev->Model1_pcpu_refcnt)) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)) && ((-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)) == 1 || (-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)) == -1)) ? (int)(-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)); (void)Model1_pao_tmp__; } switch (sizeof((*Model1_dev->Model1_pcpu_refcnt))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "qi" ((Model1_pao_T__)(-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "ri" ((Model1_pao_T__)(-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "ri" ((Model1_pao_T__)(-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "re" ((Model1_pao_T__)(-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 2: do { typedef typeof((*Model1_dev->Model1_pcpu_refcnt)) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)) && ((-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)) == 1 || (-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)) == -1)) ? (int)(-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)); (void)Model1_pao_tmp__; } switch (sizeof((*Model1_dev->Model1_pcpu_refcnt))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "qi" ((Model1_pao_T__)(-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "ri" ((Model1_pao_T__)(-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "ri" ((Model1_pao_T__)(-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "re" ((Model1_pao_T__)(-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 4: do { typedef typeof((*Model1_dev->Model1_pcpu_refcnt)) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)) && ((-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)) == 1 || (-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)) == -1)) ? (int)(-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)); (void)Model1_pao_tmp__; } switch (sizeof((*Model1_dev->Model1_pcpu_refcnt))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "qi" ((Model1_pao_T__)(-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "ri" ((Model1_pao_T__)(-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "ri" ((Model1_pao_T__)(-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "re" ((Model1_pao_T__)(-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 8: do { typedef typeof((*Model1_dev->Model1_pcpu_refcnt)) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)) && ((-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)) == 1 || (-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)) == -1)) ? (int)(-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)); (void)Model1_pao_tmp__; } switch (sizeof((*Model1_dev->Model1_pcpu_refcnt))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "qi" ((Model1_pao_T__)(-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "ri" ((Model1_pao_T__)(-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "ri" ((Model1_pao_T__)(-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "re" ((Model1_pao_T__)(-(typeof(*Model1_dev->Model1_pcpu_refcnt))(1)))); break; default: Model1___bad_percpu_size(); } } while (0);break; default: Model1___bad_size_call_parameter();break; } } while (0);
}

/**
 *	dev_hold - get reference to device
 *	@dev: network device
 *
 * Hold reference to device to keep it from being freed.
 */
static inline __attribute__((no_instrument_function)) void Model1_dev_hold(struct Model1_net_device *Model1_dev)
{
 do { do { const void *Model1___vpp_verify = (typeof((&(*Model1_dev->Model1_pcpu_refcnt)) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); switch(sizeof(*Model1_dev->Model1_pcpu_refcnt)) { case 1: do { typedef typeof((*Model1_dev->Model1_pcpu_refcnt)) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (1); (void)Model1_pao_tmp__; } switch (sizeof((*Model1_dev->Model1_pcpu_refcnt))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "qi" ((Model1_pao_T__)(1))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "ri" ((Model1_pao_T__)(1))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "ri" ((Model1_pao_T__)(1))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "re" ((Model1_pao_T__)(1))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 2: do { typedef typeof((*Model1_dev->Model1_pcpu_refcnt)) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (1); (void)Model1_pao_tmp__; } switch (sizeof((*Model1_dev->Model1_pcpu_refcnt))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "qi" ((Model1_pao_T__)(1))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "ri" ((Model1_pao_T__)(1))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "ri" ((Model1_pao_T__)(1))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "re" ((Model1_pao_T__)(1))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 4: do { typedef typeof((*Model1_dev->Model1_pcpu_refcnt)) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (1); (void)Model1_pao_tmp__; } switch (sizeof((*Model1_dev->Model1_pcpu_refcnt))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "qi" ((Model1_pao_T__)(1))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "ri" ((Model1_pao_T__)(1))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "ri" ((Model1_pao_T__)(1))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "re" ((Model1_pao_T__)(1))); break; default: Model1___bad_percpu_size(); } } while (0);break; case 8: do { typedef typeof((*Model1_dev->Model1_pcpu_refcnt)) Model1_pao_T__; const int Model1_pao_ID__ = (__builtin_constant_p(1) && ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { Model1_pao_T__ Model1_pao_tmp__; Model1_pao_tmp__ = (1); (void)Model1_pao_tmp__; } switch (sizeof((*Model1_dev->Model1_pcpu_refcnt))) { case 1: if (Model1_pao_ID__ == 1) asm("incb ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decb ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addb %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "qi" ((Model1_pao_T__)(1))); break; case 2: if (Model1_pao_ID__ == 1) asm("incw ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decw ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addw %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "ri" ((Model1_pao_T__)(1))); break; case 4: if (Model1_pao_ID__ == 1) asm("incl ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decl ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addl %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "ri" ((Model1_pao_T__)(1))); break; case 8: if (Model1_pao_ID__ == 1) asm("incq ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else if (Model1_pao_ID__ == -1) asm("decq ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt))); else asm("addq %1, ""%%""gs"":" "%" "0" : "+m" ((*Model1_dev->Model1_pcpu_refcnt)) : "re" ((Model1_pao_T__)(1))); break; default: Model1___bad_percpu_size(); } } while (0);break; default: Model1___bad_size_call_parameter();break; } } while (0);
}

/* Carrier loss detection, dial on demand. The functions netif_carrier_on
 * and _off may be called from IRQ context, but it is caller
 * who is responsible for serialization of these calls.
 *
 * The name carrier is inappropriate, these functions should really be
 * called netif_lowerlayer_*() because they represent the state of any
 * kind of lower layer not just hardware media.
 */

void Model1_linkwatch_init_dev(struct Model1_net_device *Model1_dev);
void Model1_linkwatch_fire_event(struct Model1_net_device *Model1_dev);
void Model1_linkwatch_forget_dev(struct Model1_net_device *Model1_dev);

/**
 *	netif_carrier_ok - test if carrier present
 *	@dev: network device
 *
 * Check if carrier is present on device
 */
static inline __attribute__((no_instrument_function)) bool Model1_netif_carrier_ok(const struct Model1_net_device *Model1_dev)
{
 return !(__builtin_constant_p((Model1___LINK_STATE_NOCARRIER)) ? Model1_constant_test_bit((Model1___LINK_STATE_NOCARRIER), (&Model1_dev->Model1_state)) : Model1_variable_test_bit((Model1___LINK_STATE_NOCARRIER), (&Model1_dev->Model1_state)));
}

unsigned long Model1_dev_trans_start(struct Model1_net_device *Model1_dev);

void Model1___netdev_watchdog_up(struct Model1_net_device *Model1_dev);

void Model1_netif_carrier_on(struct Model1_net_device *Model1_dev);

void Model1_netif_carrier_off(struct Model1_net_device *Model1_dev);

/**
 *	netif_dormant_on - mark device as dormant.
 *	@dev: network device
 *
 * Mark device as dormant (as per RFC2863).
 *
 * The dormant state indicates that the relevant interface is not
 * actually in a condition to pass packets (i.e., it is not 'up') but is
 * in a "pending" state, waiting for some external event.  For "on-
 * demand" interfaces, this new state identifies the situation where the
 * interface is waiting for events to place it in the up state.
 */
static inline __attribute__((no_instrument_function)) void Model1_netif_dormant_on(struct Model1_net_device *Model1_dev)
{
 if (!Model1_test_and_set_bit(Model1___LINK_STATE_DORMANT, &Model1_dev->Model1_state))
  Model1_linkwatch_fire_event(Model1_dev);
}

/**
 *	netif_dormant_off - set device as not dormant.
 *	@dev: network device
 *
 * Device is not in dormant state.
 */
static inline __attribute__((no_instrument_function)) void Model1_netif_dormant_off(struct Model1_net_device *Model1_dev)
{
 if (Model1_test_and_clear_bit(Model1___LINK_STATE_DORMANT, &Model1_dev->Model1_state))
  Model1_linkwatch_fire_event(Model1_dev);
}

/**
 *	netif_dormant - test if carrier present
 *	@dev: network device
 *
 * Check if carrier is present on device
 */
static inline __attribute__((no_instrument_function)) bool Model1_netif_dormant(const struct Model1_net_device *Model1_dev)
{
 return (__builtin_constant_p((Model1___LINK_STATE_DORMANT)) ? Model1_constant_test_bit((Model1___LINK_STATE_DORMANT), (&Model1_dev->Model1_state)) : Model1_variable_test_bit((Model1___LINK_STATE_DORMANT), (&Model1_dev->Model1_state)));
}


/**
 *	netif_oper_up - test if device is operational
 *	@dev: network device
 *
 * Check if carrier is operational
 */
static inline __attribute__((no_instrument_function)) bool Model1_netif_oper_up(const struct Model1_net_device *Model1_dev)
{
 return (Model1_dev->Model1_operstate == Model1_IF_OPER_UP ||
  Model1_dev->Model1_operstate == Model1_IF_OPER_UNKNOWN /* backward compat */);
}

/**
 *	netif_device_present - is device available or removed
 *	@dev: network device
 *
 * Check if device has not been removed from system.
 */
static inline __attribute__((no_instrument_function)) bool Model1_netif_device_present(struct Model1_net_device *Model1_dev)
{
 return (__builtin_constant_p((Model1___LINK_STATE_PRESENT)) ? Model1_constant_test_bit((Model1___LINK_STATE_PRESENT), (&Model1_dev->Model1_state)) : Model1_variable_test_bit((Model1___LINK_STATE_PRESENT), (&Model1_dev->Model1_state)));
}

void Model1_netif_device_detach(struct Model1_net_device *Model1_dev);

void Model1_netif_device_attach(struct Model1_net_device *Model1_dev);

/*
 * Network interface message level settings
 */

enum {
 Model1_NETIF_MSG_DRV = 0x0001,
 Model1_NETIF_MSG_PROBE = 0x0002,
 Model1_NETIF_MSG_LINK = 0x0004,
 Model1_NETIF_MSG_TIMER = 0x0008,
 Model1_NETIF_MSG_IFDOWN = 0x0010,
 Model1_NETIF_MSG_IFUP = 0x0020,
 Model1_NETIF_MSG_RX_ERR = 0x0040,
 Model1_NETIF_MSG_TX_ERR = 0x0080,
 Model1_NETIF_MSG_TX_QUEUED = 0x0100,
 Model1_NETIF_MSG_INTR = 0x0200,
 Model1_NETIF_MSG_TX_DONE = 0x0400,
 Model1_NETIF_MSG_RX_STATUS = 0x0800,
 Model1_NETIF_MSG_PKTDATA = 0x1000,
 Model1_NETIF_MSG_HW = 0x2000,
 Model1_NETIF_MSG_WOL = 0x4000,
};
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_netif_msg_init(int Model1_debug_value, int Model1_default_msg_enable_bits)
{
 /* use default */
 if (Model1_debug_value < 0 || Model1_debug_value >= (sizeof(Model1_u32) * 8))
  return Model1_default_msg_enable_bits;
 if (Model1_debug_value == 0) /* no output */
  return 0;
 /* set low N bits */
 return (1 << Model1_debug_value) - 1;
}

static inline __attribute__((no_instrument_function)) void Model1___netif_tx_lock(struct Model1_netdev_queue *Model1_txq, int Model1_cpu)
{
 Model1_spin_lock(&Model1_txq->Model1__xmit_lock);
 Model1_txq->Model1_xmit_lock_owner = Model1_cpu;
}

static inline __attribute__((no_instrument_function)) void Model1___netif_tx_lock_bh(struct Model1_netdev_queue *Model1_txq)
{
 Model1_spin_lock_bh(&Model1_txq->Model1__xmit_lock);
 Model1_txq->Model1_xmit_lock_owner = (({ typeof(Model1_cpu_number) Model1_pscr_ret__; do { const void *Model1___vpp_verify = (typeof((&(Model1_cpu_number)) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); switch(sizeof(Model1_cpu_number)) { case 1: Model1_pscr_ret__ = ({ typeof(Model1_cpu_number) Model1_pfo_ret__; switch (sizeof(Model1_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 2: Model1_pscr_ret__ = ({ typeof(Model1_cpu_number) Model1_pfo_ret__; switch (sizeof(Model1_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 4: Model1_pscr_ret__ = ({ typeof(Model1_cpu_number) Model1_pfo_ret__; switch (sizeof(Model1_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 8: Model1_pscr_ret__ = ({ typeof(Model1_cpu_number) Model1_pfo_ret__; switch (sizeof(Model1_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; default: Model1___bad_size_call_parameter(); break; } Model1_pscr_ret__; }));
}

static inline __attribute__((no_instrument_function)) bool Model1___netif_tx_trylock(struct Model1_netdev_queue *Model1_txq)
{
 bool Model1_ok = Model1_spin_trylock(&Model1_txq->Model1__xmit_lock);
 if (__builtin_expect(!!(Model1_ok), 1))
  Model1_txq->Model1_xmit_lock_owner = (({ typeof(Model1_cpu_number) Model1_pscr_ret__; do { const void *Model1___vpp_verify = (typeof((&(Model1_cpu_number)) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); switch(sizeof(Model1_cpu_number)) { case 1: Model1_pscr_ret__ = ({ typeof(Model1_cpu_number) Model1_pfo_ret__; switch (sizeof(Model1_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 2: Model1_pscr_ret__ = ({ typeof(Model1_cpu_number) Model1_pfo_ret__; switch (sizeof(Model1_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 4: Model1_pscr_ret__ = ({ typeof(Model1_cpu_number) Model1_pfo_ret__; switch (sizeof(Model1_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 8: Model1_pscr_ret__ = ({ typeof(Model1_cpu_number) Model1_pfo_ret__; switch (sizeof(Model1_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; default: Model1___bad_size_call_parameter(); break; } Model1_pscr_ret__; }));
 return Model1_ok;
}

static inline __attribute__((no_instrument_function)) void Model1___netif_tx_unlock(struct Model1_netdev_queue *Model1_txq)
{
 Model1_txq->Model1_xmit_lock_owner = -1;
 Model1_spin_unlock(&Model1_txq->Model1__xmit_lock);
}

static inline __attribute__((no_instrument_function)) void Model1___netif_tx_unlock_bh(struct Model1_netdev_queue *Model1_txq)
{
 Model1_txq->Model1_xmit_lock_owner = -1;
 Model1_spin_unlock_bh(&Model1_txq->Model1__xmit_lock);
}

static inline __attribute__((no_instrument_function)) void Model1_txq_trans_update(struct Model1_netdev_queue *Model1_txq)
{
 if (Model1_txq->Model1_xmit_lock_owner != -1)
  Model1_txq->Model1_trans_start = Model1_jiffies;
}

/* legacy drivers only, netdev_start_xmit() sets txq->trans_start */
static inline __attribute__((no_instrument_function)) void Model1_netif_trans_update(struct Model1_net_device *Model1_dev)
{
 struct Model1_netdev_queue *Model1_txq = Model1_netdev_get_tx_queue(Model1_dev, 0);

 if (Model1_txq->Model1_trans_start != Model1_jiffies)
  Model1_txq->Model1_trans_start = Model1_jiffies;
}

/**
 *	netif_tx_lock - grab network device transmit lock
 *	@dev: network device
 *
 * Get network device transmit lock
 */
static inline __attribute__((no_instrument_function)) void Model1_netif_tx_lock(struct Model1_net_device *Model1_dev)
{
 unsigned int Model1_i;
 int Model1_cpu;

 Model1_spin_lock(&Model1_dev->Model1_tx_global_lock);
 Model1_cpu = (({ typeof(Model1_cpu_number) Model1_pscr_ret__; do { const void *Model1___vpp_verify = (typeof((&(Model1_cpu_number)) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); switch(sizeof(Model1_cpu_number)) { case 1: Model1_pscr_ret__ = ({ typeof(Model1_cpu_number) Model1_pfo_ret__; switch (sizeof(Model1_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 2: Model1_pscr_ret__ = ({ typeof(Model1_cpu_number) Model1_pfo_ret__; switch (sizeof(Model1_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 4: Model1_pscr_ret__ = ({ typeof(Model1_cpu_number) Model1_pfo_ret__; switch (sizeof(Model1_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 8: Model1_pscr_ret__ = ({ typeof(Model1_cpu_number) Model1_pfo_ret__; switch (sizeof(Model1_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; default: Model1___bad_size_call_parameter(); break; } Model1_pscr_ret__; }));
 for (Model1_i = 0; Model1_i < Model1_dev->Model1_num_tx_queues; Model1_i++) {
  struct Model1_netdev_queue *Model1_txq = Model1_netdev_get_tx_queue(Model1_dev, Model1_i);

  /* We are the only thread of execution doing a
		 * freeze, but we have to grab the _xmit_lock in
		 * order to synchronize with threads which are in
		 * the ->hard_start_xmit() handler and already
		 * checked the frozen bit.
		 */
  Model1___netif_tx_lock(Model1_txq, Model1_cpu);
  Model1_set_bit(Model1___QUEUE_STATE_FROZEN, &Model1_txq->Model1_state);
  Model1___netif_tx_unlock(Model1_txq);
 }
}

static inline __attribute__((no_instrument_function)) void Model1_netif_tx_lock_bh(struct Model1_net_device *Model1_dev)
{
 Model1_local_bh_disable();
 Model1_netif_tx_lock(Model1_dev);
}

static inline __attribute__((no_instrument_function)) void Model1_netif_tx_unlock(struct Model1_net_device *Model1_dev)
{
 unsigned int Model1_i;

 for (Model1_i = 0; Model1_i < Model1_dev->Model1_num_tx_queues; Model1_i++) {
  struct Model1_netdev_queue *Model1_txq = Model1_netdev_get_tx_queue(Model1_dev, Model1_i);

  /* No need to grab the _xmit_lock here.  If the
		 * queue is not stopped for another reason, we
		 * force a schedule.
		 */
  Model1_clear_bit(Model1___QUEUE_STATE_FROZEN, &Model1_txq->Model1_state);
  Model1_netif_schedule_queue(Model1_txq);
 }
 Model1_spin_unlock(&Model1_dev->Model1_tx_global_lock);
}

static inline __attribute__((no_instrument_function)) void Model1_netif_tx_unlock_bh(struct Model1_net_device *Model1_dev)
{
 Model1_netif_tx_unlock(Model1_dev);
 Model1_local_bh_enable();
}
static inline __attribute__((no_instrument_function)) void Model1_netif_tx_disable(struct Model1_net_device *Model1_dev)
{
 unsigned int Model1_i;
 int Model1_cpu;

 Model1_local_bh_disable();
 Model1_cpu = (({ typeof(Model1_cpu_number) Model1_pscr_ret__; do { const void *Model1___vpp_verify = (typeof((&(Model1_cpu_number)) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); switch(sizeof(Model1_cpu_number)) { case 1: Model1_pscr_ret__ = ({ typeof(Model1_cpu_number) Model1_pfo_ret__; switch (sizeof(Model1_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 2: Model1_pscr_ret__ = ({ typeof(Model1_cpu_number) Model1_pfo_ret__; switch (sizeof(Model1_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 4: Model1_pscr_ret__ = ({ typeof(Model1_cpu_number) Model1_pfo_ret__; switch (sizeof(Model1_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 8: Model1_pscr_ret__ = ({ typeof(Model1_cpu_number) Model1_pfo_ret__; switch (sizeof(Model1_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; default: Model1___bad_size_call_parameter(); break; } Model1_pscr_ret__; }));
 for (Model1_i = 0; Model1_i < Model1_dev->Model1_num_tx_queues; Model1_i++) {
  struct Model1_netdev_queue *Model1_txq = Model1_netdev_get_tx_queue(Model1_dev, Model1_i);

  Model1___netif_tx_lock(Model1_txq, Model1_cpu);
  Model1_netif_tx_stop_queue(Model1_txq);
  Model1___netif_tx_unlock(Model1_txq);
 }
 Model1_local_bh_enable();
}

static inline __attribute__((no_instrument_function)) void Model1_netif_addr_lock(struct Model1_net_device *Model1_dev)
{
 Model1_spin_lock(&Model1_dev->Model1_addr_list_lock);
}

static inline __attribute__((no_instrument_function)) void Model1_netif_addr_lock_nested(struct Model1_net_device *Model1_dev)
{
 int Model1_subclass = 1;

 if (Model1_dev->Model1_netdev_ops->Model1_ndo_get_lock_subclass)
  Model1_subclass = Model1_dev->Model1_netdev_ops->Model1_ndo_get_lock_subclass(Model1_dev);

 do { Model1__raw_spin_lock(((void)(Model1_subclass), (Model1_spinlock_check(&Model1_dev->Model1_addr_list_lock)))); } while (0);
}

static inline __attribute__((no_instrument_function)) void Model1_netif_addr_lock_bh(struct Model1_net_device *Model1_dev)
{
 Model1_spin_lock_bh(&Model1_dev->Model1_addr_list_lock);
}

static inline __attribute__((no_instrument_function)) void Model1_netif_addr_unlock(struct Model1_net_device *Model1_dev)
{
 Model1_spin_unlock(&Model1_dev->Model1_addr_list_lock);
}

static inline __attribute__((no_instrument_function)) void Model1_netif_addr_unlock_bh(struct Model1_net_device *Model1_dev)
{
 Model1_spin_unlock_bh(&Model1_dev->Model1_addr_list_lock);
}

/*
 * dev_addrs walker. Should be used only for read access. Call with
 * rcu_read_lock held.
 */



/* These functions live elsewhere (drivers/net/net_init.c, but related) */

void Model1_ether_setup(struct Model1_net_device *Model1_dev);

/* Support for loadable net-drivers */
struct Model1_net_device *Model1_alloc_netdev_mqs(int Model1_sizeof_priv, const char *Model1_name,
        unsigned char Model1_name_assign_type,
        void (*Model1_setup)(struct Model1_net_device *),
        unsigned int Model1_txqs, unsigned int Model1_rxqs);







int Model1_register_netdev(struct Model1_net_device *Model1_dev);
void Model1_unregister_netdev(struct Model1_net_device *Model1_dev);

/* General hardware address lists handling functions */
int Model1___hw_addr_sync(struct Model1_netdev_hw_addr_list *Model1_to_list,
     struct Model1_netdev_hw_addr_list *Model1_from_list, int Model1_addr_len);
void Model1___hw_addr_unsync(struct Model1_netdev_hw_addr_list *Model1_to_list,
        struct Model1_netdev_hw_addr_list *Model1_from_list, int Model1_addr_len);
int Model1___hw_addr_sync_dev(struct Model1_netdev_hw_addr_list *Model1_list,
         struct Model1_net_device *Model1_dev,
         int (*Model1_sync)(struct Model1_net_device *, const unsigned char *),
         int (*Model1_unsync)(struct Model1_net_device *,
         const unsigned char *));
void Model1___hw_addr_unsync_dev(struct Model1_netdev_hw_addr_list *Model1_list,
     struct Model1_net_device *Model1_dev,
     int (*Model1_unsync)(struct Model1_net_device *,
     const unsigned char *));
void Model1___hw_addr_init(struct Model1_netdev_hw_addr_list *Model1_list);

/* Functions used for device addresses handling */
int Model1_dev_addr_add(struct Model1_net_device *Model1_dev, const unsigned char *Model1_addr,
   unsigned char Model1_addr_type);
int Model1_dev_addr_del(struct Model1_net_device *Model1_dev, const unsigned char *Model1_addr,
   unsigned char Model1_addr_type);
void Model1_dev_addr_flush(struct Model1_net_device *Model1_dev);
int Model1_dev_addr_init(struct Model1_net_device *Model1_dev);

/* Functions used for unicast addresses handling */
int Model1_dev_uc_add(struct Model1_net_device *Model1_dev, const unsigned char *Model1_addr);
int Model1_dev_uc_add_excl(struct Model1_net_device *Model1_dev, const unsigned char *Model1_addr);
int Model1_dev_uc_del(struct Model1_net_device *Model1_dev, const unsigned char *Model1_addr);
int Model1_dev_uc_sync(struct Model1_net_device *Model1_to, struct Model1_net_device *Model1_from);
int Model1_dev_uc_sync_multiple(struct Model1_net_device *Model1_to, struct Model1_net_device *Model1_from);
void Model1_dev_uc_unsync(struct Model1_net_device *Model1_to, struct Model1_net_device *Model1_from);
void Model1_dev_uc_flush(struct Model1_net_device *Model1_dev);
void Model1_dev_uc_init(struct Model1_net_device *Model1_dev);

/**
 *  __dev_uc_sync - Synchonize device's unicast list
 *  @dev:  device to sync
 *  @sync: function to call if address should be added
 *  @unsync: function to call if address should be removed
 *
 *  Add newly added addresses to the interface, and release
 *  addresses that have been deleted.
 */
static inline __attribute__((no_instrument_function)) int Model1___dev_uc_sync(struct Model1_net_device *Model1_dev,
    int (*Model1_sync)(struct Model1_net_device *,
         const unsigned char *),
    int (*Model1_unsync)(struct Model1_net_device *,
           const unsigned char *))
{
 return Model1___hw_addr_sync_dev(&Model1_dev->Model1_uc, Model1_dev, Model1_sync, Model1_unsync);
}

/**
 *  __dev_uc_unsync - Remove synchronized addresses from device
 *  @dev:  device to sync
 *  @unsync: function to call if address should be removed
 *
 *  Remove all addresses that were added to the device by dev_uc_sync().
 */
static inline __attribute__((no_instrument_function)) void Model1___dev_uc_unsync(struct Model1_net_device *Model1_dev,
       int (*Model1_unsync)(struct Model1_net_device *,
       const unsigned char *))
{
 Model1___hw_addr_unsync_dev(&Model1_dev->Model1_uc, Model1_dev, Model1_unsync);
}

/* Functions used for multicast addresses handling */
int Model1_dev_mc_add(struct Model1_net_device *Model1_dev, const unsigned char *Model1_addr);
int Model1_dev_mc_add_global(struct Model1_net_device *Model1_dev, const unsigned char *Model1_addr);
int Model1_dev_mc_add_excl(struct Model1_net_device *Model1_dev, const unsigned char *Model1_addr);
int Model1_dev_mc_del(struct Model1_net_device *Model1_dev, const unsigned char *Model1_addr);
int Model1_dev_mc_del_global(struct Model1_net_device *Model1_dev, const unsigned char *Model1_addr);
int Model1_dev_mc_sync(struct Model1_net_device *Model1_to, struct Model1_net_device *Model1_from);
int Model1_dev_mc_sync_multiple(struct Model1_net_device *Model1_to, struct Model1_net_device *Model1_from);
void Model1_dev_mc_unsync(struct Model1_net_device *Model1_to, struct Model1_net_device *Model1_from);
void Model1_dev_mc_flush(struct Model1_net_device *Model1_dev);
void Model1_dev_mc_init(struct Model1_net_device *Model1_dev);

/**
 *  __dev_mc_sync - Synchonize device's multicast list
 *  @dev:  device to sync
 *  @sync: function to call if address should be added
 *  @unsync: function to call if address should be removed
 *
 *  Add newly added addresses to the interface, and release
 *  addresses that have been deleted.
 */
static inline __attribute__((no_instrument_function)) int Model1___dev_mc_sync(struct Model1_net_device *Model1_dev,
    int (*Model1_sync)(struct Model1_net_device *,
         const unsigned char *),
    int (*Model1_unsync)(struct Model1_net_device *,
           const unsigned char *))
{
 return Model1___hw_addr_sync_dev(&Model1_dev->Model1_mc, Model1_dev, Model1_sync, Model1_unsync);
}

/**
 *  __dev_mc_unsync - Remove synchronized addresses from device
 *  @dev:  device to sync
 *  @unsync: function to call if address should be removed
 *
 *  Remove all addresses that were added to the device by dev_mc_sync().
 */
static inline __attribute__((no_instrument_function)) void Model1___dev_mc_unsync(struct Model1_net_device *Model1_dev,
       int (*Model1_unsync)(struct Model1_net_device *,
       const unsigned char *))
{
 Model1___hw_addr_unsync_dev(&Model1_dev->Model1_mc, Model1_dev, Model1_unsync);
}

/* Functions used for secondary unicast and multicast support */
void Model1_dev_set_rx_mode(struct Model1_net_device *Model1_dev);
void Model1___dev_set_rx_mode(struct Model1_net_device *Model1_dev);
int Model1_dev_set_promiscuity(struct Model1_net_device *Model1_dev, int Model1_inc);
int Model1_dev_set_allmulti(struct Model1_net_device *Model1_dev, int Model1_inc);
void Model1_netdev_state_change(struct Model1_net_device *Model1_dev);
void Model1_netdev_notify_peers(struct Model1_net_device *Model1_dev);
void Model1_netdev_features_change(struct Model1_net_device *Model1_dev);
/* Load a device via the kmod */
void Model1_dev_load(struct Model1_net *Model1_net, const char *Model1_name);
struct Model1_rtnl_link_stats64 *Model1_dev_get_stats(struct Model1_net_device *Model1_dev,
     struct Model1_rtnl_link_stats64 *Model1_storage);
void Model1_netdev_stats_to_stats64(struct Model1_rtnl_link_stats64 *Model1_stats64,
        const struct Model1_net_device_stats *Model1_netdev_stats);

extern int Model1_netdev_max_backlog;
extern int Model1_netdev_tstamp_prequeue;
extern int Model1_weight_p;

bool Model1_netdev_has_upper_dev(struct Model1_net_device *Model1_dev, struct Model1_net_device *Model1_upper_dev);
struct Model1_net_device *Model1_netdev_upper_get_next_dev_rcu(struct Model1_net_device *Model1_dev,
           struct Model1_list_head **Model1_iter);
struct Model1_net_device *Model1_netdev_all_upper_get_next_dev_rcu(struct Model1_net_device *Model1_dev,
           struct Model1_list_head **Model1_iter);

/* iterate through upper list, must be called under RCU read lock */






/* iterate through upper list, must be called under RCU read lock */






void *Model1_netdev_lower_get_next_private(struct Model1_net_device *Model1_dev,
        struct Model1_list_head **Model1_iter);
void *Model1_netdev_lower_get_next_private_rcu(struct Model1_net_device *Model1_dev,
     struct Model1_list_head **Model1_iter);
void *Model1_netdev_lower_get_next(struct Model1_net_device *Model1_dev,
    struct Model1_list_head **Model1_iter);







struct Model1_net_device *Model1_netdev_all_lower_get_next(struct Model1_net_device *Model1_dev,
          struct Model1_list_head **Model1_iter);
struct Model1_net_device *Model1_netdev_all_lower_get_next_rcu(struct Model1_net_device *Model1_dev,
       struct Model1_list_head **Model1_iter);
void *Model1_netdev_adjacent_get_private(struct Model1_list_head *Model1_adj_list);
void *Model1_netdev_lower_get_first_private_rcu(struct Model1_net_device *Model1_dev);
struct Model1_net_device *Model1_netdev_master_upper_dev_get(struct Model1_net_device *Model1_dev);
struct Model1_net_device *Model1_netdev_master_upper_dev_get_rcu(struct Model1_net_device *Model1_dev);
int Model1_netdev_upper_dev_link(struct Model1_net_device *Model1_dev, struct Model1_net_device *Model1_upper_dev);
int Model1_netdev_master_upper_dev_link(struct Model1_net_device *Model1_dev,
     struct Model1_net_device *Model1_upper_dev,
     void *Model1_upper_priv, void *Model1_upper_info);
void Model1_netdev_upper_dev_unlink(struct Model1_net_device *Model1_dev,
        struct Model1_net_device *Model1_upper_dev);
void Model1_netdev_adjacent_rename_links(struct Model1_net_device *Model1_dev, char *Model1_oldname);
void *Model1_netdev_lower_dev_get_private(struct Model1_net_device *Model1_dev,
       struct Model1_net_device *Model1_lower_dev);
void Model1_netdev_lower_state_changed(struct Model1_net_device *Model1_lower_dev,
    void *Model1_lower_state_info);
int Model1_netdev_default_l2upper_neigh_construct(struct Model1_net_device *Model1_dev,
        struct Model1_neighbour *Model1_n);
void Model1_netdev_default_l2upper_neigh_destroy(struct Model1_net_device *Model1_dev,
       struct Model1_neighbour *Model1_n);

/* RSS keys are 40 or 52 bytes long */

extern Model1_u8 Model1_netdev_rss_key[52] __attribute__((__section__(".data..read_mostly")));
void Model1_netdev_rss_key_fill(void *Model1_buffer, Model1_size_t Model1_len);

int Model1_dev_get_nest_level(struct Model1_net_device *Model1_dev);
int Model1_skb_checksum_help(struct Model1_sk_buff *Model1_skb);
struct Model1_sk_buff *Model1___skb_gso_segment(struct Model1_sk_buff *Model1_skb,
      Model1_netdev_features_t Model1_features, bool Model1_tx_path);
struct Model1_sk_buff *Model1_skb_mac_gso_segment(struct Model1_sk_buff *Model1_skb,
        Model1_netdev_features_t Model1_features);

struct Model1_netdev_bonding_info {
 Model1_ifslave Model1_slave;
 Model1_ifbond Model1_master;
};

struct Model1_netdev_notifier_bonding_info {
 struct Model1_netdev_notifier_info Model1_info; /* must be first */
 struct Model1_netdev_bonding_info Model1_bonding_info;
};

void Model1_netdev_bonding_info_change(struct Model1_net_device *Model1_dev,
    struct Model1_netdev_bonding_info *Model1_bonding_info);

static inline __attribute__((no_instrument_function))
struct Model1_sk_buff *Model1_skb_gso_segment(struct Model1_sk_buff *Model1_skb, Model1_netdev_features_t Model1_features)
{
 return Model1___skb_gso_segment(Model1_skb, Model1_features, true);
}
Model1___be16 Model1_skb_network_protocol(struct Model1_sk_buff *Model1_skb, int *Model1_depth);

static inline __attribute__((no_instrument_function)) bool Model1_can_checksum_protocol(Model1_netdev_features_t Model1_features,
      Model1___be16 Model1_protocol)
{
 if (Model1_protocol == (( Model1___be16)(__builtin_constant_p((Model1___u16)((0x8906))) ? ((Model1___u16)( (((Model1___u16)((0x8906)) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)((0x8906)) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16((0x8906)))))
  return !!(Model1_features & ((Model1_netdev_features_t)1 << (Model1_NETIF_F_FCOE_CRC_BIT)));

 /* Assume this is an IP checksum (not SCTP CRC) */

 if (Model1_features & ((Model1_netdev_features_t)1 << (Model1_NETIF_F_HW_CSUM_BIT))) {
  /* Can checksum everything */
  return true;
 }

 switch (Model1_protocol) {
 case (( Model1___be16)(__builtin_constant_p((Model1___u16)((0x0800))) ? ((Model1___u16)( (((Model1___u16)((0x0800)) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)((0x0800)) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16((0x0800)))):
  return !!(Model1_features & ((Model1_netdev_features_t)1 << (Model1_NETIF_F_IP_CSUM_BIT)));
 case (( Model1___be16)(__builtin_constant_p((Model1___u16)((0x86DD))) ? ((Model1___u16)( (((Model1___u16)((0x86DD)) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)((0x86DD)) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16((0x86DD)))):
  return !!(Model1_features & ((Model1_netdev_features_t)1 << (Model1_NETIF_F_IPV6_CSUM_BIT)));
 default:
  return false;
 }
}

/* Map an ethertype into IP protocol if possible */
static inline __attribute__((no_instrument_function)) int Model1_eproto_to_ipproto(int Model1_eproto)
{
 switch (Model1_eproto) {
 case (( Model1___be16)(__builtin_constant_p((Model1___u16)((0x0800))) ? ((Model1___u16)( (((Model1___u16)((0x0800)) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)((0x0800)) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16((0x0800)))):
  return Model1_IPPROTO_IP;
 case (( Model1___be16)(__builtin_constant_p((Model1___u16)((0x86DD))) ? ((Model1___u16)( (((Model1___u16)((0x86DD)) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)((0x86DD)) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16((0x86DD)))):
  return Model1_IPPROTO_IPV6;
 default:
  return -1;
 }
}


void Model1_netdev_rx_csum_fault(struct Model1_net_device *Model1_dev);





/* rx skb timestamps */
void Model1_net_enable_timestamp(void);
void Model1_net_disable_timestamp(void);


int __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function)) Model1_dev_proc_init(void);




static inline __attribute__((no_instrument_function)) Model1_netdev_tx_t Model1___netdev_start_xmit(const struct Model1_net_device_ops *Model1_ops,
           struct Model1_sk_buff *Model1_skb, struct Model1_net_device *Model1_dev,
           bool Model1_more)
{
 Model1_skb->Model1_xmit_more = Model1_more ? 1 : 0;
 return Model1_ops->Model1_ndo_start_xmit(Model1_skb, Model1_dev);
}

static inline __attribute__((no_instrument_function)) Model1_netdev_tx_t Model1_netdev_start_xmit(struct Model1_sk_buff *Model1_skb, struct Model1_net_device *Model1_dev,
         struct Model1_netdev_queue *Model1_txq, bool Model1_more)
{
 const struct Model1_net_device_ops *Model1_ops = Model1_dev->Model1_netdev_ops;
 int Model1_rc;

 Model1_rc = Model1___netdev_start_xmit(Model1_ops, Model1_skb, Model1_dev, Model1_more);
 if (Model1_rc == Model1_NETDEV_TX_OK)
  Model1_txq_trans_update(Model1_txq);

 return Model1_rc;
}

int Model1_netdev_class_create_file_ns(struct Model1_class_attribute *Model1_class_attr,
    const void *Model1_ns);
void Model1_netdev_class_remove_file_ns(struct Model1_class_attribute *Model1_class_attr,
     const void *Model1_ns);

static inline __attribute__((no_instrument_function)) int Model1_netdev_class_create_file(struct Model1_class_attribute *Model1_class_attr)
{
 return Model1_netdev_class_create_file_ns(Model1_class_attr, ((void *)0));
}

static inline __attribute__((no_instrument_function)) void Model1_netdev_class_remove_file(struct Model1_class_attribute *Model1_class_attr)
{
 Model1_netdev_class_remove_file_ns(Model1_class_attr, ((void *)0));
}

extern struct Model1_kobj_ns_type_operations Model1_net_ns_type_operations;

const char *Model1_netdev_drivername(const struct Model1_net_device *Model1_dev);

void Model1_linkwatch_run_queue(void);

static inline __attribute__((no_instrument_function)) Model1_netdev_features_t Model1_netdev_intersect_features(Model1_netdev_features_t Model1_f1,
         Model1_netdev_features_t Model1_f2)
{
 if ((Model1_f1 ^ Model1_f2) & ((Model1_netdev_features_t)1 << (Model1_NETIF_F_HW_CSUM_BIT))) {
  if (Model1_f1 & ((Model1_netdev_features_t)1 << (Model1_NETIF_F_HW_CSUM_BIT)))
   Model1_f1 |= (((Model1_netdev_features_t)1 << (Model1_NETIF_F_IP_CSUM_BIT))|((Model1_netdev_features_t)1 << (Model1_NETIF_F_IPV6_CSUM_BIT)));
  else
   Model1_f2 |= (((Model1_netdev_features_t)1 << (Model1_NETIF_F_IP_CSUM_BIT))|((Model1_netdev_features_t)1 << (Model1_NETIF_F_IPV6_CSUM_BIT)));
 }

 return Model1_f1 & Model1_f2;
}

static inline __attribute__((no_instrument_function)) Model1_netdev_features_t Model1_netdev_get_wanted_features(
 struct Model1_net_device *Model1_dev)
{
 return (Model1_dev->Model1_features & ~Model1_dev->Model1_hw_features) | Model1_dev->Model1_wanted_features;
}
Model1_netdev_features_t Model1_netdev_increment_features(Model1_netdev_features_t Model1_all,
 Model1_netdev_features_t Model1_one, Model1_netdev_features_t Model1_mask);

/* Allow TSO being used on stacked device :
 * Performing the GSO segmentation before last device
 * is a performance improvement.
 */
static inline __attribute__((no_instrument_function)) Model1_netdev_features_t Model1_netdev_add_tso_features(Model1_netdev_features_t Model1_features,
       Model1_netdev_features_t Model1_mask)
{
 return Model1_netdev_increment_features(Model1_features, (((Model1_netdev_features_t)1 << (Model1_NETIF_F_TSO_BIT)) | ((Model1_netdev_features_t)1 << (Model1_NETIF_F_TSO6_BIT)) | ((Model1_netdev_features_t)1 << (Model1_NETIF_F_TSO_ECN_BIT)) | ((Model1_netdev_features_t)1 << (Model1_NETIF_F_TSO_MANGLEID_BIT))), Model1_mask);
}

int Model1___netdev_update_features(struct Model1_net_device *Model1_dev);
void Model1_netdev_update_features(struct Model1_net_device *Model1_dev);
void Model1_netdev_change_features(struct Model1_net_device *Model1_dev);

void Model1_netif_stacked_transfer_operstate(const struct Model1_net_device *Model1_rootdev,
     struct Model1_net_device *Model1_dev);

Model1_netdev_features_t Model1_passthru_features_check(struct Model1_sk_buff *Model1_skb,
       struct Model1_net_device *Model1_dev,
       Model1_netdev_features_t Model1_features);
Model1_netdev_features_t Model1_netif_skb_features(struct Model1_sk_buff *Model1_skb);

static inline __attribute__((no_instrument_function)) bool Model1_net_gso_ok(Model1_netdev_features_t Model1_features, int Model1_gso_type)
{
 Model1_netdev_features_t Model1_feature = (Model1_netdev_features_t)Model1_gso_type << Model1_NETIF_F_GSO_SHIFT;

 /* check flags correspondence */
 do { bool Model1___cond = !(!(Model1_SKB_GSO_TCPV4 != (((Model1_netdev_features_t)1 << (Model1_NETIF_F_TSO_BIT)) >> Model1_NETIF_F_GSO_SHIFT))); extern void Model1___compiletime_assert_4065(void) ; if (Model1___cond) Model1___compiletime_assert_4065(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
 do { bool Model1___cond = !(!(Model1_SKB_GSO_UDP != (((Model1_netdev_features_t)1 << (Model1_NETIF_F_UFO_BIT)) >> Model1_NETIF_F_GSO_SHIFT))); extern void Model1___compiletime_assert_4066(void) ; if (Model1___cond) Model1___compiletime_assert_4066(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
 do { bool Model1___cond = !(!(Model1_SKB_GSO_DODGY != (((Model1_netdev_features_t)1 << (Model1_NETIF_F_GSO_ROBUST_BIT)) >> Model1_NETIF_F_GSO_SHIFT))); extern void Model1___compiletime_assert_4067(void) ; if (Model1___cond) Model1___compiletime_assert_4067(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
 do { bool Model1___cond = !(!(Model1_SKB_GSO_TCP_ECN != (((Model1_netdev_features_t)1 << (Model1_NETIF_F_TSO_ECN_BIT)) >> Model1_NETIF_F_GSO_SHIFT))); extern void Model1___compiletime_assert_4068(void) ; if (Model1___cond) Model1___compiletime_assert_4068(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
 do { bool Model1___cond = !(!(Model1_SKB_GSO_TCP_FIXEDID != (((Model1_netdev_features_t)1 << (Model1_NETIF_F_TSO_MANGLEID_BIT)) >> Model1_NETIF_F_GSO_SHIFT))); extern void Model1___compiletime_assert_4069(void) ; if (Model1___cond) Model1___compiletime_assert_4069(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
 do { bool Model1___cond = !(!(Model1_SKB_GSO_TCPV6 != (((Model1_netdev_features_t)1 << (Model1_NETIF_F_TSO6_BIT)) >> Model1_NETIF_F_GSO_SHIFT))); extern void Model1___compiletime_assert_4070(void) ; if (Model1___cond) Model1___compiletime_assert_4070(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
 do { bool Model1___cond = !(!(Model1_SKB_GSO_FCOE != (((Model1_netdev_features_t)1 << (Model1_NETIF_F_FSO_BIT)) >> Model1_NETIF_F_GSO_SHIFT))); extern void Model1___compiletime_assert_4071(void) ; if (Model1___cond) Model1___compiletime_assert_4071(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
 do { bool Model1___cond = !(!(Model1_SKB_GSO_GRE != (((Model1_netdev_features_t)1 << (Model1_NETIF_F_GSO_GRE_BIT)) >> Model1_NETIF_F_GSO_SHIFT))); extern void Model1___compiletime_assert_4072(void) ; if (Model1___cond) Model1___compiletime_assert_4072(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
 do { bool Model1___cond = !(!(Model1_SKB_GSO_GRE_CSUM != (((Model1_netdev_features_t)1 << (Model1_NETIF_F_GSO_GRE_CSUM_BIT)) >> Model1_NETIF_F_GSO_SHIFT))); extern void Model1___compiletime_assert_4073(void) ; if (Model1___cond) Model1___compiletime_assert_4073(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
 do { bool Model1___cond = !(!(Model1_SKB_GSO_IPXIP4 != (((Model1_netdev_features_t)1 << (Model1_NETIF_F_GSO_IPXIP4_BIT)) >> Model1_NETIF_F_GSO_SHIFT))); extern void Model1___compiletime_assert_4074(void) ; if (Model1___cond) Model1___compiletime_assert_4074(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
 do { bool Model1___cond = !(!(Model1_SKB_GSO_IPXIP6 != (((Model1_netdev_features_t)1 << (Model1_NETIF_F_GSO_IPXIP6_BIT)) >> Model1_NETIF_F_GSO_SHIFT))); extern void Model1___compiletime_assert_4075(void) ; if (Model1___cond) Model1___compiletime_assert_4075(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
 do { bool Model1___cond = !(!(Model1_SKB_GSO_UDP_TUNNEL != (((Model1_netdev_features_t)1 << (Model1_NETIF_F_GSO_UDP_TUNNEL_BIT)) >> Model1_NETIF_F_GSO_SHIFT))); extern void Model1___compiletime_assert_4076(void) ; if (Model1___cond) Model1___compiletime_assert_4076(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
 do { bool Model1___cond = !(!(Model1_SKB_GSO_UDP_TUNNEL_CSUM != (((Model1_netdev_features_t)1 << (Model1_NETIF_F_GSO_UDP_TUNNEL_CSUM_BIT)) >> Model1_NETIF_F_GSO_SHIFT))); extern void Model1___compiletime_assert_4077(void) ; if (Model1___cond) Model1___compiletime_assert_4077(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
 do { bool Model1___cond = !(!(Model1_SKB_GSO_PARTIAL != (((Model1_netdev_features_t)1 << (Model1_NETIF_F_GSO_PARTIAL_BIT)) >> Model1_NETIF_F_GSO_SHIFT))); extern void Model1___compiletime_assert_4078(void) ; if (Model1___cond) Model1___compiletime_assert_4078(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
 do { bool Model1___cond = !(!(Model1_SKB_GSO_TUNNEL_REMCSUM != (((Model1_netdev_features_t)1 << (Model1_NETIF_F_GSO_TUNNEL_REMCSUM_BIT)) >> Model1_NETIF_F_GSO_SHIFT))); extern void Model1___compiletime_assert_4079(void) ; if (Model1___cond) Model1___compiletime_assert_4079(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
 do { bool Model1___cond = !(!(Model1_SKB_GSO_SCTP != (((Model1_netdev_features_t)1 << (Model1_NETIF_F_GSO_SCTP_BIT)) >> Model1_NETIF_F_GSO_SHIFT))); extern void Model1___compiletime_assert_4080(void) ; if (Model1___cond) Model1___compiletime_assert_4080(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);

 return (Model1_features & Model1_feature) == Model1_feature;
}

static inline __attribute__((no_instrument_function)) bool Model1_skb_gso_ok(struct Model1_sk_buff *Model1_skb, Model1_netdev_features_t Model1_features)
{
 return Model1_net_gso_ok(Model1_features, ((struct Model1_skb_shared_info *)(Model1_skb_end_pointer(Model1_skb)))->Model1_gso_type) &&
        (!Model1_skb_has_frag_list(Model1_skb) || (Model1_features & ((Model1_netdev_features_t)1 << (Model1_NETIF_F_FRAGLIST_BIT))));
}

static inline __attribute__((no_instrument_function)) bool Model1_netif_needs_gso(struct Model1_sk_buff *Model1_skb,
       Model1_netdev_features_t Model1_features)
{
 return Model1_skb_is_gso(Model1_skb) && (!Model1_skb_gso_ok(Model1_skb, Model1_features) ||
  __builtin_expect(!!((Model1_skb->Model1_ip_summed != 3) && (Model1_skb->Model1_ip_summed != 1)), 0));

}

static inline __attribute__((no_instrument_function)) void Model1_netif_set_gso_max_size(struct Model1_net_device *Model1_dev,
       unsigned int Model1_size)
{
 Model1_dev->Model1_gso_max_size = Model1_size;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_gso_error_unwind(struct Model1_sk_buff *Model1_skb, Model1___be16 Model1_protocol,
     int Model1_pulled_hlen, Model1_u16 Model1_mac_offset,
     int Model1_mac_len)
{
 Model1_skb->Model1_protocol = Model1_protocol;
 Model1_skb->Model1_encapsulation = 1;
 Model1_skb_push(Model1_skb, Model1_pulled_hlen);
 Model1_skb_reset_transport_header(Model1_skb);
 Model1_skb->Model1_mac_header = Model1_mac_offset;
 Model1_skb->Model1_network_header = Model1_skb->Model1_mac_header + Model1_mac_len;
 Model1_skb->Model1_mac_len = Model1_mac_len;
}

static inline __attribute__((no_instrument_function)) bool Model1_netif_is_macsec(const struct Model1_net_device *Model1_dev)
{
 return Model1_dev->Model1_priv_flags & Model1_IFF_MACSEC;
}

static inline __attribute__((no_instrument_function)) bool Model1_netif_is_macvlan(const struct Model1_net_device *Model1_dev)
{
 return Model1_dev->Model1_priv_flags & Model1_IFF_MACVLAN;
}

static inline __attribute__((no_instrument_function)) bool Model1_netif_is_macvlan_port(const struct Model1_net_device *Model1_dev)
{
 return Model1_dev->Model1_priv_flags & Model1_IFF_MACVLAN_PORT;
}

static inline __attribute__((no_instrument_function)) bool Model1_netif_is_ipvlan(const struct Model1_net_device *Model1_dev)
{
 return Model1_dev->Model1_priv_flags & Model1_IFF_IPVLAN_SLAVE;
}

static inline __attribute__((no_instrument_function)) bool Model1_netif_is_ipvlan_port(const struct Model1_net_device *Model1_dev)
{
 return Model1_dev->Model1_priv_flags & Model1_IFF_IPVLAN_MASTER;
}

static inline __attribute__((no_instrument_function)) bool Model1_netif_is_bond_master(const struct Model1_net_device *Model1_dev)
{
 return Model1_dev->Model1_flags & Model1_IFF_MASTER && Model1_dev->Model1_priv_flags & Model1_IFF_BONDING;
}

static inline __attribute__((no_instrument_function)) bool Model1_netif_is_bond_slave(const struct Model1_net_device *Model1_dev)
{
 return Model1_dev->Model1_flags & Model1_IFF_SLAVE && Model1_dev->Model1_priv_flags & Model1_IFF_BONDING;
}

static inline __attribute__((no_instrument_function)) bool Model1_netif_supports_nofcs(struct Model1_net_device *Model1_dev)
{
 return Model1_dev->Model1_priv_flags & Model1_IFF_SUPP_NOFCS;
}

static inline __attribute__((no_instrument_function)) bool Model1_netif_is_l3_master(const struct Model1_net_device *Model1_dev)
{
 return Model1_dev->Model1_priv_flags & Model1_IFF_L3MDEV_MASTER;
}

static inline __attribute__((no_instrument_function)) bool Model1_netif_is_l3_slave(const struct Model1_net_device *Model1_dev)
{
 return Model1_dev->Model1_priv_flags & Model1_IFF_L3MDEV_SLAVE;
}

static inline __attribute__((no_instrument_function)) bool Model1_netif_is_bridge_master(const struct Model1_net_device *Model1_dev)
{
 return Model1_dev->Model1_priv_flags & Model1_IFF_EBRIDGE;
}

static inline __attribute__((no_instrument_function)) bool Model1_netif_is_bridge_port(const struct Model1_net_device *Model1_dev)
{
 return Model1_dev->Model1_priv_flags & Model1_IFF_BRIDGE_PORT;
}

static inline __attribute__((no_instrument_function)) bool Model1_netif_is_ovs_master(const struct Model1_net_device *Model1_dev)
{
 return Model1_dev->Model1_priv_flags & Model1_IFF_OPENVSWITCH;
}

static inline __attribute__((no_instrument_function)) bool Model1_netif_is_team_master(const struct Model1_net_device *Model1_dev)
{
 return Model1_dev->Model1_priv_flags & Model1_IFF_TEAM;
}

static inline __attribute__((no_instrument_function)) bool Model1_netif_is_team_port(const struct Model1_net_device *Model1_dev)
{
 return Model1_dev->Model1_priv_flags & Model1_IFF_TEAM_PORT;
}

static inline __attribute__((no_instrument_function)) bool Model1_netif_is_lag_master(const struct Model1_net_device *Model1_dev)
{
 return Model1_netif_is_bond_master(Model1_dev) || Model1_netif_is_team_master(Model1_dev);
}

static inline __attribute__((no_instrument_function)) bool Model1_netif_is_lag_port(const struct Model1_net_device *Model1_dev)
{
 return Model1_netif_is_bond_slave(Model1_dev) || Model1_netif_is_team_port(Model1_dev);
}

static inline __attribute__((no_instrument_function)) bool Model1_netif_is_rxfh_configured(const struct Model1_net_device *Model1_dev)
{
 return Model1_dev->Model1_priv_flags & Model1_IFF_RXFH_CONFIGURED;
}

/* This device needs to keep skb dst for qdisc enqueue or ndo_start_xmit() */
static inline __attribute__((no_instrument_function)) void Model1_netif_keep_dst(struct Model1_net_device *Model1_dev)
{
 Model1_dev->Model1_priv_flags &= ~(Model1_IFF_XMIT_DST_RELEASE | Model1_IFF_XMIT_DST_RELEASE_PERM);
}

/* return true if dev can't cope with mtu frames that need vlan tag insertion */
static inline __attribute__((no_instrument_function)) bool Model1_netif_reduces_vlan_mtu(struct Model1_net_device *Model1_dev)
{
 /* TODO: reserve and use an additional IFF bit, if we get more users */
 return Model1_dev->Model1_priv_flags & Model1_IFF_MACSEC;
}

extern struct Model1_pernet_operations Model1_loopback_net_ops;

/* Logging, debugging and troubleshooting/diagnostic helpers. */

/* netdev_printk helpers, similar to dev_printk */

static inline __attribute__((no_instrument_function)) const char *Model1_netdev_name(const struct Model1_net_device *Model1_dev)
{
 if (!Model1_dev->Model1_name[0] || Model1_strchr(Model1_dev->Model1_name, '%'))
  return "(unnamed net_device)";
 return Model1_dev->Model1_name;
}

static inline __attribute__((no_instrument_function)) const char *Model1_netdev_reg_state(const struct Model1_net_device *Model1_dev)
{
 switch (Model1_dev->Model1_reg_state) {
 case Model1_NETREG_UNINITIALIZED: return " (uninitialized)";
 case Model1_NETREG_REGISTERED: return "";
 case Model1_NETREG_UNREGISTERING: return " (unregistering)";
 case Model1_NETREG_UNREGISTERED: return " (unregistered)";
 case Model1_NETREG_RELEASED: return " (released)";
 case Model1_NETREG_DUMMY: return " (dummy)";
 }

 ({ static bool __attribute__ ((__section__(".data.unlikely"))) Model1___warned; int Model1___ret_warn_once = !!(1); if (__builtin_expect(!!(Model1___ret_warn_once && !Model1___warned), 0)) { Model1___warned = true; ({ int Model1___ret_warn_on = !!(1); if (__builtin_expect(!!(Model1___ret_warn_on), 0)) Model1_warn_slowpath_fmt("./include/linux/netdevice.h", 4245, "%s: unknown reg_state %d\n", Model1_dev->Model1_name, Model1_dev->Model1_reg_state); __builtin_expect(!!(Model1___ret_warn_on), 0); }); } __builtin_expect(!!(Model1___ret_warn_once), 0); });
 return " (unknown)";
}

__attribute__((format(printf, 3, 4)))
void Model1_netdev_printk(const char *Model1_level, const struct Model1_net_device *Model1_dev,
     const char *format, ...);
__attribute__((format(printf, 2, 3)))
void Model1_netdev_emerg(const struct Model1_net_device *Model1_dev, const char *format, ...);
__attribute__((format(printf, 2, 3)))
void Model1_netdev_alert(const struct Model1_net_device *Model1_dev, const char *format, ...);
__attribute__((format(printf, 2, 3)))
void Model1_netdev_crit(const struct Model1_net_device *Model1_dev, const char *format, ...);
__attribute__((format(printf, 2, 3)))
void Model1_netdev_err(const struct Model1_net_device *Model1_dev, const char *format, ...);
__attribute__((format(printf, 2, 3)))
void Model1_netdev_warn(const struct Model1_net_device *Model1_dev, const char *format, ...);
__attribute__((format(printf, 2, 3)))
void Model1_netdev_notice(const struct Model1_net_device *Model1_dev, const char *format, ...);
__attribute__((format(printf, 2, 3)))
void Model1_netdev_info(const struct Model1_net_device *Model1_dev, const char *format, ...);
/*
 * netdev_WARN() acts like dev_printk(), but with the key difference
 * of using a WARN/WARN_ON to get the message out, including the
 * file/line information and a backtrace.
 */




/* netif printk helpers, similar to netdev_printk */
/*
 *	The list of packet types we will receive (as opposed to discard)
 *	and the routines to invoke.
 *
 *	Why 16. Because with 16 the only overlap we get on a hash of the
 *	low nibble of the protocol value is RARP/SNAP/X.25.
 *
 *      NOTE:  That is no longer true with the addition of VLAN tags.  Not
 *             sure which should go first, but I bet it won't make much
 *             difference if we are running VLANs.  The good news is that
 *             this protocol won't be in the list unless compiled in, so
 *             the average user (w/out VLANs) will not be adversely affected.
 *             --BLG
 *
 *		0800	IP
 *		8100    802.1Q VLAN
 *		0001	802.3
 *		0002	AX.25
 *		0004	802.2
 *		8035	RARP
 *		0005	SNAP
 *		0805	X.25
 *		0806	ARP
 *		8137	IPX
 *		0009	Localtalk
 *		86DD	IPv6
 */



















struct Model1_ifaddrmsg {
 __u8 Model1_ifa_family;
 __u8 Model1_ifa_prefixlen; /* The prefix length		*/
 __u8 Model1_ifa_flags; /* Flags			*/
 __u8 Model1_ifa_scope; /* Address scope		*/
 __u32 Model1_ifa_index; /* Link index			*/
};

/*
 * Important comment:
 * IFA_ADDRESS is prefix address, rather than local interface address.
 * It makes no difference for normally configured broadcast interfaces,
 * but for point-to-point IFA_ADDRESS is DESTINATION address,
 * local address is supplied in IFA_LOCAL attribute.
 *
 * IFA_FLAGS is a u32 attribute that extends the u8 field ifa_flags.
 * If present, the value from struct ifaddrmsg will be ignored.
 */
enum {
 Model1_IFA_UNSPEC,
 Model1_IFA_ADDRESS,
 Model1_IFA_LOCAL,
 Model1_IFA_LABEL,
 Model1_IFA_BROADCAST,
 Model1_IFA_ANYCAST,
 Model1_IFA_CACHEINFO,
 Model1_IFA_MULTICAST,
 Model1_IFA_FLAGS,
 Model1___IFA_MAX,
};



/* ifa_flags */
struct Model1_ifa_cacheinfo {
 __u32 Model1_ifa_prefered;
 __u32 Model1_ifa_valid;
 __u32 Model1_cstamp; /* created timestamp, hundredths of seconds */
 __u32 Model1_tstamp; /* updated timestamp, hundredths of seconds */
};

/* backwards compatibility for userspace */


/* rtnetlink families. Values up to 127 are reserved for real address
 * families, values above 128 may be used arbitrarily.
 */




/****
 *		Routing/neighbour discovery messages.
 ****/

/* Types of messages */

enum {
 Model1_RTM_BASE = 16,


 Model1_RTM_NEWLINK = 16,

 Model1_RTM_DELLINK,

 Model1_RTM_GETLINK,

 Model1_RTM_SETLINK,


 Model1_RTM_NEWADDR = 20,

 Model1_RTM_DELADDR,

 Model1_RTM_GETADDR,


 Model1_RTM_NEWROUTE = 24,

 Model1_RTM_DELROUTE,

 Model1_RTM_GETROUTE,


 Model1_RTM_NEWNEIGH = 28,

 Model1_RTM_DELNEIGH,

 Model1_RTM_GETNEIGH,


 Model1_RTM_NEWRULE = 32,

 Model1_RTM_DELRULE,

 Model1_RTM_GETRULE,


 Model1_RTM_NEWQDISC = 36,

 Model1_RTM_DELQDISC,

 Model1_RTM_GETQDISC,


 Model1_RTM_NEWTCLASS = 40,

 Model1_RTM_DELTCLASS,

 Model1_RTM_GETTCLASS,


 Model1_RTM_NEWTFILTER = 44,

 Model1_RTM_DELTFILTER,

 Model1_RTM_GETTFILTER,


 Model1_RTM_NEWACTION = 48,

 Model1_RTM_DELACTION,

 Model1_RTM_GETACTION,


 Model1_RTM_NEWPREFIX = 52,


 Model1_RTM_GETMULTICAST = 58,


 Model1_RTM_GETANYCAST = 62,


 Model1_RTM_NEWNEIGHTBL = 64,

 Model1_RTM_GETNEIGHTBL = 66,

 Model1_RTM_SETNEIGHTBL,


 Model1_RTM_NEWNDUSEROPT = 68,


 Model1_RTM_NEWADDRLABEL = 72,

 Model1_RTM_DELADDRLABEL,

 Model1_RTM_GETADDRLABEL,


 Model1_RTM_GETDCB = 78,

 Model1_RTM_SETDCB,


 Model1_RTM_NEWNETCONF = 80,

 Model1_RTM_GETNETCONF = 82,


 Model1_RTM_NEWMDB = 84,

 Model1_RTM_DELMDB = 85,

 Model1_RTM_GETMDB = 86,


 Model1_RTM_NEWNSID = 88,

 Model1_RTM_DELNSID = 89,

 Model1_RTM_GETNSID = 90,


 Model1_RTM_NEWSTATS = 92,

 Model1_RTM_GETSTATS = 94,


 Model1___RTM_MAX,

};





/* 
   Generic structure for encapsulation of optional route information.
   It is reminiscent of sockaddr, but with sa_family replaced
   with attribute type.
 */

struct Model1_rtattr {
 unsigned short Model1_rta_len;
 unsigned short Model1_rta_type;
};

/* Macros to handle rtattributes */
/******************************************************************************
 *		Definitions used in routing table administration.
 ****/

struct Model1_rtmsg {
 unsigned char Model1_rtm_family;
 unsigned char Model1_rtm_dst_len;
 unsigned char Model1_rtm_src_len;
 unsigned char Model1_rtm_tos;

 unsigned char Model1_rtm_table; /* Routing table id */
 unsigned char Model1_rtm_protocol; /* Routing protocol; see below	*/
 unsigned char Model1_rtm_scope; /* See below */
 unsigned char Model1_rtm_type; /* See below	*/

 unsigned Model1_rtm_flags;
};

/* rtm_type */

enum {
 Model1_RTN_UNSPEC,
 Model1_RTN_UNICAST, /* Gateway or direct route	*/
 Model1_RTN_LOCAL, /* Accept locally		*/
 Model1_RTN_BROADCAST, /* Accept locally as broadcast,
				   send as broadcast */
 Model1_RTN_ANYCAST, /* Accept locally as broadcast,
				   but send as unicast */
 Model1_RTN_MULTICAST, /* Multicast route		*/
 Model1_RTN_BLACKHOLE, /* Drop				*/
 Model1_RTN_UNREACHABLE, /* Destination is unreachable   */
 Model1_RTN_PROHIBIT, /* Administratively prohibited	*/
 Model1_RTN_THROW, /* Not in this table		*/
 Model1_RTN_NAT, /* Translate this address	*/
 Model1_RTN_XRESOLVE, /* Use external resolver	*/
 Model1___RTN_MAX
};




/* rtm_protocol */
/* Values of protocol >= RTPROT_STATIC are not interpreted by kernel;
   they are just passed from user and back as is.
   It will be used by hypothetical multiple routing daemons.
   Note that protocol values should be standardized in order to
   avoid conflicts.
 */
/* rtm_scope

   Really it is not scope, but sort of distance to the destination.
   NOWHERE are reserved for not existing destinations, HOST is our
   local addresses, LINK are destinations, located on directly attached
   link and UNIVERSE is everywhere in the Universe.

   Intermediate values are also possible f.e. interior routes
   could be assigned a value between UNIVERSE and LINK.
*/

enum Model1_rt_scope_t {
 Model1_RT_SCOPE_UNIVERSE=0,
/* User defined values  */
 Model1_RT_SCOPE_SITE=200,
 Model1_RT_SCOPE_LINK=253,
 Model1_RT_SCOPE_HOST=254,
 Model1_RT_SCOPE_NOWHERE=255
};

/* rtm_flags */







/* Reserved table identifiers */

enum Model1_rt_class_t {
 Model1_RT_TABLE_UNSPEC=0,
/* User defined values */
 Model1_RT_TABLE_COMPAT=252,
 Model1_RT_TABLE_DEFAULT=253,
 Model1_RT_TABLE_MAIN=254,
 Model1_RT_TABLE_LOCAL=255,
 Model1_RT_TABLE_MAX=0xFFFFFFFF
};


/* Routing message attributes */

enum Model1_rtattr_type_t {
 Model1_RTA_UNSPEC,
 Model1_RTA_DST,
 Model1_RTA_SRC,
 Model1_RTA_IIF,
 Model1_RTA_OIF,
 Model1_RTA_GATEWAY,
 Model1_RTA_PRIORITY,
 Model1_RTA_PREFSRC,
 Model1_RTA_METRICS,
 Model1_RTA_MULTIPATH,
 Model1_RTA_PROTOINFO, /* no longer used */
 Model1_RTA_FLOW,
 Model1_RTA_CACHEINFO,
 Model1_RTA_SESSION, /* no longer used */
 Model1_RTA_MP_ALGO, /* no longer used */
 Model1_RTA_TABLE,
 Model1_RTA_MARK,
 Model1_RTA_MFC_STATS,
 Model1_RTA_VIA,
 Model1_RTA_NEWDST,
 Model1_RTA_PREF,
 Model1_RTA_ENCAP_TYPE,
 Model1_RTA_ENCAP,
 Model1_RTA_EXPIRES,
 Model1_RTA_PAD,
 Model1___RTA_MAX
};






/* RTM_MULTIPATH --- array of struct rtnexthop.
 *
 * "struct rtnexthop" describes all necessary nexthop information,
 * i.e. parameters of path to a destination via this nexthop.
 *
 * At the moment it is impossible to set different prefsrc, mtu, window
 * and rtt for different paths from multipath.
 */

struct Model1_rtnexthop {
 unsigned short Model1_rtnh_len;
 unsigned char Model1_rtnh_flags;
 unsigned char Model1_rtnh_hops;
 int Model1_rtnh_ifindex;
};

/* rtnh_flags */
/* Macros to handle hexthops */
/* RTA_VIA */
struct Model1_rtvia {
 Model1___kernel_sa_family_t Model1_rtvia_family;
 __u8 Model1_rtvia_addr[0];
};

/* RTM_CACHEINFO */

struct Model1_rta_cacheinfo {
 __u32 Model1_rta_clntref;
 __u32 Model1_rta_lastuse;
 Model1___s32 Model1_rta_expires;
 __u32 Model1_rta_error;
 __u32 Model1_rta_used;


 __u32 Model1_rta_id;
 __u32 Model1_rta_ts;
 __u32 Model1_rta_tsage;
};

/* RTM_METRICS --- array of struct rtattr with types of RTAX_* */

enum {
 Model1_RTAX_UNSPEC,

 Model1_RTAX_LOCK,

 Model1_RTAX_MTU,

 Model1_RTAX_WINDOW,

 Model1_RTAX_RTT,

 Model1_RTAX_RTTVAR,

 Model1_RTAX_SSTHRESH,

 Model1_RTAX_CWND,

 Model1_RTAX_ADVMSS,

 Model1_RTAX_REORDERING,

 Model1_RTAX_HOPLIMIT,

 Model1_RTAX_INITCWND,

 Model1_RTAX_FEATURES,

 Model1_RTAX_RTO_MIN,

 Model1_RTAX_INITRWND,

 Model1_RTAX_QUICKACK,

 Model1_RTAX_CC_ALGO,

 Model1___RTAX_MAX
};
struct Model1_rta_session {
 __u8 Model1_proto;
 __u8 Model1_pad1;
 Model1___u16 Model1_pad2;

 union {
  struct {
   Model1___u16 Model1_sport;
   Model1___u16 Model1_dport;
  } Model1_ports;

  struct {
   __u8 Model1_type;
   __u8 Model1_code;
   Model1___u16 Model1_ident;
  } Model1_icmpt;

  __u32 Model1_spi;
 } Model1_u;
};

struct Model1_rta_mfc_stats {
 __u64 Model1_mfcs_packets;
 __u64 Model1_mfcs_bytes;
 __u64 Model1_mfcs_wrong_if;
};

/****
 *		General form of address family dependent message.
 ****/

struct Model1_rtgenmsg {
 unsigned char Model1_rtgen_family;
};

/*****************************************************************
 *		Link layer specific messages.
 ****/

/* struct ifinfomsg
 * passes link level specific information, not dependent
 * on network protocol.
 */

struct Model1_ifinfomsg {
 unsigned char Model1_ifi_family;
 unsigned char Model1___ifi_pad;
 unsigned short Model1_ifi_type; /* ARPHRD_* */
 int Model1_ifi_index; /* Link index	*/
 unsigned Model1_ifi_flags; /* IFF_* flags	*/
 unsigned Model1_ifi_change; /* IFF_* change mask */
};

/********************************************************************
 *		prefix information 
 ****/

struct Model1_prefixmsg {
 unsigned char Model1_prefix_family;
 unsigned char Model1_prefix_pad1;
 unsigned short Model1_prefix_pad2;
 int Model1_prefix_ifindex;
 unsigned char Model1_prefix_type;
 unsigned char Model1_prefix_len;
 unsigned char Model1_prefix_flags;
 unsigned char Model1_prefix_pad3;
};

enum
{
 Model1_PREFIX_UNSPEC,
 Model1_PREFIX_ADDRESS,
 Model1_PREFIX_CACHEINFO,
 Model1___PREFIX_MAX
};



struct Model1_prefix_cacheinfo {
 __u32 Model1_preferred_time;
 __u32 Model1_valid_time;
};


/*****************************************************************
 *		Traffic control messages.
 ****/

struct Model1_tcmsg {
 unsigned char Model1_tcm_family;
 unsigned char Model1_tcm__pad1;
 unsigned short Model1_tcm__pad2;
 int Model1_tcm_ifindex;
 __u32 Model1_tcm_handle;
 __u32 Model1_tcm_parent;
 __u32 Model1_tcm_info;
};

enum {
 Model1_TCA_UNSPEC,
 Model1_TCA_KIND,
 Model1_TCA_OPTIONS,
 Model1_TCA_STATS,
 Model1_TCA_XSTATS,
 Model1_TCA_RATE,
 Model1_TCA_FCNT,
 Model1_TCA_STATS2,
 Model1_TCA_STAB,
 Model1_TCA_PAD,
 Model1___TCA_MAX
};






/********************************************************************
 *		Neighbor Discovery userland options
 ****/

struct Model1_nduseroptmsg {
 unsigned char Model1_nduseropt_family;
 unsigned char Model1_nduseropt_pad1;
 unsigned short Model1_nduseropt_opts_len; /* Total length of options */
 int Model1_nduseropt_ifindex;
 __u8 Model1_nduseropt_icmp_type;
 __u8 Model1_nduseropt_icmp_code;
 unsigned short Model1_nduseropt_pad2;
 unsigned int Model1_nduseropt_pad3;
 /* Followed by one or more ND options */
};

enum {
 Model1_NDUSEROPT_UNSPEC,
 Model1_NDUSEROPT_SRCADDR,
 Model1___NDUSEROPT_MAX
};
/* RTnetlink multicast groups */
enum Model1_rtnetlink_groups {
 Model1_RTNLGRP_NONE,

 Model1_RTNLGRP_LINK,

 Model1_RTNLGRP_NOTIFY,

 Model1_RTNLGRP_NEIGH,

 Model1_RTNLGRP_TC,

 Model1_RTNLGRP_IPV4_IFADDR,

 Model1_RTNLGRP_IPV4_MROUTE,

 Model1_RTNLGRP_IPV4_ROUTE,

 Model1_RTNLGRP_IPV4_RULE,

 Model1_RTNLGRP_IPV6_IFADDR,

 Model1_RTNLGRP_IPV6_MROUTE,

 Model1_RTNLGRP_IPV6_ROUTE,

 Model1_RTNLGRP_IPV6_IFINFO,

 Model1_RTNLGRP_DECnet_IFADDR,

 Model1_RTNLGRP_NOP2,
 Model1_RTNLGRP_DECnet_ROUTE,

 Model1_RTNLGRP_DECnet_RULE,

 Model1_RTNLGRP_NOP4,
 Model1_RTNLGRP_IPV6_PREFIX,

 Model1_RTNLGRP_IPV6_RULE,

 Model1_RTNLGRP_ND_USEROPT,

 Model1_RTNLGRP_PHONET_IFADDR,

 Model1_RTNLGRP_PHONET_ROUTE,

 Model1_RTNLGRP_DCB,

 Model1_RTNLGRP_IPV4_NETCONF,

 Model1_RTNLGRP_IPV6_NETCONF,

 Model1_RTNLGRP_MDB,

 Model1_RTNLGRP_MPLS_ROUTE,

 Model1_RTNLGRP_NSID,

 Model1___RTNLGRP_MAX
};


/* TC action piece */
struct Model1_tcamsg {
 unsigned char Model1_tca_family;
 unsigned char Model1_tca__pad1;
 unsigned short Model1_tca__pad2;
};





/* New extended info filters for IFLA_EXT_MASK */





/* End of information exported to user level */

extern int Model1_rtnetlink_send(struct Model1_sk_buff *Model1_skb, struct Model1_net *Model1_net, Model1_u32 Model1_pid, Model1_u32 Model1_group, int Model1_echo);
extern int Model1_rtnl_unicast(struct Model1_sk_buff *Model1_skb, struct Model1_net *Model1_net, Model1_u32 Model1_pid);
extern void Model1_rtnl_notify(struct Model1_sk_buff *Model1_skb, struct Model1_net *Model1_net, Model1_u32 Model1_pid,
   Model1_u32 Model1_group, struct Model1_nlmsghdr *Model1_nlh, Model1_gfp_t Model1_flags);
extern void Model1_rtnl_set_sk_err(struct Model1_net *Model1_net, Model1_u32 Model1_group, int error);
extern int Model1_rtnetlink_put_metrics(struct Model1_sk_buff *Model1_skb, Model1_u32 *Model1_metrics);
extern int Model1_rtnl_put_cacheinfo(struct Model1_sk_buff *Model1_skb, struct Model1_dst_entry *Model1_dst,
         Model1_u32 Model1_id, long Model1_expires, Model1_u32 error);

void Model1_rtmsg_ifinfo(int Model1_type, struct Model1_net_device *Model1_dev, unsigned Model1_change, Model1_gfp_t Model1_flags);
struct Model1_sk_buff *Model1_rtmsg_ifinfo_build_skb(int Model1_type, struct Model1_net_device *Model1_dev,
           unsigned Model1_change, Model1_gfp_t Model1_flags);
void Model1_rtmsg_ifinfo_send(struct Model1_sk_buff *Model1_skb, struct Model1_net_device *Model1_dev,
         Model1_gfp_t Model1_flags);


/* RTNL is used as a global lock for all changes to network configuration  */
extern void Model1_rtnl_lock(void);
extern void Model1_rtnl_unlock(void);
extern int Model1_rtnl_trylock(void);
extern int Model1_rtnl_is_locked(void);

extern Model1_wait_queue_head_t Model1_netdev_unregistering_wq;
extern struct Model1_mutex Model1_net_mutex;




static inline __attribute__((no_instrument_function)) bool Model1_lockdep_rtnl_is_held(void)
{
 return true;
}


/**
 * rcu_dereference_rtnl - rcu_dereference with debug checking
 * @p: The pointer to read, prior to dereferencing
 *
 * Do an rcu_dereference(p), but check caller either holds rcu_read_lock()
 * or RTNL. Note : Please prefer rtnl_dereference() or rcu_dereference()
 */



/**
 * rcu_dereference_bh_rtnl - rcu_dereference_bh with debug checking
 * @p: The pointer to read, prior to dereference
 *
 * Do an rcu_dereference_bh(p), but check caller either holds rcu_read_lock_bh()
 * or RTNL. Note : Please prefer rtnl_dereference() or rcu_dereference_bh()
 */



/**
 * rtnl_dereference - fetch RCU pointer when updates are prevented by RTNL
 * @p: The pointer to read, prior to dereferencing
 *
 * Return the value of the specified RCU-protected pointer, but omit
 * both the smp_read_barrier_depends() and the ACCESS_ONCE(), because
 * caller holds RTNL.
 */



static inline __attribute__((no_instrument_function)) struct Model1_netdev_queue *Model1_dev_ingress_queue(struct Model1_net_device *Model1_dev)
{
 return ({ do { } while (0); ; ((typeof(*(Model1_dev->Model1_ingress_queue)) *)((Model1_dev->Model1_ingress_queue))); });
}

struct Model1_netdev_queue *Model1_dev_ingress_queue_create(struct Model1_net_device *Model1_dev);


void Model1_net_inc_ingress_queue(void);
void Model1_net_dec_ingress_queue(void);







void Model1_rtnetlink_init(void);
void Model1___rtnl_unlock(void);
void Model1_rtnl_kfree_skbs(struct Model1_sk_buff *Model1_head, struct Model1_sk_buff *Model1_tail);
extern int Model1_ndo_dflt_fdb_dump(struct Model1_sk_buff *Model1_skb,
        struct Model1_netlink_callback *Model1_cb,
        struct Model1_net_device *Model1_dev,
        struct Model1_net_device *Model1_filter_dev,
        int Model1_idx);
extern int Model1_ndo_dflt_fdb_add(struct Model1_ndmsg *Model1_ndm,
       struct Model1_nlattr *Model1_tb[],
       struct Model1_net_device *Model1_dev,
       const unsigned char *Model1_addr,
       Model1_u16 Model1_vid,
       Model1_u16 Model1_flags);
extern int Model1_ndo_dflt_fdb_del(struct Model1_ndmsg *Model1_ndm,
       struct Model1_nlattr *Model1_tb[],
       struct Model1_net_device *Model1_dev,
       const unsigned char *Model1_addr,
       Model1_u16 Model1_vid);

extern int Model1_ndo_dflt_bridge_getlink(struct Model1_sk_buff *Model1_skb, Model1_u32 Model1_pid, Model1_u32 Model1_seq,
       struct Model1_net_device *Model1_dev, Model1_u16 Model1_mode,
       Model1_u32 Model1_flags, Model1_u32 Model1_mask, int Model1_nlflags,
       Model1_u32 Model1_filter_mask,
       int (*Model1_vlan_fill)(struct Model1_sk_buff *Model1_skb,
          struct Model1_net_device *Model1_dev,
          Model1_u32 Model1_filter_mask));








/*
 *	Generic neighbour manipulation
 *
 *	Authors:
 *	Pedro Roque		<roque@di.fc.ul.pt>
 *	Alexey Kuznetsov	<kuznet@ms2.inr.ac.ru>
 *
 * 	Changes:
 *
 *	Harald Welte:		<laforge@gnumonks.org>
 *		- Add neighbour cache statistics like rtstat
 */












/* ========================================================================
 *         Netlink Messages and Attributes Interface (As Seen On TV)
 * ------------------------------------------------------------------------
 *                          Messages Interface
 * ------------------------------------------------------------------------
 *
 * Message Format:
 *    <--- nlmsg_total_size(payload)  --->
 *    <-- nlmsg_msg_size(payload) ->
 *   +----------+- - -+-------------+- - -+-------- - -
 *   | nlmsghdr | Pad |   Payload   | Pad | nlmsghdr
 *   +----------+- - -+-------------+- - -+-------- - -
 *   nlmsg_data(nlh)---^                   ^
 *   nlmsg_next(nlh)-----------------------+
 *
 * Payload Format:
 *    <---------------------- nlmsg_len(nlh) --------------------->
 *    <------ hdrlen ------>       <- nlmsg_attrlen(nlh, hdrlen) ->
 *   +----------------------+- - -+--------------------------------+
 *   |     Family Header    | Pad |           Attributes           |
 *   +----------------------+- - -+--------------------------------+
 *   nlmsg_attrdata(nlh, hdrlen)---^
 *
 * Data Structures:
 *   struct nlmsghdr			netlink message header
 *
 * Message Construction:
 *   nlmsg_new()			create a new netlink message
 *   nlmsg_put()			add a netlink message to an skb
 *   nlmsg_put_answer()			callback based nlmsg_put()
 *   nlmsg_end()			finalize netlink message
 *   nlmsg_get_pos()			return current position in message
 *   nlmsg_trim()			trim part of message
 *   nlmsg_cancel()			cancel message construction
 *   nlmsg_free()			free a netlink message
 *
 * Message Sending:
 *   nlmsg_multicast()			multicast message to several groups
 *   nlmsg_unicast()			unicast a message to a single socket
 *   nlmsg_notify()			send notification message
 *
 * Message Length Calculations:
 *   nlmsg_msg_size(payload)		length of message w/o padding
 *   nlmsg_total_size(payload)		length of message w/ padding
 *   nlmsg_padlen(payload)		length of padding at tail
 *
 * Message Payload Access:
 *   nlmsg_data(nlh)			head of message payload
 *   nlmsg_len(nlh)			length of message payload
 *   nlmsg_attrdata(nlh, hdrlen)	head of attributes data
 *   nlmsg_attrlen(nlh, hdrlen)		length of attributes data
 *
 * Message Parsing:
 *   nlmsg_ok(nlh, remaining)		does nlh fit into remaining bytes?
 *   nlmsg_next(nlh, remaining)		get next netlink message
 *   nlmsg_parse()			parse attributes of a message
 *   nlmsg_find_attr()			find an attribute in a message
 *   nlmsg_for_each_msg()		loop over all messages
 *   nlmsg_validate()			validate netlink message incl. attrs
 *   nlmsg_for_each_attr()		loop over all attributes
 *
 * Misc:
 *   nlmsg_report()			report back to application?
 *
 * ------------------------------------------------------------------------
 *                          Attributes Interface
 * ------------------------------------------------------------------------
 *
 * Attribute Format:
 *    <------- nla_total_size(payload) ------->
 *    <---- nla_attr_size(payload) ----->
 *   +----------+- - -+- - - - - - - - - +- - -+-------- - -
 *   |  Header  | Pad |     Payload      | Pad |  Header
 *   +----------+- - -+- - - - - - - - - +- - -+-------- - -
 *                     <- nla_len(nla) ->      ^
 *   nla_data(nla)----^                        |
 *   nla_next(nla)-----------------------------'
 *
 * Data Structures:
 *   struct nlattr			netlink attribute header
 *
 * Attribute Construction:
 *   nla_reserve(skb, type, len)	reserve room for an attribute
 *   nla_reserve_nohdr(skb, len)	reserve room for an attribute w/o hdr
 *   nla_put(skb, type, len, data)	add attribute to skb
 *   nla_put_nohdr(skb, len, data)	add attribute w/o hdr
 *   nla_append(skb, len, data)		append data to skb
 *
 * Attribute Construction for Basic Types:
 *   nla_put_u8(skb, type, value)	add u8 attribute to skb
 *   nla_put_u16(skb, type, value)	add u16 attribute to skb
 *   nla_put_u32(skb, type, value)	add u32 attribute to skb
 *   nla_put_u64_64bits(skb, type,
 *			value, padattr)	add u64 attribute to skb
 *   nla_put_s8(skb, type, value)	add s8 attribute to skb
 *   nla_put_s16(skb, type, value)	add s16 attribute to skb
 *   nla_put_s32(skb, type, value)	add s32 attribute to skb
 *   nla_put_s64(skb, type, value,
 *               padattr)		add s64 attribute to skb
 *   nla_put_string(skb, type, str)	add string attribute to skb
 *   nla_put_flag(skb, type)		add flag attribute to skb
 *   nla_put_msecs(skb, type, jiffies,
 *                 padattr)		add msecs attribute to skb
 *   nla_put_in_addr(skb, type, addr)	add IPv4 address attribute to skb
 *   nla_put_in6_addr(skb, type, addr)	add IPv6 address attribute to skb
 *
 * Nested Attributes Construction:
 *   nla_nest_start(skb, type)		start a nested attribute
 *   nla_nest_end(skb, nla)		finalize a nested attribute
 *   nla_nest_cancel(skb, nla)		cancel nested attribute construction
 *
 * Attribute Length Calculations:
 *   nla_attr_size(payload)		length of attribute w/o padding
 *   nla_total_size(payload)		length of attribute w/ padding
 *   nla_padlen(payload)		length of padding
 *
 * Attribute Payload Access:
 *   nla_data(nla)			head of attribute payload
 *   nla_len(nla)			length of attribute payload
 *
 * Attribute Payload Access for Basic Types:
 *   nla_get_u8(nla)			get payload for a u8 attribute
 *   nla_get_u16(nla)			get payload for a u16 attribute
 *   nla_get_u32(nla)			get payload for a u32 attribute
 *   nla_get_u64(nla)			get payload for a u64 attribute
 *   nla_get_s8(nla)			get payload for a s8 attribute
 *   nla_get_s16(nla)			get payload for a s16 attribute
 *   nla_get_s32(nla)			get payload for a s32 attribute
 *   nla_get_s64(nla)			get payload for a s64 attribute
 *   nla_get_flag(nla)			return 1 if flag is true
 *   nla_get_msecs(nla)			get payload for a msecs attribute
 *
 * Attribute Misc:
 *   nla_memcpy(dest, nla, count)	copy attribute into memory
 *   nla_memcmp(nla, data, size)	compare attribute with memory area
 *   nla_strlcpy(dst, nla, size)	copy attribute to a sized string
 *   nla_strcmp(nla, str)		compare attribute with string
 *
 * Attribute Parsing:
 *   nla_ok(nla, remaining)		does nla fit into remaining bytes?
 *   nla_next(nla, remaining)		get next netlink attribute
 *   nla_validate()			validate a stream of attributes
 *   nla_validate_nested()		validate a stream of nested attributes
 *   nla_find()				find attribute in stream of attributes
 *   nla_find_nested()			find attribute in nested attributes
 *   nla_parse()			parse and validate stream of attrs
 *   nla_parse_nested()			parse nested attribuets
 *   nla_for_each_attr()		loop over all attributes
 *   nla_for_each_nested()		loop over the nested attributes
 *=========================================================================
 */

 /**
  * Standard attribute types to specify validation policy
  */
enum {
 Model1_NLA_UNSPEC,
 Model1_NLA_U8,
 Model1_NLA_U16,
 Model1_NLA_U32,
 Model1_NLA_U64,
 Model1_NLA_STRING,
 Model1_NLA_FLAG,
 Model1_NLA_MSECS,
 Model1_NLA_NESTED,
 Model1_NLA_NESTED_COMPAT,
 Model1_NLA_NUL_STRING,
 Model1_NLA_BINARY,
 Model1_NLA_S8,
 Model1_NLA_S16,
 Model1_NLA_S32,
 Model1_NLA_S64,
 Model1___NLA_TYPE_MAX,
};



/**
 * struct nla_policy - attribute validation policy
 * @type: Type of attribute or NLA_UNSPEC
 * @len: Type specific length of payload
 *
 * Policies are defined as arrays of this struct, the array must be
 * accessible by attribute type up to the highest identifier to be expected.
 *
 * Meaning of `len' field:
 *    NLA_STRING           Maximum length of string
 *    NLA_NUL_STRING       Maximum length of string (excluding NUL)
 *    NLA_FLAG             Unused
 *    NLA_BINARY           Maximum length of attribute payload
 *    NLA_NESTED           Don't use `len' field -- length verification is
 *                         done by checking len of nested header (or empty)
 *    NLA_NESTED_COMPAT    Minimum length of structure payload
 *    NLA_U8, NLA_U16,
 *    NLA_U32, NLA_U64,
 *    NLA_S8, NLA_S16,
 *    NLA_S32, NLA_S64,
 *    NLA_MSECS            Leaving the length field zero will verify the
 *                         given type fits, using it verifies minimum length
 *                         just like "All other"
 *    All other            Minimum length of attribute payload
 *
 * Example:
 * static const struct nla_policy my_policy[ATTR_MAX+1] = {
 * 	[ATTR_FOO] = { .type = NLA_U16 },
 *	[ATTR_BAR] = { .type = NLA_STRING, .len = BARSIZ },
 *	[ATTR_BAZ] = { .len = sizeof(struct mystruct) },
 * };
 */
struct Model1_nla_policy {
 Model1_u16 Model1_type;
 Model1_u16 Model1_len;
};

/**
 * struct nl_info - netlink source information
 * @nlh: Netlink message header of original request
 * @portid: Netlink PORTID of requesting application
 */
struct Model1_nl_info {
 struct Model1_nlmsghdr *Model1_nlh;
 struct Model1_net *Model1_nl_net;
 Model1_u32 Model1_portid;
};

int Model1_netlink_rcv_skb(struct Model1_sk_buff *Model1_skb,
      int (*Model1_cb)(struct Model1_sk_buff *, struct Model1_nlmsghdr *));
int Model1_nlmsg_notify(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb, Model1_u32 Model1_portid,
   unsigned int Model1_group, int Model1_report, Model1_gfp_t Model1_flags);

int Model1_nla_validate(const struct Model1_nlattr *Model1_head, int Model1_len, int Model1_maxtype,
   const struct Model1_nla_policy *Model1_policy);
int Model1_nla_parse(struct Model1_nlattr **Model1_tb, int Model1_maxtype, const struct Model1_nlattr *Model1_head,
       int Model1_len, const struct Model1_nla_policy *Model1_policy);
int Model1_nla_policy_len(const struct Model1_nla_policy *, int);
struct Model1_nlattr *Model1_nla_find(const struct Model1_nlattr *Model1_head, int Model1_len, int Model1_attrtype);
Model1_size_t Model1_nla_strlcpy(char *Model1_dst, const struct Model1_nlattr *Model1_nla, Model1_size_t Model1_dstsize);
int Model1_nla_memcpy(void *Model1_dest, const struct Model1_nlattr *Model1_src, int Model1_count);
int Model1_nla_memcmp(const struct Model1_nlattr *Model1_nla, const void *Model1_data, Model1_size_t Model1_size);
int Model1_nla_strcmp(const struct Model1_nlattr *Model1_nla, const char *Model1_str);
struct Model1_nlattr *Model1___nla_reserve(struct Model1_sk_buff *Model1_skb, int Model1_attrtype, int Model1_attrlen);
struct Model1_nlattr *Model1___nla_reserve_64bit(struct Model1_sk_buff *Model1_skb, int Model1_attrtype,
       int Model1_attrlen, int Model1_padattr);
void *Model1___nla_reserve_nohdr(struct Model1_sk_buff *Model1_skb, int Model1_attrlen);
struct Model1_nlattr *Model1_nla_reserve(struct Model1_sk_buff *Model1_skb, int Model1_attrtype, int Model1_attrlen);
struct Model1_nlattr *Model1_nla_reserve_64bit(struct Model1_sk_buff *Model1_skb, int Model1_attrtype,
     int Model1_attrlen, int Model1_padattr);
void *Model1_nla_reserve_nohdr(struct Model1_sk_buff *Model1_skb, int Model1_attrlen);
void Model1___nla_put(struct Model1_sk_buff *Model1_skb, int Model1_attrtype, int Model1_attrlen,
        const void *Model1_data);
void Model1___nla_put_64bit(struct Model1_sk_buff *Model1_skb, int Model1_attrtype, int Model1_attrlen,
       const void *Model1_data, int Model1_padattr);
void Model1___nla_put_nohdr(struct Model1_sk_buff *Model1_skb, int Model1_attrlen, const void *Model1_data);
int Model1_nla_put(struct Model1_sk_buff *Model1_skb, int Model1_attrtype, int Model1_attrlen, const void *Model1_data);
int Model1_nla_put_64bit(struct Model1_sk_buff *Model1_skb, int Model1_attrtype, int Model1_attrlen,
    const void *Model1_data, int Model1_padattr);
int Model1_nla_put_nohdr(struct Model1_sk_buff *Model1_skb, int Model1_attrlen, const void *Model1_data);
int Model1_nla_append(struct Model1_sk_buff *Model1_skb, int Model1_attrlen, const void *Model1_data);

/**************************************************************************
 * Netlink Messages
 **************************************************************************/

/**
 * nlmsg_msg_size - length of netlink message not including padding
 * @payload: length of message payload
 */
static inline __attribute__((no_instrument_function)) int Model1_nlmsg_msg_size(int Model1_payload)
{
 return ((int) ( ((sizeof(struct Model1_nlmsghdr))+4U -1) & ~(4U -1) )) + Model1_payload;
}

/**
 * nlmsg_total_size - length of netlink message including padding
 * @payload: length of message payload
 */
static inline __attribute__((no_instrument_function)) int Model1_nlmsg_total_size(int Model1_payload)
{
 return ( ((Model1_nlmsg_msg_size(Model1_payload))+4U -1) & ~(4U -1) );
}

/**
 * nlmsg_padlen - length of padding at the message's tail
 * @payload: length of message payload
 */
static inline __attribute__((no_instrument_function)) int Model1_nlmsg_padlen(int Model1_payload)
{
 return Model1_nlmsg_total_size(Model1_payload) - Model1_nlmsg_msg_size(Model1_payload);
}

/**
 * nlmsg_data - head of message payload
 * @nlh: netlink message header
 */
static inline __attribute__((no_instrument_function)) void *Model1_nlmsg_data(const struct Model1_nlmsghdr *Model1_nlh)
{
 return (unsigned char *) Model1_nlh + ((int) ( ((sizeof(struct Model1_nlmsghdr))+4U -1) & ~(4U -1) ));
}

/**
 * nlmsg_len - length of message payload
 * @nlh: netlink message header
 */
static inline __attribute__((no_instrument_function)) int Model1_nlmsg_len(const struct Model1_nlmsghdr *Model1_nlh)
{
 return Model1_nlh->Model1_nlmsg_len - ((int) ( ((sizeof(struct Model1_nlmsghdr))+4U -1) & ~(4U -1) ));
}

/**
 * nlmsg_attrdata - head of attributes data
 * @nlh: netlink message header
 * @hdrlen: length of family specific header
 */
static inline __attribute__((no_instrument_function)) struct Model1_nlattr *Model1_nlmsg_attrdata(const struct Model1_nlmsghdr *Model1_nlh,
         int Model1_hdrlen)
{
 unsigned char *Model1_data = Model1_nlmsg_data(Model1_nlh);
 return (struct Model1_nlattr *) (Model1_data + ( ((Model1_hdrlen)+4U -1) & ~(4U -1) ));
}

/**
 * nlmsg_attrlen - length of attributes data
 * @nlh: netlink message header
 * @hdrlen: length of family specific header
 */
static inline __attribute__((no_instrument_function)) int Model1_nlmsg_attrlen(const struct Model1_nlmsghdr *Model1_nlh, int Model1_hdrlen)
{
 return Model1_nlmsg_len(Model1_nlh) - ( ((Model1_hdrlen)+4U -1) & ~(4U -1) );
}

/**
 * nlmsg_ok - check if the netlink message fits into the remaining bytes
 * @nlh: netlink message header
 * @remaining: number of bytes remaining in message stream
 */
static inline __attribute__((no_instrument_function)) int Model1_nlmsg_ok(const struct Model1_nlmsghdr *Model1_nlh, int Model1_remaining)
{
 return (Model1_remaining >= (int) sizeof(struct Model1_nlmsghdr) &&
  Model1_nlh->Model1_nlmsg_len >= sizeof(struct Model1_nlmsghdr) &&
  Model1_nlh->Model1_nlmsg_len <= Model1_remaining);
}

/**
 * nlmsg_next - next netlink message in message stream
 * @nlh: netlink message header
 * @remaining: number of bytes remaining in message stream
 *
 * Returns the next netlink message in the message stream and
 * decrements remaining by the size of the current message.
 */
static inline __attribute__((no_instrument_function)) struct Model1_nlmsghdr *
Model1_nlmsg_next(const struct Model1_nlmsghdr *Model1_nlh, int *Model1_remaining)
{
 int Model1_totlen = ( ((Model1_nlh->Model1_nlmsg_len)+4U -1) & ~(4U -1) );

 *Model1_remaining -= Model1_totlen;

 return (struct Model1_nlmsghdr *) ((unsigned char *) Model1_nlh + Model1_totlen);
}

/**
 * nlmsg_parse - parse attributes of a netlink message
 * @nlh: netlink message header
 * @hdrlen: length of family specific header
 * @tb: destination array with maxtype+1 elements
 * @maxtype: maximum attribute type to be expected
 * @policy: validation policy
 *
 * See nla_parse()
 */
static inline __attribute__((no_instrument_function)) int Model1_nlmsg_parse(const struct Model1_nlmsghdr *Model1_nlh, int Model1_hdrlen,
         struct Model1_nlattr *Model1_tb[], int Model1_maxtype,
         const struct Model1_nla_policy *Model1_policy)
{
 if (Model1_nlh->Model1_nlmsg_len < Model1_nlmsg_msg_size(Model1_hdrlen))
  return -22;

 return Model1_nla_parse(Model1_tb, Model1_maxtype, Model1_nlmsg_attrdata(Model1_nlh, Model1_hdrlen),
    Model1_nlmsg_attrlen(Model1_nlh, Model1_hdrlen), Model1_policy);
}

/**
 * nlmsg_find_attr - find a specific attribute in a netlink message
 * @nlh: netlink message header
 * @hdrlen: length of familiy specific header
 * @attrtype: type of attribute to look for
 *
 * Returns the first attribute which matches the specified type.
 */
static inline __attribute__((no_instrument_function)) struct Model1_nlattr *Model1_nlmsg_find_attr(const struct Model1_nlmsghdr *Model1_nlh,
          int Model1_hdrlen, int Model1_attrtype)
{
 return Model1_nla_find(Model1_nlmsg_attrdata(Model1_nlh, Model1_hdrlen),
   Model1_nlmsg_attrlen(Model1_nlh, Model1_hdrlen), Model1_attrtype);
}

/**
 * nlmsg_validate - validate a netlink message including attributes
 * @nlh: netlinket message header
 * @hdrlen: length of familiy specific header
 * @maxtype: maximum attribute type to be expected
 * @policy: validation policy
 */
static inline __attribute__((no_instrument_function)) int Model1_nlmsg_validate(const struct Model1_nlmsghdr *Model1_nlh,
     int Model1_hdrlen, int Model1_maxtype,
     const struct Model1_nla_policy *Model1_policy)
{
 if (Model1_nlh->Model1_nlmsg_len < Model1_nlmsg_msg_size(Model1_hdrlen))
  return -22;

 return Model1_nla_validate(Model1_nlmsg_attrdata(Model1_nlh, Model1_hdrlen),
       Model1_nlmsg_attrlen(Model1_nlh, Model1_hdrlen), Model1_maxtype, Model1_policy);
}

/**
 * nlmsg_report - need to report back to application?
 * @nlh: netlink message header
 *
 * Returns 1 if a report back to the application is requested.
 */
static inline __attribute__((no_instrument_function)) int Model1_nlmsg_report(const struct Model1_nlmsghdr *Model1_nlh)
{
 return !!(Model1_nlh->Model1_nlmsg_flags & 8);
}

/**
 * nlmsg_for_each_attr - iterate over a stream of attributes
 * @pos: loop counter, set to current attribute
 * @nlh: netlink message header
 * @hdrlen: length of familiy specific header
 * @rem: initialized to len, holds bytes currently remaining in stream
 */




/**
 * nlmsg_put - Add a new netlink message to an skb
 * @skb: socket buffer to store message in
 * @portid: netlink PORTID of requesting application
 * @seq: sequence number of message
 * @type: message type
 * @payload: length of message payload
 * @flags: message flags
 *
 * Returns NULL if the tailroom of the skb is insufficient to store
 * the message header and payload.
 */
static inline __attribute__((no_instrument_function)) struct Model1_nlmsghdr *Model1_nlmsg_put(struct Model1_sk_buff *Model1_skb, Model1_u32 Model1_portid, Model1_u32 Model1_seq,
      int Model1_type, int Model1_payload, int Model1_flags)
{
 if (__builtin_expect(!!(Model1_skb_tailroom(Model1_skb) < Model1_nlmsg_total_size(Model1_payload)), 0))
  return ((void *)0);

 return Model1___nlmsg_put(Model1_skb, Model1_portid, Model1_seq, Model1_type, Model1_payload, Model1_flags);
}

/**
 * nlmsg_put_answer - Add a new callback based netlink message to an skb
 * @skb: socket buffer to store message in
 * @cb: netlink callback
 * @type: message type
 * @payload: length of message payload
 * @flags: message flags
 *
 * Returns NULL if the tailroom of the skb is insufficient to store
 * the message header and payload.
 */
static inline __attribute__((no_instrument_function)) struct Model1_nlmsghdr *Model1_nlmsg_put_answer(struct Model1_sk_buff *Model1_skb,
      struct Model1_netlink_callback *Model1_cb,
      int Model1_type, int Model1_payload,
      int Model1_flags)
{
 return Model1_nlmsg_put(Model1_skb, (*(struct Model1_netlink_skb_parms*)&((Model1_cb->Model1_skb)->Model1_cb)).Model1_portid, Model1_cb->Model1_nlh->Model1_nlmsg_seq,
    Model1_type, Model1_payload, Model1_flags);
}

/**
 * nlmsg_new - Allocate a new netlink message
 * @payload: size of the message payload
 * @flags: the type of memory to allocate.
 *
 * Use NLMSG_DEFAULT_SIZE if the size of the payload isn't known
 * and a good default is needed.
 */
static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1_nlmsg_new(Model1_size_t Model1_payload, Model1_gfp_t Model1_flags)
{
 return Model1_alloc_skb(Model1_nlmsg_total_size(Model1_payload), Model1_flags);
}

/**
 * nlmsg_end - Finalize a netlink message
 * @skb: socket buffer the message is stored in
 * @nlh: netlink message header
 *
 * Corrects the netlink message header to include the appeneded
 * attributes. Only necessary if attributes have been added to
 * the message.
 */
static inline __attribute__((no_instrument_function)) void Model1_nlmsg_end(struct Model1_sk_buff *Model1_skb, struct Model1_nlmsghdr *Model1_nlh)
{
 Model1_nlh->Model1_nlmsg_len = Model1_skb_tail_pointer(Model1_skb) - (unsigned char *)Model1_nlh;
}

/**
 * nlmsg_get_pos - return current position in netlink message
 * @skb: socket buffer the message is stored in
 *
 * Returns a pointer to the current tail of the message.
 */
static inline __attribute__((no_instrument_function)) void *Model1_nlmsg_get_pos(struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb_tail_pointer(Model1_skb);
}

/**
 * nlmsg_trim - Trim message to a mark
 * @skb: socket buffer the message is stored in
 * @mark: mark to trim to
 *
 * Trims the message to the provided mark.
 */
static inline __attribute__((no_instrument_function)) void Model1_nlmsg_trim(struct Model1_sk_buff *Model1_skb, const void *Model1_mark)
{
 if (Model1_mark) {
  ({ int Model1___ret_warn_on = !!((unsigned char *) Model1_mark < Model1_skb->Model1_data); if (__builtin_expect(!!(Model1___ret_warn_on), 0)) Model1_warn_slowpath_null("./include/net/netlink.h", 534); __builtin_expect(!!(Model1___ret_warn_on), 0); });
  Model1_skb_trim(Model1_skb, (unsigned char *) Model1_mark - Model1_skb->Model1_data);
 }
}

/**
 * nlmsg_cancel - Cancel construction of a netlink message
 * @skb: socket buffer the message is stored in
 * @nlh: netlink message header
 *
 * Removes the complete netlink message including all
 * attributes from the socket buffer again.
 */
static inline __attribute__((no_instrument_function)) void Model1_nlmsg_cancel(struct Model1_sk_buff *Model1_skb, struct Model1_nlmsghdr *Model1_nlh)
{
 Model1_nlmsg_trim(Model1_skb, Model1_nlh);
}

/**
 * nlmsg_free - free a netlink message
 * @skb: socket buffer of netlink message
 */
static inline __attribute__((no_instrument_function)) void Model1_nlmsg_free(struct Model1_sk_buff *Model1_skb)
{
 Model1_kfree_skb(Model1_skb);
}

/**
 * nlmsg_multicast - multicast a netlink message
 * @sk: netlink socket to spread messages to
 * @skb: netlink message as socket buffer
 * @portid: own netlink portid to avoid sending to yourself
 * @group: multicast group id
 * @flags: allocation flags
 */
static inline __attribute__((no_instrument_function)) int Model1_nlmsg_multicast(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb,
      Model1_u32 Model1_portid, unsigned int Model1_group, Model1_gfp_t Model1_flags)
{
 int err;

 (*(struct Model1_netlink_skb_parms*)&((Model1_skb)->Model1_cb)).Model1_dst_group = Model1_group;

 err = Model1_netlink_broadcast(Model1_sk, Model1_skb, Model1_portid, Model1_group, Model1_flags);
 if (err > 0)
  err = 0;

 return err;
}

/**
 * nlmsg_unicast - unicast a netlink message
 * @sk: netlink socket to spread message to
 * @skb: netlink message as socket buffer
 * @portid: netlink portid of the destination socket
 */
static inline __attribute__((no_instrument_function)) int Model1_nlmsg_unicast(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb, Model1_u32 Model1_portid)
{
 int err;

 err = Model1_netlink_unicast(Model1_sk, Model1_skb, Model1_portid, 0x40);
 if (err > 0)
  err = 0;

 return err;
}

/**
 * nlmsg_for_each_msg - iterate over a stream of messages
 * @pos: loop counter, set to current message
 * @head: head of message stream
 * @len: length of message stream
 * @rem: initialized to len, holds bytes currently remaining in stream
 */





/**
 * nl_dump_check_consistent - check if sequence is consistent and advertise if not
 * @cb: netlink callback structure that stores the sequence number
 * @nlh: netlink message header to write the flag to
 *
 * This function checks if the sequence (generation) number changed during dump
 * and if it did, advertises it in the netlink message header.
 *
 * The correct way to use it is to set cb->seq to the generation counter when
 * all locks for dumping have been acquired, and then call this function for
 * each message that is generated.
 *
 * Note that due to initialisation concerns, 0 is an invalid sequence number
 * and must not be used by code that uses this functionality.
 */
static inline __attribute__((no_instrument_function)) void
Model1_nl_dump_check_consistent(struct Model1_netlink_callback *Model1_cb,
    struct Model1_nlmsghdr *Model1_nlh)
{
 if (Model1_cb->Model1_prev_seq && Model1_cb->Model1_seq != Model1_cb->Model1_prev_seq)
  Model1_nlh->Model1_nlmsg_flags |= 16;
 Model1_cb->Model1_prev_seq = Model1_cb->Model1_seq;
}

/**************************************************************************
 * Netlink Attributes
 **************************************************************************/

/**
 * nla_attr_size - length of attribute not including padding
 * @payload: length of payload
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_attr_size(int Model1_payload)
{
 return ((int) (((sizeof(struct Model1_nlattr)) + 4 - 1) & ~(4 - 1))) + Model1_payload;
}

/**
 * nla_total_size - total length of attribute including padding
 * @payload: length of payload
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_total_size(int Model1_payload)
{
 return (((Model1_nla_attr_size(Model1_payload)) + 4 - 1) & ~(4 - 1));
}

/**
 * nla_padlen - length of padding at the tail of attribute
 * @payload: length of payload
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_padlen(int Model1_payload)
{
 return Model1_nla_total_size(Model1_payload) - Model1_nla_attr_size(Model1_payload);
}

/**
 * nla_type - attribute type
 * @nla: netlink attribute
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_type(const struct Model1_nlattr *Model1_nla)
{
 return Model1_nla->Model1_nla_type & ~((1 << 15) | (1 << 14));
}

/**
 * nla_data - head of payload
 * @nla: netlink attribute
 */
static inline __attribute__((no_instrument_function)) void *Model1_nla_data(const struct Model1_nlattr *Model1_nla)
{
 return (char *) Model1_nla + ((int) (((sizeof(struct Model1_nlattr)) + 4 - 1) & ~(4 - 1)));
}

/**
 * nla_len - length of payload
 * @nla: netlink attribute
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_len(const struct Model1_nlattr *Model1_nla)
{
 return Model1_nla->Model1_nla_len - ((int) (((sizeof(struct Model1_nlattr)) + 4 - 1) & ~(4 - 1)));
}

/**
 * nla_ok - check if the netlink attribute fits into the remaining bytes
 * @nla: netlink attribute
 * @remaining: number of bytes remaining in attribute stream
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_ok(const struct Model1_nlattr *Model1_nla, int Model1_remaining)
{
 return Model1_remaining >= (int) sizeof(*Model1_nla) &&
        Model1_nla->Model1_nla_len >= sizeof(*Model1_nla) &&
        Model1_nla->Model1_nla_len <= Model1_remaining;
}

/**
 * nla_next - next netlink attribute in attribute stream
 * @nla: netlink attribute
 * @remaining: number of bytes remaining in attribute stream
 *
 * Returns the next netlink attribute in the attribute stream and
 * decrements remaining by the size of the current attribute.
 */
static inline __attribute__((no_instrument_function)) struct Model1_nlattr *Model1_nla_next(const struct Model1_nlattr *Model1_nla, int *Model1_remaining)
{
 int Model1_totlen = (((Model1_nla->Model1_nla_len) + 4 - 1) & ~(4 - 1));

 *Model1_remaining -= Model1_totlen;
 return (struct Model1_nlattr *) ((char *) Model1_nla + Model1_totlen);
}

/**
 * nla_find_nested - find attribute in a set of nested attributes
 * @nla: attribute containing the nested attributes
 * @attrtype: type of attribute to look for
 *
 * Returns the first attribute which matches the specified type.
 */
static inline __attribute__((no_instrument_function)) struct Model1_nlattr *
Model1_nla_find_nested(const struct Model1_nlattr *Model1_nla, int Model1_attrtype)
{
 return Model1_nla_find(Model1_nla_data(Model1_nla), Model1_nla_len(Model1_nla), Model1_attrtype);
}

/**
 * nla_parse_nested - parse nested attributes
 * @tb: destination array with maxtype+1 elements
 * @maxtype: maximum attribute type to be expected
 * @nla: attribute containing the nested attributes
 * @policy: validation policy
 *
 * See nla_parse()
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_parse_nested(struct Model1_nlattr *Model1_tb[], int Model1_maxtype,
       const struct Model1_nlattr *Model1_nla,
       const struct Model1_nla_policy *Model1_policy)
{
 return Model1_nla_parse(Model1_tb, Model1_maxtype, Model1_nla_data(Model1_nla), Model1_nla_len(Model1_nla), Model1_policy);
}

/**
 * nla_put_u8 - Add a u8 netlink attribute to a socket buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_put_u8(struct Model1_sk_buff *Model1_skb, int Model1_attrtype, Model1_u8 Model1_value)
{
 return Model1_nla_put(Model1_skb, Model1_attrtype, sizeof(Model1_u8), &Model1_value);
}

/**
 * nla_put_u16 - Add a u16 netlink attribute to a socket buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_put_u16(struct Model1_sk_buff *Model1_skb, int Model1_attrtype, Model1_u16 Model1_value)
{
 return Model1_nla_put(Model1_skb, Model1_attrtype, sizeof(Model1_u16), &Model1_value);
}

/**
 * nla_put_be16 - Add a __be16 netlink attribute to a socket buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_put_be16(struct Model1_sk_buff *Model1_skb, int Model1_attrtype, Model1___be16 Model1_value)
{
 return Model1_nla_put(Model1_skb, Model1_attrtype, sizeof(Model1___be16), &Model1_value);
}

/**
 * nla_put_net16 - Add 16-bit network byte order netlink attribute to a socket buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_put_net16(struct Model1_sk_buff *Model1_skb, int Model1_attrtype, Model1___be16 Model1_value)
{
 return Model1_nla_put_be16(Model1_skb, Model1_attrtype | (1 << 14), Model1_value);
}

/**
 * nla_put_le16 - Add a __le16 netlink attribute to a socket buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_put_le16(struct Model1_sk_buff *Model1_skb, int Model1_attrtype, Model1___le16 Model1_value)
{
 return Model1_nla_put(Model1_skb, Model1_attrtype, sizeof(Model1___le16), &Model1_value);
}

/**
 * nla_put_u32 - Add a u32 netlink attribute to a socket buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_put_u32(struct Model1_sk_buff *Model1_skb, int Model1_attrtype, Model1_u32 Model1_value)
{
 return Model1_nla_put(Model1_skb, Model1_attrtype, sizeof(Model1_u32), &Model1_value);
}

/**
 * nla_put_be32 - Add a __be32 netlink attribute to a socket buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_put_be32(struct Model1_sk_buff *Model1_skb, int Model1_attrtype, Model1___be32 Model1_value)
{
 return Model1_nla_put(Model1_skb, Model1_attrtype, sizeof(Model1___be32), &Model1_value);
}

/**
 * nla_put_net32 - Add 32-bit network byte order netlink attribute to a socket buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_put_net32(struct Model1_sk_buff *Model1_skb, int Model1_attrtype, Model1___be32 Model1_value)
{
 return Model1_nla_put_be32(Model1_skb, Model1_attrtype | (1 << 14), Model1_value);
}

/**
 * nla_put_le32 - Add a __le32 netlink attribute to a socket buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_put_le32(struct Model1_sk_buff *Model1_skb, int Model1_attrtype, Model1___le32 Model1_value)
{
 return Model1_nla_put(Model1_skb, Model1_attrtype, sizeof(Model1___le32), &Model1_value);
}

/**
 * nla_put_u64_64bit - Add a u64 netlink attribute to a skb and align it
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 * @padattr: attribute type for the padding
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_put_u64_64bit(struct Model1_sk_buff *Model1_skb, int Model1_attrtype,
        Model1_u64 Model1_value, int Model1_padattr)
{
 return Model1_nla_put_64bit(Model1_skb, Model1_attrtype, sizeof(Model1_u64), &Model1_value, Model1_padattr);
}

/**
 * nla_put_be64 - Add a __be64 netlink attribute to a socket buffer and align it
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 * @padattr: attribute type for the padding
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_put_be64(struct Model1_sk_buff *Model1_skb, int Model1_attrtype, Model1___be64 Model1_value,
          int Model1_padattr)
{
 return Model1_nla_put_64bit(Model1_skb, Model1_attrtype, sizeof(Model1___be64), &Model1_value, Model1_padattr);
}

/**
 * nla_put_net64 - Add 64-bit network byte order nlattr to a skb and align it
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 * @padattr: attribute type for the padding
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_put_net64(struct Model1_sk_buff *Model1_skb, int Model1_attrtype, Model1___be64 Model1_value,
    int Model1_padattr)
{
 return Model1_nla_put_be64(Model1_skb, Model1_attrtype | (1 << 14), Model1_value,
       Model1_padattr);
}

/**
 * nla_put_le64 - Add a __le64 netlink attribute to a socket buffer and align it
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 * @padattr: attribute type for the padding
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_put_le64(struct Model1_sk_buff *Model1_skb, int Model1_attrtype, Model1___le64 Model1_value,
          int Model1_padattr)
{
 return Model1_nla_put_64bit(Model1_skb, Model1_attrtype, sizeof(Model1___le64), &Model1_value, Model1_padattr);
}

/**
 * nla_put_s8 - Add a s8 netlink attribute to a socket buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_put_s8(struct Model1_sk_buff *Model1_skb, int Model1_attrtype, Model1_s8 Model1_value)
{
 return Model1_nla_put(Model1_skb, Model1_attrtype, sizeof(Model1_s8), &Model1_value);
}

/**
 * nla_put_s16 - Add a s16 netlink attribute to a socket buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_put_s16(struct Model1_sk_buff *Model1_skb, int Model1_attrtype, Model1_s16 Model1_value)
{
 return Model1_nla_put(Model1_skb, Model1_attrtype, sizeof(Model1_s16), &Model1_value);
}

/**
 * nla_put_s32 - Add a s32 netlink attribute to a socket buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_put_s32(struct Model1_sk_buff *Model1_skb, int Model1_attrtype, Model1_s32 Model1_value)
{
 return Model1_nla_put(Model1_skb, Model1_attrtype, sizeof(Model1_s32), &Model1_value);
}

/**
 * nla_put_s64 - Add a s64 netlink attribute to a socket buffer and align it
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @value: numeric value
 * @padattr: attribute type for the padding
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_put_s64(struct Model1_sk_buff *Model1_skb, int Model1_attrtype, Model1_s64 Model1_value,
         int Model1_padattr)
{
 return Model1_nla_put_64bit(Model1_skb, Model1_attrtype, sizeof(Model1_s64), &Model1_value, Model1_padattr);
}

/**
 * nla_put_string - Add a string netlink attribute to a socket buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @str: NUL terminated string
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_put_string(struct Model1_sk_buff *Model1_skb, int Model1_attrtype,
     const char *Model1_str)
{
 return Model1_nla_put(Model1_skb, Model1_attrtype, Model1_strlen(Model1_str) + 1, Model1_str);
}

/**
 * nla_put_flag - Add a flag netlink attribute to a socket buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_put_flag(struct Model1_sk_buff *Model1_skb, int Model1_attrtype)
{
 return Model1_nla_put(Model1_skb, Model1_attrtype, 0, ((void *)0));
}

/**
 * nla_put_msecs - Add a msecs netlink attribute to a skb and align it
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @njiffies: number of jiffies to convert to msecs
 * @padattr: attribute type for the padding
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_put_msecs(struct Model1_sk_buff *Model1_skb, int Model1_attrtype,
    unsigned long Model1_njiffies, int Model1_padattr)
{
 Model1_u64 Model1_tmp = Model1_jiffies_to_msecs(Model1_njiffies);

 return Model1_nla_put_64bit(Model1_skb, Model1_attrtype, sizeof(Model1_u64), &Model1_tmp, Model1_padattr);
}

/**
 * nla_put_in_addr - Add an IPv4 address netlink attribute to a socket
 * buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @addr: IPv4 address
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_put_in_addr(struct Model1_sk_buff *Model1_skb, int Model1_attrtype,
      Model1___be32 Model1_addr)
{
 return Model1_nla_put_be32(Model1_skb, Model1_attrtype, Model1_addr);
}

/**
 * nla_put_in6_addr - Add an IPv6 address netlink attribute to a socket
 * buffer
 * @skb: socket buffer to add attribute to
 * @attrtype: attribute type
 * @addr: IPv6 address
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_put_in6_addr(struct Model1_sk_buff *Model1_skb, int Model1_attrtype,
       const struct Model1_in6_addr *Model1_addr)
{
 return Model1_nla_put(Model1_skb, Model1_attrtype, sizeof(*Model1_addr), Model1_addr);
}

/**
 * nla_get_u32 - return payload of u32 attribute
 * @nla: u32 netlink attribute
 */
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_nla_get_u32(const struct Model1_nlattr *Model1_nla)
{
 return *(Model1_u32 *) Model1_nla_data(Model1_nla);
}

/**
 * nla_get_be32 - return payload of __be32 attribute
 * @nla: __be32 netlink attribute
 */
static inline __attribute__((no_instrument_function)) Model1___be32 Model1_nla_get_be32(const struct Model1_nlattr *Model1_nla)
{
 return *(Model1___be32 *) Model1_nla_data(Model1_nla);
}

/**
 * nla_get_le32 - return payload of __le32 attribute
 * @nla: __le32 netlink attribute
 */
static inline __attribute__((no_instrument_function)) Model1___le32 Model1_nla_get_le32(const struct Model1_nlattr *Model1_nla)
{
 return *(Model1___le32 *) Model1_nla_data(Model1_nla);
}

/**
 * nla_get_u16 - return payload of u16 attribute
 * @nla: u16 netlink attribute
 */
static inline __attribute__((no_instrument_function)) Model1_u16 Model1_nla_get_u16(const struct Model1_nlattr *Model1_nla)
{
 return *(Model1_u16 *) Model1_nla_data(Model1_nla);
}

/**
 * nla_get_be16 - return payload of __be16 attribute
 * @nla: __be16 netlink attribute
 */
static inline __attribute__((no_instrument_function)) Model1___be16 Model1_nla_get_be16(const struct Model1_nlattr *Model1_nla)
{
 return *(Model1___be16 *) Model1_nla_data(Model1_nla);
}

/**
 * nla_get_le16 - return payload of __le16 attribute
 * @nla: __le16 netlink attribute
 */
static inline __attribute__((no_instrument_function)) Model1___le16 Model1_nla_get_le16(const struct Model1_nlattr *Model1_nla)
{
 return *(Model1___le16 *) Model1_nla_data(Model1_nla);
}

/**
 * nla_get_u8 - return payload of u8 attribute
 * @nla: u8 netlink attribute
 */
static inline __attribute__((no_instrument_function)) Model1_u8 Model1_nla_get_u8(const struct Model1_nlattr *Model1_nla)
{
 return *(Model1_u8 *) Model1_nla_data(Model1_nla);
}

/**
 * nla_get_u64 - return payload of u64 attribute
 * @nla: u64 netlink attribute
 */
static inline __attribute__((no_instrument_function)) Model1_u64 Model1_nla_get_u64(const struct Model1_nlattr *Model1_nla)
{
 Model1_u64 Model1_tmp;

 Model1_nla_memcpy(&Model1_tmp, Model1_nla, sizeof(Model1_tmp));

 return Model1_tmp;
}

/**
 * nla_get_be64 - return payload of __be64 attribute
 * @nla: __be64 netlink attribute
 */
static inline __attribute__((no_instrument_function)) Model1___be64 Model1_nla_get_be64(const struct Model1_nlattr *Model1_nla)
{
 Model1___be64 Model1_tmp;

 Model1_nla_memcpy(&Model1_tmp, Model1_nla, sizeof(Model1_tmp));

 return Model1_tmp;
}

/**
 * nla_get_le64 - return payload of __le64 attribute
 * @nla: __le64 netlink attribute
 */
static inline __attribute__((no_instrument_function)) Model1___le64 Model1_nla_get_le64(const struct Model1_nlattr *Model1_nla)
{
 return *(Model1___le64 *) Model1_nla_data(Model1_nla);
}

/**
 * nla_get_s32 - return payload of s32 attribute
 * @nla: s32 netlink attribute
 */
static inline __attribute__((no_instrument_function)) Model1_s32 Model1_nla_get_s32(const struct Model1_nlattr *Model1_nla)
{
 return *(Model1_s32 *) Model1_nla_data(Model1_nla);
}

/**
 * nla_get_s16 - return payload of s16 attribute
 * @nla: s16 netlink attribute
 */
static inline __attribute__((no_instrument_function)) Model1_s16 Model1_nla_get_s16(const struct Model1_nlattr *Model1_nla)
{
 return *(Model1_s16 *) Model1_nla_data(Model1_nla);
}

/**
 * nla_get_s8 - return payload of s8 attribute
 * @nla: s8 netlink attribute
 */
static inline __attribute__((no_instrument_function)) Model1_s8 Model1_nla_get_s8(const struct Model1_nlattr *Model1_nla)
{
 return *(Model1_s8 *) Model1_nla_data(Model1_nla);
}

/**
 * nla_get_s64 - return payload of s64 attribute
 * @nla: s64 netlink attribute
 */
static inline __attribute__((no_instrument_function)) Model1_s64 Model1_nla_get_s64(const struct Model1_nlattr *Model1_nla)
{
 Model1_s64 Model1_tmp;

 Model1_nla_memcpy(&Model1_tmp, Model1_nla, sizeof(Model1_tmp));

 return Model1_tmp;
}

/**
 * nla_get_flag - return payload of flag attribute
 * @nla: flag netlink attribute
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_get_flag(const struct Model1_nlattr *Model1_nla)
{
 return !!Model1_nla;
}

/**
 * nla_get_msecs - return payload of msecs attribute
 * @nla: msecs netlink attribute
 *
 * Returns the number of milliseconds in jiffies.
 */
static inline __attribute__((no_instrument_function)) unsigned long Model1_nla_get_msecs(const struct Model1_nlattr *Model1_nla)
{
 Model1_u64 Model1_msecs = Model1_nla_get_u64(Model1_nla);

 return Model1_msecs_to_jiffies((unsigned long) Model1_msecs);
}

/**
 * nla_get_in_addr - return payload of IPv4 address attribute
 * @nla: IPv4 address netlink attribute
 */
static inline __attribute__((no_instrument_function)) Model1___be32 Model1_nla_get_in_addr(const struct Model1_nlattr *Model1_nla)
{
 return *(Model1___be32 *) Model1_nla_data(Model1_nla);
}

/**
 * nla_get_in6_addr - return payload of IPv6 address attribute
 * @nla: IPv6 address netlink attribute
 */
static inline __attribute__((no_instrument_function)) struct Model1_in6_addr Model1_nla_get_in6_addr(const struct Model1_nlattr *Model1_nla)
{
 struct Model1_in6_addr Model1_tmp;

 Model1_nla_memcpy(&Model1_tmp, Model1_nla, sizeof(Model1_tmp));
 return Model1_tmp;
}

/**
 * nla_nest_start - Start a new level of nested attributes
 * @skb: socket buffer to add attributes to
 * @attrtype: attribute type of container
 *
 * Returns the container attribute
 */
static inline __attribute__((no_instrument_function)) struct Model1_nlattr *Model1_nla_nest_start(struct Model1_sk_buff *Model1_skb, int Model1_attrtype)
{
 struct Model1_nlattr *Model1_start = (struct Model1_nlattr *)Model1_skb_tail_pointer(Model1_skb);

 if (Model1_nla_put(Model1_skb, Model1_attrtype, 0, ((void *)0)) < 0)
  return ((void *)0);

 return Model1_start;
}

/**
 * nla_nest_end - Finalize nesting of attributes
 * @skb: socket buffer the attributes are stored in
 * @start: container attribute
 *
 * Corrects the container attribute header to include the all
 * appeneded attributes.
 *
 * Returns the total data length of the skb.
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_nest_end(struct Model1_sk_buff *Model1_skb, struct Model1_nlattr *Model1_start)
{
 Model1_start->Model1_nla_len = Model1_skb_tail_pointer(Model1_skb) - (unsigned char *)Model1_start;
 return Model1_skb->Model1_len;
}

/**
 * nla_nest_cancel - Cancel nesting of attributes
 * @skb: socket buffer the message is stored in
 * @start: container attribute
 *
 * Removes the container attribute and including all nested
 * attributes. Returns -EMSGSIZE
 */
static inline __attribute__((no_instrument_function)) void Model1_nla_nest_cancel(struct Model1_sk_buff *Model1_skb, struct Model1_nlattr *Model1_start)
{
 Model1_nlmsg_trim(Model1_skb, Model1_start);
}

/**
 * nla_validate_nested - Validate a stream of nested attributes
 * @start: container attribute
 * @maxtype: maximum attribute type to be expected
 * @policy: validation policy
 *
 * Validates all attributes in the nested attribute stream against the
 * specified policy. Attributes with a type exceeding maxtype will be
 * ignored. See documenation of struct nla_policy for more details.
 *
 * Returns 0 on success or a negative error code.
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_validate_nested(const struct Model1_nlattr *Model1_start, int Model1_maxtype,
          const struct Model1_nla_policy *Model1_policy)
{
 return Model1_nla_validate(Model1_nla_data(Model1_start), Model1_nla_len(Model1_start), Model1_maxtype, Model1_policy);
}

/**
 * nla_need_padding_for_64bit - test 64-bit alignment of the next attribute
 * @skb: socket buffer the message is stored in
 *
 * Return true if padding is needed to align the next attribute (nla_data()) to
 * a 64-bit aligned area.
 */
static inline __attribute__((no_instrument_function)) bool Model1_nla_need_padding_for_64bit(struct Model1_sk_buff *Model1_skb)
{
 return false;
}

/**
 * nla_align_64bit - 64-bit align the nla_data() of next attribute
 * @skb: socket buffer the message is stored in
 * @padattr: attribute type for the padding
 *
 * Conditionally emit a padding netlink attribute in order to make
 * the next attribute we emit have a 64-bit aligned nla_data() area.
 * This will only be done in architectures which do not have
 * CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS defined.
 *
 * Returns zero on success or a negative error code.
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_align_64bit(struct Model1_sk_buff *Model1_skb, int Model1_padattr)
{
 if (Model1_nla_need_padding_for_64bit(Model1_skb) &&
     !Model1_nla_reserve(Model1_skb, Model1_padattr, 0))
  return -90;

 return 0;
}

/**
 * nla_total_size_64bit - total length of attribute including padding
 * @payload: length of payload
 */
static inline __attribute__((no_instrument_function)) int Model1_nla_total_size_64bit(int Model1_payload)
{
 return (((Model1_nla_attr_size(Model1_payload)) + 4 - 1) & ~(4 - 1))



  ;
}

/**
 * nla_for_each_attr - iterate over a stream of attributes
 * @pos: loop counter, set to current attribute
 * @head: head of attribute stream
 * @len: length of attribute stream
 * @rem: initialized to len, holds bytes currently remaining in stream
 */





/**
 * nla_for_each_nested - iterate over nested attributes
 * @pos: loop counter, set to current attribute
 * @nla: attribute containing the nested attributes
 * @rem: initialized to len, holds bytes currently remaining in stream
 */



/**
 * nla_is_last - Test if attribute is last in stream
 * @nla: attribute to test
 * @rem: bytes remaining in stream
 */
static inline __attribute__((no_instrument_function)) bool Model1_nla_is_last(const struct Model1_nlattr *Model1_nla, int Model1_rem)
{
 return Model1_nla->Model1_nla_len == Model1_rem;
}

typedef int (*Model1_rtnl_doit_func)(struct Model1_sk_buff *, struct Model1_nlmsghdr *);
typedef int (*Model1_rtnl_dumpit_func)(struct Model1_sk_buff *, struct Model1_netlink_callback *);
typedef Model1_u16 (*Model1_rtnl_calcit_func)(struct Model1_sk_buff *, struct Model1_nlmsghdr *);

int Model1___rtnl_register(int Model1_protocol, int Model1_msgtype,
      Model1_rtnl_doit_func, Model1_rtnl_dumpit_func, Model1_rtnl_calcit_func);
void Model1_rtnl_register(int Model1_protocol, int Model1_msgtype,
     Model1_rtnl_doit_func, Model1_rtnl_dumpit_func, Model1_rtnl_calcit_func);
int Model1_rtnl_unregister(int Model1_protocol, int Model1_msgtype);
void Model1_rtnl_unregister_all(int Model1_protocol);

static inline __attribute__((no_instrument_function)) int Model1_rtnl_msg_family(const struct Model1_nlmsghdr *Model1_nlh)
{
 if (Model1_nlmsg_len(Model1_nlh) >= sizeof(struct Model1_rtgenmsg))
  return ((struct Model1_rtgenmsg *) Model1_nlmsg_data(Model1_nlh))->Model1_rtgen_family;
 else
  return 0;
}

/**
 *	struct rtnl_link_ops - rtnetlink link operations
 *
 *	@list: Used internally
 *	@kind: Identifier
 *	@maxtype: Highest device specific netlink attribute number
 *	@policy: Netlink policy for device specific attribute validation
 *	@validate: Optional validation function for netlink/changelink parameters
 *	@priv_size: sizeof net_device private space
 *	@setup: net_device setup function
 *	@newlink: Function for configuring and registering a new device
 *	@changelink: Function for changing parameters of an existing device
 *	@dellink: Function to remove a device
 *	@get_size: Function to calculate required room for dumping device
 *		   specific netlink attributes
 *	@fill_info: Function to dump device specific netlink attributes
 *	@get_xstats_size: Function to calculate required room for dumping device
 *			  specific statistics
 *	@fill_xstats: Function to dump device specific statistics
 *	@get_num_tx_queues: Function to determine number of transmit queues
 *			    to create when creating a new device.
 *	@get_num_rx_queues: Function to determine number of receive queues
 *			    to create when creating a new device.
 *	@get_link_net: Function to get the i/o netns of the device
 *	@get_linkxstats_size: Function to calculate the required room for
 *			      dumping device-specific extended link stats
 *	@fill_linkxstats: Function to dump device-specific extended link stats
 */
struct Model1_rtnl_link_ops {
 struct Model1_list_head Model1_list;

 const char *Model1_kind;

 Model1_size_t Model1_priv_size;
 void (*Model1_setup)(struct Model1_net_device *Model1_dev);

 int Model1_maxtype;
 const struct Model1_nla_policy *Model1_policy;
 int (*Model1_validate)(struct Model1_nlattr *Model1_tb[],
         struct Model1_nlattr *Model1_data[]);

 int (*Model1_newlink)(struct Model1_net *Model1_src_net,
        struct Model1_net_device *Model1_dev,
        struct Model1_nlattr *Model1_tb[],
        struct Model1_nlattr *Model1_data[]);
 int (*Model1_changelink)(struct Model1_net_device *Model1_dev,
           struct Model1_nlattr *Model1_tb[],
           struct Model1_nlattr *Model1_data[]);
 void (*Model1_dellink)(struct Model1_net_device *Model1_dev,
        struct Model1_list_head *Model1_head);

 Model1_size_t (*Model1_get_size)(const struct Model1_net_device *Model1_dev);
 int (*Model1_fill_info)(struct Model1_sk_buff *Model1_skb,
          const struct Model1_net_device *Model1_dev);

 Model1_size_t (*Model1_get_xstats_size)(const struct Model1_net_device *Model1_dev);
 int (*Model1_fill_xstats)(struct Model1_sk_buff *Model1_skb,
            const struct Model1_net_device *Model1_dev);
 unsigned int (*Model1_get_num_tx_queues)(void);
 unsigned int (*Model1_get_num_rx_queues)(void);

 int Model1_slave_maxtype;
 const struct Model1_nla_policy *Model1_slave_policy;
 int (*Model1_slave_validate)(struct Model1_nlattr *Model1_tb[],
        struct Model1_nlattr *Model1_data[]);
 int (*Model1_slave_changelink)(struct Model1_net_device *Model1_dev,
          struct Model1_net_device *Model1_slave_dev,
          struct Model1_nlattr *Model1_tb[],
          struct Model1_nlattr *Model1_data[]);
 Model1_size_t (*Model1_get_slave_size)(const struct Model1_net_device *Model1_dev,
        const struct Model1_net_device *Model1_slave_dev);
 int (*Model1_fill_slave_info)(struct Model1_sk_buff *Model1_skb,
         const struct Model1_net_device *Model1_dev,
         const struct Model1_net_device *Model1_slave_dev);
 struct Model1_net *(*Model1_get_link_net)(const struct Model1_net_device *Model1_dev);
 Model1_size_t (*Model1_get_linkxstats_size)(const struct Model1_net_device *Model1_dev,
             int Model1_attr);
 int (*Model1_fill_linkxstats)(struct Model1_sk_buff *Model1_skb,
         const struct Model1_net_device *Model1_dev,
         int *Model1_prividx, int Model1_attr);
};

int Model1___rtnl_link_register(struct Model1_rtnl_link_ops *Model1_ops);
void Model1___rtnl_link_unregister(struct Model1_rtnl_link_ops *Model1_ops);

int Model1_rtnl_link_register(struct Model1_rtnl_link_ops *Model1_ops);
void Model1_rtnl_link_unregister(struct Model1_rtnl_link_ops *Model1_ops);

/**
 * 	struct rtnl_af_ops - rtnetlink address family operations
 *
 *	@list: Used internally
 * 	@family: Address family
 * 	@fill_link_af: Function to fill IFLA_AF_SPEC with address family
 * 		       specific netlink attributes.
 * 	@get_link_af_size: Function to calculate size of address family specific
 * 			   netlink attributes.
 *	@validate_link_af: Validate a IFLA_AF_SPEC attribute, must check attr
 *			   for invalid configuration settings.
 * 	@set_link_af: Function to parse a IFLA_AF_SPEC attribute and modify
 *		      net_device accordingly.
 */
struct Model1_rtnl_af_ops {
 struct Model1_list_head Model1_list;
 int Model1_family;

 int (*Model1_fill_link_af)(struct Model1_sk_buff *Model1_skb,
      const struct Model1_net_device *Model1_dev,
      Model1_u32 Model1_ext_filter_mask);
 Model1_size_t (*Model1_get_link_af_size)(const struct Model1_net_device *Model1_dev,
          Model1_u32 Model1_ext_filter_mask);

 int (*Model1_validate_link_af)(const struct Model1_net_device *Model1_dev,
          const struct Model1_nlattr *Model1_attr);
 int (*Model1_set_link_af)(struct Model1_net_device *Model1_dev,
            const struct Model1_nlattr *Model1_attr);
};

void Model1___rtnl_af_unregister(struct Model1_rtnl_af_ops *Model1_ops);

void Model1_rtnl_af_register(struct Model1_rtnl_af_ops *Model1_ops);
void Model1_rtnl_af_unregister(struct Model1_rtnl_af_ops *Model1_ops);

struct Model1_net *Model1_rtnl_link_get_net(struct Model1_net *Model1_src_net, struct Model1_nlattr *Model1_tb[]);
struct Model1_net_device *Model1_rtnl_create_link(struct Model1_net *Model1_net, const char *Model1_ifname,
        unsigned char Model1_name_assign_type,
        const struct Model1_rtnl_link_ops *Model1_ops,
        struct Model1_nlattr *Model1_tb[]);
int Model1_rtnl_delete_link(struct Model1_net_device *Model1_dev);
int Model1_rtnl_configure_link(struct Model1_net_device *Model1_dev, const struct Model1_ifinfomsg *Model1_ifm);

int Model1_rtnl_nla_parse_ifla(struct Model1_nlattr **Model1_tb, const struct Model1_nlattr *Model1_head, int Model1_len);

/*
 * NUD stands for "neighbor unreachability detection"
 */





struct Model1_neighbour;

enum {
 Model1_NEIGH_VAR_MCAST_PROBES,
 Model1_NEIGH_VAR_UCAST_PROBES,
 Model1_NEIGH_VAR_APP_PROBES,
 Model1_NEIGH_VAR_MCAST_REPROBES,
 Model1_NEIGH_VAR_RETRANS_TIME,
 Model1_NEIGH_VAR_BASE_REACHABLE_TIME,
 Model1_NEIGH_VAR_DELAY_PROBE_TIME,
 Model1_NEIGH_VAR_GC_STALETIME,
 Model1_NEIGH_VAR_QUEUE_LEN_BYTES,
 Model1_NEIGH_VAR_PROXY_QLEN,
 Model1_NEIGH_VAR_ANYCAST_DELAY,
 Model1_NEIGH_VAR_PROXY_DELAY,
 Model1_NEIGH_VAR_LOCKTIME,

 /* Following are used as a second way to access one of the above */
 Model1_NEIGH_VAR_QUEUE_LEN, /* same data as NEIGH_VAR_QUEUE_LEN_BYTES */
 Model1_NEIGH_VAR_RETRANS_TIME_MS, /* same data as NEIGH_VAR_RETRANS_TIME */
 Model1_NEIGH_VAR_BASE_REACHABLE_TIME_MS, /* same data as NEIGH_VAR_BASE_REACHABLE_TIME */
 /* Following are used by "default" only */
 Model1_NEIGH_VAR_GC_INTERVAL,
 Model1_NEIGH_VAR_GC_THRESH1,
 Model1_NEIGH_VAR_GC_THRESH2,
 Model1_NEIGH_VAR_GC_THRESH3,
 Model1_NEIGH_VAR_MAX
};

struct Model1_neigh_parms {
 Model1_possible_net_t Model1_net;
 struct Model1_net_device *Model1_dev;
 struct Model1_list_head Model1_list;
 int (*Model1_neigh_setup)(struct Model1_neighbour *);
 void (*Model1_neigh_cleanup)(struct Model1_neighbour *);
 struct Model1_neigh_table *Model1_tbl;

 void *Model1_sysctl_table;

 int Model1_dead;
 Model1_atomic_t Model1_refcnt;
 struct Model1_callback_head Model1_callback_head;

 int Model1_reachable_time;
 int Model1_data[(Model1_NEIGH_VAR_LOCKTIME + 1)];
 unsigned long Model1_data_state[((((Model1_NEIGH_VAR_LOCKTIME + 1)) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];
};

static inline __attribute__((no_instrument_function)) void Model1_neigh_var_set(struct Model1_neigh_parms *Model1_p, int Model1_index, int Model1_val)
{
 Model1_set_bit(Model1_index, Model1_p->Model1_data_state);
 Model1_p->Model1_data[Model1_index] = Model1_val;
}



/* In ndo_neigh_setup, NEIGH_VAR_INIT should be used.
 * In other cases, NEIGH_VAR_SET should be used.
 */



static inline __attribute__((no_instrument_function)) void Model1_neigh_parms_data_state_setall(struct Model1_neigh_parms *Model1_p)
{
 Model1_bitmap_fill(Model1_p->Model1_data_state, (Model1_NEIGH_VAR_LOCKTIME + 1));
}

static inline __attribute__((no_instrument_function)) void Model1_neigh_parms_data_state_cleanall(struct Model1_neigh_parms *Model1_p)
{
 Model1_bitmap_zero(Model1_p->Model1_data_state, (Model1_NEIGH_VAR_LOCKTIME + 1));
}

struct Model1_neigh_statistics {
 unsigned long Model1_allocs; /* number of allocated neighs */
 unsigned long Model1_destroys; /* number of destroyed neighs */
 unsigned long Model1_hash_grows; /* number of hash resizes */

 unsigned long Model1_res_failed; /* number of failed resolutions */

 unsigned long Model1_lookups; /* number of lookups */
 unsigned long Model1_hits; /* number of hits (among lookups) */

 unsigned long Model1_rcv_probes_mcast; /* number of received mcast ipv6 */
 unsigned long Model1_rcv_probes_ucast; /* number of received ucast ipv6 */

 unsigned long Model1_periodic_gc_runs; /* number of periodic GC runs */
 unsigned long Model1_forced_gc_runs; /* number of forced GC runs */

 unsigned long Model1_unres_discards; /* number of unresolved drops */
 unsigned long Model1_table_fulls; /* times even gc couldn't help */
};



struct Model1_neighbour {
 struct Model1_neighbour *Model1_next;
 struct Model1_neigh_table *Model1_tbl;
 struct Model1_neigh_parms *Model1_parms;
 unsigned long Model1_confirmed;
 unsigned long Model1_updated;
 Model1_rwlock_t Model1_lock;
 Model1_atomic_t Model1_refcnt;
 struct Model1_sk_buff_head Model1_arp_queue;
 unsigned int Model1_arp_queue_len_bytes;
 struct Model1_timer_list Model1_timer;
 unsigned long Model1_used;
 Model1_atomic_t Model1_probes;
 __u8 Model1_flags;
 __u8 Model1_nud_state;
 __u8 Model1_type;
 __u8 Model1_dead;
 Model1_seqlock_t Model1_ha_lock;
 unsigned char Model1_ha[((((32)) + ((typeof((32)))((sizeof(unsigned long))) - 1)) & ~((typeof((32)))((sizeof(unsigned long))) - 1))];
 struct Model1_hh_cache Model1_hh;
 int (*Model1_output)(struct Model1_neighbour *, struct Model1_sk_buff *);
 const struct Model1_neigh_ops *Model1_ops;
 struct Model1_callback_head Model1_rcu;
 struct Model1_net_device *Model1_dev;
 Model1_u8 Model1_primary_key[0];
};

struct Model1_neigh_ops {
 int Model1_family;
 void (*Model1_solicit)(struct Model1_neighbour *, struct Model1_sk_buff *);
 void (*Model1_error_report)(struct Model1_neighbour *, struct Model1_sk_buff *);
 int (*Model1_output)(struct Model1_neighbour *, struct Model1_sk_buff *);
 int (*Model1_connected_output)(struct Model1_neighbour *, struct Model1_sk_buff *);
};

struct Model1_pneigh_entry {
 struct Model1_pneigh_entry *Model1_next;
 Model1_possible_net_t Model1_net;
 struct Model1_net_device *Model1_dev;
 Model1_u8 Model1_flags;
 Model1_u8 Model1_key[0];
};

/*
 *	neighbour table manipulation
 */



struct Model1_neigh_hash_table {
 struct Model1_neighbour **Model1_hash_buckets;
 unsigned int Model1_hash_shift;
 __u32 Model1_hash_rnd[4];
 struct Model1_callback_head Model1_rcu;
};


struct Model1_neigh_table {
 int Model1_family;
 int Model1_entry_size;
 int Model1_key_len;
 Model1___be16 Model1_protocol;
 __u32 (*Model1_hash)(const void *Model1_pkey,
     const struct Model1_net_device *Model1_dev,
     __u32 *Model1_hash_rnd);
 bool (*Model1_key_eq)(const struct Model1_neighbour *, const void *Model1_pkey);
 int (*Model1_constructor)(struct Model1_neighbour *);
 int (*Model1_pconstructor)(struct Model1_pneigh_entry *);
 void (*Model1_pdestructor)(struct Model1_pneigh_entry *);
 void (*Model1_proxy_redo)(struct Model1_sk_buff *Model1_skb);
 char *Model1_id;
 struct Model1_neigh_parms Model1_parms;
 struct Model1_list_head Model1_parms_list;
 int Model1_gc_interval;
 int Model1_gc_thresh1;
 int Model1_gc_thresh2;
 int Model1_gc_thresh3;
 unsigned long Model1_last_flush;
 struct Model1_delayed_work Model1_gc_work;
 struct Model1_timer_list Model1_proxy_timer;
 struct Model1_sk_buff_head Model1_proxy_queue;
 Model1_atomic_t Model1_entries;
 Model1_rwlock_t Model1_lock;
 unsigned long Model1_last_rand;
 struct Model1_neigh_statistics *Model1_stats;
 struct Model1_neigh_hash_table *Model1_nht;
 struct Model1_pneigh_entry **Model1_phash_buckets;
};

enum {
 Model1_NEIGH_ARP_TABLE = 0,
 Model1_NEIGH_ND_TABLE = 1,
 Model1_NEIGH_DN_TABLE = 2,
 Model1_NEIGH_NR_TABLES,
 Model1_NEIGH_LINK_TABLE = Model1_NEIGH_NR_TABLES /* Pseudo table for neigh_xmit */
};

static inline __attribute__((no_instrument_function)) int Model1_neigh_parms_family(struct Model1_neigh_parms *Model1_p)
{
 return Model1_p->Model1_tbl->Model1_family;
}




static inline __attribute__((no_instrument_function)) void *Model1_neighbour_priv(const struct Model1_neighbour *Model1_n)
{
 return (char *)Model1_n + Model1_n->Model1_tbl->Model1_entry_size;
}

/* flags for neigh_update() */







static inline __attribute__((no_instrument_function)) bool Model1_neigh_key_eq16(const struct Model1_neighbour *Model1_n, const void *Model1_pkey)
{
 return *(const Model1_u16 *)Model1_n->Model1_primary_key == *(const Model1_u16 *)Model1_pkey;
}

static inline __attribute__((no_instrument_function)) bool Model1_neigh_key_eq32(const struct Model1_neighbour *Model1_n, const void *Model1_pkey)
{
 return *(const Model1_u32 *)Model1_n->Model1_primary_key == *(const Model1_u32 *)Model1_pkey;
}

static inline __attribute__((no_instrument_function)) bool Model1_neigh_key_eq128(const struct Model1_neighbour *Model1_n, const void *Model1_pkey)
{
 const Model1_u32 *Model1_n32 = (const Model1_u32 *)Model1_n->Model1_primary_key;
 const Model1_u32 *Model1_p32 = Model1_pkey;

 return ((Model1_n32[0] ^ Model1_p32[0]) | (Model1_n32[1] ^ Model1_p32[1]) |
  (Model1_n32[2] ^ Model1_p32[2]) | (Model1_n32[3] ^ Model1_p32[3])) == 0;
}

static inline __attribute__((no_instrument_function)) struct Model1_neighbour *Model1____neigh_lookup_noref(
 struct Model1_neigh_table *Model1_tbl,
 bool (*Model1_key_eq)(const struct Model1_neighbour *Model1_n, const void *Model1_pkey),
 __u32 (*Model1_hash)(const void *Model1_pkey,
        const struct Model1_net_device *Model1_dev,
        __u32 *Model1_hash_rnd),
 const void *Model1_pkey,
 struct Model1_net_device *Model1_dev)
{
 struct Model1_neigh_hash_table *Model1_nht = ({ typeof(*(Model1_tbl->Model1_nht)) *Model1_________p1 = (typeof(*(Model1_tbl->Model1_nht)) *)({ typeof((Model1_tbl->Model1_nht)) Model1__________p1 = ({ union { typeof((Model1_tbl->Model1_nht)) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&((Model1_tbl->Model1_nht)), Model1___u.Model1___c, sizeof((Model1_tbl->Model1_nht))); else Model1___read_once_size_nocheck(&((Model1_tbl->Model1_nht)), Model1___u.Model1___c, sizeof((Model1_tbl->Model1_nht))); Model1___u.Model1___val; }); typeof(*((Model1_tbl->Model1_nht))) *Model1____typecheck_p __attribute__((unused)); do { } while (0); (Model1__________p1); }); do { } while (0); ; ((typeof(*(Model1_tbl->Model1_nht)) *)(Model1_________p1)); });
 struct Model1_neighbour *Model1_n;
 Model1_u32 Model1_hash_val;

 Model1_hash_val = Model1_hash(Model1_pkey, Model1_dev, Model1_nht->Model1_hash_rnd) >> (32 - Model1_nht->Model1_hash_shift);
 for (Model1_n = ({ typeof(*(Model1_nht->Model1_hash_buckets[Model1_hash_val])) *Model1_________p1 = (typeof(*(Model1_nht->Model1_hash_buckets[Model1_hash_val])) *)({ typeof((Model1_nht->Model1_hash_buckets[Model1_hash_val])) Model1__________p1 = ({ union { typeof((Model1_nht->Model1_hash_buckets[Model1_hash_val])) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&((Model1_nht->Model1_hash_buckets[Model1_hash_val])), Model1___u.Model1___c, sizeof((Model1_nht->Model1_hash_buckets[Model1_hash_val]))); else Model1___read_once_size_nocheck(&((Model1_nht->Model1_hash_buckets[Model1_hash_val])), Model1___u.Model1___c, sizeof((Model1_nht->Model1_hash_buckets[Model1_hash_val]))); Model1___u.Model1___val; }); typeof(*((Model1_nht->Model1_hash_buckets[Model1_hash_val]))) *Model1____typecheck_p __attribute__((unused)); do { } while (0); (Model1__________p1); }); do { } while (0); ; ((typeof(*(Model1_nht->Model1_hash_buckets[Model1_hash_val])) *)(Model1_________p1)); });
      Model1_n != ((void *)0);
      Model1_n = ({ typeof(*(Model1_n->Model1_next)) *Model1_________p1 = (typeof(*(Model1_n->Model1_next)) *)({ typeof((Model1_n->Model1_next)) Model1__________p1 = ({ union { typeof((Model1_n->Model1_next)) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&((Model1_n->Model1_next)), Model1___u.Model1___c, sizeof((Model1_n->Model1_next))); else Model1___read_once_size_nocheck(&((Model1_n->Model1_next)), Model1___u.Model1___c, sizeof((Model1_n->Model1_next))); Model1___u.Model1___val; }); typeof(*((Model1_n->Model1_next))) *Model1____typecheck_p __attribute__((unused)); do { } while (0); (Model1__________p1); }); do { } while (0); ; ((typeof(*(Model1_n->Model1_next)) *)(Model1_________p1)); })) {
  if (Model1_n->Model1_dev == Model1_dev && Model1_key_eq(Model1_n, Model1_pkey))
   return Model1_n;
 }

 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model1_neighbour *Model1___neigh_lookup_noref(struct Model1_neigh_table *Model1_tbl,
           const void *Model1_pkey,
           struct Model1_net_device *Model1_dev)
{
 return Model1____neigh_lookup_noref(Model1_tbl, Model1_tbl->Model1_key_eq, Model1_tbl->Model1_hash, Model1_pkey, Model1_dev);
}

void Model1_neigh_table_init(int Model1_index, struct Model1_neigh_table *Model1_tbl);
int Model1_neigh_table_clear(int Model1_index, struct Model1_neigh_table *Model1_tbl);
struct Model1_neighbour *Model1_neigh_lookup(struct Model1_neigh_table *Model1_tbl, const void *Model1_pkey,
          struct Model1_net_device *Model1_dev);
struct Model1_neighbour *Model1_neigh_lookup_nodev(struct Model1_neigh_table *Model1_tbl, struct Model1_net *Model1_net,
         const void *Model1_pkey);
struct Model1_neighbour *Model1___neigh_create(struct Model1_neigh_table *Model1_tbl, const void *Model1_pkey,
     struct Model1_net_device *Model1_dev, bool Model1_want_ref);
static inline __attribute__((no_instrument_function)) struct Model1_neighbour *Model1_neigh_create(struct Model1_neigh_table *Model1_tbl,
          const void *Model1_pkey,
          struct Model1_net_device *Model1_dev)
{
 return Model1___neigh_create(Model1_tbl, Model1_pkey, Model1_dev, true);
}
void Model1_neigh_destroy(struct Model1_neighbour *Model1_neigh);
int Model1___neigh_event_send(struct Model1_neighbour *Model1_neigh, struct Model1_sk_buff *Model1_skb);
int Model1_neigh_update(struct Model1_neighbour *Model1_neigh, const Model1_u8 *Model1_lladdr, Model1_u8 Model1_new, Model1_u32 Model1_flags);
void Model1___neigh_set_probe_once(struct Model1_neighbour *Model1_neigh);
void Model1_neigh_changeaddr(struct Model1_neigh_table *Model1_tbl, struct Model1_net_device *Model1_dev);
int Model1_neigh_ifdown(struct Model1_neigh_table *Model1_tbl, struct Model1_net_device *Model1_dev);
int Model1_neigh_resolve_output(struct Model1_neighbour *Model1_neigh, struct Model1_sk_buff *Model1_skb);
int Model1_neigh_connected_output(struct Model1_neighbour *Model1_neigh, struct Model1_sk_buff *Model1_skb);
int Model1_neigh_direct_output(struct Model1_neighbour *Model1_neigh, struct Model1_sk_buff *Model1_skb);
struct Model1_neighbour *Model1_neigh_event_ns(struct Model1_neigh_table *Model1_tbl,
      Model1_u8 *Model1_lladdr, void *Model1_saddr,
      struct Model1_net_device *Model1_dev);

struct Model1_neigh_parms *Model1_neigh_parms_alloc(struct Model1_net_device *Model1_dev,
          struct Model1_neigh_table *Model1_tbl);
void Model1_neigh_parms_release(struct Model1_neigh_table *Model1_tbl, struct Model1_neigh_parms *Model1_parms);

static inline __attribute__((no_instrument_function))
struct Model1_net *Model1_neigh_parms_net(const struct Model1_neigh_parms *Model1_parms)
{
 return Model1_read_pnet(&Model1_parms->Model1_net);
}

unsigned long Model1_neigh_rand_reach_time(unsigned long Model1_base);

void Model1_pneigh_enqueue(struct Model1_neigh_table *Model1_tbl, struct Model1_neigh_parms *Model1_p,
      struct Model1_sk_buff *Model1_skb);
struct Model1_pneigh_entry *Model1_pneigh_lookup(struct Model1_neigh_table *Model1_tbl, struct Model1_net *Model1_net,
       const void *Model1_key, struct Model1_net_device *Model1_dev,
       int Model1_creat);
struct Model1_pneigh_entry *Model1___pneigh_lookup(struct Model1_neigh_table *Model1_tbl, struct Model1_net *Model1_net,
         const void *Model1_key, struct Model1_net_device *Model1_dev);
int Model1_pneigh_delete(struct Model1_neigh_table *Model1_tbl, struct Model1_net *Model1_net, const void *Model1_key,
    struct Model1_net_device *Model1_dev);

static inline __attribute__((no_instrument_function)) struct Model1_net *Model1_pneigh_net(const struct Model1_pneigh_entry *Model1_pneigh)
{
 return Model1_read_pnet(&Model1_pneigh->Model1_net);
}

void Model1_neigh_app_ns(struct Model1_neighbour *Model1_n);
void Model1_neigh_for_each(struct Model1_neigh_table *Model1_tbl,
      void (*Model1_cb)(struct Model1_neighbour *, void *), void *Model1_cookie);
void Model1___neigh_for_each_release(struct Model1_neigh_table *Model1_tbl,
         int (*Model1_cb)(struct Model1_neighbour *));
int Model1_neigh_xmit(int Model1_fam, struct Model1_net_device *, const void *, struct Model1_sk_buff *);
void Model1_pneigh_for_each(struct Model1_neigh_table *Model1_tbl,
       void (*Model1_cb)(struct Model1_pneigh_entry *));

struct Model1_neigh_seq_state {
 struct Model1_seq_net_private Model1_p;
 struct Model1_neigh_table *Model1_tbl;
 struct Model1_neigh_hash_table *Model1_nht;
 void *(*Model1_neigh_sub_iter)(struct Model1_neigh_seq_state *Model1_state,
    struct Model1_neighbour *Model1_n, Model1_loff_t *Model1_pos);
 unsigned int Model1_bucket;
 unsigned int Model1_flags;



};
void *Model1_neigh_seq_start(struct Model1_seq_file *, Model1_loff_t *, struct Model1_neigh_table *,
        unsigned int);
void *Model1_neigh_seq_next(struct Model1_seq_file *, void *, Model1_loff_t *);
void Model1_neigh_seq_stop(struct Model1_seq_file *, void *);

int Model1_neigh_proc_dointvec(struct Model1_ctl_table *Model1_ctl, int Model1_write,
   void *Model1_buffer, Model1_size_t *Model1_lenp, Model1_loff_t *Model1_ppos);
int Model1_neigh_proc_dointvec_jiffies(struct Model1_ctl_table *Model1_ctl, int Model1_write,
    void *Model1_buffer,
    Model1_size_t *Model1_lenp, Model1_loff_t *Model1_ppos);
int Model1_neigh_proc_dointvec_ms_jiffies(struct Model1_ctl_table *Model1_ctl, int Model1_write,
       void *Model1_buffer,
       Model1_size_t *Model1_lenp, Model1_loff_t *Model1_ppos);

int Model1_neigh_sysctl_register(struct Model1_net_device *Model1_dev, struct Model1_neigh_parms *Model1_p,
     Model1_proc_handler *Model1_proc_handler);
void Model1_neigh_sysctl_unregister(struct Model1_neigh_parms *Model1_p);

static inline __attribute__((no_instrument_function)) void Model1___neigh_parms_put(struct Model1_neigh_parms *Model1_parms)
{
 Model1_atomic_dec(&Model1_parms->Model1_refcnt);
}

static inline __attribute__((no_instrument_function)) struct Model1_neigh_parms *Model1_neigh_parms_clone(struct Model1_neigh_parms *Model1_parms)
{
 Model1_atomic_inc(&Model1_parms->Model1_refcnt);
 return Model1_parms;
}

/*
 *	Neighbour references
 */

static inline __attribute__((no_instrument_function)) void Model1_neigh_release(struct Model1_neighbour *Model1_neigh)
{
 if (Model1_atomic_dec_and_test(&Model1_neigh->Model1_refcnt))
  Model1_neigh_destroy(Model1_neigh);
}

static inline __attribute__((no_instrument_function)) struct Model1_neighbour * Model1_neigh_clone(struct Model1_neighbour *Model1_neigh)
{
 if (Model1_neigh)
  Model1_atomic_inc(&Model1_neigh->Model1_refcnt);
 return Model1_neigh;
}



static inline __attribute__((no_instrument_function)) int Model1_neigh_event_send(struct Model1_neighbour *Model1_neigh, struct Model1_sk_buff *Model1_skb)
{
 unsigned long Model1_now = Model1_jiffies;

 if (Model1_neigh->Model1_used != Model1_now)
  Model1_neigh->Model1_used = Model1_now;
 if (!(Model1_neigh->Model1_nud_state&((0x80|0x40|0x02)|0x08|0x10)))
  return Model1___neigh_event_send(Model1_neigh, Model1_skb);
 return 0;
}
static inline __attribute__((no_instrument_function)) int Model1_neigh_hh_output(const struct Model1_hh_cache *Model1_hh, struct Model1_sk_buff *Model1_skb)
{
 unsigned int Model1_seq;
 int Model1_hh_len;

 do {
  Model1_seq = Model1_read_seqbegin(&Model1_hh->Model1_hh_lock);
  Model1_hh_len = Model1_hh->Model1_hh_len;
  if (__builtin_expect(!!(Model1_hh_len <= 16), 1)) {
   /* this is inlined by gcc */
   ({ Model1_size_t Model1___len = (16); void *Model1___ret; if (__builtin_constant_p(16) && Model1___len >= 64) Model1___ret = Model1___memcpy((Model1_skb->Model1_data - 16), (Model1_hh->Model1_hh_data), Model1___len); else Model1___ret = __builtin_memcpy((Model1_skb->Model1_data - 16), (Model1_hh->Model1_hh_data), Model1___len); Model1___ret; });
  } else {
   int Model1_hh_alen = (((Model1_hh_len)+(16 -1))&~(16 - 1));

   ({ Model1_size_t Model1___len = (Model1_hh_alen); void *Model1___ret; if (__builtin_constant_p(Model1_hh_alen) && Model1___len >= 64) Model1___ret = Model1___memcpy((Model1_skb->Model1_data - Model1_hh_alen), (Model1_hh->Model1_hh_data), Model1___len); else Model1___ret = __builtin_memcpy((Model1_skb->Model1_data - Model1_hh_alen), (Model1_hh->Model1_hh_data), Model1___len); Model1___ret; });
  }
 } while (Model1_read_seqretry(&Model1_hh->Model1_hh_lock, Model1_seq));

 Model1_skb_push(Model1_skb, Model1_hh_len);
 return Model1_dev_queue_xmit(Model1_skb);
}

static inline __attribute__((no_instrument_function)) struct Model1_neighbour *
Model1___neigh_lookup(struct Model1_neigh_table *Model1_tbl, const void *Model1_pkey, struct Model1_net_device *Model1_dev, int Model1_creat)
{
 struct Model1_neighbour *Model1_n = Model1_neigh_lookup(Model1_tbl, Model1_pkey, Model1_dev);

 if (Model1_n || !Model1_creat)
  return Model1_n;

 Model1_n = Model1_neigh_create(Model1_tbl, Model1_pkey, Model1_dev);
 return Model1_IS_ERR(Model1_n) ? ((void *)0) : Model1_n;
}

static inline __attribute__((no_instrument_function)) struct Model1_neighbour *
Model1___neigh_lookup_errno(struct Model1_neigh_table *Model1_tbl, const void *Model1_pkey,
  struct Model1_net_device *Model1_dev)
{
 struct Model1_neighbour *Model1_n = Model1_neigh_lookup(Model1_tbl, Model1_pkey, Model1_dev);

 if (Model1_n)
  return Model1_n;

 return Model1_neigh_create(Model1_tbl, Model1_pkey, Model1_dev);
}

struct Model1_neighbour_cb {
 unsigned long Model1_sched_next;
 unsigned int Model1_flags;
};





static inline __attribute__((no_instrument_function)) void Model1_neigh_ha_snapshot(char *Model1_dst, const struct Model1_neighbour *Model1_n,
         const struct Model1_net_device *Model1_dev)
{
 unsigned int Model1_seq;

 do {
  Model1_seq = Model1_read_seqbegin(&Model1_n->Model1_ha_lock);
  ({ Model1_size_t Model1___len = (Model1_dev->Model1_addr_len); void *Model1___ret; if (__builtin_constant_p(Model1_dev->Model1_addr_len) && Model1___len >= 64) Model1___ret = Model1___memcpy((Model1_dst), (Model1_n->Model1_ha), Model1___len); else Model1___ret = __builtin_memcpy((Model1_dst), (Model1_n->Model1_ha), Model1___len); Model1___ret; });
 } while (Model1_read_seqretry(&Model1_n->Model1_ha_lock, Model1_seq));
}






/* Each dst_entry has reference count and sits in some parent list(s).
 * When it is removed from parent list, it is "freed" (dst_free).
 * After this it enters dead state (dst->obsolete > 0) and if its refcnt
 * is zero, it can be destroyed immediately, otherwise it is added
 * to gc list and garbage collector periodically checks the refcnt.
 */

struct Model1_sk_buff;

struct Model1_dst_entry {
 struct Model1_callback_head Model1_callback_head;
 struct Model1_dst_entry *Model1_child;
 struct Model1_net_device *Model1_dev;
 struct Model1_dst_ops *Model1_ops;
 unsigned long Model1__metrics;
 unsigned long Model1_expires;
 struct Model1_dst_entry *Model1_path;
 struct Model1_dst_entry *Model1_from;

 struct Model1_xfrm_state *Model1_xfrm;



 int (*Model1_input)(struct Model1_sk_buff *);
 int (*Model1_output)(struct Model1_net *Model1_net, struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);

 unsigned short Model1_flags;
 unsigned short Model1_pending_confirm;

 short error;

 /* A non-zero value of dst->obsolete forces by-hand validation
	 * of the route entry.  Positive values are set by the generic
	 * dst layer to indicate that the entry has been forcefully
	 * destroyed.
	 *
	 * Negative values are used by the implementation layer code to
	 * force invocation of the dst_ops->check() method.
	 */
 short Model1_obsolete;




 unsigned short Model1_header_len; /* more space at head required */
 unsigned short Model1_trailer_len; /* space to reserve at tail */



 __u32 Model1___pad2;



 /*
	 * Align __refcnt to a 64 bytes alignment
	 * (L1_CACHE_SIZE would be too much)
	 */
 long Model1___pad_to_align_refcnt[2];

 /*
	 * __refcnt wants to be on a different cache line from
	 * input/output/ops or performance tanks badly
	 */
 Model1_atomic_t Model1___refcnt; /* client references	*/
 int Model1___use;
 unsigned long Model1_lastuse;
 struct Model1_lwtunnel_state *Model1_lwtstate;
 union {
  struct Model1_dst_entry *Model1_next;
  struct Model1_rtable *Model1_rt_next;
  struct Model1_rt6_info *Model1_rt6_next;
  struct Model1_dn_route *Model1_dn_next;
 };
};

Model1_u32 *Model1_dst_cow_metrics_generic(struct Model1_dst_entry *Model1_dst, unsigned long old);
extern const Model1_u32 Model1_dst_default_metrics[];







static inline __attribute__((no_instrument_function)) bool Model1_dst_metrics_read_only(const struct Model1_dst_entry *Model1_dst)
{
 return Model1_dst->Model1__metrics & 0x1UL;
}

void Model1___dst_destroy_metrics_generic(struct Model1_dst_entry *Model1_dst, unsigned long old);

static inline __attribute__((no_instrument_function)) void Model1_dst_destroy_metrics_generic(struct Model1_dst_entry *Model1_dst)
{
 unsigned long Model1_val = Model1_dst->Model1__metrics;
 if (!(Model1_val & 0x1UL))
  Model1___dst_destroy_metrics_generic(Model1_dst, Model1_val);
}

static inline __attribute__((no_instrument_function)) Model1_u32 *Model1_dst_metrics_write_ptr(struct Model1_dst_entry *Model1_dst)
{
 unsigned long Model1_p = Model1_dst->Model1__metrics;

 do { if (__builtin_expect(!!(!Model1_p), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/net/dst.h"), "i" (137), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);

 if (Model1_p & 0x1UL)
  return Model1_dst->Model1_ops->Model1_cow_metrics(Model1_dst, Model1_p);
 return ((Model1_u32 *)((Model1_p) & ~0x3UL));
}

/* This may only be invoked before the entry has reached global
 * visibility.
 */
static inline __attribute__((no_instrument_function)) void Model1_dst_init_metrics(struct Model1_dst_entry *Model1_dst,
        const Model1_u32 *Model1_src_metrics,
        bool Model1_read_only)
{
 Model1_dst->Model1__metrics = ((unsigned long) Model1_src_metrics) |
  (Model1_read_only ? 0x1UL : 0);
}

static inline __attribute__((no_instrument_function)) void Model1_dst_copy_metrics(struct Model1_dst_entry *Model1_dest, const struct Model1_dst_entry *Model1_src)
{
 Model1_u32 *Model1_dst_metrics = Model1_dst_metrics_write_ptr(Model1_dest);

 if (Model1_dst_metrics) {
  Model1_u32 *Model1_src_metrics = ((Model1_u32 *)(((Model1_src)->Model1__metrics) & ~0x3UL));

  ({ Model1_size_t Model1___len = ((Model1___RTAX_MAX - 1) * sizeof(Model1_u32)); void *Model1___ret; if (__builtin_constant_p((Model1___RTAX_MAX - 1) * sizeof(Model1_u32)) && Model1___len >= 64) Model1___ret = Model1___memcpy((Model1_dst_metrics), (Model1_src_metrics), Model1___len); else Model1___ret = __builtin_memcpy((Model1_dst_metrics), (Model1_src_metrics), Model1___len); Model1___ret; });
 }
}

static inline __attribute__((no_instrument_function)) Model1_u32 *Model1_dst_metrics_ptr(struct Model1_dst_entry *Model1_dst)
{
 return ((Model1_u32 *)(((Model1_dst)->Model1__metrics) & ~0x3UL));
}

static inline __attribute__((no_instrument_function)) Model1_u32
Model1_dst_metric_raw(const struct Model1_dst_entry *Model1_dst, const int Model1_metric)
{
 Model1_u32 *Model1_p = ((Model1_u32 *)(((Model1_dst)->Model1__metrics) & ~0x3UL));

 return Model1_p[Model1_metric-1];
}

static inline __attribute__((no_instrument_function)) Model1_u32
Model1_dst_metric(const struct Model1_dst_entry *Model1_dst, const int Model1_metric)
{
 ({ static bool __attribute__ ((__section__(".data.unlikely"))) Model1___warned; int Model1___ret_warn_once = !!(Model1_metric == Model1_RTAX_HOPLIMIT || Model1_metric == Model1_RTAX_ADVMSS || Model1_metric == Model1_RTAX_MTU); if (__builtin_expect(!!(Model1___ret_warn_once && !Model1___warned), 0)) { Model1___warned = true; ({ int Model1___ret_warn_on = !!(1); if (__builtin_expect(!!(Model1___ret_warn_on), 0)) Model1_warn_slowpath_null("./include/net/dst.h", 184); __builtin_expect(!!(Model1___ret_warn_on), 0); }); } __builtin_expect(!!(Model1___ret_warn_once), 0); });


 return Model1_dst_metric_raw(Model1_dst, Model1_metric);
}

static inline __attribute__((no_instrument_function)) Model1_u32
Model1_dst_metric_advmss(const struct Model1_dst_entry *Model1_dst)
{
#if CY_ABSTRACT3 //advmss is from device, which is too low details
     return 1460; //MTU (1500) - TCP header (20) - IP header (20)
#else
 Model1_u32 Model1_advmss = Model1_dst_metric_raw(Model1_dst, Model1_RTAX_ADVMSS);

 if (!Model1_advmss)
  Model1_advmss = Model1_dst->Model1_ops->Model1_default_advmss(Model1_dst);

 return Model1_advmss;
#endif
}

static inline __attribute__((no_instrument_function)) void Model1_dst_metric_set(struct Model1_dst_entry *Model1_dst, int Model1_metric, Model1_u32 Model1_val)
{
 Model1_u32 *Model1_p = Model1_dst_metrics_write_ptr(Model1_dst);

 if (Model1_p)
  Model1_p[Model1_metric-1] = Model1_val;
}

/* Kernel-internal feature bits that are unallocated in user space. */





static inline __attribute__((no_instrument_function)) Model1_u32
Model1_dst_feature(const struct Model1_dst_entry *Model1_dst, Model1_u32 Model1_feature)
{
 return Model1_dst_metric(Model1_dst, Model1_RTAX_FEATURES) & Model1_feature;
}

static inline __attribute__((no_instrument_function)) Model1_u32 Model1_dst_mtu(const struct Model1_dst_entry *Model1_dst)
{
 return Model1_dst->Model1_ops->Model1_mtu(Model1_dst);
}

/* RTT metrics are stored in milliseconds for user ABI, but used as jiffies */
static inline __attribute__((no_instrument_function)) unsigned long Model1_dst_metric_rtt(const struct Model1_dst_entry *Model1_dst, int Model1_metric)
{
 return Model1_msecs_to_jiffies(Model1_dst_metric(Model1_dst, Model1_metric));
}

static inline __attribute__((no_instrument_function)) Model1_u32
Model1_dst_allfrag(const struct Model1_dst_entry *Model1_dst)
{
 int Model1_ret = Model1_dst_feature(Model1_dst, (1 << 3));
 return Model1_ret;
}

static inline __attribute__((no_instrument_function)) int
Model1_dst_metric_locked(const struct Model1_dst_entry *Model1_dst, int Model1_metric)
{
 return Model1_dst_metric(Model1_dst, Model1_RTAX_LOCK) & (1<<Model1_metric);
}

static inline __attribute__((no_instrument_function)) void Model1_dst_hold(struct Model1_dst_entry *Model1_dst)
{
 /*
	 * If your kernel compilation stops here, please check
	 * __pad_to_align_refcnt declaration in struct dst_entry
	 */
 do { bool Model1___cond = !(!(__builtin_offsetof(struct Model1_dst_entry, Model1___refcnt) & 63)); extern void Model1___compiletime_assert_249(void) ; if (Model1___cond) Model1___compiletime_assert_249(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
 Model1_atomic_inc(&Model1_dst->Model1___refcnt);
}

static inline __attribute__((no_instrument_function)) void Model1_dst_use(struct Model1_dst_entry *Model1_dst, unsigned long Model1_time)
{
 Model1_dst_hold(Model1_dst);
 Model1_dst->Model1___use++;
 Model1_dst->Model1_lastuse = Model1_time;
}

static inline __attribute__((no_instrument_function)) void Model1_dst_use_noref(struct Model1_dst_entry *Model1_dst, unsigned long Model1_time)
{
 Model1_dst->Model1___use++;
 Model1_dst->Model1_lastuse = Model1_time;
}

static inline __attribute__((no_instrument_function)) struct Model1_dst_entry *Model1_dst_clone(struct Model1_dst_entry *Model1_dst)
{
 if (Model1_dst)
  Model1_atomic_inc(&Model1_dst->Model1___refcnt);
 return Model1_dst;
}

void Model1_dst_release(struct Model1_dst_entry *Model1_dst);

static inline __attribute__((no_instrument_function)) void Model1_refdst_drop(unsigned long Model1_refdst)
{
 if (!(Model1_refdst & 1UL))
  Model1_dst_release((struct Model1_dst_entry *)(Model1_refdst & ~(1UL)));
}

/**
 * skb_dst_drop - drops skb dst
 * @skb: buffer
 *
 * Drops dst reference count if a reference was taken.
 */
static inline __attribute__((no_instrument_function)) void Model1_skb_dst_drop(struct Model1_sk_buff *Model1_skb)
{
 #if CY_ABSTRACT1
 #else
 if (Model1_skb->Model1__skb_refdst) {
  Model1_refdst_drop(Model1_skb->Model1__skb_refdst);
  Model1_skb->Model1__skb_refdst = 0UL;
 }
 #endif
}

static inline __attribute__((no_instrument_function)) void Model1___skb_dst_copy(struct Model1_sk_buff *Model1_nskb, unsigned long Model1_refdst)
{
 Model1_nskb->Model1__skb_refdst = Model1_refdst;
 if (!(Model1_nskb->Model1__skb_refdst & 1UL))
  Model1_dst_clone(Model1_skb_dst(Model1_nskb));
}

static inline __attribute__((no_instrument_function)) void Model1_skb_dst_copy(struct Model1_sk_buff *Model1_nskb, const struct Model1_sk_buff *Model1_oskb)
{
 Model1___skb_dst_copy(Model1_nskb, Model1_oskb->Model1__skb_refdst);
}

/**
 * skb_dst_force - makes sure skb dst is refcounted
 * @skb: buffer
 *
 * If dst is not yet refcounted, let's do it
 */
static inline __attribute__((no_instrument_function)) void Model1_skb_dst_force(struct Model1_sk_buff *Model1_skb)
{
 if (Model1_skb_dst_is_noref(Model1_skb)) {
  ({ int Model1___ret_warn_on = !!(!Model1_rcu_read_lock_held()); if (__builtin_expect(!!(Model1___ret_warn_on), 0)) Model1_warn_slowpath_null("./include/net/dst.h", 316); __builtin_expect(!!(Model1___ret_warn_on), 0); });
  Model1_skb->Model1__skb_refdst &= ~1UL;
  Model1_dst_clone(Model1_skb_dst(Model1_skb));
 }
}

/**
 * dst_hold_safe - Take a reference on a dst if possible
 * @dst: pointer to dst entry
 *
 * This helper returns false if it could not safely
 * take a reference on a dst.
 */
static inline __attribute__((no_instrument_function)) bool Model1_dst_hold_safe(struct Model1_dst_entry *Model1_dst)
{
 if (Model1_dst->Model1_flags & 0x0010)
  return Model1_atomic_add_unless((&Model1_dst->Model1___refcnt), 1, 0);
 Model1_dst_hold(Model1_dst);
 return true;
}

/**
 * skb_dst_force_safe - makes sure skb dst is refcounted
 * @skb: buffer
 *
 * If dst is not yet refcounted and not destroyed, grab a ref on it.
 */
static inline __attribute__((no_instrument_function)) void Model1_skb_dst_force_safe(struct Model1_sk_buff *Model1_skb)
{
 if (Model1_skb_dst_is_noref(Model1_skb)) {
  struct Model1_dst_entry *Model1_dst = Model1_skb_dst(Model1_skb);

  if (!Model1_dst_hold_safe(Model1_dst))
   Model1_dst = ((void *)0);

  Model1_skb->Model1__skb_refdst = (unsigned long)Model1_dst;
 }
}


/**
 *	__skb_tunnel_rx - prepare skb for rx reinsert
 *	@skb: buffer
 *	@dev: tunnel device
 *	@net: netns for packet i/o
 *
 *	After decapsulation, packet is going to re-enter (netif_rx()) our stack,
 *	so make some cleanups. (no accounting done)
 */
static inline __attribute__((no_instrument_function)) void Model1___skb_tunnel_rx(struct Model1_sk_buff *Model1_skb, struct Model1_net_device *Model1_dev,
       struct Model1_net *Model1_net)
{
 Model1_skb->Model1_dev = Model1_dev;

 /*
	 * Clear hash so that we can recalulate the hash for the
	 * encapsulated packet, unless we have already determine the hash
	 * over the L4 4-tuple.
	 */
 Model1_skb_clear_hash_if_not_l4(Model1_skb);
 Model1_skb_set_queue_mapping(Model1_skb, 0);
 Model1_skb_scrub_packet(Model1_skb, !Model1_net_eq(Model1_net, Model1_dev_net(Model1_dev)));
}

/**
 *	skb_tunnel_rx - prepare skb for rx reinsert
 *	@skb: buffer
 *	@dev: tunnel device
 *
 *	After decapsulation, packet is going to re-enter (netif_rx()) our stack,
 *	so make some cleanups, and perform accounting.
 *	Note: this accounting is not SMP safe.
 */
static inline __attribute__((no_instrument_function)) void Model1_skb_tunnel_rx(struct Model1_sk_buff *Model1_skb, struct Model1_net_device *Model1_dev,
     struct Model1_net *Model1_net)
{
 /* TODO : stats should be SMP safe */
 Model1_dev->Model1_stats.Model1_rx_packets++;
 Model1_dev->Model1_stats.Model1_rx_bytes += Model1_skb->Model1_len;
 Model1___skb_tunnel_rx(Model1_skb, Model1_dev, Model1_net);
}

static inline __attribute__((no_instrument_function)) Model1_u32 Model1_dst_tclassid(const struct Model1_sk_buff *Model1_skb)
{







 return 0;
}

int Model1_dst_discard_out(struct Model1_net *Model1_net, struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);
static inline __attribute__((no_instrument_function)) int Model1_dst_discard(struct Model1_sk_buff *Model1_skb)
{
 return Model1_dst_discard_out(&Model1_init_net, Model1_skb->Model1_sk, Model1_skb);
}
void *Model1_dst_alloc(struct Model1_dst_ops *Model1_ops, struct Model1_net_device *Model1_dev, int Model1_initial_ref,
  int Model1_initial_obsolete, unsigned short Model1_flags);
void Model1_dst_init(struct Model1_dst_entry *Model1_dst, struct Model1_dst_ops *Model1_ops,
       struct Model1_net_device *Model1_dev, int Model1_initial_ref, int Model1_initial_obsolete,
       unsigned short Model1_flags);
void Model1___dst_free(struct Model1_dst_entry *Model1_dst);
struct Model1_dst_entry *Model1_dst_destroy(struct Model1_dst_entry *Model1_dst);

static inline __attribute__((no_instrument_function)) void Model1_dst_free(struct Model1_dst_entry *Model1_dst)
{
 if (Model1_dst->Model1_obsolete > 0)
  return;
 if (!Model1_atomic_read(&Model1_dst->Model1___refcnt)) {
  Model1_dst = Model1_dst_destroy(Model1_dst);
  if (!Model1_dst)
   return;
 }
 Model1___dst_free(Model1_dst);
}

static inline __attribute__((no_instrument_function)) void Model1_dst_rcu_free(struct Model1_callback_head *Model1_head)
{
 struct Model1_dst_entry *Model1_dst = ({ const typeof( ((struct Model1_dst_entry *)0)->Model1_callback_head ) *Model1___mptr = (Model1_head); (struct Model1_dst_entry *)( (char *)Model1___mptr - __builtin_offsetof(struct Model1_dst_entry, Model1_callback_head) );});
 Model1_dst_free(Model1_dst);
}

static inline __attribute__((no_instrument_function)) void Model1_dst_confirm(struct Model1_dst_entry *Model1_dst)
{
 Model1_dst->Model1_pending_confirm = 1;
}

static inline __attribute__((no_instrument_function)) int Model1_dst_neigh_output(struct Model1_dst_entry *Model1_dst, struct Model1_neighbour *Model1_n,
       struct Model1_sk_buff *Model1_skb)
{
 const struct Model1_hh_cache *Model1_hh;

 if (Model1_dst->Model1_pending_confirm) {
  unsigned long Model1_now = Model1_jiffies;

  Model1_dst->Model1_pending_confirm = 0;
  /* avoid dirtying neighbour */
  if (Model1_n->Model1_confirmed != Model1_now)
   Model1_n->Model1_confirmed = Model1_now;
 }

 Model1_hh = &Model1_n->Model1_hh;
 if ((Model1_n->Model1_nud_state & (0x80|0x40|0x02)) && Model1_hh->Model1_hh_len)
  return Model1_neigh_hh_output(Model1_hh, Model1_skb);
 else
  return Model1_n->Model1_output(Model1_n, Model1_skb);
}

static inline __attribute__((no_instrument_function)) struct Model1_neighbour *Model1_dst_neigh_lookup(const struct Model1_dst_entry *Model1_dst, const void *Model1_daddr)
{
 struct Model1_neighbour *Model1_n = Model1_dst->Model1_ops->Model1_neigh_lookup(Model1_dst, ((void *)0), Model1_daddr);
 return Model1_IS_ERR(Model1_n) ? ((void *)0) : Model1_n;
}

static inline __attribute__((no_instrument_function)) struct Model1_neighbour *Model1_dst_neigh_lookup_skb(const struct Model1_dst_entry *Model1_dst,
           struct Model1_sk_buff *Model1_skb)
{
 struct Model1_neighbour *Model1_n = Model1_dst->Model1_ops->Model1_neigh_lookup(Model1_dst, Model1_skb, ((void *)0));
 return Model1_IS_ERR(Model1_n) ? ((void *)0) : Model1_n;
}

static inline __attribute__((no_instrument_function)) void Model1_dst_link_failure(struct Model1_sk_buff *Model1_skb)
{
 struct Model1_dst_entry *Model1_dst = Model1_skb_dst(Model1_skb);
 if (Model1_dst && Model1_dst->Model1_ops && Model1_dst->Model1_ops->Model1_link_failure)
  Model1_dst->Model1_ops->Model1_link_failure(Model1_skb);
}

static inline __attribute__((no_instrument_function)) void Model1_dst_set_expires(struct Model1_dst_entry *Model1_dst, int Model1_timeout)
{
 unsigned long Model1_expires = Model1_jiffies + Model1_timeout;

 if (Model1_expires == 0)
  Model1_expires = 1;

 if (Model1_dst->Model1_expires == 0 || (({ unsigned long Model1___dummy; typeof(Model1_dst->Model1_expires) Model1___dummy2; (void)(&Model1___dummy == &Model1___dummy2); 1; }) && ({ unsigned long Model1___dummy; typeof(Model1_expires) Model1___dummy2; (void)(&Model1___dummy == &Model1___dummy2); 1; }) && ((long)((Model1_expires) - (Model1_dst->Model1_expires)) < 0)))
  Model1_dst->Model1_expires = Model1_expires;
}

/* Output packet to network from transport.  */
static inline __attribute__((no_instrument_function)) int Model1_dst_output(struct Model1_net *Model1_net, struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb_dst(Model1_skb)->Model1_output(Model1_net, Model1_sk, Model1_skb);
}

/* Input packet from network to transport.  */
static inline __attribute__((no_instrument_function)) int Model1_dst_input(struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb_dst(Model1_skb)->Model1_input(Model1_skb);
}

static inline __attribute__((no_instrument_function)) struct Model1_dst_entry *Model1_dst_check(struct Model1_dst_entry *Model1_dst, Model1_u32 Model1_cookie)
{
 if (Model1_dst->Model1_obsolete)
  Model1_dst = Model1_dst->Model1_ops->Model1_check(Model1_dst, Model1_cookie);
 return Model1_dst;
}

void Model1_dst_subsys_init(void);

/* Flags for xfrm_lookup flags argument. */
enum {
 Model1_XFRM_LOOKUP_ICMP = 1 << 0,
 Model1_XFRM_LOOKUP_QUEUE = 1 << 1,
 Model1_XFRM_LOOKUP_KEEP_DST_REF = 1 << 2,
};

struct Model1_flowi;
struct Model1_dst_entry *Model1_xfrm_lookup(struct Model1_net *Model1_net, struct Model1_dst_entry *Model1_dst_orig,
         const struct Model1_flowi *Model1_fl, const struct Model1_sock *Model1_sk,
         int Model1_flags);

struct Model1_dst_entry *Model1_xfrm_lookup_route(struct Model1_net *Model1_net, struct Model1_dst_entry *Model1_dst_orig,
        const struct Model1_flowi *Model1_fl, const struct Model1_sock *Model1_sk,
        int Model1_flags);

/* skb attached with this dst needs transformation if dst->xfrm is valid */
static inline __attribute__((no_instrument_function)) struct Model1_xfrm_state *Model1_dst_xfrm(const struct Model1_dst_entry *Model1_dst)
{
 return Model1_dst->Model1_xfrm;
}
/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for the TCP module.
 *
 * Version:	@(#)tcp.h	1.0.5	05/23/93
 *
 * Authors:	Ross Biro
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */







/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for the TCP protocol.
 *
 * Version:	@(#)tcp.h	1.0.2	04/28/93
 *
 * Author:	Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */






/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for the AF_INET socket handler.
 *
 * Version:	@(#)sock.h	1.0.4	05/13/93
 *
 * Authors:	Ross Biro
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *		Corey Minyard <wf-rch!minyard@relay.EU.net>
 *		Florian La Roche <flla@stud.uni-sb.de>
 *
 * Fixes:
 *		Alan Cox	:	Volatiles in skbuff pointers. See
 *					skbuff comments. May be overdone,
 *					better to prove they can be removed
 *					than the reverse.
 *		Alan Cox	:	Added a zapped field for tcp to note
 *					a socket is reset and must stay shut up
 *		Alan Cox	:	New fields for options
 *	Pauline Middelink	:	identd support
 *		Alan Cox	:	Eliminate low level recv/recvfrom
 *		David S. Miller	:	New socket lookup architecture.
 *              Steve Whitehouse:       Default routines for sock_ops
 *              Arnaldo C. Melo :	removed net_pinfo, tp_pinfo and made
 *              			protinfo be just a void pointer, as the
 *              			protocol specific parts were moved to
 *              			respective headers and ipv4/v6, etc now
 *              			use private slabcaches for its socks
 *              Pedro Hortas	:	New flags field for socket options
 *
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */






static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_pagefault_disabled_inc(void)
{
 Model1_get_current()->Model1_pagefault_disabled++;
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_pagefault_disabled_dec(void)
{
 Model1_get_current()->Model1_pagefault_disabled--;
 ({ int Model1___ret_warn_on = !!(Model1_get_current()->Model1_pagefault_disabled < 0); if (__builtin_expect(!!(Model1___ret_warn_on), 0)) Model1_warn_slowpath_null("./include/linux/uaccess.h", 15); __builtin_expect(!!(Model1___ret_warn_on), 0); });
}

/*
 * These routines enable/disable the pagefault handler. If disabled, it will
 * not take any locks and go straight to the fixup table.
 *
 * User access methods will not sleep when called from a pagefault_disabled()
 * environment.
 */
static inline __attribute__((no_instrument_function)) void Model1_pagefault_disable(void)
{
 Model1_pagefault_disabled_inc();
 /*
	 * make sure to have issued the store before a pagefault
	 * can hit.
	 */
 __asm__ __volatile__("": : :"memory");
}

static inline __attribute__((no_instrument_function)) void Model1_pagefault_enable(void)
{
 /*
	 * make sure to issue those last loads/stores before enabling
	 * the pagefault handler again.
	 */
 __asm__ __volatile__("": : :"memory");
 Model1_pagefault_disabled_dec();
}

/*
 * Is the pagefault handler disabled? If so, user access methods will not sleep.
 */


/*
 * The pagefault handler is in general disabled by pagefault_disable() or
 * when in irq context (via in_atomic()).
 *
 * This function should only be used by the fault handlers. Other users should
 * stick to pagefault_disabled().
 * Please NEVER use preempt_disable() to disable the fault handler. With
 * !CONFIG_PREEMPT_COUNT, this is like a NOP. So the handler won't be disabled.
 * in_atomic() will report different values based on !CONFIG_PREEMPT_COUNT.
 */
/*
 * probe_kernel_read(): safely attempt to read from a location
 * @dst: pointer to the buffer that shall take the data
 * @src: address to read from
 * @size: size of the data chunk
 *
 * Safely read from address @src to the buffer at @dst.  If a kernel fault
 * happens, handle that and return -EFAULT.
 */
extern long Model1_probe_kernel_read(void *Model1_dst, const void *Model1_src, Model1_size_t Model1_size);
extern long Model1___probe_kernel_read(void *Model1_dst, const void *Model1_src, Model1_size_t Model1_size);

/*
 * probe_kernel_write(): safely attempt to write to a location
 * @dst: address to write to
 * @src: pointer to the data that shall be written
 * @size: size of the data chunk
 *
 * Safely write to address @dst from the buffer at @src.  If a kernel fault
 * happens, handle that and return -EFAULT.
 */
extern long __attribute__((no_instrument_function)) Model1_probe_kernel_write(void *Model1_dst, const void *Model1_src, Model1_size_t Model1_size);
extern long __attribute__((no_instrument_function)) Model1___probe_kernel_write(void *Model1_dst, const void *Model1_src, Model1_size_t Model1_size);

extern long Model1_strncpy_from_unsafe(char *Model1_dst, const void *Model1_unsafe_addr, long Model1_count);

/**
 * probe_kernel_address(): safely attempt to read from a location
 * @addr: address to read from
 * @retval: read into this variable
 *
 * Returns 0 on success, or -EFAULT.
 */







struct Model1_page_counter {
 Model1_atomic_long_t Model1_count;
 unsigned long Model1_limit;
 struct Model1_page_counter *Model1_parent;

 /* legacy */
 unsigned long Model1_watermark;
 unsigned long Model1_failcnt;
};







static inline __attribute__((no_instrument_function)) void Model1_page_counter_init(struct Model1_page_counter *Model1_counter,
         struct Model1_page_counter *Model1_parent)
{
 Model1_atomic_long_set(&Model1_counter->Model1_count, 0);
 Model1_counter->Model1_limit = (((long)(~0UL>>1)) / ((1UL) << 12));
 Model1_counter->Model1_parent = Model1_parent;
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_page_counter_read(struct Model1_page_counter *Model1_counter)
{
 return Model1_atomic_long_read(&Model1_counter->Model1_count);
}

void Model1_page_counter_cancel(struct Model1_page_counter *Model1_counter, unsigned long Model1_nr_pages);
void Model1_page_counter_charge(struct Model1_page_counter *Model1_counter, unsigned long Model1_nr_pages);
bool Model1_page_counter_try_charge(struct Model1_page_counter *Model1_counter,
        unsigned long Model1_nr_pages,
        struct Model1_page_counter **Model1_fail);
void Model1_page_counter_uncharge(struct Model1_page_counter *Model1_counter, unsigned long Model1_nr_pages);
int Model1_page_counter_limit(struct Model1_page_counter *Model1_counter, unsigned long Model1_limit);
int Model1_page_counter_memparse(const char *Model1_buf, const char *Model1_max,
     unsigned long *Model1_nr_pages);

static inline __attribute__((no_instrument_function)) void Model1_page_counter_reset_watermark(struct Model1_page_counter *Model1_counter)
{
 Model1_counter->Model1_watermark = Model1_page_counter_read(Model1_counter);
}
/* memcontrol.h - Memory Controller
 *
 * Copyright IBM Corporation, 2007
 * Author Balbir Singh <balbir@linux.vnet.ibm.com>
 *
 * Copyright 2007 OpenVZ SWsoft Inc
 * Author: Pavel Emelianov <xemul@openvz.org>
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 */
/*
 *  include/linux/eventfd.h
 *
 *  Copyright (C) 2007  Davide Libenzi <davidel@xmailserver.org>
 *
 */







/*
 * CAREFUL: Check include/uapi/asm-generic/fcntl.h when defining
 * new flags, since they might collide with O_* ones. We want
 * to re-use O_* flags that couldn't possibly have a meaning
 * from eventfd, in order to leave a free define-space for
 * shared O_* flags.
 */







struct Model1_file;



struct Model1_file *Model1_eventfd_file_create(unsigned int Model1_count, int Model1_flags);
struct Model1_eventfd_ctx *Model1_eventfd_ctx_get(struct Model1_eventfd_ctx *Model1_ctx);
void Model1_eventfd_ctx_put(struct Model1_eventfd_ctx *Model1_ctx);
struct Model1_file *Model1_eventfd_fget(int Model1_fd);
struct Model1_eventfd_ctx *Model1_eventfd_ctx_fdget(int Model1_fd);
struct Model1_eventfd_ctx *Model1_eventfd_ctx_fileget(struct Model1_file *Model1_file);
__u64 Model1_eventfd_signal(struct Model1_eventfd_ctx *Model1_ctx, __u64 Model1_n);
Model1_ssize_t Model1_eventfd_ctx_read(struct Model1_eventfd_ctx *Model1_ctx, int Model1_no_wait, __u64 *Model1_cnt);
int Model1_eventfd_ctx_remove_wait_queue(struct Model1_eventfd_ctx *Model1_ctx, Model1_wait_queue_t *Model1_wait,
      __u64 *Model1_cnt);

struct Model1_vmpressure {
 unsigned long Model1_scanned;
 unsigned long Model1_reclaimed;

 unsigned long Model1_tree_scanned;
 unsigned long Model1_tree_reclaimed;
 /* The lock is used to keep the scanned/reclaimed above in sync. */
 struct Model1_spinlock Model1_sr_lock;

 /* The list of vmpressure_event structs. */
 struct Model1_list_head Model1_events;
 /* Have to grab the lock on events traversal or modifications. */
 struct Model1_mutex Model1_events_lock;

 struct Model1_work_struct Model1_work;
};

struct Model1_mem_cgroup;
static inline __attribute__((no_instrument_function)) void Model1_vmpressure(Model1_gfp_t Model1_gfp, struct Model1_mem_cgroup *Model1_memcg, bool Model1_tree,
         unsigned long Model1_scanned, unsigned long Model1_reclaimed) {}
static inline __attribute__((no_instrument_function)) void Model1_vmpressure_prio(Model1_gfp_t Model1_gfp, struct Model1_mem_cgroup *Model1_memcg,
       int Model1_prio) {}


/*
 * include/linux/writeback.h
 */







/*
 * Floating proportions with flexible aging period
 *
 *  Copyright (C) 2011, SUSE, Jan Kara <jack@suse.cz>
 */
/*
 * When maximum proportion of some event type is specified, this is the
 * precision with which we allow limitting. Note that this creates an upper
 * bound on the number of events per period like
 *   ULLONG_MAX >> FPROP_FRAC_SHIFT.
 */



/*
 * ---- Global proportion definitions ----
 */
struct Model1_fprop_global {
 /* Number of events in the current period */
 struct Model1_percpu_counter Model1_events;
 /* Current period */
 unsigned int Model1_period;
 /* Synchronization with period transitions */
 Model1_seqcount_t Model1_sequence;
};

int Model1_fprop_global_init(struct Model1_fprop_global *Model1_p, Model1_gfp_t Model1_gfp);
void Model1_fprop_global_destroy(struct Model1_fprop_global *Model1_p);
bool Model1_fprop_new_period(struct Model1_fprop_global *Model1_p, int Model1_periods);

/*
 *  ---- SINGLE ----
 */
struct Model1_fprop_local_single {
 /* the local events counter */
 unsigned long Model1_events;
 /* Period in which we last updated events */
 unsigned int Model1_period;
 Model1_raw_spinlock_t Model1_lock; /* Protect period and numerator */
};





int Model1_fprop_local_init_single(struct Model1_fprop_local_single *Model1_pl);
void Model1_fprop_local_destroy_single(struct Model1_fprop_local_single *Model1_pl);
void Model1___fprop_inc_single(struct Model1_fprop_global *Model1_p, struct Model1_fprop_local_single *Model1_pl);
void Model1_fprop_fraction_single(struct Model1_fprop_global *Model1_p,
 struct Model1_fprop_local_single *Model1_pl, unsigned long *Model1_numerator,
 unsigned long *Model1_denominator);

static inline __attribute__((no_instrument_function))
void Model1_fprop_inc_single(struct Model1_fprop_global *Model1_p, struct Model1_fprop_local_single *Model1_pl)
{
 unsigned long Model1_flags;

 do { do { ({ unsigned long Model1___dummy; typeof(Model1_flags) Model1___dummy2; (void)(&Model1___dummy == &Model1___dummy2); 1; }); Model1_flags = Model1_arch_local_irq_save(); } while (0); } while (0);
 Model1___fprop_inc_single(Model1_p, Model1_pl);
 do { do { ({ unsigned long Model1___dummy; typeof(Model1_flags) Model1___dummy2; (void)(&Model1___dummy == &Model1___dummy2); 1; }); Model1_arch_local_irq_restore(Model1_flags); } while (0); } while (0);
}

/*
 * ---- PERCPU ----
 */
struct Model1_fprop_local_percpu {
 /* the local events counter */
 struct Model1_percpu_counter Model1_events;
 /* Period in which we last updated events */
 unsigned int Model1_period;
 Model1_raw_spinlock_t Model1_lock; /* Protect period and numerator */
};

int Model1_fprop_local_init_percpu(struct Model1_fprop_local_percpu *Model1_pl, Model1_gfp_t Model1_gfp);
void Model1_fprop_local_destroy_percpu(struct Model1_fprop_local_percpu *Model1_pl);
void Model1___fprop_inc_percpu(struct Model1_fprop_global *Model1_p, struct Model1_fprop_local_percpu *Model1_pl);
void Model1___fprop_inc_percpu_max(struct Model1_fprop_global *Model1_p, struct Model1_fprop_local_percpu *Model1_pl,
       int Model1_max_frac);
void Model1_fprop_fraction_percpu(struct Model1_fprop_global *Model1_p,
 struct Model1_fprop_local_percpu *Model1_pl, unsigned long *Model1_numerator,
 unsigned long *Model1_denominator);

static inline __attribute__((no_instrument_function))
void Model1_fprop_inc_percpu(struct Model1_fprop_global *Model1_p, struct Model1_fprop_local_percpu *Model1_pl)
{
 unsigned long Model1_flags;

 do { do { ({ unsigned long Model1___dummy; typeof(Model1_flags) Model1___dummy2; (void)(&Model1___dummy == &Model1___dummy2); 1; }); Model1_flags = Model1_arch_local_irq_save(); } while (0); } while (0);
 Model1___fprop_inc_percpu(Model1_p, Model1_pl);
 do { do { ({ unsigned long Model1___dummy; typeof(Model1_flags) Model1___dummy2; (void)(&Model1___dummy == &Model1___dummy2); 1; }); Model1_arch_local_irq_restore(Model1_flags); } while (0); } while (0);
}
struct Model1_page;
struct Model1_device;
struct Model1_dentry;

/*
 * Bits in bdi_writeback.state
 */
enum Model1_wb_state {
 Model1_WB_registered, /* bdi_register() was done */
 Model1_WB_writeback_running, /* Writeback is in progress */
 Model1_WB_has_dirty_io, /* Dirty inodes on ->b_{dirty|io|more_io} */
};

enum Model1_wb_congested_state {
 Model1_WB_async_congested, /* The async (write) queue is getting full */
 Model1_WB_sync_congested, /* The sync queue is getting full */
};

typedef int (Model1_congested_fn)(void *, int);

enum Model1_wb_stat_item {
 Model1_WB_RECLAIMABLE,
 Model1_WB_WRITEBACK,
 Model1_WB_DIRTIED,
 Model1_WB_WRITTEN,
 Model1_NR_WB_STAT_ITEMS
};



/*
 * For cgroup writeback, multiple wb's may map to the same blkcg.  Those
 * wb's can operate mostly independently but should share the congested
 * state.  To facilitate such sharing, the congested state is tracked using
 * the following struct which is created on demand, indexed by blkcg ID on
 * its bdi, and refcounted.
 */
struct Model1_bdi_writeback_congested {
 unsigned long Model1_state; /* WB_[a]sync_congested flags */
 Model1_atomic_t Model1_refcnt; /* nr of attached wb's and blkg */






};

/*
 * Each wb (bdi_writeback) can perform writeback operations, is measured
 * and throttled, independently.  Without cgroup writeback, each bdi
 * (bdi_writeback) is served by its embedded bdi->wb.
 *
 * On the default hierarchy, blkcg implicitly enables memcg.  This allows
 * using memcg's page ownership for attributing writeback IOs, and every
 * memcg - blkcg combination can be served by its own wb by assigning a
 * dedicated wb to each memcg, which enables isolation across different
 * cgroups and propagation of IO back pressure down from the IO layer upto
 * the tasks which are generating the dirty pages to be written back.
 *
 * A cgroup wb is indexed on its bdi by the ID of the associated memcg,
 * refcounted with the number of inodes attached to it, and pins the memcg
 * and the corresponding blkcg.  As the corresponding blkcg for a memcg may
 * change as blkcg is disabled and enabled higher up in the hierarchy, a wb
 * is tested for blkcg after lookup and removed from index on mismatch so
 * that a new wb for the combination can be created.
 */
struct Model1_bdi_writeback {
 struct Model1_backing_dev_info *Model1_bdi; /* our parent bdi */

 unsigned long Model1_state; /* Always use atomic bitops on this */
 unsigned long Model1_last_old_flush; /* last old data flush */

 struct Model1_list_head Model1_b_dirty; /* dirty inodes */
 struct Model1_list_head Model1_b_io; /* parked for writeback */
 struct Model1_list_head Model1_b_more_io; /* parked for more writeback */
 struct Model1_list_head Model1_b_dirty_time; /* time stamps are dirty */
 Model1_spinlock_t Model1_list_lock; /* protects the b_* lists */

 struct Model1_percpu_counter Model1_stat[Model1_NR_WB_STAT_ITEMS];

 struct Model1_bdi_writeback_congested *Model1_congested;

 unsigned long Model1_bw_time_stamp; /* last time write bw is updated */
 unsigned long Model1_dirtied_stamp;
 unsigned long Model1_written_stamp; /* pages written at bw_time_stamp */
 unsigned long Model1_write_bandwidth; /* the estimated write bandwidth */
 unsigned long Model1_avg_write_bandwidth; /* further smoothed write bw, > 0 */

 /*
	 * The base dirty throttle rate, re-calculated on every 200ms.
	 * All the bdi tasks' dirty rate will be curbed under it.
	 * @dirty_ratelimit tracks the estimated @balanced_dirty_ratelimit
	 * in small steps and is much more smooth/stable than the latter.
	 */
 unsigned long Model1_dirty_ratelimit;
 unsigned long Model1_balanced_dirty_ratelimit;

 struct Model1_fprop_local_percpu Model1_completions;
 int Model1_dirty_exceeded;

 Model1_spinlock_t Model1_work_lock; /* protects work_list & dwork scheduling */
 struct Model1_list_head Model1_work_list;
 struct Model1_delayed_work Model1_dwork; /* work item used for writeback */

 struct Model1_list_head Model1_bdi_node; /* anchored at bdi->wb_list */
};

struct Model1_backing_dev_info {
 struct Model1_list_head Model1_bdi_list;
 unsigned long Model1_ra_pages; /* max readahead in PAGE_SIZE units */
 unsigned int Model1_capabilities; /* Device capabilities */
 Model1_congested_fn *Model1_congested_fn; /* Function pointer if device is md/dm */
 void *Model1_congested_data; /* Pointer to aux data for congested func */

 char *Model1_name;

 unsigned int Model1_min_ratio;
 unsigned int Model1_max_ratio, Model1_max_prop_frac;

 /*
	 * Sum of avg_write_bw of wbs with dirty inodes.  > 0 if there are
	 * any dirty wbs, which is depended upon by bdi_has_dirty().
	 */
 Model1_atomic_long_t Model1_tot_write_bandwidth;

 struct Model1_bdi_writeback Model1_wb; /* the root writeback info for this bdi */
 struct Model1_list_head Model1_wb_list; /* list of all wbs */





 struct Model1_bdi_writeback_congested *Model1_wb_congested;

 Model1_wait_queue_head_t Model1_wb_waitq;

 struct Model1_device *Model1_dev;
 struct Model1_device *Model1_owner;

 struct Model1_timer_list Model1_laptop_mode_wb_timer;


 struct Model1_dentry *Model1_debug_dir;
 struct Model1_dentry *Model1_debug_stats;

};

enum {
 Model1_BLK_RW_ASYNC = 0,
 Model1_BLK_RW_SYNC = 1,
};

void Model1_clear_wb_congested(struct Model1_bdi_writeback_congested *Model1_congested, int Model1_sync);
void Model1_set_wb_congested(struct Model1_bdi_writeback_congested *Model1_congested, int Model1_sync);

static inline __attribute__((no_instrument_function)) void Model1_clear_bdi_congested(struct Model1_backing_dev_info *Model1_bdi, int Model1_sync)
{
 Model1_clear_wb_congested(Model1_bdi->Model1_wb.Model1_congested, Model1_sync);
}

static inline __attribute__((no_instrument_function)) void Model1_set_bdi_congested(struct Model1_backing_dev_info *Model1_bdi, int Model1_sync)
{
 Model1_set_wb_congested(Model1_bdi->Model1_wb.Model1_congested, Model1_sync);
}
static inline __attribute__((no_instrument_function)) bool Model1_wb_tryget(struct Model1_bdi_writeback *Model1_wb)
{
 return true;
}

static inline __attribute__((no_instrument_function)) void Model1_wb_get(struct Model1_bdi_writeback *Model1_wb)
{
}

static inline __attribute__((no_instrument_function)) void Model1_wb_put(struct Model1_bdi_writeback *Model1_wb)
{
}

static inline __attribute__((no_instrument_function)) bool Model1_wb_dying(struct Model1_bdi_writeback *Model1_wb)
{
 return false;
}

extern __attribute__((section(".data..percpu" ""))) __typeof__(int) Model1_dirty_throttle_leaks;

/*
 * The 1/4 region under the global dirty thresh is for smooth dirty throttling:
 *
 *	(thresh - thresh/DIRTY_FULL_SCOPE, thresh)
 *
 * Further beyond, all dirtier tasks will enter a loop waiting (possibly long
 * time) for the dirty pages to drop, unless written enough pages.
 *
 * The global dirty threshold is normally equal to the global dirty limit,
 * except when the system suddenly allocates a lot of anonymous memory and
 * knocks down the global dirty threshold quickly, in which case the global
 * dirty limit will follow down slowly to prevent livelocking all dirtier tasks.
 */



struct Model1_backing_dev_info;

/*
 * fs/fs-writeback.c
 */
enum Model1_writeback_sync_modes {
 Model1_WB_SYNC_NONE, /* Don't wait on anything */
 Model1_WB_SYNC_ALL, /* Wait on every mapping */
};

/*
 * why some writeback work was initiated
 */
enum Model1_wb_reason {
 Model1_WB_REASON_BACKGROUND,
 Model1_WB_REASON_TRY_TO_FREE_PAGES,
 Model1_WB_REASON_SYNC,
 Model1_WB_REASON_PERIODIC,
 Model1_WB_REASON_LAPTOP_TIMER,
 Model1_WB_REASON_FREE_MORE_MEM,
 Model1_WB_REASON_FS_FREE_SPACE,
 /*
	 * There is no bdi forker thread any more and works are done
	 * by emergency worker, however, this is TPs userland visible
	 * and we'll be exposing exactly the same information,
	 * so it has a mismatch name.
	 */
 Model1_WB_REASON_FORKER_THREAD,

 Model1_WB_REASON_MAX,
};

/*
 * A control structure which tells the writeback code what to do.  These are
 * always on the stack, and hence need no locking.  They are always initialised
 * in a manner such that unspecified fields are set to zero.
 */
struct Model1_writeback_control {
 long Model1_nr_to_write; /* Write this many pages, and decrement
					   this for each page written */
 long Model1_pages_skipped; /* Pages which were not written */

 /*
	 * For a_ops->writepages(): if start or end are non-zero then this is
	 * a hint that the filesystem need only write out the pages inside that
	 * byterange.  The byte at `end' is included in the writeout request.
	 */
 Model1_loff_t Model1_range_start;
 Model1_loff_t Model1_range_end;

 enum Model1_writeback_sync_modes Model1_sync_mode;

 unsigned Model1_for_kupdate:1; /* A kupdate writeback */
 unsigned Model1_for_background:1; /* A background writeback */
 unsigned Model1_tagged_writepages:1; /* tag-and-write to avoid livelock */
 unsigned Model1_for_reclaim:1; /* Invoked from the page allocator */
 unsigned Model1_range_cyclic:1; /* range_start is cyclic */
 unsigned Model1_for_sync:1; /* sync(2) WB_SYNC_ALL writeback */
};

/*
 * A wb_domain represents a domain that wb's (bdi_writeback's) belong to
 * and are measured against each other in.  There always is one global
 * domain, global_wb_domain, that every wb in the system is a member of.
 * This allows measuring the relative bandwidth of each wb to distribute
 * dirtyable memory accordingly.
 */
struct Model1_wb_domain {
 Model1_spinlock_t Model1_lock;

 /*
	 * Scale the writeback cache size proportional to the relative
	 * writeout speed.
	 *
	 * We do this by keeping a floating proportion between BDIs, based
	 * on page writeback completions [end_page_writeback()]. Those
	 * devices that write out pages fastest will get the larger share,
	 * while the slower will get a smaller share.
	 *
	 * We use page writeout completions because we are interested in
	 * getting rid of dirty pages. Having them written out is the
	 * primary goal.
	 *
	 * We introduce a concept of time, a period over which we measure
	 * these events, because demand can/will vary over time. The length
	 * of this period itself is measured in page writeback completions.
	 */
 struct Model1_fprop_global Model1_completions;
 struct Model1_timer_list Model1_period_timer; /* timer for aging of completions */
 unsigned long Model1_period_time;

 /*
	 * The dirtyable memory and dirty threshold could be suddenly
	 * knocked down by a large amount (eg. on the startup of KVM in a
	 * swapless system). This may throw the system into deep dirty
	 * exceeded state and throttle heavy/light dirtiers alike. To
	 * retain good responsiveness, maintain global_dirty_limit for
	 * tracking slowly down to the knocked down dirty threshold.
	 *
	 * Both fields are protected by ->lock.
	 */
 unsigned long Model1_dirty_limit_tstamp;
 unsigned long Model1_dirty_limit;
};

/**
 * wb_domain_size_changed - memory available to a wb_domain has changed
 * @dom: wb_domain of interest
 *
 * This function should be called when the amount of memory available to
 * @dom has changed.  It resets @dom's dirty limit parameters to prevent
 * the past values which don't match the current configuration from skewing
 * dirty throttling.  Without this, when memory size of a wb_domain is
 * greatly reduced, the dirty throttling logic may allow too many pages to
 * be dirtied leading to consecutive unnecessary OOMs and may get stuck in
 * that situation.
 */
static inline __attribute__((no_instrument_function)) void Model1_wb_domain_size_changed(struct Model1_wb_domain *Model1_dom)
{
 Model1_spin_lock(&Model1_dom->Model1_lock);
 Model1_dom->Model1_dirty_limit_tstamp = Model1_jiffies;
 Model1_dom->Model1_dirty_limit = 0;
 Model1_spin_unlock(&Model1_dom->Model1_lock);
}

/*
 * fs/fs-writeback.c
 */
struct Model1_bdi_writeback;
void Model1_writeback_inodes_sb(struct Model1_super_block *, enum Model1_wb_reason Model1_reason);
void Model1_writeback_inodes_sb_nr(struct Model1_super_block *, unsigned long Model1_nr,
       enum Model1_wb_reason Model1_reason);
bool Model1_try_to_writeback_inodes_sb(struct Model1_super_block *, enum Model1_wb_reason Model1_reason);
bool Model1_try_to_writeback_inodes_sb_nr(struct Model1_super_block *, unsigned long Model1_nr,
       enum Model1_wb_reason Model1_reason);
void Model1_sync_inodes_sb(struct Model1_super_block *);
void Model1_wakeup_flusher_threads(long Model1_nr_pages, enum Model1_wb_reason Model1_reason);
void Model1_inode_wait_for_writeback(struct Model1_inode *Model1_inode);

/* writeback.h requires fs.h; it, too, is not included from here. */
static inline __attribute__((no_instrument_function)) void Model1_wait_on_inode(struct Model1_inode *Model1_inode)
{
 do { Model1__cond_resched(); } while (0);
 Model1_wait_on_bit(&Model1_inode->Model1_i_state, 3, 2);
}
static inline __attribute__((no_instrument_function)) void Model1_inode_attach_wb(struct Model1_inode *Model1_inode, struct Model1_page *Model1_page)
{
}

static inline __attribute__((no_instrument_function)) void Model1_inode_detach_wb(struct Model1_inode *Model1_inode)
{
}

static inline __attribute__((no_instrument_function)) void Model1_wbc_attach_and_unlock_inode(struct Model1_writeback_control *Model1_wbc,
            struct Model1_inode *Model1_inode)

{
 Model1_spin_unlock(&Model1_inode->Model1_i_lock);
}

static inline __attribute__((no_instrument_function)) void Model1_wbc_attach_fdatawrite_inode(struct Model1_writeback_control *Model1_wbc,
            struct Model1_inode *Model1_inode)
{
}

static inline __attribute__((no_instrument_function)) void Model1_wbc_detach_inode(struct Model1_writeback_control *Model1_wbc)
{
}

static inline __attribute__((no_instrument_function)) void Model1_wbc_init_bio(struct Model1_writeback_control *Model1_wbc, struct Model1_bio *Model1_bio)
{
}

static inline __attribute__((no_instrument_function)) void Model1_wbc_account_io(struct Model1_writeback_control *Model1_wbc,
      struct Model1_page *Model1_page, Model1_size_t Model1_bytes)
{
}

static inline __attribute__((no_instrument_function)) void Model1_cgroup_writeback_umount(void)
{
}



/*
 * mm/page-writeback.c
 */

void Model1_laptop_io_completion(struct Model1_backing_dev_info *Model1_info);
void Model1_laptop_sync_completion(void);
void Model1_laptop_mode_sync(struct Model1_work_struct *Model1_work);
void Model1_laptop_mode_timer_fn(unsigned long Model1_data);



void Model1_throttle_vm_writeout(Model1_gfp_t Model1_gfp_mask);
bool Model1_node_dirty_ok(struct Model1_pglist_data *Model1_pgdat);
int Model1_wb_domain_init(struct Model1_wb_domain *Model1_dom, Model1_gfp_t Model1_gfp);




extern struct Model1_wb_domain Model1_global_wb_domain;

/* These are exported to sysctl. */
extern int Model1_dirty_background_ratio;
extern unsigned long Model1_dirty_background_bytes;
extern int Model1_vm_dirty_ratio;
extern unsigned long Model1_vm_dirty_bytes;
extern unsigned int Model1_dirty_writeback_interval;
extern unsigned int Model1_dirty_expire_interval;
extern unsigned int Model1_dirtytime_expire_interval;
extern int Model1_vm_highmem_is_dirtyable;
extern int Model1_block_dump;
extern int Model1_laptop_mode;

extern int Model1_dirty_background_ratio_handler(struct Model1_ctl_table *Model1_table, int Model1_write,
  void *Model1_buffer, Model1_size_t *Model1_lenp,
  Model1_loff_t *Model1_ppos);
extern int Model1_dirty_background_bytes_handler(struct Model1_ctl_table *Model1_table, int Model1_write,
  void *Model1_buffer, Model1_size_t *Model1_lenp,
  Model1_loff_t *Model1_ppos);
extern int Model1_dirty_ratio_handler(struct Model1_ctl_table *Model1_table, int Model1_write,
  void *Model1_buffer, Model1_size_t *Model1_lenp,
  Model1_loff_t *Model1_ppos);
extern int Model1_dirty_bytes_handler(struct Model1_ctl_table *Model1_table, int Model1_write,
  void *Model1_buffer, Model1_size_t *Model1_lenp,
  Model1_loff_t *Model1_ppos);
int Model1_dirtytime_interval_handler(struct Model1_ctl_table *Model1_table, int Model1_write,
          void *Model1_buffer, Model1_size_t *Model1_lenp, Model1_loff_t *Model1_ppos);

struct Model1_ctl_table;
int Model1_dirty_writeback_centisecs_handler(struct Model1_ctl_table *, int,
          void *, Model1_size_t *, Model1_loff_t *);

void Model1_global_dirty_limits(unsigned long *Model1_pbackground, unsigned long *Model1_pdirty);
unsigned long Model1_wb_calc_thresh(struct Model1_bdi_writeback *Model1_wb, unsigned long Model1_thresh);

void Model1_wb_update_bandwidth(struct Model1_bdi_writeback *Model1_wb, unsigned long Model1_start_time);
void Model1_page_writeback_init(void);
void Model1_balance_dirty_pages_ratelimited(struct Model1_address_space *Model1_mapping);
bool Model1_wb_over_bg_thresh(struct Model1_bdi_writeback *Model1_wb);

typedef int (*Model1_writepage_t)(struct Model1_page *Model1_page, struct Model1_writeback_control *Model1_wbc,
    void *Model1_data);

int Model1_generic_writepages(struct Model1_address_space *Model1_mapping,
         struct Model1_writeback_control *Model1_wbc);
void Model1_tag_pages_for_writeback(struct Model1_address_space *Model1_mapping,
        unsigned long Model1_start, unsigned long Model1_end);
int Model1_write_cache_pages(struct Model1_address_space *Model1_mapping,
        struct Model1_writeback_control *Model1_wbc, Model1_writepage_t Model1_writepage,
        void *Model1_data);
int Model1_do_writepages(struct Model1_address_space *Model1_mapping, struct Model1_writeback_control *Model1_wbc);
void Model1_writeback_set_ratelimit(void);
void Model1_tag_pages_for_writeback(struct Model1_address_space *Model1_mapping,
        unsigned long Model1_start, unsigned long Model1_end);

void Model1_account_page_redirty(struct Model1_page *Model1_page);

void Model1_sb_mark_inode_writeback(struct Model1_inode *Model1_inode);
void Model1_sb_clear_inode_writeback(struct Model1_inode *Model1_inode);


struct Model1_mem_cgroup;
struct Model1_page;
struct Model1_mm_struct;
struct Model1_kmem_cache;

/*
 * The corresponding mem_cgroup_stat_names is defined in mm/memcontrol.c,
 * These two lists should keep in accord with each other.
 */
enum Model1_mem_cgroup_stat_index {
 /*
	 * For MEM_CONTAINER_TYPE_ALL, usage = pagecache + rss.
	 */
 Model1_MEM_CGROUP_STAT_CACHE, /* # of pages charged as cache */
 Model1_MEM_CGROUP_STAT_RSS, /* # of pages charged as anon rss */
 Model1_MEM_CGROUP_STAT_RSS_HUGE, /* # of pages charged as anon huge */
 Model1_MEM_CGROUP_STAT_FILE_MAPPED, /* # of pages charged as file rss */
 Model1_MEM_CGROUP_STAT_DIRTY, /* # of dirty pages in page cache */
 Model1_MEM_CGROUP_STAT_WRITEBACK, /* # of pages under writeback */
 Model1_MEM_CGROUP_STAT_SWAP, /* # of pages, swapped out */
 Model1_MEM_CGROUP_STAT_NSTATS,
 /* default hierarchy stats */
 Model1_MEMCG_KERNEL_STACK_KB = Model1_MEM_CGROUP_STAT_NSTATS,
 Model1_MEMCG_SLAB_RECLAIMABLE,
 Model1_MEMCG_SLAB_UNRECLAIMABLE,
 Model1_MEMCG_SOCK,
 Model1_MEMCG_NR_STAT,
};

struct Model1_mem_cgroup_reclaim_cookie {
 Model1_pg_data_t *Model1_pgdat;
 int Model1_priority;
 unsigned int Model1_generation;
};

enum Model1_mem_cgroup_events_index {
 Model1_MEM_CGROUP_EVENTS_PGPGIN, /* # of pages paged in */
 Model1_MEM_CGROUP_EVENTS_PGPGOUT, /* # of pages paged out */
 Model1_MEM_CGROUP_EVENTS_PGFAULT, /* # of page-faults */
 Model1_MEM_CGROUP_EVENTS_PGMAJFAULT, /* # of major page-faults */
 Model1_MEM_CGROUP_EVENTS_NSTATS,
 /* default hierarchy events */
 Model1_MEMCG_LOW = Model1_MEM_CGROUP_EVENTS_NSTATS,
 Model1_MEMCG_HIGH,
 Model1_MEMCG_MAX,
 Model1_MEMCG_OOM,
 Model1_MEMCG_NR_EVENTS,
};

/*
 * Per memcg event counter is incremented at every pagein/pageout. With THP,
 * it will be incremated by the number of pages. This counter is used for
 * for trigger some periodic events. This is straightforward and better
 * than using jiffies etc. to handle periodic memcg event.
 */
enum Model1_mem_cgroup_events_target {
 Model1_MEM_CGROUP_TARGET_THRESH,
 Model1_MEM_CGROUP_TARGET_SOFTLIMIT,
 Model1_MEM_CGROUP_TARGET_NUMAINFO,
 Model1_MEM_CGROUP_NTARGETS,
};
struct Model1_mem_cgroup;

static inline __attribute__((no_instrument_function)) bool Model1_mem_cgroup_disabled(void)
{
 return true;
}

static inline __attribute__((no_instrument_function)) void Model1_mem_cgroup_events(struct Model1_mem_cgroup *Model1_memcg,
         enum Model1_mem_cgroup_events_index Model1_idx,
         unsigned int Model1_nr)
{
}

static inline __attribute__((no_instrument_function)) bool Model1_mem_cgroup_low(struct Model1_mem_cgroup *Model1_root,
      struct Model1_mem_cgroup *Model1_memcg)
{
 return false;
}

static inline __attribute__((no_instrument_function)) int Model1_mem_cgroup_try_charge(struct Model1_page *Model1_page, struct Model1_mm_struct *Model1_mm,
     Model1_gfp_t Model1_gfp_mask,
     struct Model1_mem_cgroup **Model1_memcgp,
     bool Model1_compound)
{
 *Model1_memcgp = ((void *)0);
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model1_mem_cgroup_commit_charge(struct Model1_page *Model1_page,
         struct Model1_mem_cgroup *Model1_memcg,
         bool Model1_lrucare, bool Model1_compound)
{
}

static inline __attribute__((no_instrument_function)) void Model1_mem_cgroup_cancel_charge(struct Model1_page *Model1_page,
         struct Model1_mem_cgroup *Model1_memcg,
         bool Model1_compound)
{
}

static inline __attribute__((no_instrument_function)) void Model1_mem_cgroup_uncharge(struct Model1_page *Model1_page)
{
}

static inline __attribute__((no_instrument_function)) void Model1_mem_cgroup_uncharge_list(struct Model1_list_head *Model1_page_list)
{
}

static inline __attribute__((no_instrument_function)) void Model1_mem_cgroup_migrate(struct Model1_page *old, struct Model1_page *Model1_new)
{
}

static inline __attribute__((no_instrument_function)) struct Model1_lruvec *Model1_mem_cgroup_lruvec(struct Model1_pglist_data *Model1_pgdat,
    struct Model1_mem_cgroup *Model1_memcg)
{
 return Model1_node_lruvec(Model1_pgdat);
}

static inline __attribute__((no_instrument_function)) struct Model1_lruvec *Model1_mem_cgroup_page_lruvec(struct Model1_page *Model1_page,
          struct Model1_pglist_data *Model1_pgdat)
{
 return &Model1_pgdat->Model1_lruvec;
}

static inline __attribute__((no_instrument_function)) bool Model1_mm_match_cgroup(struct Model1_mm_struct *Model1_mm,
  struct Model1_mem_cgroup *Model1_memcg)
{
 return true;
}

static inline __attribute__((no_instrument_function)) bool Model1_task_in_mem_cgroup(struct Model1_task_struct *Model1_task,
          const struct Model1_mem_cgroup *Model1_memcg)
{
 return true;
}

static inline __attribute__((no_instrument_function)) struct Model1_mem_cgroup *
Model1_mem_cgroup_iter(struct Model1_mem_cgroup *Model1_root,
  struct Model1_mem_cgroup *Model1_prev,
  struct Model1_mem_cgroup_reclaim_cookie *Model1_reclaim)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) void Model1_mem_cgroup_iter_break(struct Model1_mem_cgroup *Model1_root,
      struct Model1_mem_cgroup *Model1_prev)
{
}

static inline __attribute__((no_instrument_function)) unsigned short Model1_mem_cgroup_id(struct Model1_mem_cgroup *Model1_memcg)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) struct Model1_mem_cgroup *Model1_mem_cgroup_from_id(unsigned short Model1_id)
{
 ({ static bool __attribute__ ((__section__(".data.unlikely"))) Model1___warned; int Model1___ret_warn_once = !!(Model1_id); if (__builtin_expect(!!(Model1___ret_warn_once && !Model1___warned), 0)) { Model1___warned = true; ({ int Model1___ret_warn_on = !!(1); if (__builtin_expect(!!(Model1___ret_warn_on), 0)) Model1_warn_slowpath_null("./include/linux/memcontrol.h", 649); __builtin_expect(!!(Model1___ret_warn_on), 0); }); } __builtin_expect(!!(Model1___ret_warn_once), 0); });
 /* XXX: This should always return root_mem_cgroup */
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) bool Model1_mem_cgroup_online(struct Model1_mem_cgroup *Model1_memcg)
{
 return true;
}

static inline __attribute__((no_instrument_function)) unsigned long
Model1_mem_cgroup_get_lru_size(struct Model1_lruvec *Model1_lruvec, enum Model1_lru_list Model1_lru)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) unsigned long
Model1_mem_cgroup_node_nr_lru_pages(struct Model1_mem_cgroup *Model1_memcg,
        int Model1_nid, unsigned int Model1_lru_mask)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) void
Model1_mem_cgroup_print_oom_info(struct Model1_mem_cgroup *Model1_memcg, struct Model1_task_struct *Model1_p)
{
}

static inline __attribute__((no_instrument_function)) void Model1_lock_page_memcg(struct Model1_page *Model1_page)
{
}

static inline __attribute__((no_instrument_function)) void Model1_unlock_page_memcg(struct Model1_page *Model1_page)
{
}

static inline __attribute__((no_instrument_function)) void Model1_mem_cgroup_handle_over_high(void)
{
}

static inline __attribute__((no_instrument_function)) void Model1_mem_cgroup_oom_enable(void)
{
}

static inline __attribute__((no_instrument_function)) void Model1_mem_cgroup_oom_disable(void)
{
}

static inline __attribute__((no_instrument_function)) bool Model1_task_in_memcg_oom(struct Model1_task_struct *Model1_p)
{
 return false;
}

static inline __attribute__((no_instrument_function)) bool Model1_mem_cgroup_oom_synchronize(bool Model1_wait)
{
 return false;
}

static inline __attribute__((no_instrument_function)) void Model1_mem_cgroup_inc_page_stat(struct Model1_page *Model1_page,
         enum Model1_mem_cgroup_stat_index Model1_idx)
{
}

static inline __attribute__((no_instrument_function)) void Model1_mem_cgroup_dec_page_stat(struct Model1_page *Model1_page,
         enum Model1_mem_cgroup_stat_index Model1_idx)
{
}

static inline __attribute__((no_instrument_function))
unsigned long Model1_mem_cgroup_soft_limit_reclaim(Model1_pg_data_t *Model1_pgdat, int Model1_order,
         Model1_gfp_t Model1_gfp_mask,
         unsigned long *Model1_total_scanned)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model1_mem_cgroup_split_huge_fixup(struct Model1_page *Model1_head)
{
}

static inline __attribute__((no_instrument_function))
void Model1_mem_cgroup_count_vm_event(struct Model1_mm_struct *Model1_mm, enum Model1_vm_event_item Model1_idx)
{
}
static inline __attribute__((no_instrument_function)) struct Model1_wb_domain *Model1_mem_cgroup_wb_domain(struct Model1_bdi_writeback *Model1_wb)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) void Model1_mem_cgroup_wb_stats(struct Model1_bdi_writeback *Model1_wb,
           unsigned long *Model1_pfilepages,
           unsigned long *Model1_pheadroom,
           unsigned long *Model1_pdirty,
           unsigned long *Model1_pwriteback)
{
}



struct Model1_sock;
void Model1_sock_update_memcg(struct Model1_sock *Model1_sk);
void Model1_sock_release_memcg(struct Model1_sock *Model1_sk);
bool Model1_mem_cgroup_charge_skmem(struct Model1_mem_cgroup *Model1_memcg, unsigned int Model1_nr_pages);
void Model1_mem_cgroup_uncharge_skmem(struct Model1_mem_cgroup *Model1_memcg, unsigned int Model1_nr_pages);
static inline __attribute__((no_instrument_function)) bool Model1_mem_cgroup_under_socket_pressure(struct Model1_mem_cgroup *Model1_memcg)
{
 return false;
}


struct Model1_kmem_cache *Model1_memcg_kmem_get_cache(struct Model1_kmem_cache *Model1_cachep);
void Model1_memcg_kmem_put_cache(struct Model1_kmem_cache *Model1_cachep);
int Model1_memcg_kmem_charge_memcg(struct Model1_page *Model1_page, Model1_gfp_t Model1_gfp, int Model1_order,
       struct Model1_mem_cgroup *Model1_memcg);
int Model1_memcg_kmem_charge(struct Model1_page *Model1_page, Model1_gfp_t Model1_gfp, int Model1_order);
void Model1_memcg_kmem_uncharge(struct Model1_page *Model1_page, int Model1_order);
static inline __attribute__((no_instrument_function)) bool Model1_memcg_kmem_enabled(void)
{
 return false;
}

static inline __attribute__((no_instrument_function)) int Model1_memcg_cache_id(struct Model1_mem_cgroup *Model1_memcg)
{
 return -1;
}

static inline __attribute__((no_instrument_function)) void Model1_memcg_get_cache_ids(void)
{
}

static inline __attribute__((no_instrument_function)) void Model1_memcg_put_cache_ids(void)
{
}

static inline __attribute__((no_instrument_function)) void Model1_memcg_kmem_update_page_stat(struct Model1_page *Model1_page,
    enum Model1_mem_cgroup_stat_index Model1_idx, int Model1_val)
{
}




/*
 * Linux Socket Filter Data Structures
 */








enum {
 Model1_TCA_STATS_UNSPEC,
 Model1_TCA_STATS_BASIC,
 Model1_TCA_STATS_RATE_EST,
 Model1_TCA_STATS_QUEUE,
 Model1_TCA_STATS_APP,
 Model1_TCA_STATS_RATE_EST64,
 Model1_TCA_STATS_PAD,
 Model1___TCA_STATS_MAX,
};


/**
 * struct gnet_stats_basic - byte/packet throughput statistics
 * @bytes: number of seen bytes
 * @packets: number of seen packets
 */
struct Model1_gnet_stats_basic {
 __u64 Model1_bytes;
 __u32 Model1_packets;
};
struct Model1_gnet_stats_basic_packed {
 __u64 Model1_bytes;
 __u32 Model1_packets;
} __attribute__ ((packed));

/**
 * struct gnet_stats_rate_est - rate estimator
 * @bps: current byte rate
 * @pps: current packet rate
 */
struct Model1_gnet_stats_rate_est {
 __u32 Model1_bps;
 __u32 Model1_pps;
};

/**
 * struct gnet_stats_rate_est64 - rate estimator
 * @bps: current byte rate
 * @pps: current packet rate
 */
struct Model1_gnet_stats_rate_est64 {
 __u64 Model1_bps;
 __u64 Model1_pps;
};

/**
 * struct gnet_stats_queue - queuing statistics
 * @qlen: queue length
 * @backlog: backlog size of queue
 * @drops: number of dropped packets
 * @requeues: number of requeues
 * @overlimits: number of enqueues over the limit
 */
struct Model1_gnet_stats_queue {
 __u32 Model1_qlen;
 __u32 Model1_backlog;
 __u32 Model1_drops;
 __u32 Model1_requeues;
 __u32 Model1_overlimits;
};

/**
 * struct gnet_estimator - rate estimator configuration
 * @interval: sampling period
 * @ewma_log: the log of measurement window weight
 */
struct Model1_gnet_estimator {
 signed char Model1_interval;
 unsigned char Model1_ewma_log;
};




struct Model1_gnet_stats_basic_cpu {
 struct Model1_gnet_stats_basic_packed Model1_bstats;
 struct Model1_u64_stats_sync Model1_syncp;
};

struct Model1_gnet_dump {
 Model1_spinlock_t * Model1_lock;
 struct Model1_sk_buff * Model1_skb;
 struct Model1_nlattr * Model1_tail;

 /* Backward compatibility */
 int Model1_compat_tc_stats;
 int Model1_compat_xstats;
 int Model1_padattr;
 void * Model1_xstats;
 int Model1_xstats_len;
 struct Model1_tc_stats Model1_tc_stats;
};

int Model1_gnet_stats_start_copy(struct Model1_sk_buff *Model1_skb, int Model1_type, Model1_spinlock_t *Model1_lock,
     struct Model1_gnet_dump *Model1_d, int Model1_padattr);

int Model1_gnet_stats_start_copy_compat(struct Model1_sk_buff *Model1_skb, int Model1_type,
     int Model1_tc_stats_type, int Model1_xstats_type,
     Model1_spinlock_t *Model1_lock, struct Model1_gnet_dump *Model1_d,
     int Model1_padattr);

int Model1_gnet_stats_copy_basic(const Model1_seqcount_t *Model1_running,
     struct Model1_gnet_dump *Model1_d,
     struct Model1_gnet_stats_basic_cpu *Model1_cpu,
     struct Model1_gnet_stats_basic_packed *Model1_b);
void Model1___gnet_stats_copy_basic(const Model1_seqcount_t *Model1_running,
        struct Model1_gnet_stats_basic_packed *Model1_bstats,
        struct Model1_gnet_stats_basic_cpu *Model1_cpu,
        struct Model1_gnet_stats_basic_packed *Model1_b);
int Model1_gnet_stats_copy_rate_est(struct Model1_gnet_dump *Model1_d,
        const struct Model1_gnet_stats_basic_packed *Model1_b,
        struct Model1_gnet_stats_rate_est64 *Model1_r);
int Model1_gnet_stats_copy_queue(struct Model1_gnet_dump *Model1_d,
     struct Model1_gnet_stats_queue *Model1_cpu_q,
     struct Model1_gnet_stats_queue *Model1_q, __u32 Model1_qlen);
int Model1_gnet_stats_copy_app(struct Model1_gnet_dump *Model1_d, void *Model1_st, int Model1_len);

int Model1_gnet_stats_finish_copy(struct Model1_gnet_dump *Model1_d);

int Model1_gen_new_estimator(struct Model1_gnet_stats_basic_packed *Model1_bstats,
        struct Model1_gnet_stats_basic_cpu *Model1_cpu_bstats,
        struct Model1_gnet_stats_rate_est64 *Model1_rate_est,
        Model1_spinlock_t *Model1_stats_lock,
        Model1_seqcount_t *Model1_running, struct Model1_nlattr *Model1_opt);
void Model1_gen_kill_estimator(struct Model1_gnet_stats_basic_packed *Model1_bstats,
   struct Model1_gnet_stats_rate_est64 *Model1_rate_est);
int Model1_gen_replace_estimator(struct Model1_gnet_stats_basic_packed *Model1_bstats,
     struct Model1_gnet_stats_basic_cpu *Model1_cpu_bstats,
     struct Model1_gnet_stats_rate_est64 *Model1_rate_est,
     Model1_spinlock_t *Model1_stats_lock,
     Model1_seqcount_t *Model1_running, struct Model1_nlattr *Model1_opt);
bool Model1_gen_estimator_active(const struct Model1_gnet_stats_basic_packed *Model1_bstats,
     const struct Model1_gnet_stats_rate_est64 *Model1_rate_est);


struct Model1_Qdisc_ops;
struct Model1_qdisc_walker;
struct Model1_tcf_walker;
struct Model1_module;

struct Model1_qdisc_rate_table {
 struct Model1_tc_ratespec Model1_rate;
 Model1_u32 Model1_data[256];
 struct Model1_qdisc_rate_table *Model1_next;
 int Model1_refcnt;
};

enum Model1_qdisc_state_t {
 Model1___QDISC_STATE_SCHED,
 Model1___QDISC_STATE_DEACTIVATED,
};

struct Model1_qdisc_size_table {
 struct Model1_callback_head Model1_rcu;
 struct Model1_list_head Model1_list;
 struct Model1_tc_sizespec Model1_szopts;
 int Model1_refcnt;
 Model1_u16 Model1_data[];
};

struct Model1_Qdisc {
 int (*Model1_enqueue)(struct Model1_sk_buff *Model1_skb,
        struct Model1_Qdisc *Model1_sch,
        struct Model1_sk_buff **Model1_to_free);
 struct Model1_sk_buff * (*Model1_dequeue)(struct Model1_Qdisc *Model1_sch);
 unsigned int Model1_flags;
 Model1_u32 Model1_limit;
 const struct Model1_Qdisc_ops *Model1_ops;
 struct Model1_qdisc_size_table *Model1_stab;
 struct Model1_list_head Model1_list;
 Model1_u32 Model1_handle;
 Model1_u32 Model1_parent;
 void *Model1_u32_node;

 struct Model1_netdev_queue *Model1_dev_queue;

 struct Model1_gnet_stats_rate_est64 Model1_rate_est;
 struct Model1_gnet_stats_basic_cpu *Model1_cpu_bstats;
 struct Model1_gnet_stats_queue *Model1_cpu_qstats;

 /*
	 * For performance sake on SMP, we put highly modified fields at the end
	 */
 struct Model1_sk_buff *Model1_gso_skb __attribute__((__aligned__((1 << (6)))));
 struct Model1_sk_buff_head Model1_q;
 struct Model1_gnet_stats_basic_packed Model1_bstats;
 Model1_seqcount_t Model1_running;
 struct Model1_gnet_stats_queue Model1_qstats;
 unsigned long Model1_state;
 struct Model1_Qdisc *Model1_next_sched;
 struct Model1_sk_buff *Model1_skb_bad_txq;
 struct Model1_callback_head Model1_callback_head;
 int Model1_padded;
 Model1_atomic_t Model1_refcnt;

 Model1_spinlock_t Model1_busylock __attribute__((__aligned__((1 << (6)))));
};

static inline __attribute__((no_instrument_function)) bool Model1_qdisc_is_running(const struct Model1_Qdisc *Model1_qdisc)
{
 return (Model1_raw_read_seqcount(&Model1_qdisc->Model1_running) & 1) ? true : false;
}

static inline __attribute__((no_instrument_function)) bool Model1_qdisc_run_begin(struct Model1_Qdisc *Model1_qdisc)
{
 if (Model1_qdisc_is_running(Model1_qdisc))
  return false;
 /* Variant of write_seqcount_begin() telling lockdep a trylock
	 * was attempted.
	 */
 Model1_raw_write_seqcount_begin(&Model1_qdisc->Model1_running);
 do { } while (0);
 return true;
}

static inline __attribute__((no_instrument_function)) void Model1_qdisc_run_end(struct Model1_Qdisc *Model1_qdisc)
{
 Model1_write_seqcount_end(&Model1_qdisc->Model1_running);
}

static inline __attribute__((no_instrument_function)) bool Model1_qdisc_may_bulk(const struct Model1_Qdisc *Model1_qdisc)
{
 return Model1_qdisc->Model1_flags & 0x10;
}

static inline __attribute__((no_instrument_function)) int Model1_qdisc_avail_bulklimit(const struct Model1_netdev_queue *Model1_txq)
{

 /* Non-BQL migrated drivers will return 0, too. */
 return Model1_dql_avail(&Model1_txq->Model1_dql);



}

struct Model1_Qdisc_class_ops {
 /* Child qdisc manipulation */
 struct Model1_netdev_queue * (*Model1_select_queue)(struct Model1_Qdisc *, struct Model1_tcmsg *);
 int (*Model1_graft)(struct Model1_Qdisc *, unsigned long Model1_cl,
     struct Model1_Qdisc *, struct Model1_Qdisc **);
 struct Model1_Qdisc * (*Model1_leaf)(struct Model1_Qdisc *, unsigned long Model1_cl);
 void (*Model1_qlen_notify)(struct Model1_Qdisc *, unsigned long);

 /* Class manipulation routines */
 unsigned long (*Model1_get)(struct Model1_Qdisc *, Model1_u32 Model1_classid);
 void (*Model1_put)(struct Model1_Qdisc *, unsigned long);
 int (*Model1_change)(struct Model1_Qdisc *, Model1_u32, Model1_u32,
     struct Model1_nlattr **, unsigned long *);
 int (*Model1_delete)(struct Model1_Qdisc *, unsigned long);
 void (*Model1_walk)(struct Model1_Qdisc *, struct Model1_qdisc_walker * Model1_arg);

 /* Filter manipulation */
 struct Model1_tcf_proto ** (*Model1_tcf_chain)(struct Model1_Qdisc *, unsigned long);
 bool (*Model1_tcf_cl_offload)(Model1_u32 Model1_classid);
 unsigned long (*Model1_bind_tcf)(struct Model1_Qdisc *, unsigned long,
     Model1_u32 Model1_classid);
 void (*Model1_unbind_tcf)(struct Model1_Qdisc *, unsigned long);

 /* rtnetlink specific */
 int (*Model1_dump)(struct Model1_Qdisc *, unsigned long,
     struct Model1_sk_buff *Model1_skb, struct Model1_tcmsg*);
 int (*Model1_dump_stats)(struct Model1_Qdisc *, unsigned long,
     struct Model1_gnet_dump *);
};

struct Model1_Qdisc_ops {
 struct Model1_Qdisc_ops *Model1_next;
 const struct Model1_Qdisc_class_ops *Model1_cl_ops;
 char Model1_id[16];
 int Model1_priv_size;

 int (*Model1_enqueue)(struct Model1_sk_buff *Model1_skb,
        struct Model1_Qdisc *Model1_sch,
        struct Model1_sk_buff **Model1_to_free);
 struct Model1_sk_buff * (*Model1_dequeue)(struct Model1_Qdisc *);
 struct Model1_sk_buff * (*Model1_peek)(struct Model1_Qdisc *);

 int (*Model1_init)(struct Model1_Qdisc *, struct Model1_nlattr *Model1_arg);
 void (*Model1_reset)(struct Model1_Qdisc *);
 void (*Model1_destroy)(struct Model1_Qdisc *);
 int (*Model1_change)(struct Model1_Qdisc *, struct Model1_nlattr *Model1_arg);
 void (*Model1_attach)(struct Model1_Qdisc *);

 int (*Model1_dump)(struct Model1_Qdisc *, struct Model1_sk_buff *);
 int (*Model1_dump_stats)(struct Model1_Qdisc *, struct Model1_gnet_dump *);

 struct Model1_module *Model1_owner;
};


struct Model1_tcf_result {
 unsigned long Model1_class;
 Model1_u32 Model1_classid;
};

struct Model1_tcf_proto_ops {
 struct Model1_list_head Model1_head;
 char Model1_kind[16];

 int (*Model1_classify)(struct Model1_sk_buff *,
         const struct Model1_tcf_proto *,
         struct Model1_tcf_result *);
 int (*Model1_init)(struct Model1_tcf_proto*);
 bool (*Model1_destroy)(struct Model1_tcf_proto*, bool);

 unsigned long (*Model1_get)(struct Model1_tcf_proto*, Model1_u32 Model1_handle);
 int (*Model1_change)(struct Model1_net *Model1_net, struct Model1_sk_buff *,
     struct Model1_tcf_proto*, unsigned long,
     Model1_u32 Model1_handle, struct Model1_nlattr **,
     unsigned long *, bool);
 int (*Model1_delete)(struct Model1_tcf_proto*, unsigned long);
 void (*Model1_walk)(struct Model1_tcf_proto*, struct Model1_tcf_walker *Model1_arg);

 /* rtnetlink specific */
 int (*Model1_dump)(struct Model1_net*, struct Model1_tcf_proto*, unsigned long,
     struct Model1_sk_buff *Model1_skb, struct Model1_tcmsg*);

 struct Model1_module *Model1_owner;
};

struct Model1_tcf_proto {
 /* Fast access part */
 struct Model1_tcf_proto *Model1_next;
 void *Model1_root;
 int (*Model1_classify)(struct Model1_sk_buff *,
         const struct Model1_tcf_proto *,
         struct Model1_tcf_result *);
 Model1___be16 Model1_protocol;

 /* All the rest */
 Model1_u32 Model1_prio;
 Model1_u32 Model1_classid;
 struct Model1_Qdisc *Model1_q;
 void *Model1_data;
 const struct Model1_tcf_proto_ops *Model1_ops;
 struct Model1_callback_head Model1_rcu;
};

struct Model1_qdisc_skb_cb {
 unsigned int Model1_pkt_len;
 Model1_u16 Model1_slave_dev_queue_mapping;
 Model1_u16 Model1_tc_classid;

 unsigned char Model1_data[20];
};

static inline __attribute__((no_instrument_function)) void Model1_qdisc_cb_private_validate(const struct Model1_sk_buff *Model1_skb, int Model1_sz)
{
 struct Model1_qdisc_skb_cb *Model1_qcb;

 do { bool Model1___cond = !(!(sizeof(Model1_skb->Model1_cb) < __builtin_offsetof(struct Model1_qdisc_skb_cb, Model1_data) + Model1_sz)); extern void Model1___compiletime_assert_245(void) ; if (Model1___cond) Model1___compiletime_assert_245(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
 do { bool Model1___cond = !(!(sizeof(Model1_qcb->Model1_data) < Model1_sz)); extern void Model1___compiletime_assert_246(void) ; if (Model1___cond) Model1___compiletime_assert_246(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
}

static inline __attribute__((no_instrument_function)) int Model1_qdisc_qlen(const struct Model1_Qdisc *Model1_q)
{
 return Model1_q->Model1_q.Model1_qlen;
}

static inline __attribute__((no_instrument_function)) struct Model1_qdisc_skb_cb *Model1_qdisc_skb_cb(const struct Model1_sk_buff *Model1_skb)
{
 return (struct Model1_qdisc_skb_cb *)Model1_skb->Model1_cb;
}

static inline __attribute__((no_instrument_function)) Model1_spinlock_t *Model1_qdisc_lock(struct Model1_Qdisc *Model1_qdisc)
{
 return &Model1_qdisc->Model1_q.Model1_lock;
}

static inline __attribute__((no_instrument_function)) struct Model1_Qdisc *Model1_qdisc_root(const struct Model1_Qdisc *Model1_qdisc)
{
 struct Model1_Qdisc *Model1_q = ({ typeof(*(Model1_qdisc->Model1_dev_queue->Model1_qdisc)) *Model1_________p1 = (typeof(*(Model1_qdisc->Model1_dev_queue->Model1_qdisc)) *)({ typeof((Model1_qdisc->Model1_dev_queue->Model1_qdisc)) Model1__________p1 = ({ union { typeof((Model1_qdisc->Model1_dev_queue->Model1_qdisc)) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&((Model1_qdisc->Model1_dev_queue->Model1_qdisc)), Model1___u.Model1___c, sizeof((Model1_qdisc->Model1_dev_queue->Model1_qdisc))); else Model1___read_once_size_nocheck(&((Model1_qdisc->Model1_dev_queue->Model1_qdisc)), Model1___u.Model1___c, sizeof((Model1_qdisc->Model1_dev_queue->Model1_qdisc))); Model1___u.Model1___val; }); typeof(*((Model1_qdisc->Model1_dev_queue->Model1_qdisc))) *Model1____typecheck_p __attribute__((unused)); do { } while (0); (Model1__________p1); }); do { } while (0); ; ((typeof(*(Model1_qdisc->Model1_dev_queue->Model1_qdisc)) *)(Model1_________p1)); });

 return Model1_q;
}

static inline __attribute__((no_instrument_function)) struct Model1_Qdisc *Model1_qdisc_root_sleeping(const struct Model1_Qdisc *Model1_qdisc)
{
 return Model1_qdisc->Model1_dev_queue->Model1_qdisc_sleeping;
}

/* The qdisc root lock is a mechanism by which to top level
 * of a qdisc tree can be locked from any qdisc node in the
 * forest.  This allows changing the configuration of some
 * aspect of the qdisc tree while blocking out asynchronous
 * qdisc access in the packet processing paths.
 *
 * It is only legal to do this when the root will not change
 * on us.  Otherwise we'll potentially lock the wrong qdisc
 * root.  This is enforced by holding the RTNL semaphore, which
 * all users of this lock accessor must do.
 */
static inline __attribute__((no_instrument_function)) Model1_spinlock_t *Model1_qdisc_root_lock(const struct Model1_Qdisc *Model1_qdisc)
{
 struct Model1_Qdisc *Model1_root = Model1_qdisc_root(Model1_qdisc);

 do { if (__builtin_expect(!!(!Model1_rtnl_is_locked()), 0)) { Model1_printk("\001" "3" "RTNL: assertion failed at %s (%d)\n", "./include/net/sch_generic.h", 291); Model1_dump_stack(); } } while(0);
 return Model1_qdisc_lock(Model1_root);
}

static inline __attribute__((no_instrument_function)) Model1_spinlock_t *Model1_qdisc_root_sleeping_lock(const struct Model1_Qdisc *Model1_qdisc)
{
 struct Model1_Qdisc *Model1_root = Model1_qdisc_root_sleeping(Model1_qdisc);

 do { if (__builtin_expect(!!(!Model1_rtnl_is_locked()), 0)) { Model1_printk("\001" "3" "RTNL: assertion failed at %s (%d)\n", "./include/net/sch_generic.h", 299); Model1_dump_stack(); } } while(0);
 return Model1_qdisc_lock(Model1_root);
}

static inline __attribute__((no_instrument_function)) Model1_seqcount_t *Model1_qdisc_root_sleeping_running(const struct Model1_Qdisc *Model1_qdisc)
{
 struct Model1_Qdisc *Model1_root = Model1_qdisc_root_sleeping(Model1_qdisc);

 do { if (__builtin_expect(!!(!Model1_rtnl_is_locked()), 0)) { Model1_printk("\001" "3" "RTNL: assertion failed at %s (%d)\n", "./include/net/sch_generic.h", 307); Model1_dump_stack(); } } while(0);
 return &Model1_root->Model1_running;
}

static inline __attribute__((no_instrument_function)) struct Model1_net_device *Model1_qdisc_dev(const struct Model1_Qdisc *Model1_qdisc)
{
 return Model1_qdisc->Model1_dev_queue->Model1_dev;
}

static inline __attribute__((no_instrument_function)) void Model1_sch_tree_lock(const struct Model1_Qdisc *Model1_q)
{
 Model1_spin_lock_bh(Model1_qdisc_root_sleeping_lock(Model1_q));
}

static inline __attribute__((no_instrument_function)) void Model1_sch_tree_unlock(const struct Model1_Qdisc *Model1_q)
{
 Model1_spin_unlock_bh(Model1_qdisc_root_sleeping_lock(Model1_q));
}




extern struct Model1_Qdisc Model1_noop_qdisc;
extern struct Model1_Qdisc_ops Model1_noop_qdisc_ops;
extern struct Model1_Qdisc_ops Model1_pfifo_fast_ops;
extern struct Model1_Qdisc_ops Model1_mq_qdisc_ops;
extern struct Model1_Qdisc_ops Model1_noqueue_qdisc_ops;
extern const struct Model1_Qdisc_ops *Model1_default_qdisc_ops;
static inline __attribute__((no_instrument_function)) const struct Model1_Qdisc_ops *
Model1_get_default_qdisc_ops(const struct Model1_net_device *Model1_dev, int Model1_ntx)
{
 return Model1_ntx < Model1_dev->Model1_real_num_tx_queues ?
   Model1_default_qdisc_ops : &Model1_pfifo_fast_ops;
}

struct Model1_Qdisc_class_common {
 Model1_u32 Model1_classid;
 struct Model1_hlist_node Model1_hnode;
};

struct Model1_Qdisc_class_hash {
 struct Model1_hlist_head *Model1_hash;
 unsigned int Model1_hashsize;
 unsigned int Model1_hashmask;
 unsigned int Model1_hashelems;
};

static inline __attribute__((no_instrument_function)) unsigned int Model1_qdisc_class_hash(Model1_u32 Model1_id, Model1_u32 Model1_mask)
{
 Model1_id ^= Model1_id >> 8;
 Model1_id ^= Model1_id >> 4;
 return Model1_id & Model1_mask;
}

static inline __attribute__((no_instrument_function)) struct Model1_Qdisc_class_common *
Model1_qdisc_class_find(const struct Model1_Qdisc_class_hash *Model1_hash, Model1_u32 Model1_id)
{
 struct Model1_Qdisc_class_common *Model1_cl;
 unsigned int Model1_h;

 Model1_h = Model1_qdisc_class_hash(Model1_id, Model1_hash->Model1_hashmask);
 for (Model1_cl = ({ typeof((&Model1_hash->Model1_hash[Model1_h])->Model1_first) Model1_____ptr = ((&Model1_hash->Model1_hash[Model1_h])->Model1_first); Model1_____ptr ? ({ const typeof( ((typeof(*(Model1_cl)) *)0)->Model1_hnode ) *Model1___mptr = (Model1_____ptr); (typeof(*(Model1_cl)) *)( (char *)Model1___mptr - __builtin_offsetof(typeof(*(Model1_cl)), Model1_hnode) );}) : ((void *)0); }); Model1_cl; Model1_cl = ({ typeof((Model1_cl)->Model1_hnode.Model1_next) Model1_____ptr = ((Model1_cl)->Model1_hnode.Model1_next); Model1_____ptr ? ({ const typeof( ((typeof(*(Model1_cl)) *)0)->Model1_hnode ) *Model1___mptr = (Model1_____ptr); (typeof(*(Model1_cl)) *)( (char *)Model1___mptr - __builtin_offsetof(typeof(*(Model1_cl)), Model1_hnode) );}) : ((void *)0); })) {
  if (Model1_cl->Model1_classid == Model1_id)
   return Model1_cl;
 }
 return ((void *)0);
}

int Model1_qdisc_class_hash_init(struct Model1_Qdisc_class_hash *);
void Model1_qdisc_class_hash_insert(struct Model1_Qdisc_class_hash *,
        struct Model1_Qdisc_class_common *);
void Model1_qdisc_class_hash_remove(struct Model1_Qdisc_class_hash *,
        struct Model1_Qdisc_class_common *);
void Model1_qdisc_class_hash_grow(struct Model1_Qdisc *, struct Model1_Qdisc_class_hash *);
void Model1_qdisc_class_hash_destroy(struct Model1_Qdisc_class_hash *);

void Model1_dev_init_scheduler(struct Model1_net_device *Model1_dev);
void Model1_dev_shutdown(struct Model1_net_device *Model1_dev);
void Model1_dev_activate(struct Model1_net_device *Model1_dev);
void Model1_dev_deactivate(struct Model1_net_device *Model1_dev);
void Model1_dev_deactivate_many(struct Model1_list_head *Model1_head);
struct Model1_Qdisc *Model1_dev_graft_qdisc(struct Model1_netdev_queue *Model1_dev_queue,
         struct Model1_Qdisc *Model1_qdisc);
void Model1_qdisc_reset(struct Model1_Qdisc *Model1_qdisc);
void Model1_qdisc_destroy(struct Model1_Qdisc *Model1_qdisc);
void Model1_qdisc_tree_reduce_backlog(struct Model1_Qdisc *Model1_qdisc, unsigned int Model1_n,
          unsigned int Model1_len);
struct Model1_Qdisc *Model1_qdisc_alloc(struct Model1_netdev_queue *Model1_dev_queue,
     const struct Model1_Qdisc_ops *Model1_ops);
struct Model1_Qdisc *Model1_qdisc_create_dflt(struct Model1_netdev_queue *Model1_dev_queue,
    const struct Model1_Qdisc_ops *Model1_ops, Model1_u32 Model1_parentid);
void Model1___qdisc_calculate_pkt_len(struct Model1_sk_buff *Model1_skb,
          const struct Model1_qdisc_size_table *Model1_stab);
bool Model1_tcf_destroy(struct Model1_tcf_proto *Model1_tp, bool Model1_force);
void Model1_tcf_destroy_chain(struct Model1_tcf_proto **Model1_fl);
int Model1_skb_do_redirect(struct Model1_sk_buff *);

static inline __attribute__((no_instrument_function)) bool Model1_skb_at_tc_ingress(const struct Model1_sk_buff *Model1_skb)
{

 return ((((Model1_skb->Model1_tc_verd)) & ((((((((1))<<(2))-1)) << ((((12)))))))) >> ((((12))))) & 0x1;



}

/* Reset all TX qdiscs greater then index of a device.  */
static inline __attribute__((no_instrument_function)) void Model1_qdisc_reset_all_tx_gt(struct Model1_net_device *Model1_dev, unsigned int Model1_i)
{
 struct Model1_Qdisc *Model1_qdisc;

 for (; Model1_i < Model1_dev->Model1_num_tx_queues; Model1_i++) {
  Model1_qdisc = ({ do { } while (0); ; ((typeof(*(Model1_netdev_get_tx_queue(Model1_dev, Model1_i)->Model1_qdisc)) *)((Model1_netdev_get_tx_queue(Model1_dev, Model1_i)->Model1_qdisc))); });
  if (Model1_qdisc) {
   Model1_spin_lock_bh(Model1_qdisc_lock(Model1_qdisc));
   Model1_qdisc_reset(Model1_qdisc);
   Model1_spin_unlock_bh(Model1_qdisc_lock(Model1_qdisc));
  }
 }
}

static inline __attribute__((no_instrument_function)) void Model1_qdisc_reset_all_tx(struct Model1_net_device *Model1_dev)
{
 Model1_qdisc_reset_all_tx_gt(Model1_dev, 0);
}

/* Are all TX queues of the device empty?  */
static inline __attribute__((no_instrument_function)) bool Model1_qdisc_all_tx_empty(const struct Model1_net_device *Model1_dev)
{
 unsigned int Model1_i;

 Model1_rcu_read_lock();
 for (Model1_i = 0; Model1_i < Model1_dev->Model1_num_tx_queues; Model1_i++) {
  struct Model1_netdev_queue *Model1_txq = Model1_netdev_get_tx_queue(Model1_dev, Model1_i);
  const struct Model1_Qdisc *Model1_q = ({ typeof(*(Model1_txq->Model1_qdisc)) *Model1_________p1 = (typeof(*(Model1_txq->Model1_qdisc)) *)({ typeof((Model1_txq->Model1_qdisc)) Model1__________p1 = ({ union { typeof((Model1_txq->Model1_qdisc)) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&((Model1_txq->Model1_qdisc)), Model1___u.Model1___c, sizeof((Model1_txq->Model1_qdisc))); else Model1___read_once_size_nocheck(&((Model1_txq->Model1_qdisc)), Model1___u.Model1___c, sizeof((Model1_txq->Model1_qdisc))); Model1___u.Model1___val; }); typeof(*((Model1_txq->Model1_qdisc))) *Model1____typecheck_p __attribute__((unused)); do { } while (0); (Model1__________p1); }); do { } while (0); ; ((typeof(*(Model1_txq->Model1_qdisc)) *)(Model1_________p1)); });

  if (Model1_q->Model1_q.Model1_qlen) {
   Model1_rcu_read_unlock();
   return false;
  }
 }
 Model1_rcu_read_unlock();
 return true;
}

/* Are any of the TX qdiscs changing?  */
static inline __attribute__((no_instrument_function)) bool Model1_qdisc_tx_changing(const struct Model1_net_device *Model1_dev)
{
 unsigned int Model1_i;

 for (Model1_i = 0; Model1_i < Model1_dev->Model1_num_tx_queues; Model1_i++) {
  struct Model1_netdev_queue *Model1_txq = Model1_netdev_get_tx_queue(Model1_dev, Model1_i);
  if (({ typeof(*(Model1_txq->Model1_qdisc)) *Model1__________p1 = (typeof(*(Model1_txq->Model1_qdisc)) *)({ union { typeof((Model1_txq->Model1_qdisc)) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&((Model1_txq->Model1_qdisc)), Model1___u.Model1___c, sizeof((Model1_txq->Model1_qdisc))); else Model1___read_once_size_nocheck(&((Model1_txq->Model1_qdisc)), Model1___u.Model1___c, sizeof((Model1_txq->Model1_qdisc))); Model1___u.Model1___val; }); ; ((typeof(*(Model1_txq->Model1_qdisc)) *)(Model1__________p1)); }) != Model1_txq->Model1_qdisc_sleeping)
   return true;
 }
 return false;
}

/* Is the device using the noop qdisc on all queues?  */
static inline __attribute__((no_instrument_function)) bool Model1_qdisc_tx_is_noop(const struct Model1_net_device *Model1_dev)
{
 unsigned int Model1_i;

 for (Model1_i = 0; Model1_i < Model1_dev->Model1_num_tx_queues; Model1_i++) {
  struct Model1_netdev_queue *Model1_txq = Model1_netdev_get_tx_queue(Model1_dev, Model1_i);
  if (({ typeof(*(Model1_txq->Model1_qdisc)) *Model1__________p1 = (typeof(*(Model1_txq->Model1_qdisc)) *)({ union { typeof((Model1_txq->Model1_qdisc)) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&((Model1_txq->Model1_qdisc)), Model1___u.Model1___c, sizeof((Model1_txq->Model1_qdisc))); else Model1___read_once_size_nocheck(&((Model1_txq->Model1_qdisc)), Model1___u.Model1___c, sizeof((Model1_txq->Model1_qdisc))); Model1___u.Model1___val; }); ; ((typeof(*(Model1_txq->Model1_qdisc)) *)(Model1__________p1)); }) != &Model1_noop_qdisc)
   return false;
 }
 return true;
}

static inline __attribute__((no_instrument_function)) unsigned int Model1_qdisc_pkt_len(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_qdisc_skb_cb(Model1_skb)->Model1_pkt_len;
}

/* additional qdisc xmit flags (NET_XMIT_MASK in linux/netdevice.h) */
enum Model1_net_xmit_qdisc_t {
 Model1___NET_XMIT_STOLEN = 0x00010000,
 Model1___NET_XMIT_BYPASS = 0x00020000,
};







static inline __attribute__((no_instrument_function)) void Model1_qdisc_calculate_pkt_len(struct Model1_sk_buff *Model1_skb,
        const struct Model1_Qdisc *Model1_sch)
{

 struct Model1_qdisc_size_table *Model1_stab = ({ typeof(*(Model1_sch->Model1_stab)) *Model1_________p1 = (typeof(*(Model1_sch->Model1_stab)) *)({ typeof((Model1_sch->Model1_stab)) Model1__________p1 = ({ union { typeof((Model1_sch->Model1_stab)) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&((Model1_sch->Model1_stab)), Model1___u.Model1___c, sizeof((Model1_sch->Model1_stab))); else Model1___read_once_size_nocheck(&((Model1_sch->Model1_stab)), Model1___u.Model1___c, sizeof((Model1_sch->Model1_stab))); Model1___u.Model1___val; }); typeof(*((Model1_sch->Model1_stab))) *Model1____typecheck_p __attribute__((unused)); do { } while (0); (Model1__________p1); }); do { } while (0); ; ((typeof(*(Model1_sch->Model1_stab)) *)(Model1_________p1)); });

 if (Model1_stab)
  Model1___qdisc_calculate_pkt_len(Model1_skb, Model1_stab);

}

static inline __attribute__((no_instrument_function)) int Model1_qdisc_enqueue(struct Model1_sk_buff *Model1_skb, struct Model1_Qdisc *Model1_sch,
    struct Model1_sk_buff **Model1_to_free)
{
 Model1_qdisc_calculate_pkt_len(Model1_skb, Model1_sch);
 return Model1_sch->Model1_enqueue(Model1_skb, Model1_sch, Model1_to_free);
}

static inline __attribute__((no_instrument_function)) bool Model1_qdisc_is_percpu_stats(const struct Model1_Qdisc *Model1_q)
{
 return Model1_q->Model1_flags & 0x20;
}

static inline __attribute__((no_instrument_function)) void Model1__bstats_update(struct Model1_gnet_stats_basic_packed *Model1_bstats,
      __u64 Model1_bytes, __u32 Model1_packets)
{
 Model1_bstats->Model1_bytes += Model1_bytes;
 Model1_bstats->Model1_packets += Model1_packets;
}

static inline __attribute__((no_instrument_function)) void Model1_bstats_update(struct Model1_gnet_stats_basic_packed *Model1_bstats,
     const struct Model1_sk_buff *Model1_skb)
{
 Model1__bstats_update(Model1_bstats,
         Model1_qdisc_pkt_len(Model1_skb),
         Model1_skb_is_gso(Model1_skb) ? ((struct Model1_skb_shared_info *)(Model1_skb_end_pointer(Model1_skb)))->Model1_gso_segs : 1);
}

static inline __attribute__((no_instrument_function)) void Model1__bstats_cpu_update(struct Model1_gnet_stats_basic_cpu *Model1_bstats,
          __u64 Model1_bytes, __u32 Model1_packets)
{
 Model1_u64_stats_update_begin(&Model1_bstats->Model1_syncp);
 Model1__bstats_update(&Model1_bstats->Model1_bstats, Model1_bytes, Model1_packets);
 Model1_u64_stats_update_end(&Model1_bstats->Model1_syncp);
}

static inline __attribute__((no_instrument_function)) void Model1_bstats_cpu_update(struct Model1_gnet_stats_basic_cpu *Model1_bstats,
         const struct Model1_sk_buff *Model1_skb)
{
 Model1_u64_stats_update_begin(&Model1_bstats->Model1_syncp);
 Model1_bstats_update(&Model1_bstats->Model1_bstats, Model1_skb);
 Model1_u64_stats_update_end(&Model1_bstats->Model1_syncp);
}

static inline __attribute__((no_instrument_function)) void Model1_qdisc_bstats_cpu_update(struct Model1_Qdisc *Model1_sch,
        const struct Model1_sk_buff *Model1_skb)
{
 Model1_bstats_cpu_update(({ do { const void *Model1___vpp_verify = (typeof((Model1_sch->Model1_cpu_bstats) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); ({ unsigned long Model1_tcp_ptr__; asm volatile("add " "%%""gs"":" "%" "1" ", %0" : "=r" (Model1_tcp_ptr__) : "m" (Model1_this_cpu_off), "0" (Model1_sch->Model1_cpu_bstats)); (typeof(*(Model1_sch->Model1_cpu_bstats)) *)Model1_tcp_ptr__; }); }), Model1_skb);
}

static inline __attribute__((no_instrument_function)) void Model1_qdisc_bstats_update(struct Model1_Qdisc *Model1_sch,
           const struct Model1_sk_buff *Model1_skb)
{
 Model1_bstats_update(&Model1_sch->Model1_bstats, Model1_skb);
}

static inline __attribute__((no_instrument_function)) void Model1_qdisc_qstats_backlog_dec(struct Model1_Qdisc *Model1_sch,
         const struct Model1_sk_buff *Model1_skb)
{
 Model1_sch->Model1_qstats.Model1_backlog -= Model1_qdisc_pkt_len(Model1_skb);
}

static inline __attribute__((no_instrument_function)) void Model1_qdisc_qstats_backlog_inc(struct Model1_Qdisc *Model1_sch,
         const struct Model1_sk_buff *Model1_skb)
{
 Model1_sch->Model1_qstats.Model1_backlog += Model1_qdisc_pkt_len(Model1_skb);
}

static inline __attribute__((no_instrument_function)) void Model1___qdisc_qstats_drop(struct Model1_Qdisc *Model1_sch, int Model1_count)
{
 Model1_sch->Model1_qstats.Model1_drops += Model1_count;
}

static inline __attribute__((no_instrument_function)) void Model1_qstats_drop_inc(struct Model1_gnet_stats_queue *Model1_qstats)
{
 Model1_qstats->Model1_drops++;
}

static inline __attribute__((no_instrument_function)) void Model1_qstats_overlimit_inc(struct Model1_gnet_stats_queue *Model1_qstats)
{
 Model1_qstats->Model1_overlimits++;
}

static inline __attribute__((no_instrument_function)) void Model1_qdisc_qstats_drop(struct Model1_Qdisc *Model1_sch)
{
 Model1_qstats_drop_inc(&Model1_sch->Model1_qstats);
}

static inline __attribute__((no_instrument_function)) void Model1_qdisc_qstats_cpu_drop(struct Model1_Qdisc *Model1_sch)
{
 Model1_qstats_drop_inc(({ do { const void *Model1___vpp_verify = (typeof((Model1_sch->Model1_cpu_qstats) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); ({ unsigned long Model1_tcp_ptr__; asm volatile("add " "%%""gs"":" "%" "1" ", %0" : "=r" (Model1_tcp_ptr__) : "m" (Model1_this_cpu_off), "0" (Model1_sch->Model1_cpu_qstats)); (typeof(*(Model1_sch->Model1_cpu_qstats)) *)Model1_tcp_ptr__; }); }));
}

static inline __attribute__((no_instrument_function)) void Model1_qdisc_qstats_overlimit(struct Model1_Qdisc *Model1_sch)
{
 Model1_sch->Model1_qstats.Model1_overlimits++;
}

static inline __attribute__((no_instrument_function)) int Model1___qdisc_enqueue_tail(struct Model1_sk_buff *Model1_skb, struct Model1_Qdisc *Model1_sch,
           struct Model1_sk_buff_head *Model1_list)
{
 Model1___skb_queue_tail(Model1_list, Model1_skb);
 Model1_qdisc_qstats_backlog_inc(Model1_sch, Model1_skb);

 return 0x00;
}

static inline __attribute__((no_instrument_function)) int Model1_qdisc_enqueue_tail(struct Model1_sk_buff *Model1_skb, struct Model1_Qdisc *Model1_sch)
{
 return Model1___qdisc_enqueue_tail(Model1_skb, Model1_sch, &Model1_sch->Model1_q);
}

static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1___qdisc_dequeue_head(struct Model1_Qdisc *Model1_sch,
         struct Model1_sk_buff_head *Model1_list)
{
 struct Model1_sk_buff *Model1_skb = Model1___skb_dequeue(Model1_list);

 if (__builtin_expect(!!(Model1_skb != ((void *)0)), 1)) {
  Model1_qdisc_qstats_backlog_dec(Model1_sch, Model1_skb);
  Model1_qdisc_bstats_update(Model1_sch, Model1_skb);
 }

 return Model1_skb;
}

static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1_qdisc_dequeue_head(struct Model1_Qdisc *Model1_sch)
{
 return Model1___qdisc_dequeue_head(Model1_sch, &Model1_sch->Model1_q);
}

/* Instead of calling kfree_skb() while root qdisc lock is held,
 * queue the skb for future freeing at end of __dev_xmit_skb()
 */
static inline __attribute__((no_instrument_function)) void Model1___qdisc_drop(struct Model1_sk_buff *Model1_skb, struct Model1_sk_buff **Model1_to_free)
{
 Model1_skb->Model1_next = *Model1_to_free;
 *Model1_to_free = Model1_skb;
}

static inline __attribute__((no_instrument_function)) unsigned int Model1___qdisc_queue_drop_head(struct Model1_Qdisc *Model1_sch,
         struct Model1_sk_buff_head *Model1_list,
         struct Model1_sk_buff **Model1_to_free)
{
 struct Model1_sk_buff *Model1_skb = Model1___skb_dequeue(Model1_list);

 if (__builtin_expect(!!(Model1_skb != ((void *)0)), 1)) {
  unsigned int Model1_len = Model1_qdisc_pkt_len(Model1_skb);

  Model1_qdisc_qstats_backlog_dec(Model1_sch, Model1_skb);
  Model1___qdisc_drop(Model1_skb, Model1_to_free);
  return Model1_len;
 }

 return 0;
}

static inline __attribute__((no_instrument_function)) unsigned int Model1_qdisc_queue_drop_head(struct Model1_Qdisc *Model1_sch,
       struct Model1_sk_buff **Model1_to_free)
{
 return Model1___qdisc_queue_drop_head(Model1_sch, &Model1_sch->Model1_q, Model1_to_free);
}

static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1_qdisc_peek_head(struct Model1_Qdisc *Model1_sch)
{
 return Model1_skb_peek(&Model1_sch->Model1_q);
}

/* generic pseudo peek method for non-work-conserving qdisc */
static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1_qdisc_peek_dequeued(struct Model1_Qdisc *Model1_sch)
{
 /* we can reuse ->gso_skb because peek isn't called for root qdiscs */
 if (!Model1_sch->Model1_gso_skb) {
  Model1_sch->Model1_gso_skb = Model1_sch->Model1_dequeue(Model1_sch);
  if (Model1_sch->Model1_gso_skb) {
   /* it's still part of the queue */
   Model1_qdisc_qstats_backlog_inc(Model1_sch, Model1_sch->Model1_gso_skb);
   Model1_sch->Model1_q.Model1_qlen++;
  }
 }

 return Model1_sch->Model1_gso_skb;
}

/* use instead of qdisc->dequeue() for all qdiscs queried with ->peek() */
static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1_qdisc_dequeue_peeked(struct Model1_Qdisc *Model1_sch)
{
 struct Model1_sk_buff *Model1_skb = Model1_sch->Model1_gso_skb;

 if (Model1_skb) {
  Model1_sch->Model1_gso_skb = ((void *)0);
  Model1_qdisc_qstats_backlog_dec(Model1_sch, Model1_skb);
  Model1_sch->Model1_q.Model1_qlen--;
 } else {
  Model1_skb = Model1_sch->Model1_dequeue(Model1_sch);
 }

 return Model1_skb;
}

static inline __attribute__((no_instrument_function)) void Model1___qdisc_reset_queue(struct Model1_sk_buff_head *Model1_list)
{
 /*
	 * We do not know the backlog in bytes of this list, it
	 * is up to the caller to correct it
	 */
 if (!Model1_skb_queue_empty(Model1_list)) {
  Model1_rtnl_kfree_skbs(Model1_list->Model1_next, Model1_list->Model1_prev);
  Model1___skb_queue_head_init(Model1_list);
 }
}

static inline __attribute__((no_instrument_function)) void Model1_qdisc_reset_queue(struct Model1_Qdisc *Model1_sch)
{
 Model1___qdisc_reset_queue(&Model1_sch->Model1_q);
 Model1_sch->Model1_qstats.Model1_backlog = 0;
}

static inline __attribute__((no_instrument_function)) struct Model1_Qdisc *Model1_qdisc_replace(struct Model1_Qdisc *Model1_sch, struct Model1_Qdisc *Model1_new,
       struct Model1_Qdisc **Model1_pold)
{
 struct Model1_Qdisc *old;

 Model1_sch_tree_lock(Model1_sch);
 old = *Model1_pold;
 *Model1_pold = Model1_new;
 if (old != ((void *)0)) {
  Model1_qdisc_tree_reduce_backlog(old, old->Model1_q.Model1_qlen, old->Model1_qstats.Model1_backlog);
  Model1_qdisc_reset(old);
 }
 Model1_sch_tree_unlock(Model1_sch);

 return old;
}

static inline __attribute__((no_instrument_function)) void Model1_rtnl_qdisc_drop(struct Model1_sk_buff *Model1_skb, struct Model1_Qdisc *Model1_sch)
{
 Model1_rtnl_kfree_skbs(Model1_skb, Model1_skb);
 Model1_qdisc_qstats_drop(Model1_sch);
}


static inline __attribute__((no_instrument_function)) int Model1_qdisc_drop(struct Model1_sk_buff *Model1_skb, struct Model1_Qdisc *Model1_sch,
        struct Model1_sk_buff **Model1_to_free)
{
 Model1___qdisc_drop(Model1_skb, Model1_to_free);
 Model1_qdisc_qstats_drop(Model1_sch);

 return 0x01;
}

/* Length to Time (L2T) lookup in a qdisc_rate_table, to determine how
   long it will take to send a packet given its size.
 */
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_qdisc_l2t(struct Model1_qdisc_rate_table* Model1_rtab, unsigned int Model1_pktlen)
{
 int Model1_slot = Model1_pktlen + Model1_rtab->Model1_rate.Model1_cell_align + Model1_rtab->Model1_rate.Model1_overhead;
 if (Model1_slot < 0)
  Model1_slot = 0;
 Model1_slot >>= Model1_rtab->Model1_rate.Model1_cell_log;
 if (Model1_slot > 255)
  return Model1_rtab->Model1_data[255]*(Model1_slot >> 8) + Model1_rtab->Model1_data[Model1_slot & 0xFF];
 return Model1_rtab->Model1_data[Model1_slot];
}

struct Model1_psched_ratecfg {
 Model1_u64 Model1_rate_bytes_ps; /* bytes per second */
 Model1_u32 Model1_mult;
 Model1_u16 Model1_overhead;
 Model1_u8 Model1_linklayer;
 Model1_u8 Model1_shift;
};

static inline __attribute__((no_instrument_function)) Model1_u64 Model1_psched_l2t_ns(const struct Model1_psched_ratecfg *Model1_r,
    unsigned int Model1_len)
{
 Model1_len += Model1_r->Model1_overhead;

 if (__builtin_expect(!!(Model1_r->Model1_linklayer == Model1_TC_LINKLAYER_ATM), 0))
  return ((Model1_u64)((((Model1_len) + (48) - 1) / (48))*53) * Model1_r->Model1_mult) >> Model1_r->Model1_shift;

 return ((Model1_u64)Model1_len * Model1_r->Model1_mult) >> Model1_r->Model1_shift;
}

void Model1_psched_ratecfg_precompute(struct Model1_psched_ratecfg *Model1_r,
          const struct Model1_tc_ratespec *Model1_conf,
          Model1_u64 Model1_rate64);

static inline __attribute__((no_instrument_function)) void Model1_psched_ratecfg_getrate(struct Model1_tc_ratespec *Model1_res,
       const struct Model1_psched_ratecfg *Model1_r)
{
 memset(Model1_res, 0, sizeof(*Model1_res));

 /* legacy struct tc_ratespec has a 32bit @rate field
	 * Qdisc using 64bit rate should add new attributes
	 * in order to maintain compatibility.
	 */
 Model1_res->Model1_rate = ({ Model1_u64 Model1___min1 = (Model1_r->Model1_rate_bytes_ps); Model1_u64 Model1___min2 = (~0U); Model1___min1 < Model1___min2 ? Model1___min1: Model1___min2; });

 Model1_res->Model1_overhead = Model1_r->Model1_overhead;
 Model1_res->Model1_linklayer = (Model1_r->Model1_linklayer & 0x0F);
}




/* Caches aren't brain-dead on the intel. */




/* Keep includes the same across arches.  */


/*
 * The cache doesn't need to be flushed when TLB entries change when
 * the cache is mapped to physical memory, not virtual memory
 */



/*
 * The set_memory_* API can be used to change various attributes of a virtual
 * address range. The attributes include:
 * Cachability   : UnCached, WriteCombining, WriteThrough, WriteBack
 * Executability : eXeutable, NoteXecutable
 * Read/Write    : ReadOnly, ReadWrite
 * Presence      : NotPresent
 *
 * Within a category, the attributes are mutually exclusive.
 *
 * The implementation of this API will take care of various aspects that
 * are associated with changing such attributes, such as:
 * - Flushing TLBs
 * - Flushing CPU caches
 * - Making sure aliases of the memory behind the mapping don't violate
 *   coherency rules as defined by the CPU in the system.
 *
 * What this API does not do:
 * - Provide exclusion between various callers - including callers that
 *   operation on other mappings of the same physical page
 * - Restore default attributes when a page is freed
 * - Guarantee that mappings other than the requested one are
 *   in any state, other than that these do not violate rules for
 *   the CPU you have. Do not depend on any effects on other mappings,
 *   CPUs other than the one you have may have more relaxed rules.
 * The caller is required to take care of these.
 */

int Model1__set_memory_uc(unsigned long Model1_addr, int Model1_numpages);
int Model1__set_memory_wc(unsigned long Model1_addr, int Model1_numpages);
int Model1__set_memory_wt(unsigned long Model1_addr, int Model1_numpages);
int Model1__set_memory_wb(unsigned long Model1_addr, int Model1_numpages);
int Model1_set_memory_uc(unsigned long Model1_addr, int Model1_numpages);
int Model1_set_memory_wc(unsigned long Model1_addr, int Model1_numpages);
int Model1_set_memory_wt(unsigned long Model1_addr, int Model1_numpages);
int Model1_set_memory_wb(unsigned long Model1_addr, int Model1_numpages);
int Model1_set_memory_x(unsigned long Model1_addr, int Model1_numpages);
int Model1_set_memory_nx(unsigned long Model1_addr, int Model1_numpages);
int Model1_set_memory_ro(unsigned long Model1_addr, int Model1_numpages);
int Model1_set_memory_rw(unsigned long Model1_addr, int Model1_numpages);
int Model1_set_memory_np(unsigned long Model1_addr, int Model1_numpages);
int Model1_set_memory_4k(unsigned long Model1_addr, int Model1_numpages);

int Model1_set_memory_array_uc(unsigned long *Model1_addr, int Model1_addrinarray);
int Model1_set_memory_array_wc(unsigned long *Model1_addr, int Model1_addrinarray);
int Model1_set_memory_array_wt(unsigned long *Model1_addr, int Model1_addrinarray);
int Model1_set_memory_array_wb(unsigned long *Model1_addr, int Model1_addrinarray);

int Model1_set_pages_array_uc(struct Model1_page **Model1_pages, int Model1_addrinarray);
int Model1_set_pages_array_wc(struct Model1_page **Model1_pages, int Model1_addrinarray);
int Model1_set_pages_array_wt(struct Model1_page **Model1_pages, int Model1_addrinarray);
int Model1_set_pages_array_wb(struct Model1_page **Model1_pages, int Model1_addrinarray);

/*
 * For legacy compatibility with the old APIs, a few functions
 * are provided that work on a "struct page".
 * These functions operate ONLY on the 1:1 kernel mapping of the
 * memory that the struct page represents, and internally just
 * call the set_memory_* function. See the description of the
 * set_memory_* function for more details on conventions.
 *
 * These APIs should be considered *deprecated* and are likely going to
 * be removed in the future.
 * The reason for this is the implicit operation on the 1:1 mapping only,
 * making this not a generally useful API.
 *
 * Specifically, many users of the old APIs had a virtual address,
 * called virt_to_page() or vmalloc_to_page() on that address to
 * get a struct page* that the old API required.
 * To convert these cases, use set_memory_*() on the original
 * virtual address, do not use these functions.
 */

int Model1_set_pages_uc(struct Model1_page *Model1_page, int Model1_numpages);
int Model1_set_pages_wb(struct Model1_page *Model1_page, int Model1_numpages);
int Model1_set_pages_x(struct Model1_page *Model1_page, int Model1_numpages);
int Model1_set_pages_nx(struct Model1_page *Model1_page, int Model1_numpages);
int Model1_set_pages_ro(struct Model1_page *Model1_page, int Model1_numpages);
int Model1_set_pages_rw(struct Model1_page *Model1_page, int Model1_numpages);


void Model1_clflush_cache_range(void *Model1_addr, unsigned int Model1_size);



extern const int Model1_rodata_test_data;
extern int Model1_kernel_set_to_readonly;
void Model1_set_kernel_text_rw(void);
void Model1_set_kernel_text_ro(void);




static inline __attribute__((no_instrument_function)) int Model1_rodata_test(void)
{
 return 0;
}

/*
 * Linux Socket Filter Data Structures
 */










/* Instruction classes */
/* ld/ldx fields */
/* alu/jmp fields */

/*
 * Current version of the filter code architecture.
 */



/*
 *	Try and keep these values and structures similar to BSD, especially
 *	the BPF code definitions which need to match so you can share filters
 */

struct Model1_sock_filter { /* Filter block */
 Model1___u16 Model1_code; /* Actual filter code */
 __u8 Model1_jt; /* Jump true */
 __u8 Model1_jf; /* Jump false */
 __u32 Model1_k; /* Generic multiuse field */
};

struct Model1_sock_fprog { /* Required for SO_ATTACH_FILTER. */
 unsigned short Model1_len; /* Number of filter blocks */
 struct Model1_sock_filter *Model1_filter;
};

/* ret - BPF_K and BPF_X also apply */



/* misc */




/*
 * Macros for filter block array initializers.
 */







/*
 * Number of scratch memory words for: BPF_ST and BPF_STX
 */


/* RATIONALE. Negative offsets are invalid in BPF.
   We use them to reference ancillary data.
   Unlike introduction new instructions, it does not break
   existing compilers/optimizers.
 */
/* Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of version 2 of the GNU General Public
 * License as published by the Free Software Foundation.
 */






/* Extended instruction set based on top of classic BPF */

/* instruction classes */


/* ld/ldx fields */



/* alu/jmp fields */



/* change endianness of a register */
/* Register numbers */
enum {
 Model1_BPF_REG_0 = 0,
 Model1_BPF_REG_1,
 Model1_BPF_REG_2,
 Model1_BPF_REG_3,
 Model1_BPF_REG_4,
 Model1_BPF_REG_5,
 Model1_BPF_REG_6,
 Model1_BPF_REG_7,
 Model1_BPF_REG_8,
 Model1_BPF_REG_9,
 Model1_BPF_REG_10,
 Model1___MAX_BPF_REG,
};

/* BPF has 10 general purpose 64-bit registers and stack frame. */


struct Model1_bpf_insn {
 __u8 Model1_code; /* opcode */
 __u8 Model1_dst_reg:4; /* dest register */
 __u8 Model1_src_reg:4; /* source register */
 Model1___s16 Model1_off; /* signed offset */
 Model1___s32 Model1_imm; /* signed immediate constant */
};

/* BPF syscall commands, see bpf(2) man-page for details. */
enum Model1_bpf_cmd {
 Model1_BPF_MAP_CREATE,
 Model1_BPF_MAP_LOOKUP_ELEM,
 Model1_BPF_MAP_UPDATE_ELEM,
 Model1_BPF_MAP_DELETE_ELEM,
 Model1_BPF_MAP_GET_NEXT_KEY,
 Model1_BPF_PROG_LOAD,
 Model1_BPF_OBJ_PIN,
 Model1_BPF_OBJ_GET,
};

enum Model1_bpf_map_type {
 Model1_BPF_MAP_TYPE_UNSPEC,
 Model1_BPF_MAP_TYPE_HASH,
 Model1_BPF_MAP_TYPE_ARRAY,
 Model1_BPF_MAP_TYPE_PROG_ARRAY,
 Model1_BPF_MAP_TYPE_PERF_EVENT_ARRAY,
 Model1_BPF_MAP_TYPE_PERCPU_HASH,
 Model1_BPF_MAP_TYPE_PERCPU_ARRAY,
 Model1_BPF_MAP_TYPE_STACK_TRACE,
 Model1_BPF_MAP_TYPE_CGROUP_ARRAY,
};

enum Model1_bpf_prog_type {
 Model1_BPF_PROG_TYPE_UNSPEC,
 Model1_BPF_PROG_TYPE_SOCKET_FILTER,
 Model1_BPF_PROG_TYPE_KPROBE,
 Model1_BPF_PROG_TYPE_SCHED_CLS,
 Model1_BPF_PROG_TYPE_SCHED_ACT,
 Model1_BPF_PROG_TYPE_TRACEPOINT,
 Model1_BPF_PROG_TYPE_XDP,
};



/* flags for BPF_MAP_UPDATE_ELEM command */






union Model1_bpf_attr {
 struct { /* anonymous struct used by BPF_MAP_CREATE command */
  __u32 Model1_map_type; /* one of enum bpf_map_type */
  __u32 Model1_key_size; /* size of key in bytes */
  __u32 Model1_value_size; /* size of value in bytes */
  __u32 Model1_max_entries; /* max number of entries in a map */
  __u32 Model1_map_flags; /* prealloc or not */
 };

 struct { /* anonymous struct used by BPF_MAP_*_ELEM commands */
  __u32 Model1_map_fd;
  __u64 __attribute__((aligned(8))) Model1_key;
  union {
   __u64 __attribute__((aligned(8))) Model1_value;
   __u64 __attribute__((aligned(8))) Model1_next_key;
  };
  __u64 Model1_flags;
 };

 struct { /* anonymous struct used by BPF_PROG_LOAD command */
  __u32 Model1_prog_type; /* one of enum bpf_prog_type */
  __u32 Model1_insn_cnt;
  __u64 __attribute__((aligned(8))) Model1_insns;
  __u64 __attribute__((aligned(8))) Model1_license;
  __u32 Model1_log_level; /* verbosity level of verifier */
  __u32 Model1_log_size; /* size of user buffer */
  __u64 __attribute__((aligned(8))) Model1_log_buf; /* user supplied buffer */
  __u32 Model1_kern_version; /* checked when prog_type=kprobe */
 };

 struct { /* anonymous struct used by BPF_OBJ_* commands */
  __u64 __attribute__((aligned(8))) Model1_pathname;
  __u32 Model1_bpf_fd;
 };
} __attribute__((aligned(8)));

/* integer value in 'imm' field of BPF_CALL instruction selects which helper
 * function eBPF program intends to call
 */
enum Model1_bpf_func_id {
 Model1_BPF_FUNC_unspec,
 Model1_BPF_FUNC_map_lookup_elem, /* void *map_lookup_elem(&map, &key) */
 Model1_BPF_FUNC_map_update_elem, /* int map_update_elem(&map, &key, &value, flags) */
 Model1_BPF_FUNC_map_delete_elem, /* int map_delete_elem(&map, &key) */
 Model1_BPF_FUNC_probe_read, /* int bpf_probe_read(void *dst, int size, void *src) */
 Model1_BPF_FUNC_ktime_get_ns, /* u64 bpf_ktime_get_ns(void) */
 Model1_BPF_FUNC_trace_printk, /* int bpf_trace_printk(const char *fmt, int fmt_size, ...) */
 Model1_BPF_FUNC_get_prandom_u32, /* u32 prandom_u32(void) */
 Model1_BPF_FUNC_get_smp_processor_id, /* u32 raw_smp_processor_id(void) */

 /**
	 * skb_store_bytes(skb, offset, from, len, flags) - store bytes into packet
	 * @skb: pointer to skb
	 * @offset: offset within packet from skb->mac_header
	 * @from: pointer where to copy bytes from
	 * @len: number of bytes to store into packet
	 * @flags: bit 0 - if true, recompute skb->csum
	 *         other bits - reserved
	 * Return: 0 on success
	 */
 Model1_BPF_FUNC_skb_store_bytes,

 /**
	 * l3_csum_replace(skb, offset, from, to, flags) - recompute IP checksum
	 * @skb: pointer to skb
	 * @offset: offset within packet where IP checksum is located
	 * @from: old value of header field
	 * @to: new value of header field
	 * @flags: bits 0-3 - size of header field
	 *         other bits - reserved
	 * Return: 0 on success
	 */
 Model1_BPF_FUNC_l3_csum_replace,

 /**
	 * l4_csum_replace(skb, offset, from, to, flags) - recompute TCP/UDP checksum
	 * @skb: pointer to skb
	 * @offset: offset within packet where TCP/UDP checksum is located
	 * @from: old value of header field
	 * @to: new value of header field
	 * @flags: bits 0-3 - size of header field
	 *         bit 4 - is pseudo header
	 *         other bits - reserved
	 * Return: 0 on success
	 */
 Model1_BPF_FUNC_l4_csum_replace,

 /**
	 * bpf_tail_call(ctx, prog_array_map, index) - jump into another BPF program
	 * @ctx: context pointer passed to next program
	 * @prog_array_map: pointer to map which type is BPF_MAP_TYPE_PROG_ARRAY
	 * @index: index inside array that selects specific program to run
	 * Return: 0 on success
	 */
 Model1_BPF_FUNC_tail_call,

 /**
	 * bpf_clone_redirect(skb, ifindex, flags) - redirect to another netdev
	 * @skb: pointer to skb
	 * @ifindex: ifindex of the net device
	 * @flags: bit 0 - if set, redirect to ingress instead of egress
	 *         other bits - reserved
	 * Return: 0 on success
	 */
 Model1_BPF_FUNC_clone_redirect,

 /**
	 * u64 bpf_get_current_pid_tgid(void)
	 * Return: current->tgid << 32 | current->pid
	 */
 Model1_BPF_FUNC_get_current_pid_tgid,

 /**
	 * u64 bpf_get_current_uid_gid(void)
	 * Return: current_gid << 32 | current_uid
	 */
 Model1_BPF_FUNC_get_current_uid_gid,

 /**
	 * bpf_get_current_comm(char *buf, int size_of_buf)
	 * stores current->comm into buf
	 * Return: 0 on success
	 */
 Model1_BPF_FUNC_get_current_comm,

 /**
	 * bpf_get_cgroup_classid(skb) - retrieve a proc's classid
	 * @skb: pointer to skb
	 * Return: classid if != 0
	 */
 Model1_BPF_FUNC_get_cgroup_classid,
 Model1_BPF_FUNC_skb_vlan_push, /* bpf_skb_vlan_push(skb, vlan_proto, vlan_tci) */
 Model1_BPF_FUNC_skb_vlan_pop, /* bpf_skb_vlan_pop(skb) */

 /**
	 * bpf_skb_[gs]et_tunnel_key(skb, key, size, flags)
	 * retrieve or populate tunnel metadata
	 * @skb: pointer to skb
	 * @key: pointer to 'struct bpf_tunnel_key'
	 * @size: size of 'struct bpf_tunnel_key'
	 * @flags: room for future extensions
	 * Retrun: 0 on success
	 */
 Model1_BPF_FUNC_skb_get_tunnel_key,
 Model1_BPF_FUNC_skb_set_tunnel_key,
 Model1_BPF_FUNC_perf_event_read, /* u64 bpf_perf_event_read(&map, index) */
 /**
	 * bpf_redirect(ifindex, flags) - redirect to another netdev
	 * @ifindex: ifindex of the net device
	 * @flags: bit 0 - if set, redirect to ingress instead of egress
	 *         other bits - reserved
	 * Return: TC_ACT_REDIRECT
	 */
 Model1_BPF_FUNC_redirect,

 /**
	 * bpf_get_route_realm(skb) - retrieve a dst's tclassid
	 * @skb: pointer to skb
	 * Return: realm if != 0
	 */
 Model1_BPF_FUNC_get_route_realm,

 /**
	 * bpf_perf_event_output(ctx, map, index, data, size) - output perf raw sample
	 * @ctx: struct pt_regs*
	 * @map: pointer to perf_event_array map
	 * @index: index of event in the map
	 * @data: data on stack to be output as raw data
	 * @size: size of data
	 * Return: 0 on success
	 */
 Model1_BPF_FUNC_perf_event_output,
 Model1_BPF_FUNC_skb_load_bytes,

 /**
	 * bpf_get_stackid(ctx, map, flags) - walk user or kernel stack and return id
	 * @ctx: struct pt_regs*
	 * @map: pointer to stack_trace map
	 * @flags: bits 0-7 - numer of stack frames to skip
	 *         bit 8 - collect user stack instead of kernel
	 *         bit 9 - compare stacks by hash only
	 *         bit 10 - if two different stacks hash into the same stackid
	 *                  discard old
	 *         other bits - reserved
	 * Return: >= 0 stackid on success or negative error
	 */
 Model1_BPF_FUNC_get_stackid,

 /**
	 * bpf_csum_diff(from, from_size, to, to_size, seed) - calculate csum diff
	 * @from: raw from buffer
	 * @from_size: length of from buffer
	 * @to: raw to buffer
	 * @to_size: length of to buffer
	 * @seed: optional seed
	 * Return: csum result
	 */
 Model1_BPF_FUNC_csum_diff,

 /**
	 * bpf_skb_[gs]et_tunnel_opt(skb, opt, size)
	 * retrieve or populate tunnel options metadata
	 * @skb: pointer to skb
	 * @opt: pointer to raw tunnel option data
	 * @size: size of @opt
	 * Return: 0 on success for set, option size for get
	 */
 Model1_BPF_FUNC_skb_get_tunnel_opt,
 Model1_BPF_FUNC_skb_set_tunnel_opt,

 /**
	 * bpf_skb_change_proto(skb, proto, flags)
	 * Change protocol of the skb. Currently supported is
	 * v4 -> v6, v6 -> v4 transitions. The helper will also
	 * resize the skb. eBPF program is expected to fill the
	 * new headers via skb_store_bytes and lX_csum_replace.
	 * @skb: pointer to skb
	 * @proto: new skb->protocol type
	 * @flags: reserved
	 * Return: 0 on success or negative error
	 */
 Model1_BPF_FUNC_skb_change_proto,

 /**
	 * bpf_skb_change_type(skb, type)
	 * Change packet type of skb.
	 * @skb: pointer to skb
	 * @type: new skb->pkt_type type
	 * Return: 0 on success or negative error
	 */
 Model1_BPF_FUNC_skb_change_type,

 /**
	 * bpf_skb_under_cgroup(skb, map, index) - Check cgroup2 membership of skb
	 * @skb: pointer to skb
	 * @map: pointer to bpf_map in BPF_MAP_TYPE_CGROUP_ARRAY type
	 * @index: index of the cgroup in the bpf_map
	 * Return:
	 *   == 0 skb failed the cgroup2 descendant test
	 *   == 1 skb succeeded the cgroup2 descendant test
	 *    < 0 error
	 */
 Model1_BPF_FUNC_skb_under_cgroup,

 /**
	 * bpf_get_hash_recalc(skb)
	 * Retrieve and possibly recalculate skb->hash.
	 * @skb: pointer to skb
	 * Return: hash
	 */
 Model1_BPF_FUNC_get_hash_recalc,

 /**
	 * u64 bpf_get_current_task(void)
	 * Returns current task_struct
	 * Return: current
	 */
 Model1_BPF_FUNC_get_current_task,

 /**
	 * bpf_probe_write_user(void *dst, void *src, int len)
	 * safely attempt to write to a location
	 * @dst: destination address in userspace
	 * @src: source address on stack
	 * @len: number of bytes to copy
	 * Return: 0 on success or negative error
	 */
 Model1_BPF_FUNC_probe_write_user,

 Model1___BPF_FUNC_MAX_ID,
};

/* All flags used by eBPF helper functions, placed here. */

/* BPF_FUNC_skb_store_bytes flags. */



/* BPF_FUNC_l3_csum_replace and BPF_FUNC_l4_csum_replace flags.
 * First 4 bits are for passing the header field size.
 */


/* BPF_FUNC_l4_csum_replace flags. */



/* BPF_FUNC_clone_redirect and BPF_FUNC_redirect flags. */


/* BPF_FUNC_skb_set_tunnel_key and BPF_FUNC_skb_get_tunnel_key flags. */


/* BPF_FUNC_get_stackid flags. */





/* BPF_FUNC_skb_set_tunnel_key flags. */



/* BPF_FUNC_perf_event_output and BPF_FUNC_perf_event_read flags. */


/* BPF_FUNC_perf_event_output for sk_buff input context. */


/* user accessible mirror of in-kernel sk_buff.
 * new fields can only be added to the end of this structure
 */
struct Model1___sk_buff {
 __u32 Model1_len;
 __u32 Model1_pkt_type;
 __u32 Model1_mark;
 __u32 Model1_queue_mapping;
 __u32 Model1_protocol;
 __u32 Model1_vlan_present;
 __u32 Model1_vlan_tci;
 __u32 Model1_vlan_proto;
 __u32 Model1_priority;
 __u32 Model1_ingress_ifindex;
 __u32 Model1_ifindex;
 __u32 Model1_tc_index;
 __u32 Model1_cb[5];
 __u32 Model1_hash;
 __u32 Model1_tc_classid;
 __u32 Model1_data;
 __u32 Model1_data_end;
};

struct Model1_bpf_tunnel_key {
 __u32 Model1_tunnel_id;
 union {
  __u32 Model1_remote_ipv4;
  __u32 Model1_remote_ipv6[4];
 };
 __u8 Model1_tunnel_tos;
 __u8 Model1_tunnel_ttl;
 Model1___u16 Model1_tunnel_ext;
 __u32 Model1_tunnel_label;
};

/* User return codes for XDP prog type.
 * A valid XDP program must return one of these defined values. All other
 * return codes are reserved for future use. Unknown return codes will result
 * in packet drop.
 */
enum Model1_xdp_action {
 Model1_XDP_ABORTED = 0,
 Model1_XDP_DROP,
 Model1_XDP_PASS,
 Model1_XDP_TX,
};

/* user accessible metadata for XDP packet hook
 * new fields must be added to the end of this structure
 */
struct Model1_xdp_md {
 __u32 Model1_data;
 __u32 Model1_data_end;
};

struct Model1_sk_buff;
struct Model1_sock;
struct Model1_seccomp_data;
struct Model1_bpf_prog_aux;

/* ArgX, context and stack frame pointer register positions. Note,
 * Arg1, Arg2, Arg3, etc are used as argument mappings of function
 * calls in BPF_CALL instruction.
 */
/* Additional register mappings for converted user programs. */




/* Kernel hidden auxiliary/helper register for hardening step.
 * Only used by eBPF JITs. It's nothing more than a temporary
 * register that JITs use internally, only that here it's part
 * of eBPF instructions that have been rewritten for blinding
 * constants. See JIT pre-step in bpf_jit_blind_constants().
 */



/* BPF program can access up to 512 bytes of stack space. */


/* Helper macros for filter block array initializers. */

/* ALU ops on registers, bpf_add|sub|...: dst_reg += src_reg */
/* ALU ops on immediates, bpf_add|sub|...: dst_reg += imm32 */
/* Endianess conversion, cpu_to_{l,b}e(), {l,b}e_to_cpu() */
/* Short form of mov, dst_reg = src_reg */
/* Short form of mov, dst_reg = imm32 */
/* BPF_LD_IMM64 macro encodes single 'load 64-bit immediate' insn */
/* pseudo BPF_LD_IMM64 insn used to refer to process-local map_fd */



/* Short form of mov based on type, BPF_X: dst_reg = src_reg, BPF_K: dst_reg = imm32 */
/* Direct packet access, R0 = *(uint *) (skb->data + imm32) */
/* Indirect packet access, R0 = *(uint *) (skb->data + src_reg + imm32) */
/* Memory load, dst_reg = *(uint *) (src_reg + off16) */
/* Memory store, *(uint *) (dst_reg + off16) = src_reg */
/* Atomic memory add, *(uint *)(dst_reg + off16) += src_reg */
/* Memory store, *(uint *) (dst_reg + off16) = imm32 */
/* Conditional jumps against registers, if (dst_reg 'op' src_reg) goto pc + off16 */
/* Conditional jumps against immediates, if (dst_reg 'op' imm32) goto pc + off16 */
/* Function call */
/* Raw code statement block */
/* Program exit */
/* Internal classic blocks for direct assignment */
/* A struct sock_filter is architecture independent. */
struct Model1_compat_sock_fprog {
 Model1_u16 Model1_len;
 Model1_compat_uptr_t Model1_filter; /* struct sock_filter * */
};


struct Model1_sock_fprog_kern {
 Model1_u16 Model1_len;
 struct Model1_sock_filter *Model1_filter;
};

struct Model1_bpf_binary_header {
 unsigned int Model1_pages;
 Model1_u8 Model1_image[];
};

struct Model1_bpf_prog {
 Model1_u16 Model1_pages; /* Number of allocated pages */
                               ;
 Model1_u16 Model1_jited:1, /* Is our filter JIT'ed? */
    Model1_gpl_compatible:1, /* Is filter GPL compatible? */
    Model1_cb_access:1, /* Is control block accessed? */
    Model1_dst_needed:1; /* Do we need dst entry? */
                             ;
 Model1_u32 Model1_len; /* Number of filter blocks */
 enum Model1_bpf_prog_type Model1_type; /* Type of BPF program */
 struct Model1_bpf_prog_aux *Model1_aux; /* Auxiliary fields */
 struct Model1_sock_fprog_kern *Model1_orig_prog; /* Original BPF program */
 unsigned int (*Model1_bpf_func)(const struct Model1_sk_buff *Model1_skb,
         const struct Model1_bpf_insn *Model1_filter);
 /* Instructions for interpreter */
 union {
  struct Model1_sock_filter Model1_insns[0];
  struct Model1_bpf_insn Model1_insnsi[0];
 };
};

struct Model1_sk_filter {
 Model1_atomic_t Model1_refcnt;
 struct Model1_callback_head Model1_rcu;
 struct Model1_bpf_prog *Model1_prog;
};





struct Model1_bpf_skb_data_end {
 struct Model1_qdisc_skb_cb Model1_qdisc_cb;
 void *Model1_data_end;
};

struct Model1_xdp_buff {
 void *Model1_data;
 void *Model1_data_end;
};

/* compute the linear packet data range [data, data_end) which
 * will be accessed by cls_bpf and act_bpf programs
 */
static inline __attribute__((no_instrument_function)) void Model1_bpf_compute_data_end(struct Model1_sk_buff *Model1_skb)
{
 struct Model1_bpf_skb_data_end *Model1_cb = (struct Model1_bpf_skb_data_end *)Model1_skb->Model1_cb;

 do { bool Model1___cond = !(!(sizeof(*Model1_cb) > (sizeof(((struct Model1_sk_buff*)0)->Model1_cb)))); extern void Model1___compiletime_assert_383(void) ; if (Model1___cond) Model1___compiletime_assert_383(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
 Model1_cb->Model1_data_end = Model1_skb->Model1_data + Model1_skb_headlen(Model1_skb);
}

static inline __attribute__((no_instrument_function)) Model1_u8 *Model1_bpf_skb_cb(struct Model1_sk_buff *Model1_skb)
{
 /* eBPF programs may read/write skb->cb[] area to transfer meta
	 * data between tail calls. Since this also needs to work with
	 * tc, that scratch memory is mapped to qdisc_skb_cb's data area.
	 *
	 * In some socket filter cases, the cb unfortunately needs to be
	 * saved/restored so that protocol specific skb->cb[] data won't
	 * be lost. In any case, due to unpriviledged eBPF programs
	 * attached to sockets, we need to clear the bpf_skb_cb() area
	 * to not leak previous contents to user space.
	 */
 do { bool Model1___cond = !(!((sizeof(((struct Model1___sk_buff*)0)->Model1_cb)) != 20)); extern void Model1___compiletime_assert_399(void) ; if (Model1___cond) Model1___compiletime_assert_399(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
 do { bool Model1___cond = !(!((sizeof(((struct Model1___sk_buff*)0)->Model1_cb)) != (sizeof(((struct Model1_qdisc_skb_cb*)0)->Model1_data)))); extern void Model1___compiletime_assert_401(void) ; if (Model1___cond) Model1___compiletime_assert_401(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);


 return Model1_qdisc_skb_cb(Model1_skb)->Model1_data;
}

static inline __attribute__((no_instrument_function)) Model1_u32 Model1_bpf_prog_run_save_cb(const struct Model1_bpf_prog *Model1_prog,
           struct Model1_sk_buff *Model1_skb)
{
 Model1_u8 *Model1_cb_data = Model1_bpf_skb_cb(Model1_skb);
 Model1_u8 Model1_cb_saved[20];
 Model1_u32 Model1_res;

 if (__builtin_expect(!!(Model1_prog->Model1_cb_access), 0)) {
  ({ Model1_size_t Model1___len = (sizeof(Model1_cb_saved)); void *Model1___ret; if (__builtin_constant_p(sizeof(Model1_cb_saved)) && Model1___len >= 64) Model1___ret = Model1___memcpy((Model1_cb_saved), (Model1_cb_data), Model1___len); else Model1___ret = __builtin_memcpy((Model1_cb_saved), (Model1_cb_data), Model1___len); Model1___ret; });
  memset(Model1_cb_data, 0, sizeof(Model1_cb_saved));
 }

 Model1_res = (*Model1_prog->Model1_bpf_func)(Model1_skb, Model1_prog->Model1_insnsi);

 if (__builtin_expect(!!(Model1_prog->Model1_cb_access), 0))
  ({ Model1_size_t Model1___len = (sizeof(Model1_cb_saved)); void *Model1___ret; if (__builtin_constant_p(sizeof(Model1_cb_saved)) && Model1___len >= 64) Model1___ret = Model1___memcpy((Model1_cb_data), (Model1_cb_saved), Model1___len); else Model1___ret = __builtin_memcpy((Model1_cb_data), (Model1_cb_saved), Model1___len); Model1___ret; });

 return Model1_res;
}

static inline __attribute__((no_instrument_function)) Model1_u32 Model1_bpf_prog_run_clear_cb(const struct Model1_bpf_prog *Model1_prog,
     struct Model1_sk_buff *Model1_skb)
{
 Model1_u8 *Model1_cb_data = Model1_bpf_skb_cb(Model1_skb);

 if (__builtin_expect(!!(Model1_prog->Model1_cb_access), 0))
  memset(Model1_cb_data, 0, 20);

 return (*Model1_prog->Model1_bpf_func)(Model1_skb, Model1_prog->Model1_insnsi);
}

static inline __attribute__((no_instrument_function)) Model1_u32 Model1_bpf_prog_run_xdp(const struct Model1_bpf_prog *Model1_prog,
       struct Model1_xdp_buff *Model1_xdp)
{
 Model1_u32 Model1_ret;

 Model1_rcu_read_lock();
 Model1_ret = (*Model1_prog->Model1_bpf_func)((void *)Model1_xdp, Model1_prog->Model1_insnsi);
 Model1_rcu_read_unlock();

 return Model1_ret;
}

static inline __attribute__((no_instrument_function)) unsigned int Model1_bpf_prog_size(unsigned int Model1_proglen)
{
 return ({ typeof(sizeof(struct Model1_bpf_prog)) Model1__max1 = (sizeof(struct Model1_bpf_prog)); typeof(__builtin_offsetof(struct Model1_bpf_prog, Model1_insns[Model1_proglen])) Model1__max2 = (__builtin_offsetof(struct Model1_bpf_prog, Model1_insns[Model1_proglen])); (void) (&Model1__max1 == &Model1__max2); Model1__max1 > Model1__max2 ? Model1__max1 : Model1__max2; });

}

static inline __attribute__((no_instrument_function)) bool Model1_bpf_prog_was_classic(const struct Model1_bpf_prog *Model1_prog)
{
 /* When classic BPF programs have been loaded and the arch
	 * does not have a classic BPF JIT (anymore), they have been
	 * converted via bpf_migrate_filter() to eBPF and thus always
	 * have an unspec program type.
	 */
 return Model1_prog->Model1_type == Model1_BPF_PROG_TYPE_UNSPEC;
}
static inline __attribute__((no_instrument_function)) void Model1_bpf_prog_lock_ro(struct Model1_bpf_prog *Model1_fp)
{
}

static inline __attribute__((no_instrument_function)) void Model1_bpf_prog_unlock_ro(struct Model1_bpf_prog *Model1_fp)
{
}


int Model1_sk_filter_trim_cap(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb, unsigned int Model1_cap);
static inline __attribute__((no_instrument_function)) int Model1_sk_filter(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb)
{
 return Model1_sk_filter_trim_cap(Model1_sk, Model1_skb, 1);
}

struct Model1_bpf_prog *Model1_bpf_prog_select_runtime(struct Model1_bpf_prog *Model1_fp, int *err);
void Model1_bpf_prog_free(struct Model1_bpf_prog *Model1_fp);

struct Model1_bpf_prog *Model1_bpf_prog_alloc(unsigned int Model1_size, Model1_gfp_t Model1_gfp_extra_flags);
struct Model1_bpf_prog *Model1_bpf_prog_realloc(struct Model1_bpf_prog *Model1_fp_old, unsigned int Model1_size,
      Model1_gfp_t Model1_gfp_extra_flags);
void Model1___bpf_prog_free(struct Model1_bpf_prog *Model1_fp);

static inline __attribute__((no_instrument_function)) void Model1_bpf_prog_unlock_free(struct Model1_bpf_prog *Model1_fp)
{
 Model1_bpf_prog_unlock_ro(Model1_fp);
 Model1___bpf_prog_free(Model1_fp);
}

typedef int (*Model1_bpf_aux_classic_check_t)(struct Model1_sock_filter *Model1_filter,
           unsigned int Model1_flen);

int Model1_bpf_prog_create(struct Model1_bpf_prog **Model1_pfp, struct Model1_sock_fprog_kern *Model1_fprog);
int Model1_bpf_prog_create_from_user(struct Model1_bpf_prog **Model1_pfp, struct Model1_sock_fprog *Model1_fprog,
         Model1_bpf_aux_classic_check_t Model1_trans, bool Model1_save_orig);
void Model1_bpf_prog_destroy(struct Model1_bpf_prog *Model1_fp);

int Model1_sk_attach_filter(struct Model1_sock_fprog *Model1_fprog, struct Model1_sock *Model1_sk);
int Model1_sk_attach_bpf(Model1_u32 Model1_ufd, struct Model1_sock *Model1_sk);
int Model1_sk_reuseport_attach_filter(struct Model1_sock_fprog *Model1_fprog, struct Model1_sock *Model1_sk);
int Model1_sk_reuseport_attach_bpf(Model1_u32 Model1_ufd, struct Model1_sock *Model1_sk);
int Model1_sk_detach_filter(struct Model1_sock *Model1_sk);
int Model1_sk_get_filter(struct Model1_sock *Model1_sk, struct Model1_sock_filter *Model1_filter,
    unsigned int Model1_len);

bool Model1_sk_filter_charge(struct Model1_sock *Model1_sk, struct Model1_sk_filter *Model1_fp);
void Model1_sk_filter_uncharge(struct Model1_sock *Model1_sk, struct Model1_sk_filter *Model1_fp);

Model1_u64 Model1___bpf_call_base(Model1_u64 Model1_r1, Model1_u64 Model1_r2, Model1_u64 Model1_r3, Model1_u64 Model1_r4, Model1_u64 Model1_r5);

struct Model1_bpf_prog *Model1_bpf_int_jit_compile(struct Model1_bpf_prog *Model1_prog);
bool Model1_bpf_helper_changes_skb_data(void *func);

struct Model1_bpf_prog *Model1_bpf_patch_insn_single(struct Model1_bpf_prog *Model1_prog, Model1_u32 Model1_off,
           const struct Model1_bpf_insn *Model1_patch, Model1_u32 Model1_len);
void Model1_bpf_warn_invalid_xdp_action(Model1_u32 Model1_act);
static inline __attribute__((no_instrument_function)) void Model1_bpf_jit_compile(struct Model1_bpf_prog *Model1_fp)
{
}

static inline __attribute__((no_instrument_function)) void Model1_bpf_jit_free(struct Model1_bpf_prog *Model1_fp)
{
 Model1_bpf_prog_unlock_free(Model1_fp);
}




static inline __attribute__((no_instrument_function)) bool Model1_bpf_needs_clear_a(const struct Model1_sock_filter *Model1_first)
{
 switch (Model1_first->Model1_code) {
 case 0x06 | 0x00:
 case 0x00 | 0x00 | 0x80:
  return false;

 case 0x00 | 0x00 | 0x20:
 case 0x00 | 0x08 | 0x20:
 case 0x00 | 0x10 | 0x20:
  if (Model1_first->Model1_k == (-0x1000) + 40)
   return true;
  return false;

 default:
  return true;
 }
}

static inline __attribute__((no_instrument_function)) Model1_u16 Model1_bpf_anc_helper(const struct Model1_sock_filter *Model1_ftest)
{
 do { if (__builtin_expect(!!(Model1_ftest->Model1_code & (1UL << (15))), 0)) do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("./include/linux/filter.h"), "i" (624), "i" (sizeof(struct Model1_bug_entry))); do { } while (1); } while (0); } while (0);

 switch (Model1_ftest->Model1_code) {
 case 0x00 | 0x00 | 0x20:
 case 0x00 | 0x08 | 0x20:
 case 0x00 | 0x10 | 0x20:


  switch (Model1_ftest->Model1_k) {
  case (-0x1000) + 0: return (1UL << (15)) | 0;
  case (-0x1000) + 4: return (1UL << (15)) | 4;
  case (-0x1000) + 8: return (1UL << (15)) | 8;
  case (-0x1000) + 12: return (1UL << (15)) | 12;
  case (-0x1000) + 16: return (1UL << (15)) | 16;
  case (-0x1000) + 20: return (1UL << (15)) | 20;
  case (-0x1000) + 24: return (1UL << (15)) | 24;
  case (-0x1000) + 28: return (1UL << (15)) | 28;
  case (-0x1000) + 32: return (1UL << (15)) | 32;
  case (-0x1000) + 36: return (1UL << (15)) | 36;
  case (-0x1000) + 40: return (1UL << (15)) | 40;
  case (-0x1000) + 44: return (1UL << (15)) | 44;
  case (-0x1000) + 48: return (1UL << (15)) | 48;
  case (-0x1000) + 52: return (1UL << (15)) | 52;
  case (-0x1000) + 56: return (1UL << (15)) | 56;
  case (-0x1000) + 60: return (1UL << (15)) | 60;
  }
  /* Fallthrough. */
 default:
  return Model1_ftest->Model1_code;
 }
}

void *Model1_bpf_internal_load_pointer_neg_helper(const struct Model1_sk_buff *Model1_skb,
        int Model1_k, unsigned int Model1_size);

static inline __attribute__((no_instrument_function)) void *Model1_bpf_load_pointer(const struct Model1_sk_buff *Model1_skb, int Model1_k,
         unsigned int Model1_size, void *Model1_buffer)
{
 if (Model1_k >= 0)
  return Model1_skb_header_pointer(Model1_skb, Model1_k, Model1_size, Model1_buffer);

 return Model1_bpf_internal_load_pointer_neg_helper(Model1_skb, Model1_k, Model1_size);
}

static inline __attribute__((no_instrument_function)) int Model1_bpf_tell_extensions(void)
{
 return 64;
}





/*
 * RCU-protected list version
 */



/**
 * hlist_nulls_del_init_rcu - deletes entry from hash list with re-initialization
 * @n: the element to delete from the hash list.
 *
 * Note: hlist_nulls_unhashed() on the node return true after this. It is
 * useful for RCU based read lockfree traversal if the writer side
 * must know if the list entry is still hashed or already unhashed.
 *
 * In particular, it means that we can not poison the forward pointers
 * that may still be used for walking the hash list and we can only
 * zero the pprev pointer so list_unhashed() will return true after
 * this.
 *
 * The caller must take whatever precautions are necessary (such as
 * holding appropriate locks) to avoid racing with another
 * list-mutation primitive, such as hlist_nulls_add_head_rcu() or
 * hlist_nulls_del_rcu(), running on this same list.  However, it is
 * perfectly legal to run concurrently with the _rcu list-traversal
 * primitives, such as hlist_nulls_for_each_entry_rcu().
 */
static inline __attribute__((no_instrument_function)) void Model1_hlist_nulls_del_init_rcu(struct Model1_hlist_nulls_node *Model1_n)
{
 if (!Model1_hlist_nulls_unhashed(Model1_n)) {
  Model1___hlist_nulls_del(Model1_n);
  Model1_n->Model1_pprev = ((void *)0);
 }
}







/**
 * hlist_nulls_del_rcu - deletes entry from hash list without re-initialization
 * @n: the element to delete from the hash list.
 *
 * Note: hlist_nulls_unhashed() on entry does not return true after this,
 * the entry is in an undefined state. It is useful for RCU based
 * lockfree traversal.
 *
 * In particular, it means that we can not poison the forward
 * pointers that may still be used for walking the hash list.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as hlist_nulls_add_head_rcu()
 * or hlist_nulls_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * hlist_nulls_for_each_entry().
 */
static inline __attribute__((no_instrument_function)) void Model1_hlist_nulls_del_rcu(struct Model1_hlist_nulls_node *Model1_n)
{
 Model1___hlist_nulls_del(Model1_n);
 Model1_n->Model1_pprev = ((void *) 0x200 + (0xdead000000000000UL));
}

/**
 * hlist_nulls_add_head_rcu
 * @n: the element to add to the hash list.
 * @h: the list to add to.
 *
 * Description:
 * Adds the specified element to the specified hlist_nulls,
 * while permitting racing traversals.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as hlist_nulls_add_head_rcu()
 * or hlist_nulls_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * hlist_nulls_for_each_entry_rcu(), used to prevent memory-consistency
 * problems on Alpha CPUs.  Regardless of the type of CPU, the
 * list-traversal primitive must be guarded by rcu_read_lock().
 */
static inline __attribute__((no_instrument_function)) void Model1_hlist_nulls_add_head_rcu(struct Model1_hlist_nulls_node *Model1_n,
     struct Model1_hlist_nulls_head *Model1_h)
{
 struct Model1_hlist_nulls_node *Model1_first = Model1_h->Model1_first;

 Model1_n->Model1_next = Model1_first;
 Model1_n->Model1_pprev = &Model1_h->Model1_first;
 ({ Model1_uintptr_t Model1__r_a_p__v = (Model1_uintptr_t)(Model1_n); if (__builtin_constant_p(Model1_n) && (Model1__r_a_p__v) == (Model1_uintptr_t)((void *)0)) ({ union { typeof(((*((struct Model1_hlist_nulls_node **)&(Model1_h)->Model1_first)))) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(((*((struct Model1_hlist_nulls_node **)&(Model1_h)->Model1_first))))) ((typeof((*((struct Model1_hlist_nulls_node **)&(Model1_h)->Model1_first))))(Model1__r_a_p__v)) }; Model1___write_once_size(&(((*((struct Model1_hlist_nulls_node **)&(Model1_h)->Model1_first)))), Model1___u.Model1___c, sizeof(((*((struct Model1_hlist_nulls_node **)&(Model1_h)->Model1_first))))); Model1___u.Model1___val; }); else do { do { bool Model1___cond = !((sizeof(*&(*((struct Model1_hlist_nulls_node **)&(Model1_h)->Model1_first))) == sizeof(char) || sizeof(*&(*((struct Model1_hlist_nulls_node **)&(Model1_h)->Model1_first))) == sizeof(short) || sizeof(*&(*((struct Model1_hlist_nulls_node **)&(Model1_h)->Model1_first))) == sizeof(int) || sizeof(*&(*((struct Model1_hlist_nulls_node **)&(Model1_h)->Model1_first))) == sizeof(long))); extern void Model1___compiletime_assert_97(void) ; if (Model1___cond) Model1___compiletime_assert_97(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&(*((struct Model1_hlist_nulls_node **)&(Model1_h)->Model1_first))) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(*&(*((struct Model1_hlist_nulls_node **)&(Model1_h)->Model1_first)))) ((typeof(*((typeof((*((struct Model1_hlist_nulls_node **)&(Model1_h)->Model1_first))))Model1__r_a_p__v)) *)((typeof((*((struct Model1_hlist_nulls_node **)&(Model1_h)->Model1_first))))Model1__r_a_p__v)) }; Model1___write_once_size(&(*&(*((struct Model1_hlist_nulls_node **)&(Model1_h)->Model1_first))), Model1___u.Model1___c, sizeof(*&(*((struct Model1_hlist_nulls_node **)&(Model1_h)->Model1_first)))); Model1___u.Model1___val; }); } while (0); Model1__r_a_p__v; });
 if (!Model1_is_a_nulls(Model1_first))
  Model1_first->Model1_pprev = &Model1_n->Model1_next;
}

/**
 * hlist_nulls_add_tail_rcu
 * @n: the element to add to the hash list.
 * @h: the list to add to.
 *
 * Description:
 * Adds the specified element to the end of the specified hlist_nulls,
 * while permitting racing traversals.  NOTE: tail insertion requires
 * list traversal.
 *
 * The caller must take whatever precautions are necessary
 * (such as holding appropriate locks) to avoid racing
 * with another list-mutation primitive, such as hlist_nulls_add_head_rcu()
 * or hlist_nulls_del_rcu(), running on this same list.
 * However, it is perfectly legal to run concurrently with
 * the _rcu list-traversal primitives, such as
 * hlist_nulls_for_each_entry_rcu(), used to prevent memory-consistency
 * problems on Alpha CPUs.  Regardless of the type of CPU, the
 * list-traversal primitive must be guarded by rcu_read_lock().
 */
static inline __attribute__((no_instrument_function)) void Model1_hlist_nulls_add_tail_rcu(struct Model1_hlist_nulls_node *Model1_n,
     struct Model1_hlist_nulls_head *Model1_h)
{
 struct Model1_hlist_nulls_node *Model1_i, *Model1_last = ((void *)0);

 for (Model1_i = (*((struct Model1_hlist_nulls_node **)&(Model1_h)->Model1_first)); !Model1_is_a_nulls(Model1_i);
      Model1_i = (*((struct Model1_hlist_nulls_node **)&(Model1_i)->Model1_next)))
  Model1_last = Model1_i;

 if (Model1_last) {
  Model1_n->Model1_next = Model1_last->Model1_next;
  Model1_n->Model1_pprev = &Model1_last->Model1_next;
  ({ Model1_uintptr_t Model1__r_a_p__v = (Model1_uintptr_t)(Model1_n); if (__builtin_constant_p(Model1_n) && (Model1__r_a_p__v) == (Model1_uintptr_t)((void *)0)) ({ union { typeof(((*((struct Model1_hlist_nulls_node **)&(Model1_last)->Model1_next)))) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(((*((struct Model1_hlist_nulls_node **)&(Model1_last)->Model1_next))))) ((typeof((*((struct Model1_hlist_nulls_node **)&(Model1_last)->Model1_next))))(Model1__r_a_p__v)) }; Model1___write_once_size(&(((*((struct Model1_hlist_nulls_node **)&(Model1_last)->Model1_next)))), Model1___u.Model1___c, sizeof(((*((struct Model1_hlist_nulls_node **)&(Model1_last)->Model1_next))))); Model1___u.Model1___val; }); else do { do { bool Model1___cond = !((sizeof(*&(*((struct Model1_hlist_nulls_node **)&(Model1_last)->Model1_next))) == sizeof(char) || sizeof(*&(*((struct Model1_hlist_nulls_node **)&(Model1_last)->Model1_next))) == sizeof(short) || sizeof(*&(*((struct Model1_hlist_nulls_node **)&(Model1_last)->Model1_next))) == sizeof(int) || sizeof(*&(*((struct Model1_hlist_nulls_node **)&(Model1_last)->Model1_next))) == sizeof(long))); extern void Model1___compiletime_assert_134(void) ; if (Model1___cond) Model1___compiletime_assert_134(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&(*((struct Model1_hlist_nulls_node **)&(Model1_last)->Model1_next))) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(*&(*((struct Model1_hlist_nulls_node **)&(Model1_last)->Model1_next)))) ((typeof(*((typeof((*((struct Model1_hlist_nulls_node **)&(Model1_last)->Model1_next))))Model1__r_a_p__v)) *)((typeof((*((struct Model1_hlist_nulls_node **)&(Model1_last)->Model1_next))))Model1__r_a_p__v)) }; Model1___write_once_size(&(*&(*((struct Model1_hlist_nulls_node **)&(Model1_last)->Model1_next))), Model1___u.Model1___c, sizeof(*&(*((struct Model1_hlist_nulls_node **)&(Model1_last)->Model1_next)))); Model1___u.Model1___val; }); } while (0); Model1__r_a_p__v; });
 } else {
  Model1_hlist_nulls_add_head_rcu(Model1_n, Model1_h);
 }
}

/**
 * hlist_nulls_for_each_entry_rcu - iterate over rcu list of given type
 * @tpos:	the type * to use as a loop cursor.
 * @pos:	the &struct hlist_nulls_node to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the hlist_nulls_node within the struct.
 *
 * The barrier() is needed to make sure compiler doesn't cache first element [1],
 * as this loop can be restarted [2]
 * [1] Documentation/atomic_ops.txt around line 114
 * [2] Documentation/RCU/rculist_nulls.txt around line 146
 */



/* These are specified by iBCS2 */







/* The rest seem to be more-or-less nonstandard. Check them! */
struct Model1_pollfd {
 int Model1_fd;
 short Model1_events;
 short Model1_revents;
};

extern struct Model1_ctl_table Model1_epoll_table[]; /* for sysctl */
/* ~832 bytes of stack space used max in sys_select/sys_poll before allocating
   additional memory. */
struct Model1_poll_table_struct;

/* 
 * structures and helpers for f_op->poll implementations
 */
typedef void (*Model1_poll_queue_proc)(struct Model1_file *, Model1_wait_queue_head_t *, struct Model1_poll_table_struct *);

/*
 * Do not touch the structure directly, use the access functions
 * poll_does_not_wait() and poll_requested_events() instead.
 */
typedef struct Model1_poll_table_struct {
 Model1_poll_queue_proc Model1__qproc;
 unsigned long Model1__key;
} Model1_poll_table;

static inline __attribute__((no_instrument_function)) void Model1_poll_wait(struct Model1_file * Model1_filp, Model1_wait_queue_head_t * Model1_wait_address, Model1_poll_table *Model1_p)
{
 if (Model1_p && Model1_p->Model1__qproc && Model1_wait_address)
  Model1_p->Model1__qproc(Model1_filp, Model1_wait_address, Model1_p);
}

/*
 * Return true if it is guaranteed that poll will not wait. This is the case
 * if the poll() of another file descriptor in the set got an event, so there
 * is no need for waiting.
 */
static inline __attribute__((no_instrument_function)) bool Model1_poll_does_not_wait(const Model1_poll_table *Model1_p)
{
 return Model1_p == ((void *)0) || Model1_p->Model1__qproc == ((void *)0);
}

/*
 * Return the set of events that the application wants to poll for.
 * This is useful for drivers that need to know whether a DMA transfer has
 * to be started implicitly on poll(). You typically only want to do that
 * if the application is actually polling for POLLIN and/or POLLOUT.
 */
static inline __attribute__((no_instrument_function)) unsigned long Model1_poll_requested_events(const Model1_poll_table *Model1_p)
{
 return Model1_p ? Model1_p->Model1__key : ~0UL;
}

static inline __attribute__((no_instrument_function)) void Model1_init_poll_funcptr(Model1_poll_table *Model1_pt, Model1_poll_queue_proc Model1_qproc)
{
 Model1_pt->Model1__qproc = Model1_qproc;
 Model1_pt->Model1__key = ~0UL; /* all events enabled */
}

struct Model1_poll_table_entry {
 struct Model1_file *Model1_filp;
 unsigned long Model1_key;
 Model1_wait_queue_t Model1_wait;
 Model1_wait_queue_head_t *Model1_wait_address;
};

/*
 * Structures and helpers for select/poll syscall
 */
struct Model1_poll_wqueues {
 Model1_poll_table Model1_pt;
 struct Model1_poll_table_page *Model1_table;
 struct Model1_task_struct *Model1_polling_task;
 int Model1_triggered;
 int error;
 int Model1_inline_index;
 struct Model1_poll_table_entry Model1_inline_entries[((832 - 256) / sizeof(struct Model1_poll_table_entry))];
};

extern void Model1_poll_initwait(struct Model1_poll_wqueues *Model1_pwq);
extern void Model1_poll_freewait(struct Model1_poll_wqueues *Model1_pwq);
extern int Model1_poll_schedule_timeout(struct Model1_poll_wqueues *Model1_pwq, int Model1_state,
     Model1_ktime_t *Model1_expires, unsigned long Model1_slack);
extern Model1_u64 Model1_select_estimate_accuracy(struct Model1_timespec *Model1_tv);


static inline __attribute__((no_instrument_function)) int Model1_poll_schedule(struct Model1_poll_wqueues *Model1_pwq, int Model1_state)
{
 return Model1_poll_schedule_timeout(Model1_pwq, Model1_state, ((void *)0), 0);
}

/*
 * Scalable version of the fd_set.
 */

typedef struct {
 unsigned long *Model1_in, *Model1_out, *Model1_ex;
 unsigned long *Model1_res_in, *Model1_res_out, *Model1_res_ex;
} Model1_fd_set_bits;

/*
 * How many longwords for "nr" bits?
 */




/*
 * We do a VERIFY_WRITE here even though we are only reading this time:
 * we'll write to it eventually..
 *
 * Use "unsigned long" accesses to let user-mode fd_set's be long-aligned.
 */
static inline __attribute__((no_instrument_function))
int Model1_get_fd_set(unsigned long Model1_nr, void *Model1_ufdset, unsigned long *Model1_fdset)
{
 Model1_nr = ((((Model1_nr)+(8*sizeof(long))-1)/(8*sizeof(long)))*sizeof(long));
 if (Model1_ufdset)
  return Model1_copy_from_user(Model1_fdset, Model1_ufdset, Model1_nr) ? -14 : 0;

 memset(Model1_fdset, 0, Model1_nr);
 return 0;
}

static inline __attribute__((no_instrument_function)) unsigned long __attribute__((warn_unused_result))
Model1_set_fd_set(unsigned long Model1_nr, void *Model1_ufdset, unsigned long *Model1_fdset)
{
 if (Model1_ufdset)
  return Model1___copy_to_user(Model1_ufdset, Model1_fdset, ((((Model1_nr)+(8*sizeof(long))-1)/(8*sizeof(long)))*sizeof(long)));
 return 0;
}

static inline __attribute__((no_instrument_function))
void Model1_zero_fd_set(unsigned long Model1_nr, unsigned long *Model1_fdset)
{
 memset(Model1_fdset, 0, ((((Model1_nr)+(8*sizeof(long))-1)/(8*sizeof(long)))*sizeof(long)));
}



extern int Model1_do_select(int Model1_n, Model1_fd_set_bits *Model1_fds, struct Model1_timespec *Model1_end_time);
extern int Model1_do_sys_poll(struct Model1_pollfd * Model1_ufds, unsigned int Model1_nfds,
         struct Model1_timespec *Model1_end_time);
extern int Model1_core_sys_select(int Model1_n, Model1_fd_set *Model1_inp, Model1_fd_set *Model1_outp,
      Model1_fd_set *Model1_exp, struct Model1_timespec *Model1_end_time);

extern int Model1_poll_select_set_timeout(struct Model1_timespec *Model1_to, Model1_time64_t Model1_sec,
       long Model1_nsec);




/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for the TCP protocol sk_state field.
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */



enum {
 Model1_TCP_ESTABLISHED = 1,
 Model1_TCP_SYN_SENT,
 Model1_TCP_SYN_RECV,
 Model1_TCP_FIN_WAIT1,
 Model1_TCP_FIN_WAIT2,
 Model1_TCP_TIME_WAIT,
 Model1_TCP_CLOSE,
 Model1_TCP_CLOSE_WAIT,
 Model1_TCP_LAST_ACK,
 Model1_TCP_LISTEN,
 Model1_TCP_CLOSING, /* Now a valid state */
 Model1_TCP_NEW_SYN_RECV,

 Model1_TCP_MAX_STATES /* Leave at the end! */
};





enum {
 Model1_TCPF_ESTABLISHED = (1 << 1),
 Model1_TCPF_SYN_SENT = (1 << 2),
 Model1_TCPF_SYN_RECV = (1 << 3),
 Model1_TCPF_FIN_WAIT1 = (1 << 4),
 Model1_TCPF_FIN_WAIT2 = (1 << 5),
 Model1_TCPF_TIME_WAIT = (1 << 6),
 Model1_TCPF_CLOSE = (1 << 7),
 Model1_TCPF_CLOSE_WAIT = (1 << 8),
 Model1_TCPF_LAST_ACK = (1 << 9),
 Model1_TCPF_LISTEN = (1 << 10),
 Model1_TCPF_CLOSING = (1 << 11),
 Model1_TCPF_NEW_SYN_RECV = (1 << 12),
};
/*
 * Userspace API for hardware time stamping of network packets
 *
 * Copyright (C) 2008,2009 Intel Corporation
 * Author: Patrick Ohly <patrick.ohly@intel.com>
 *
 */






/* SO_TIMESTAMPING gets an integer bit field comprised of these values */
enum {
 Model1_SOF_TIMESTAMPING_TX_HARDWARE = (1<<0),
 Model1_SOF_TIMESTAMPING_TX_SOFTWARE = (1<<1),
 Model1_SOF_TIMESTAMPING_RX_HARDWARE = (1<<2),
 Model1_SOF_TIMESTAMPING_RX_SOFTWARE = (1<<3),
 Model1_SOF_TIMESTAMPING_SOFTWARE = (1<<4),
 Model1_SOF_TIMESTAMPING_SYS_HARDWARE = (1<<5),
 Model1_SOF_TIMESTAMPING_RAW_HARDWARE = (1<<6),
 Model1_SOF_TIMESTAMPING_OPT_ID = (1<<7),
 Model1_SOF_TIMESTAMPING_TX_SCHED = (1<<8),
 Model1_SOF_TIMESTAMPING_TX_ACK = (1<<9),
 Model1_SOF_TIMESTAMPING_OPT_CMSG = (1<<10),
 Model1_SOF_TIMESTAMPING_OPT_TSONLY = (1<<11),

 Model1_SOF_TIMESTAMPING_LAST = Model1_SOF_TIMESTAMPING_OPT_TSONLY,
 Model1_SOF_TIMESTAMPING_MASK = (Model1_SOF_TIMESTAMPING_LAST - 1) |
     Model1_SOF_TIMESTAMPING_LAST
};

/*
 * SO_TIMESTAMPING flags are either for recording a packet timestamp or for
 * reporting the timestamp to user space.
 * Recording flags can be set both via socket options and control messages.
 */





/**
 * struct hwtstamp_config - %SIOCGHWTSTAMP and %SIOCSHWTSTAMP parameter
 *
 * @flags:	no flags defined right now, must be zero for %SIOCSHWTSTAMP
 * @tx_type:	one of HWTSTAMP_TX_*
 * @rx_filter:	one of HWTSTAMP_FILTER_*
 *
 * %SIOCGHWTSTAMP and %SIOCSHWTSTAMP expect a &struct ifreq with a
 * ifr_data pointer to this structure.  For %SIOCSHWTSTAMP, if the
 * driver or hardware does not support the requested @rx_filter value,
 * the driver may use a more general filter mode.  In this case
 * @rx_filter will indicate the actual mode on return.
 */
struct Model1_hwtstamp_config {
 int Model1_flags;
 int Model1_tx_type;
 int Model1_rx_filter;
};

/* possible values for hwtstamp_config->tx_type */
enum Model1_hwtstamp_tx_types {
 /*
	 * No outgoing packet will need hardware time stamping;
	 * should a packet arrive which asks for it, no hardware
	 * time stamping will be done.
	 */
 Model1_HWTSTAMP_TX_OFF,

 /*
	 * Enables hardware time stamping for outgoing packets;
	 * the sender of the packet decides which are to be
	 * time stamped by setting %SOF_TIMESTAMPING_TX_SOFTWARE
	 * before sending the packet.
	 */
 Model1_HWTSTAMP_TX_ON,

 /*
	 * Enables time stamping for outgoing packets just as
	 * HWTSTAMP_TX_ON does, but also enables time stamp insertion
	 * directly into Sync packets. In this case, transmitted Sync
	 * packets will not received a time stamp via the socket error
	 * queue.
	 */
 Model1_HWTSTAMP_TX_ONESTEP_SYNC,
};

/* possible values for hwtstamp_config->rx_filter */
enum Model1_hwtstamp_rx_filters {
 /* time stamp no incoming packet at all */
 Model1_HWTSTAMP_FILTER_NONE,

 /* time stamp any incoming packet */
 Model1_HWTSTAMP_FILTER_ALL,

 /* return value: time stamp all packets requested plus some others */
 Model1_HWTSTAMP_FILTER_SOME,

 /* PTP v1, UDP, any kind of event packet */
 Model1_HWTSTAMP_FILTER_PTP_V1_L4_EVENT,
 /* PTP v1, UDP, Sync packet */
 Model1_HWTSTAMP_FILTER_PTP_V1_L4_SYNC,
 /* PTP v1, UDP, Delay_req packet */
 Model1_HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ,
 /* PTP v2, UDP, any kind of event packet */
 Model1_HWTSTAMP_FILTER_PTP_V2_L4_EVENT,
 /* PTP v2, UDP, Sync packet */
 Model1_HWTSTAMP_FILTER_PTP_V2_L4_SYNC,
 /* PTP v2, UDP, Delay_req packet */
 Model1_HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ,

 /* 802.AS1, Ethernet, any kind of event packet */
 Model1_HWTSTAMP_FILTER_PTP_V2_L2_EVENT,
 /* 802.AS1, Ethernet, Sync packet */
 Model1_HWTSTAMP_FILTER_PTP_V2_L2_SYNC,
 /* 802.AS1, Ethernet, Delay_req packet */
 Model1_HWTSTAMP_FILTER_PTP_V2_L2_DELAY_REQ,

 /* PTP v2/802.AS1, any layer, any kind of event packet */
 Model1_HWTSTAMP_FILTER_PTP_V2_EVENT,
 /* PTP v2/802.AS1, any layer, Sync packet */
 Model1_HWTSTAMP_FILTER_PTP_V2_SYNC,
 /* PTP v2/802.AS1, any layer, Delay_req packet */
 Model1_HWTSTAMP_FILTER_PTP_V2_DELAY_REQ,
};

/*
 * This structure really needs to be cleaned up.
 * Most of it is for TCP, and not used by any of
 * the other protocols.
 */

/* Define this to get the SOCK_DBG debugging facility. */
/* This is the per-socket lock.  The spinlock provides a synchronization
 * between user contexts and software interrupt processing, whereas the
 * mini-semaphore synchronizes multiple users amongst themselves.
 */
typedef struct {
 Model1_spinlock_t Model1_slock;
 int Model1_owned;
 Model1_wait_queue_head_t Model1_wq;
 /*
	 * We express the mutex-alike socket_lock semantics
	 * to the lock validator by explicitly managing
	 * the slock as a lock variant (in addition to
	 * the slock itself):
	 */



} Model1_socket_lock_t;

struct Model1_sock;
struct Model1_proto;
struct Model1_net;

typedef __u32 Model1___portpair;
typedef __u64 Model1___addrpair;

/**
 *	struct sock_common - minimal network layer representation of sockets
 *	@skc_daddr: Foreign IPv4 addr
 *	@skc_rcv_saddr: Bound local IPv4 addr
 *	@skc_hash: hash value used with various protocol lookup tables
 *	@skc_u16hashes: two u16 hash values used by UDP lookup tables
 *	@skc_dport: placeholder for inet_dport/tw_dport
 *	@skc_num: placeholder for inet_num/tw_num
 *	@skc_family: network address family
 *	@skc_state: Connection state
 *	@skc_reuse: %SO_REUSEADDR setting
 *	@skc_reuseport: %SO_REUSEPORT setting
 *	@skc_bound_dev_if: bound device index if != 0
 *	@skc_bind_node: bind hash linkage for various protocol lookup tables
 *	@skc_portaddr_node: second hash linkage for UDP/UDP-Lite protocol
 *	@skc_prot: protocol handlers inside a network family
 *	@skc_net: reference to the network namespace of this socket
 *	@skc_node: main hash linkage for various protocol lookup tables
 *	@skc_nulls_node: main hash linkage for TCP/UDP/UDP-Lite protocol
 *	@skc_tx_queue_mapping: tx queue number for this connection
 *	@skc_flags: place holder for sk_flags
 *		%SO_LINGER (l_onoff), %SO_BROADCAST, %SO_KEEPALIVE,
 *		%SO_OOBINLINE settings, %SO_TIMESTAMPING settings
 *	@skc_incoming_cpu: record/match cpu processing incoming packets
 *	@skc_refcnt: reference count
 *
 *	This is the minimal network layer representation of sockets, the header
 *	for struct sock and struct inet_timewait_sock.
 */
struct Model1_sock_common {
 /* skc_daddr and skc_rcv_saddr must be grouped on a 8 bytes aligned
	 * address on 64bit arches : cf INET_MATCH()
	 */
 union {
  Model1___addrpair Model1_skc_addrpair;
  struct {
   Model1___be32 Model1_skc_daddr;
   Model1___be32 Model1_skc_rcv_saddr;
  };
 };
 union {
  unsigned int Model1_skc_hash;
  Model1___u16 Model1_skc_u16hashes[2];
 };
 /* skc_dport && skc_num must be grouped as well */
 union {
  Model1___portpair Model1_skc_portpair;
  struct {
   Model1___be16 Model1_skc_dport;
   Model1___u16 Model1_skc_num;
  };
 };

 unsigned short Model1_skc_family;
 volatile unsigned char Model1_skc_state;
 unsigned char Model1_skc_reuse:4;
 unsigned char Model1_skc_reuseport:1;
 unsigned char Model1_skc_ipv6only:1;
 unsigned char Model1_skc_net_refcnt:1;
 int Model1_skc_bound_dev_if;
 union {
  struct Model1_hlist_node Model1_skc_bind_node;
  struct Model1_hlist_node Model1_skc_portaddr_node;
 };
 struct Model1_proto *Model1_skc_prot;
 Model1_possible_net_t Model1_skc_net;


 struct Model1_in6_addr Model1_skc_v6_daddr;
 struct Model1_in6_addr Model1_skc_v6_rcv_saddr;


 Model1_atomic64_t Model1_skc_cookie;

 /* following fields are padding to force
	 * offset(struct sock, sk_refcnt) == 128 on 64bit arches
	 * assuming IPV6 is enabled. We use this padding differently
	 * for different kind of 'sockets'
	 */
 union {
  unsigned long Model1_skc_flags;
  struct Model1_sock *Model1_skc_listener; /* request_sock */
  struct Model1_inet_timewait_death_row *Model1_skc_tw_dr; /* inet_timewait_sock */
 };
 /*
	 * fields between dontcopy_begin/dontcopy_end
	 * are not copied in sock_copy()
	 */
 /* private: */
 int Model1_skc_dontcopy_begin[0];
 /* public: */
 union {
  struct Model1_hlist_node Model1_skc_node;
  struct Model1_hlist_nulls_node Model1_skc_nulls_node;
 };
 int Model1_skc_tx_queue_mapping;
 union {
  int Model1_skc_incoming_cpu;
  Model1_u32 Model1_skc_rcv_wnd;
  Model1_u32 Model1_skc_tw_rcv_nxt; /* struct tcp_timewait_sock  */
 };

 Model1_atomic_t Model1_skc_refcnt;
 /* private: */
 int Model1_skc_dontcopy_end[0];
 union {
  Model1_u32 Model1_skc_rxhash;
  Model1_u32 Model1_skc_window_clamp;
  Model1_u32 Model1_skc_tw_snd_nxt; /* struct tcp_timewait_sock */
 };
 /* public: */
};

/**
  *	struct sock - network layer representation of sockets
  *	@__sk_common: shared layout with inet_timewait_sock
  *	@sk_shutdown: mask of %SEND_SHUTDOWN and/or %RCV_SHUTDOWN
  *	@sk_userlocks: %SO_SNDBUF and %SO_RCVBUF settings
  *	@sk_lock:	synchronizer
  *	@sk_rcvbuf: size of receive buffer in bytes
  *	@sk_wq: sock wait queue and async head
  *	@sk_rx_dst: receive input route used by early demux
  *	@sk_dst_cache: destination cache
  *	@sk_policy: flow policy
  *	@sk_receive_queue: incoming packets
  *	@sk_wmem_alloc: transmit queue bytes committed
  *	@sk_write_queue: Packet sending queue
  *	@sk_omem_alloc: "o" is "option" or "other"
  *	@sk_wmem_queued: persistent queue size
  *	@sk_forward_alloc: space allocated forward
  *	@sk_napi_id: id of the last napi context to receive data for sk
  *	@sk_ll_usec: usecs to busypoll when there is no data
  *	@sk_allocation: allocation mode
  *	@sk_pacing_rate: Pacing rate (if supported by transport/packet scheduler)
  *	@sk_max_pacing_rate: Maximum pacing rate (%SO_MAX_PACING_RATE)
  *	@sk_sndbuf: size of send buffer in bytes
  *	@sk_no_check_tx: %SO_NO_CHECK setting, set checksum in TX packets
  *	@sk_no_check_rx: allow zero checksum in RX packets
  *	@sk_route_caps: route capabilities (e.g. %NETIF_F_TSO)
  *	@sk_route_nocaps: forbidden route capabilities (e.g NETIF_F_GSO_MASK)
  *	@sk_gso_type: GSO type (e.g. %SKB_GSO_TCPV4)
  *	@sk_gso_max_size: Maximum GSO segment size to build
  *	@sk_gso_max_segs: Maximum number of GSO segments
  *	@sk_lingertime: %SO_LINGER l_linger setting
  *	@sk_backlog: always used with the per-socket spinlock held
  *	@sk_callback_lock: used with the callbacks in the end of this struct
  *	@sk_error_queue: rarely used
  *	@sk_prot_creator: sk_prot of original sock creator (see ipv6_setsockopt,
  *			  IPV6_ADDRFORM for instance)
  *	@sk_err: last error
  *	@sk_err_soft: errors that don't cause failure but are the cause of a
  *		      persistent failure not just 'timed out'
  *	@sk_drops: raw/udp drops counter
  *	@sk_ack_backlog: current listen backlog
  *	@sk_max_ack_backlog: listen backlog set in listen()
  *	@sk_priority: %SO_PRIORITY setting
  *	@sk_type: socket type (%SOCK_STREAM, etc)
  *	@sk_protocol: which protocol this socket belongs in this network family
  *	@sk_peer_pid: &struct pid for this socket's peer
  *	@sk_peer_cred: %SO_PEERCRED setting
  *	@sk_rcvlowat: %SO_RCVLOWAT setting
  *	@sk_rcvtimeo: %SO_RCVTIMEO setting
  *	@sk_sndtimeo: %SO_SNDTIMEO setting
  *	@sk_txhash: computed flow hash for use on transmit
  *	@sk_filter: socket filtering instructions
  *	@sk_timer: sock cleanup timer
  *	@sk_stamp: time stamp of last packet received
  *	@sk_tsflags: SO_TIMESTAMPING socket options
  *	@sk_tskey: counter to disambiguate concurrent tstamp requests
  *	@sk_socket: Identd and reporting IO signals
  *	@sk_user_data: RPC layer private data
  *	@sk_frag: cached page frag
  *	@sk_peek_off: current peek_offset value
  *	@sk_send_head: front of stuff to transmit
  *	@sk_security: used by security modules
  *	@sk_mark: generic packet mark
  *	@sk_cgrp_data: cgroup data for this cgroup
  *	@sk_memcg: this socket's memory cgroup association
  *	@sk_write_pending: a write to stream socket waits to start
  *	@sk_state_change: callback to indicate change in the state of the sock
  *	@sk_data_ready: callback to indicate there is data to be processed
  *	@sk_write_space: callback to indicate there is bf sending space available
  *	@sk_error_report: callback to indicate errors (e.g. %MSG_ERRQUEUE)
  *	@sk_backlog_rcv: callback to process the backlog
  *	@sk_destruct: called at sock freeing time, i.e. when all refcnt == 0
  *	@sk_reuseport_cb: reuseport group container
 */
struct Model1_sock {
 /*
	 * Now struct inet_timewait_sock also uses sock_common, so please just
	 * don't add nothing before this first member (__sk_common) --acme
	 */
 struct Model1_sock_common Model1___sk_common;
 Model1_socket_lock_t Model1_sk_lock;
 struct Model1_sk_buff_head Model1_sk_receive_queue;
 /*
	 * The backlog queue is special, it is always used with
	 * the per-socket spinlock held and requires low latency
	 * access. Therefore we special case it's implementation.
	 * Note : rmem_alloc is in this structure to fill a hole
	 * on 64bit arches, not because its logically part of
	 * backlog.
	 */
 struct {
  Model1_atomic_t Model1_rmem_alloc;
  int Model1_len;
  struct Model1_sk_buff *Model1_head;
  struct Model1_sk_buff *Model1_tail;
 } Model1_sk_backlog;

 int Model1_sk_forward_alloc;

 __u32 Model1_sk_txhash;

 unsigned int Model1_sk_napi_id;
 unsigned int Model1_sk_ll_usec;

 Model1_atomic_t Model1_sk_drops;
 int Model1_sk_rcvbuf;

 struct Model1_sk_filter *Model1_sk_filter;
 union {
  struct Model1_socket_wq *Model1_sk_wq;
  struct Model1_socket_wq *Model1_sk_wq_raw;
 };

 struct Model1_xfrm_policy *Model1_sk_policy[2];

 struct Model1_dst_entry *Model1_sk_rx_dst;
 struct Model1_dst_entry *Model1_sk_dst_cache;
 /* Note: 32bit hole on 64bit arches */
 Model1_atomic_t Model1_sk_wmem_alloc;
 Model1_atomic_t Model1_sk_omem_alloc;
 int Model1_sk_sndbuf;
 struct Model1_sk_buff_head Model1_sk_write_queue;

 /*
	 * Because of non atomicity rules, all
	 * changes are protected by socket lock.
	 */
                                ;
 unsigned int Model1_sk_padding : 2,
    Model1_sk_no_check_tx : 1,
    Model1_sk_no_check_rx : 1,
    Model1_sk_userlocks : 4,
    Model1_sk_protocol : 8,
    Model1_sk_type : 16;

                              ;

 int Model1_sk_wmem_queued;
 Model1_gfp_t Model1_sk_allocation;
 Model1_u32 Model1_sk_pacing_rate; /* bytes per second */
 Model1_u32 Model1_sk_max_pacing_rate;
 Model1_netdev_features_t Model1_sk_route_caps;
 Model1_netdev_features_t Model1_sk_route_nocaps;
 int Model1_sk_gso_type;
 unsigned int Model1_sk_gso_max_size;
 Model1_u16 Model1_sk_gso_max_segs;
 int Model1_sk_rcvlowat;
 unsigned long Model1_sk_lingertime;
 struct Model1_sk_buff_head Model1_sk_error_queue;
 struct Model1_proto *Model1_sk_prot_creator;
 Model1_rwlock_t Model1_sk_callback_lock;
 int Model1_sk_err,
    Model1_sk_err_soft;
 Model1_u32 Model1_sk_ack_backlog;
 Model1_u32 Model1_sk_max_ack_backlog;
 __u32 Model1_sk_priority;
 __u32 Model1_sk_mark;
 struct Model1_pid *Model1_sk_peer_pid;
 const struct Model1_cred *Model1_sk_peer_cred;
 long Model1_sk_rcvtimeo;
 long Model1_sk_sndtimeo;
 struct Model1_timer_list Model1_sk_timer;
 Model1_ktime_t Model1_sk_stamp;
 Model1_u16 Model1_sk_tsflags;
 Model1_u8 Model1_sk_shutdown;
 Model1_u32 Model1_sk_tskey;
 struct Model1_socket *Model1_sk_socket;
 void *Model1_sk_user_data;
 struct Model1_page_frag Model1_sk_frag;
 struct Model1_sk_buff *Model1_sk_send_head;
 Model1___s32 Model1_sk_peek_off;
 int Model1_sk_write_pending;

 void *Model1_sk_security;

 struct Model1_sock_cgroup_data Model1_sk_cgrp_data;
 struct Model1_mem_cgroup *Model1_sk_memcg;
 void (*Model1_sk_state_change)(struct Model1_sock *Model1_sk);
 void (*Model1_sk_data_ready)(struct Model1_sock *Model1_sk);
 void (*Model1_sk_write_space)(struct Model1_sock *Model1_sk);
 void (*Model1_sk_error_report)(struct Model1_sock *Model1_sk);
 int (*Model1_sk_backlog_rcv)(struct Model1_sock *Model1_sk,
        struct Model1_sk_buff *Model1_skb);
 void (*Model1_sk_destruct)(struct Model1_sock *Model1_sk);
 struct Model1_sock_reuseport *Model1_sk_reuseport_cb;
 struct Model1_callback_head Model1_sk_rcu;
};






/*
 * SK_CAN_REUSE and SK_NO_REUSE on a socket mean that the socket is OK
 * or not whether his port will be reused by someone else. SK_FORCE_REUSE
 * on a socket means that the socket will reuse everybody else's port
 * without looking at the other's sk_reuse value.
 */





int Model1_sk_set_peek_off(struct Model1_sock *Model1_sk, int Model1_val);

static inline __attribute__((no_instrument_function)) int Model1_sk_peek_offset(struct Model1_sock *Model1_sk, int Model1_flags)
{
 if (__builtin_expect(!!(Model1_flags & 2), 0)) {
  Model1_s32 Model1_off = ({ union { typeof(Model1_sk->Model1_sk_peek_off) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&(Model1_sk->Model1_sk_peek_off), Model1___u.Model1___c, sizeof(Model1_sk->Model1_sk_peek_off)); else Model1___read_once_size_nocheck(&(Model1_sk->Model1_sk_peek_off), Model1___u.Model1___c, sizeof(Model1_sk->Model1_sk_peek_off)); Model1___u.Model1___val; });
  if (Model1_off >= 0)
   return Model1_off;
 }

 return 0;
}

static inline __attribute__((no_instrument_function)) void Model1_sk_peek_offset_bwd(struct Model1_sock *Model1_sk, int Model1_val)
{
 Model1_s32 Model1_off = ({ union { typeof(Model1_sk->Model1_sk_peek_off) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&(Model1_sk->Model1_sk_peek_off), Model1___u.Model1___c, sizeof(Model1_sk->Model1_sk_peek_off)); else Model1___read_once_size_nocheck(&(Model1_sk->Model1_sk_peek_off), Model1___u.Model1___c, sizeof(Model1_sk->Model1_sk_peek_off)); Model1___u.Model1___val; });

 if (__builtin_expect(!!(Model1_off >= 0), 0)) {
  Model1_off = ({ Model1_s32 Model1___max1 = (Model1_off - Model1_val); Model1_s32 Model1___max2 = (0); Model1___max1 > Model1___max2 ? Model1___max1: Model1___max2; });
  ({ union { typeof(Model1_sk->Model1_sk_peek_off) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(Model1_sk->Model1_sk_peek_off)) (Model1_off) }; Model1___write_once_size(&(Model1_sk->Model1_sk_peek_off), Model1___u.Model1___c, sizeof(Model1_sk->Model1_sk_peek_off)); Model1___u.Model1___val; });
 }
}

static inline __attribute__((no_instrument_function)) void Model1_sk_peek_offset_fwd(struct Model1_sock *Model1_sk, int Model1_val)
{
 Model1_sk_peek_offset_bwd(Model1_sk, -Model1_val);
}

/*
 * Hashed lists helper routines
 */
static inline __attribute__((no_instrument_function)) struct Model1_sock *Model1_sk_entry(const struct Model1_hlist_node *Model1_node)
{
 return ({ const typeof( ((struct Model1_sock *)0)->Model1___sk_common.Model1_skc_node ) *Model1___mptr = (Model1_node); (struct Model1_sock *)( (char *)Model1___mptr - __builtin_offsetof(struct Model1_sock, Model1___sk_common.Model1_skc_node) );});
}

static inline __attribute__((no_instrument_function)) struct Model1_sock *Model1___sk_head(const struct Model1_hlist_head *Model1_head)
{
 return ({ const typeof( ((struct Model1_sock *)0)->Model1___sk_common.Model1_skc_node ) *Model1___mptr = (Model1_head->Model1_first); (struct Model1_sock *)( (char *)Model1___mptr - __builtin_offsetof(struct Model1_sock, Model1___sk_common.Model1_skc_node) );});
}

static inline __attribute__((no_instrument_function)) struct Model1_sock *Model1_sk_head(const struct Model1_hlist_head *Model1_head)
{
 return Model1_hlist_empty(Model1_head) ? ((void *)0) : Model1___sk_head(Model1_head);
}

static inline __attribute__((no_instrument_function)) struct Model1_sock *Model1___sk_nulls_head(const struct Model1_hlist_nulls_head *Model1_head)
{
 return ({ const typeof( ((struct Model1_sock *)0)->Model1___sk_common.Model1_skc_nulls_node ) *Model1___mptr = (Model1_head->Model1_first); (struct Model1_sock *)( (char *)Model1___mptr - __builtin_offsetof(struct Model1_sock, Model1___sk_common.Model1_skc_nulls_node) );});
}

static inline __attribute__((no_instrument_function)) struct Model1_sock *Model1_sk_nulls_head(const struct Model1_hlist_nulls_head *Model1_head)
{
 return Model1_hlist_nulls_empty(Model1_head) ? ((void *)0) : Model1___sk_nulls_head(Model1_head);
}

static inline __attribute__((no_instrument_function)) struct Model1_sock *Model1_sk_next(const struct Model1_sock *Model1_sk)
{
 return Model1_sk->Model1___sk_common.Model1_skc_node.Model1_next ?
  ({ const typeof( ((struct Model1_sock *)0)->Model1___sk_common.Model1_skc_node ) *Model1___mptr = (Model1_sk->Model1___sk_common.Model1_skc_node.Model1_next); (struct Model1_sock *)( (char *)Model1___mptr - __builtin_offsetof(struct Model1_sock, Model1___sk_common.Model1_skc_node) );}) : ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model1_sock *Model1_sk_nulls_next(const struct Model1_sock *Model1_sk)
{
 return (!Model1_is_a_nulls(Model1_sk->Model1___sk_common.Model1_skc_nulls_node.Model1_next)) ?
  ({ const typeof( ((struct Model1_sock *)0)->Model1___sk_common.Model1_skc_nulls_node ) *Model1___mptr = (Model1_sk->Model1___sk_common.Model1_skc_nulls_node.Model1_next); (struct Model1_sock *)( (char *)Model1___mptr - __builtin_offsetof(struct Model1_sock, Model1___sk_common.Model1_skc_nulls_node) );}) :

  ((void *)0);
}

static inline __attribute__((no_instrument_function)) bool Model1_sk_unhashed(const struct Model1_sock *Model1_sk)
{
 return Model1_hlist_unhashed(&Model1_sk->Model1___sk_common.Model1_skc_node);
}

static inline __attribute__((no_instrument_function)) bool Model1_sk_hashed(const struct Model1_sock *Model1_sk)
{
 return !Model1_sk_unhashed(Model1_sk);
}

static inline __attribute__((no_instrument_function)) void Model1_sk_node_init(struct Model1_hlist_node *Model1_node)
{
 Model1_node->Model1_pprev = ((void *)0);
}

static inline __attribute__((no_instrument_function)) void Model1_sk_nulls_node_init(struct Model1_hlist_nulls_node *Model1_node)
{
 Model1_node->Model1_pprev = ((void *)0);
}

static inline __attribute__((no_instrument_function)) void Model1___sk_del_node(struct Model1_sock *Model1_sk)
{
 Model1___hlist_del(&Model1_sk->Model1___sk_common.Model1_skc_node);
}

/* NB: equivalent to hlist_del_init_rcu */
static inline __attribute__((no_instrument_function)) bool Model1___sk_del_node_init(struct Model1_sock *Model1_sk)
{
 if (Model1_sk_hashed(Model1_sk)) {
  Model1___sk_del_node(Model1_sk);
  Model1_sk_node_init(&Model1_sk->Model1___sk_common.Model1_skc_node);
  return true;
 }
 return false;
}

/* Grab socket reference count. This operation is valid only
   when sk is ALREADY grabbed f.e. it is found in hash table
   or a list and the lookup is made under lock preventing hash table
   modifications.
 */

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_sock_hold(struct Model1_sock *Model1_sk)
{
 Model1_atomic_inc(&Model1_sk->Model1___sk_common.Model1_skc_refcnt);
}

/* Ungrab socket in the context, which assumes that socket refcnt
   cannot hit zero, f.e. it is true in context of any socketcall.
 */
static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1___sock_put(struct Model1_sock *Model1_sk)
{
 Model1_atomic_dec(&Model1_sk->Model1___sk_common.Model1_skc_refcnt);
}

static inline __attribute__((no_instrument_function)) bool Model1_sk_del_node_init(struct Model1_sock *Model1_sk)
{
 bool Model1_rc = Model1___sk_del_node_init(Model1_sk);

 if (Model1_rc) {
  /* paranoid for a while -acme */
  ({ int Model1___ret_warn_on = !!(Model1_atomic_read(&Model1_sk->Model1___sk_common.Model1_skc_refcnt) == 1); if (__builtin_expect(!!(Model1___ret_warn_on), 0)) Model1_warn_slowpath_null("./include/net/sock.h", 598); __builtin_expect(!!(Model1___ret_warn_on), 0); });
  Model1___sock_put(Model1_sk);
 }
 return Model1_rc;
}


static inline __attribute__((no_instrument_function)) bool Model1___sk_nulls_del_node_init_rcu(struct Model1_sock *Model1_sk)
{
 if (Model1_sk_hashed(Model1_sk)) {
  Model1_hlist_nulls_del_init_rcu(&Model1_sk->Model1___sk_common.Model1_skc_nulls_node);
  return true;
 }
 return false;
}

static inline __attribute__((no_instrument_function)) bool Model1_sk_nulls_del_node_init_rcu(struct Model1_sock *Model1_sk)
{
 bool Model1_rc = Model1___sk_nulls_del_node_init_rcu(Model1_sk);

 if (Model1_rc) {
  /* paranoid for a while -acme */
  ({ int Model1___ret_warn_on = !!(Model1_atomic_read(&Model1_sk->Model1___sk_common.Model1_skc_refcnt) == 1); if (__builtin_expect(!!(Model1___ret_warn_on), 0)) Model1_warn_slowpath_null("./include/net/sock.h", 620); __builtin_expect(!!(Model1___ret_warn_on), 0); });
  Model1___sock_put(Model1_sk);
 }
 return Model1_rc;
}

static inline __attribute__((no_instrument_function)) void Model1___sk_add_node(struct Model1_sock *Model1_sk, struct Model1_hlist_head *Model1_list)
{
 Model1_hlist_add_head(&Model1_sk->Model1___sk_common.Model1_skc_node, Model1_list);
}

static inline __attribute__((no_instrument_function)) void Model1_sk_add_node(struct Model1_sock *Model1_sk, struct Model1_hlist_head *Model1_list)
{
 Model1_sock_hold(Model1_sk);
 Model1___sk_add_node(Model1_sk, Model1_list);
}

static inline __attribute__((no_instrument_function)) void Model1_sk_add_node_rcu(struct Model1_sock *Model1_sk, struct Model1_hlist_head *Model1_list)
{
 Model1_sock_hold(Model1_sk);
 if (1 && Model1_sk->Model1___sk_common.Model1_skc_reuseport &&
     Model1_sk->Model1___sk_common.Model1_skc_family == 10)
  Model1_hlist_add_tail_rcu(&Model1_sk->Model1___sk_common.Model1_skc_node, Model1_list);
 else
  Model1_hlist_add_head_rcu(&Model1_sk->Model1___sk_common.Model1_skc_node, Model1_list);
}

static inline __attribute__((no_instrument_function)) void Model1___sk_nulls_add_node_rcu(struct Model1_sock *Model1_sk, struct Model1_hlist_nulls_head *Model1_list)
{
 if (1 && Model1_sk->Model1___sk_common.Model1_skc_reuseport &&
     Model1_sk->Model1___sk_common.Model1_skc_family == 10)
  Model1_hlist_nulls_add_tail_rcu(&Model1_sk->Model1___sk_common.Model1_skc_nulls_node, Model1_list);
 else
  Model1_hlist_nulls_add_head_rcu(&Model1_sk->Model1___sk_common.Model1_skc_nulls_node, Model1_list);
}

static inline __attribute__((no_instrument_function)) void Model1_sk_nulls_add_node_rcu(struct Model1_sock *Model1_sk, struct Model1_hlist_nulls_head *Model1_list)
{
 Model1_sock_hold(Model1_sk);
 Model1___sk_nulls_add_node_rcu(Model1_sk, Model1_list);
}

static inline __attribute__((no_instrument_function)) void Model1___sk_del_bind_node(struct Model1_sock *Model1_sk)
{
 Model1___hlist_del(&Model1_sk->Model1___sk_common.Model1_skc_bind_node);
}

static inline __attribute__((no_instrument_function)) void Model1_sk_add_bind_node(struct Model1_sock *Model1_sk,
     struct Model1_hlist_head *Model1_list)
{
 Model1_hlist_add_head(&Model1_sk->Model1___sk_common.Model1_skc_bind_node, Model1_list);
}
/**
 * sk_for_each_entry_offset_rcu - iterate over a list at a given struct offset
 * @tpos:	the type * to use as a loop cursor.
 * @pos:	the &struct hlist_node to use as a loop cursor.
 * @head:	the head for your list.
 * @offset:	offset of hlist_node within the struct.
 *
 */






static inline __attribute__((no_instrument_function)) struct Model1_user_namespace *Model1_sk_user_ns(struct Model1_sock *Model1_sk)
{
 /* Careful only use this in a context where these parameters
	 * can not change and must all be valid, such as recvmsg from
	 * userspace.
	 */
 return Model1_sk->Model1_sk_socket->Model1_file->Model1_f_cred->Model1_user_ns;
}

/* Sock flags */
enum Model1_sock_flags {
 Model1_SOCK_DEAD,
 Model1_SOCK_DONE,
 Model1_SOCK_URGINLINE,
 Model1_SOCK_KEEPOPEN,
 Model1_SOCK_LINGER,
 Model1_SOCK_DESTROY,
 Model1_SOCK_BROADCAST,
 Model1_SOCK_TIMESTAMP,
 Model1_SOCK_ZAPPED,
 Model1_SOCK_USE_WRITE_QUEUE, /* whether to call sk->sk_write_space in sock_wfree */
 Model1_SOCK_DBG, /* %SO_DEBUG setting */
 Model1_SOCK_RCVTSTAMP, /* %SO_TIMESTAMP setting */
 Model1_SOCK_RCVTSTAMPNS, /* %SO_TIMESTAMPNS setting */
 Model1_SOCK_LOCALROUTE, /* route locally only, %SO_DONTROUTE setting */
 Model1_SOCK_QUEUE_SHRUNK, /* write queue has been shrunk recently */
 Model1_SOCK_MEMALLOC, /* VM depends on this socket for swapping */
 Model1_SOCK_TIMESTAMPING_RX_SOFTWARE, /* %SOF_TIMESTAMPING_RX_SOFTWARE */
 Model1_SOCK_FASYNC, /* fasync() active */
 Model1_SOCK_RXQ_OVFL,
 Model1_SOCK_ZEROCOPY, /* buffers from userspace */
 Model1_SOCK_WIFI_STATUS, /* push wifi status to userspace */
 Model1_SOCK_NOFCS, /* Tell NIC not to do the Ethernet FCS.
		     * Will use last 4 bytes of packet sent from
		     * user-space instead.
		     */
 Model1_SOCK_FILTER_LOCKED, /* Filter cannot be changed anymore */
 Model1_SOCK_SELECT_ERR_QUEUE, /* Wake select on error queue */
 Model1_SOCK_RCU_FREE, /* wait rcu grace period in sk_destruct() */
};



static inline __attribute__((no_instrument_function)) void Model1_sock_copy_flags(struct Model1_sock *Model1_nsk, struct Model1_sock *Model1_osk)
{
 Model1_nsk->Model1___sk_common.Model1_skc_flags = Model1_osk->Model1___sk_common.Model1_skc_flags;
}

static inline __attribute__((no_instrument_function)) void Model1_sock_set_flag(struct Model1_sock *Model1_sk, enum Model1_sock_flags Model1_flag)
{
 Model1___set_bit(Model1_flag, &Model1_sk->Model1___sk_common.Model1_skc_flags);
}

static inline __attribute__((no_instrument_function)) void Model1_sock_reset_flag(struct Model1_sock *Model1_sk, enum Model1_sock_flags Model1_flag)
{
 Model1___clear_bit(Model1_flag, &Model1_sk->Model1___sk_common.Model1_skc_flags);
}

static inline __attribute__((no_instrument_function)) bool Model1_sock_flag(const struct Model1_sock *Model1_sk, enum Model1_sock_flags Model1_flag)
{
 return (__builtin_constant_p((Model1_flag)) ? Model1_constant_test_bit((Model1_flag), (&Model1_sk->Model1___sk_common.Model1_skc_flags)) : Model1_variable_test_bit((Model1_flag), (&Model1_sk->Model1___sk_common.Model1_skc_flags)));
}


extern struct Model1_static_key Model1_memalloc_socks;
static inline __attribute__((no_instrument_function)) int Model1_sk_memalloc_socks(void)
{
 return Model1_static_key_false(&Model1_memalloc_socks);
}
static inline __attribute__((no_instrument_function)) Model1_gfp_t Model1_sk_gfp_mask(const struct Model1_sock *Model1_sk, Model1_gfp_t Model1_gfp_mask)
{
 return Model1_gfp_mask | (Model1_sk->Model1_sk_allocation & (( Model1_gfp_t)0x2000u));
}

static inline __attribute__((no_instrument_function)) void Model1_sk_acceptq_removed(struct Model1_sock *Model1_sk)
{
 Model1_sk->Model1_sk_ack_backlog--;
}

static inline __attribute__((no_instrument_function)) void Model1_sk_acceptq_added(struct Model1_sock *Model1_sk)
{
 Model1_sk->Model1_sk_ack_backlog++;
}

static inline __attribute__((no_instrument_function)) bool Model1_sk_acceptq_is_full(const struct Model1_sock *Model1_sk)
{
 return Model1_sk->Model1_sk_ack_backlog > Model1_sk->Model1_sk_max_ack_backlog;
}

/*
 * Compute minimal free write space needed to queue new packets.
 */
static inline __attribute__((no_instrument_function)) int Model1_sk_stream_min_wspace(const struct Model1_sock *Model1_sk)
{
 return Model1_sk->Model1_sk_wmem_queued >> 1;
}

static inline __attribute__((no_instrument_function)) int Model1_sk_stream_wspace(const struct Model1_sock *Model1_sk)
{
 return Model1_sk->Model1_sk_sndbuf - Model1_sk->Model1_sk_wmem_queued;
}

void Model1_sk_stream_write_space(struct Model1_sock *Model1_sk);

/* OOB backlog add */
static inline __attribute__((no_instrument_function)) void Model1___sk_add_backlog(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb)
{
 /* dont let skb dst not refcounted, we are going to leave rcu lock */
 Model1_skb_dst_force_safe(Model1_skb);

 if (!Model1_sk->Model1_sk_backlog.Model1_tail)
  Model1_sk->Model1_sk_backlog.Model1_head = Model1_skb;
 else
  Model1_sk->Model1_sk_backlog.Model1_tail->Model1_next = Model1_skb;

 Model1_sk->Model1_sk_backlog.Model1_tail = Model1_skb;
 Model1_skb->Model1_next = ((void *)0);
}

/*
 * Take into account size of receive queue and backlog queue
 * Do not take into account this skb truesize,
 * to allow even a single big packet to come.
 */
static inline __attribute__((no_instrument_function)) bool Model1_sk_rcvqueues_full(const struct Model1_sock *Model1_sk, unsigned int Model1_limit)
{
 unsigned int Model1_qsize = Model1_sk->Model1_sk_backlog.Model1_len + Model1_atomic_read(&Model1_sk->Model1_sk_backlog.Model1_rmem_alloc);

 return Model1_qsize > Model1_limit;
}

/* The per-socket spinlock must be held here. */
static inline __attribute__((no_instrument_function)) __attribute__((warn_unused_result)) int Model1_sk_add_backlog(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb,
           unsigned int Model1_limit)
{
 if (Model1_sk_rcvqueues_full(Model1_sk, Model1_limit))
  return -105;

 /*
	 * If the skb was allocated from pfmemalloc reserves, only
	 * allow SOCK_MEMALLOC sockets to use it as this socket is
	 * helping free memory
	 */
 if (Model1_skb_pfmemalloc(Model1_skb) && !Model1_sock_flag(Model1_sk, Model1_SOCK_MEMALLOC))
  return -12;

 Model1___sk_add_backlog(Model1_sk, Model1_skb);
 Model1_sk->Model1_sk_backlog.Model1_len += Model1_skb->Model1_truesize;
 return 0;
}

int Model1___sk_backlog_rcv(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);

static inline __attribute__((no_instrument_function)) int Model1_sk_backlog_rcv(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb)
{
 if (Model1_sk_memalloc_socks() && Model1_skb_pfmemalloc(Model1_skb))
  return Model1___sk_backlog_rcv(Model1_sk, Model1_skb);

 #if CY_ABSTRACT7
  return Model1_tcp_v4_do_rcv(Model1_sk, Model1_skb);
 #else
 return Model1_sk->Model1_sk_backlog_rcv(Model1_sk, Model1_skb);
 #endif
}

static inline __attribute__((no_instrument_function)) void Model1_sk_incoming_cpu_update(struct Model1_sock *Model1_sk)
{
#if CY_ABSTRACT6
    Model1_sk->Model1___sk_common.Model1_skc_incoming_cpu = 0; //raw_smp_processor_id() = 0; https://elixir.bootlin.com/linux/v4.8/source/include/linux/smp.h#L130
#else
 Model1_sk->Model1___sk_common.Model1_skc_incoming_cpu = (({ typeof(Model1_cpu_number) Model1_pscr_ret__; do { const void *Model1___vpp_verify = (typeof((&(Model1_cpu_number)) + 0))((void *)0); (void)Model1___vpp_verify; } while (0); switch(sizeof(Model1_cpu_number)) { case 1: Model1_pscr_ret__ = ({ typeof(Model1_cpu_number) Model1_pfo_ret__; switch (sizeof(Model1_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 2: Model1_pscr_ret__ = ({ typeof(Model1_cpu_number) Model1_pfo_ret__; switch (sizeof(Model1_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 4: Model1_pscr_ret__ = ({ typeof(Model1_cpu_number) Model1_pfo_ret__; switch (sizeof(Model1_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; case 8: Model1_pscr_ret__ = ({ typeof(Model1_cpu_number) Model1_pfo_ret__; switch (sizeof(Model1_cpu_number)) { case 1: asm("mov" "b ""%%""gs"":" "%" "1"",%0" : "=q" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 2: asm("mov" "w ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 4: asm("mov" "l ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; case 8: asm("mov" "q ""%%""gs"":" "%" "1"",%0" : "=r" (Model1_pfo_ret__) : "m" (Model1_cpu_number)); break; default: Model1___bad_percpu_size(); } Model1_pfo_ret__; }); break; default: Model1___bad_size_call_parameter(); break; } Model1_pscr_ret__; }));
#endif
}

static inline __attribute__((no_instrument_function)) void Model1_sock_rps_record_flow_hash(__u32 Model1_hash)
{

 struct Model1_rps_sock_flow_table *Model1_sock_flow_table;

 Model1_rcu_read_lock();
 Model1_sock_flow_table = ({ typeof(*(Model1_rps_sock_flow_table)) *Model1_________p1 = (typeof(*(Model1_rps_sock_flow_table)) *)({ typeof((Model1_rps_sock_flow_table)) Model1__________p1 = ({ union { typeof((Model1_rps_sock_flow_table)) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&((Model1_rps_sock_flow_table)), Model1___u.Model1___c, sizeof((Model1_rps_sock_flow_table))); else Model1___read_once_size_nocheck(&((Model1_rps_sock_flow_table)), Model1___u.Model1___c, sizeof((Model1_rps_sock_flow_table))); Model1___u.Model1___val; }); typeof(*((Model1_rps_sock_flow_table))) *Model1____typecheck_p __attribute__((unused)); do { } while (0); (Model1__________p1); }); do { } while (0); ; ((typeof(*(Model1_rps_sock_flow_table)) *)(Model1_________p1)); });
 Model1_rps_record_sock_flow(Model1_sock_flow_table, Model1_hash);
 Model1_rcu_read_unlock();

}

static inline __attribute__((no_instrument_function)) void Model1_sock_rps_record_flow(const struct Model1_sock *Model1_sk)
{

 Model1_sock_rps_record_flow_hash(Model1_sk->Model1___sk_common.Model1_skc_rxhash);

}

static inline __attribute__((no_instrument_function)) void Model1_sock_rps_save_rxhash(struct Model1_sock *Model1_sk,
     const struct Model1_sk_buff *Model1_skb)
{

 if (__builtin_expect(!!(Model1_sk->Model1___sk_common.Model1_skc_rxhash != Model1_skb->Model1_hash), 0))
  Model1_sk->Model1___sk_common.Model1_skc_rxhash = Model1_skb->Model1_hash;

}

static inline __attribute__((no_instrument_function)) void Model1_sock_rps_reset_rxhash(struct Model1_sock *Model1_sk)
{

 Model1_sk->Model1___sk_common.Model1_skc_rxhash = 0;

}
int Model1_sk_stream_wait_connect(struct Model1_sock *Model1_sk, long *Model1_timeo_p);
int Model1_sk_stream_wait_memory(struct Model1_sock *Model1_sk, long *Model1_timeo_p);
void Model1_sk_stream_wait_close(struct Model1_sock *Model1_sk, long Model1_timeo_p);
int Model1_sk_stream_error(struct Model1_sock *Model1_sk, int Model1_flags, int err);
void Model1_sk_stream_kill_queues(struct Model1_sock *Model1_sk);
void Model1_sk_set_memalloc(struct Model1_sock *Model1_sk);
void Model1_sk_clear_memalloc(struct Model1_sock *Model1_sk);

void Model1___sk_flush_backlog(struct Model1_sock *Model1_sk);

static inline __attribute__((no_instrument_function)) bool Model1_sk_flush_backlog(struct Model1_sock *Model1_sk)
{
 if (__builtin_expect(!!(({ union { typeof(Model1_sk->Model1_sk_backlog.Model1_tail) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&(Model1_sk->Model1_sk_backlog.Model1_tail), Model1___u.Model1___c, sizeof(Model1_sk->Model1_sk_backlog.Model1_tail)); else Model1___read_once_size_nocheck(&(Model1_sk->Model1_sk_backlog.Model1_tail), Model1___u.Model1___c, sizeof(Model1_sk->Model1_sk_backlog.Model1_tail)); Model1___u.Model1___val; })), 0)) {
  Model1___sk_flush_backlog(Model1_sk);
  return true;
 }
 return false;
}

int Model1_sk_wait_data(struct Model1_sock *Model1_sk, long *Model1_timeo, const struct Model1_sk_buff *Model1_skb);

struct Model1_request_sock_ops;
struct Model1_timewait_sock_ops;
struct Model1_inet_hashinfo;
struct Model1_raw_hashinfo;
struct Model1_module;

/*
 * caches using SLAB_DESTROY_BY_RCU should let .next pointer from nulls nodes
 * un-modified. Special care is taken when initializing object to zero.
 */
static inline __attribute__((no_instrument_function)) void Model1_sk_prot_clear_nulls(struct Model1_sock *Model1_sk, int Model1_size)
{
 if (__builtin_offsetof(struct Model1_sock, Model1___sk_common.Model1_skc_node.Model1_next) != 0)
  memset(Model1_sk, 0, __builtin_offsetof(struct Model1_sock, Model1___sk_common.Model1_skc_node.Model1_next));
 memset(&Model1_sk->Model1___sk_common.Model1_skc_node.Model1_pprev, 0,
        Model1_size - __builtin_offsetof(struct Model1_sock, Model1___sk_common.Model1_skc_node.Model1_pprev));
}

/* Networking protocol blocks we attach to sockets.
 * socket layer -> transport layer interface
 */
struct Model1_proto {
 void (*Model1_close)(struct Model1_sock *Model1_sk,
     long Model1_timeout);
 int (*Model1_connect)(struct Model1_sock *Model1_sk,
     struct Model1_sockaddr *Model1_uaddr,
     int Model1_addr_len);
 int (*Model1_disconnect)(struct Model1_sock *Model1_sk, int Model1_flags);

 struct Model1_sock * (*Model1_accept)(struct Model1_sock *Model1_sk, int Model1_flags, int *err);

 int (*Model1_ioctl)(struct Model1_sock *Model1_sk, int Model1_cmd,
      unsigned long Model1_arg);
 int (*Model1_init)(struct Model1_sock *Model1_sk);
 void (*Model1_destroy)(struct Model1_sock *Model1_sk);
 void (*Model1_shutdown)(struct Model1_sock *Model1_sk, int Model1_how);
 int (*Model1_setsockopt)(struct Model1_sock *Model1_sk, int Model1_level,
     int Model1_optname, char *Model1_optval,
     unsigned int Model1_optlen);
 int (*Model1_getsockopt)(struct Model1_sock *Model1_sk, int Model1_level,
     int Model1_optname, char *Model1_optval,
     int *Model1_option);

 int (*Model1_compat_setsockopt)(struct Model1_sock *Model1_sk,
     int Model1_level,
     int Model1_optname, char *Model1_optval,
     unsigned int Model1_optlen);
 int (*Model1_compat_getsockopt)(struct Model1_sock *Model1_sk,
     int Model1_level,
     int Model1_optname, char *Model1_optval,
     int *Model1_option);
 int (*Model1_compat_ioctl)(struct Model1_sock *Model1_sk,
     unsigned int Model1_cmd, unsigned long Model1_arg);

 int (*Model1_sendmsg)(struct Model1_sock *Model1_sk, struct Model1_msghdr *Model1_msg,
        Model1_size_t Model1_len);
 int (*Model1_recvmsg)(struct Model1_sock *Model1_sk, struct Model1_msghdr *Model1_msg,
        Model1_size_t Model1_len, int Model1_noblock, int Model1_flags,
        int *Model1_addr_len);
 int (*Model1_sendpage)(struct Model1_sock *Model1_sk, struct Model1_page *Model1_page,
     int Model1_offset, Model1_size_t Model1_size, int Model1_flags);
 int (*Model1_bind)(struct Model1_sock *Model1_sk,
     struct Model1_sockaddr *Model1_uaddr, int Model1_addr_len);

 int (*Model1_backlog_rcv) (struct Model1_sock *Model1_sk,
      struct Model1_sk_buff *Model1_skb);

 void (*Model1_release_cb)(struct Model1_sock *Model1_sk);

 /* Keeping track of sk's, looking them up, and port selection methods. */
 int (*Model1_hash)(struct Model1_sock *Model1_sk);
 void (*Model1_unhash)(struct Model1_sock *Model1_sk);
 void (*Model1_rehash)(struct Model1_sock *Model1_sk);
 int (*Model1_get_port)(struct Model1_sock *Model1_sk, unsigned short Model1_snum);
 void (*Model1_clear_sk)(struct Model1_sock *Model1_sk, int Model1_size);

 /* Keeping track of sockets in use */

 unsigned int Model1_inuse_idx;


 bool (*Model1_stream_memory_free)(const struct Model1_sock *Model1_sk);
 /* Memory pressure */
 void (*Model1_enter_memory_pressure)(struct Model1_sock *Model1_sk);
 Model1_atomic_long_t *Model1_memory_allocated; /* Current allocated memory. */
 struct Model1_percpu_counter *Model1_sockets_allocated; /* Current number of sockets. */
 /*
	 * Pressure flag: try to collapse.
	 * Technical note: it is used by multiple contexts non atomically.
	 * All the __sk_mem_schedule() is of this nature: accounting
	 * is strict, actions are advisory and have some latency.
	 */
 int *Model1_memory_pressure;
 long *Model1_sysctl_mem;
 int *Model1_sysctl_wmem;
 int *Model1_sysctl_rmem;
 int Model1_max_header;
 bool Model1_no_autobind;

 struct Model1_kmem_cache *Model1_slab;
 unsigned int Model1_obj_size;
 int Model1_slab_flags;

 struct Model1_percpu_counter *Model1_orphan_count;

 struct Model1_request_sock_ops *Model1_rsk_prot;
 struct Model1_timewait_sock_ops *Model1_twsk_prot;

 union {
  struct Model1_inet_hashinfo *Model1_hashinfo;
  struct Model1_udp_table *Model1_udp_table;
  struct Model1_raw_hashinfo *Model1_raw_hash;
 } Model1_h;

 struct Model1_module *Model1_owner;

 char Model1_name[32];

 struct Model1_list_head Model1_node;



 int (*Model1_diag_destroy)(struct Model1_sock *Model1_sk, int err);
};

int Model1_proto_register(struct Model1_proto *Model1_prot, int Model1_alloc_slab);
void Model1_proto_unregister(struct Model1_proto *Model1_prot);
static inline __attribute__((no_instrument_function)) bool Model1_sk_stream_memory_free(const struct Model1_sock *Model1_sk)
{
 if (Model1_sk->Model1_sk_wmem_queued >= Model1_sk->Model1_sk_sndbuf)
  return false;

 return Model1_sk->Model1___sk_common.Model1_skc_prot->Model1_stream_memory_free ?
  Model1_sk->Model1___sk_common.Model1_skc_prot->Model1_stream_memory_free(Model1_sk) : true;
}

static inline __attribute__((no_instrument_function)) bool Model1_sk_stream_is_writeable(const struct Model1_sock *Model1_sk)
{
 return Model1_sk_stream_wspace(Model1_sk) >= Model1_sk_stream_min_wspace(Model1_sk) &&
        Model1_sk_stream_memory_free(Model1_sk);
}


static inline __attribute__((no_instrument_function)) bool Model1_sk_has_memory_pressure(const struct Model1_sock *Model1_sk)
{
 return Model1_sk->Model1___sk_common.Model1_skc_prot->Model1_memory_pressure != ((void *)0);
}

static inline __attribute__((no_instrument_function)) bool Model1_sk_under_memory_pressure(const struct Model1_sock *Model1_sk)
{
 if (!Model1_sk->Model1___sk_common.Model1_skc_prot->Model1_memory_pressure)
  return false;

 if (0 && Model1_sk->Model1_sk_memcg &&
     Model1_mem_cgroup_under_socket_pressure(Model1_sk->Model1_sk_memcg))
  return true;

 return !!*Model1_sk->Model1___sk_common.Model1_skc_prot->Model1_memory_pressure;
}

static inline __attribute__((no_instrument_function)) void Model1_sk_leave_memory_pressure(struct Model1_sock *Model1_sk)
{
 int *Model1_memory_pressure = Model1_sk->Model1___sk_common.Model1_skc_prot->Model1_memory_pressure;

 if (!Model1_memory_pressure)
  return;

 if (*Model1_memory_pressure)
  *Model1_memory_pressure = 0;
}

#if CY_ABSTRACT7
void Model1_tcp_enter_memory_pressure(struct Model1_sock *Model1_sk);
#endif
static inline __attribute__((no_instrument_function)) void Model1_sk_enter_memory_pressure(struct Model1_sock *Model1_sk)
{
 #if CY_ABSTRACT7
  Model1_tcp_enter_memory_pressure(Model1_sk);
 #else
 if (!Model1_sk->Model1___sk_common.Model1_skc_prot->Model1_enter_memory_pressure)
  return;

 Model1_sk->Model1___sk_common.Model1_skc_prot->Model1_enter_memory_pressure(Model1_sk);
 #endif
}

static inline __attribute__((no_instrument_function)) long Model1_sk_prot_mem_limits(const struct Model1_sock *Model1_sk, int Model1_index)
{
 return Model1_sk->Model1___sk_common.Model1_skc_prot->Model1_sysctl_mem[Model1_index];
}

static inline __attribute__((no_instrument_function)) long
Model1_sk_memory_allocated(const struct Model1_sock *Model1_sk)
{
 return Model1_atomic_long_read(Model1_sk->Model1___sk_common.Model1_skc_prot->Model1_memory_allocated);
}

static inline __attribute__((no_instrument_function)) long
Model1_sk_memory_allocated_add(struct Model1_sock *Model1_sk, int Model1_amt)
{
 return Model1_atomic_long_add_return(Model1_amt, Model1_sk->Model1___sk_common.Model1_skc_prot->Model1_memory_allocated);
}

static inline __attribute__((no_instrument_function)) void
Model1_sk_memory_allocated_sub(struct Model1_sock *Model1_sk, int Model1_amt)
{
 Model1_atomic_long_sub(Model1_amt, Model1_sk->Model1___sk_common.Model1_skc_prot->Model1_memory_allocated);
}

static inline __attribute__((no_instrument_function)) void Model1_sk_sockets_allocated_dec(struct Model1_sock *Model1_sk)
{
 Model1_percpu_counter_dec(Model1_sk->Model1___sk_common.Model1_skc_prot->Model1_sockets_allocated);
}

static inline __attribute__((no_instrument_function)) void Model1_sk_sockets_allocated_inc(struct Model1_sock *Model1_sk)
{
 Model1_percpu_counter_inc(Model1_sk->Model1___sk_common.Model1_skc_prot->Model1_sockets_allocated);
}

static inline __attribute__((no_instrument_function)) int
Model1_sk_sockets_allocated_read_positive(struct Model1_sock *Model1_sk)
{
 return Model1_percpu_counter_read_positive(Model1_sk->Model1___sk_common.Model1_skc_prot->Model1_sockets_allocated);
}

static inline __attribute__((no_instrument_function)) int
Model1_proto_sockets_allocated_sum_positive(struct Model1_proto *Model1_prot)
{
 return Model1_percpu_counter_sum_positive(Model1_prot->Model1_sockets_allocated);
}

static inline __attribute__((no_instrument_function)) long
Model1_proto_memory_allocated(struct Model1_proto *Model1_prot)
{
 return Model1_atomic_long_read(Model1_prot->Model1_memory_allocated);
}

static inline __attribute__((no_instrument_function)) bool
Model1_proto_memory_pressure(struct Model1_proto *Model1_prot)
{
 if (!Model1_prot->Model1_memory_pressure)
  return false;
 return !!*Model1_prot->Model1_memory_pressure;
}



/* Called with local bh disabled */
void Model1_sock_prot_inuse_add(struct Model1_net *Model1_net, struct Model1_proto *Model1_prot, int Model1_inc);
int Model1_sock_prot_inuse_get(struct Model1_net *Model1_net, struct Model1_proto *Model1_proto);
/* With per-bucket locks this operation is not-atomic, so that
 * this version is not worse.
 */
static inline __attribute__((no_instrument_function)) int Model1___sk_prot_rehash(struct Model1_sock *Model1_sk)
{
 Model1_sk->Model1___sk_common.Model1_skc_prot->Model1_unhash(Model1_sk);
 return Model1_sk->Model1___sk_common.Model1_skc_prot->Model1_hash(Model1_sk);
}

void Model1_sk_prot_clear_portaddr_nulls(struct Model1_sock *Model1_sk, int Model1_size);

/* About 10 seconds */


/* Sockets 0-1023 can't be bound to unless you are superuser */
struct Model1_socket_alloc {
 struct Model1_socket Model1_socket;
 struct Model1_inode Model1_vfs_inode;
};

static inline __attribute__((no_instrument_function)) struct Model1_socket *Model1_SOCKET_I(struct Model1_inode *Model1_inode)
{
 return &({ const typeof( ((struct Model1_socket_alloc *)0)->Model1_vfs_inode ) *Model1___mptr = (Model1_inode); (struct Model1_socket_alloc *)( (char *)Model1___mptr - __builtin_offsetof(struct Model1_socket_alloc, Model1_vfs_inode) );})->Model1_socket;
}

static inline __attribute__((no_instrument_function)) struct Model1_inode *Model1_SOCK_INODE(struct Model1_socket *Model1_socket)
{
 return &({ const typeof( ((struct Model1_socket_alloc *)0)->Model1_socket ) *Model1___mptr = (Model1_socket); (struct Model1_socket_alloc *)( (char *)Model1___mptr - __builtin_offsetof(struct Model1_socket_alloc, Model1_socket) );})->Model1_vfs_inode;
}

/*
 * Functions for memory accounting
 */
int Model1___sk_mem_schedule(struct Model1_sock *Model1_sk, int Model1_size, int Model1_kind);
void Model1___sk_mem_reclaim(struct Model1_sock *Model1_sk, int Model1_amount);






static inline __attribute__((no_instrument_function)) int Model1_sk_mem_pages(int Model1_amt)
{
 return (Model1_amt + ((int)((1UL) << 12)) - 1) >> ( __builtin_constant_p(((int)((1UL) << 12))) ? ( (((int)((1UL) << 12))) < 1 ? Model1_____ilog2_NaN() : (((int)((1UL) << 12))) & (1ULL << 63) ? 63 : (((int)((1UL) << 12))) & (1ULL << 62) ? 62 : (((int)((1UL) << 12))) & (1ULL << 61) ? 61 : (((int)((1UL) << 12))) & (1ULL << 60) ? 60 : (((int)((1UL) << 12))) & (1ULL << 59) ? 59 : (((int)((1UL) << 12))) & (1ULL << 58) ? 58 : (((int)((1UL) << 12))) & (1ULL << 57) ? 57 : (((int)((1UL) << 12))) & (1ULL << 56) ? 56 : (((int)((1UL) << 12))) & (1ULL << 55) ? 55 : (((int)((1UL) << 12))) & (1ULL << 54) ? 54 : (((int)((1UL) << 12))) & (1ULL << 53) ? 53 : (((int)((1UL) << 12))) & (1ULL << 52) ? 52 : (((int)((1UL) << 12))) & (1ULL << 51) ? 51 : (((int)((1UL) << 12))) & (1ULL << 50) ? 50 : (((int)((1UL) << 12))) & (1ULL << 49) ? 49 : (((int)((1UL) << 12))) & (1ULL << 48) ? 48 : (((int)((1UL) << 12))) & (1ULL << 47) ? 47 : (((int)((1UL) << 12))) & (1ULL << 46) ? 46 : (((int)((1UL) << 12))) & (1ULL << 45) ? 45 : (((int)((1UL) << 12))) & (1ULL << 44) ? 44 : (((int)((1UL) << 12))) & (1ULL << 43) ? 43 : (((int)((1UL) << 12))) & (1ULL << 42) ? 42 : (((int)((1UL) << 12))) & (1ULL << 41) ? 41 : (((int)((1UL) << 12))) & (1ULL << 40) ? 40 : (((int)((1UL) << 12))) & (1ULL << 39) ? 39 : (((int)((1UL) << 12))) & (1ULL << 38) ? 38 : (((int)((1UL) << 12))) & (1ULL << 37) ? 37 : (((int)((1UL) << 12))) & (1ULL << 36) ? 36 : (((int)((1UL) << 12))) & (1ULL << 35) ? 35 : (((int)((1UL) << 12))) & (1ULL << 34) ? 34 : (((int)((1UL) << 12))) & (1ULL << 33) ? 33 : (((int)((1UL) << 12))) & (1ULL << 32) ? 32 : (((int)((1UL) << 12))) & (1ULL << 31) ? 31 : (((int)((1UL) << 12))) & (1ULL << 30) ? 30 : (((int)((1UL) << 12))) & (1ULL << 29) ? 29 : (((int)((1UL) << 12))) & (1ULL << 28) ? 28 : (((int)((1UL) << 12))) & (1ULL << 27) ? 27 : (((int)((1UL) << 12))) & (1ULL << 26) ? 26 : (((int)((1UL) << 12))) & (1ULL << 25) ? 25 : (((int)((1UL) << 12))) & (1ULL << 24) ? 24 : (((int)((1UL) << 12))) & (1ULL << 23) ? 23 : (((int)((1UL) << 12))) & (1ULL << 22) ? 22 : (((int)((1UL) << 12))) & (1ULL << 21) ? 21 : (((int)((1UL) << 12))) & (1ULL << 20) ? 20 : (((int)((1UL) << 12))) & (1ULL << 19) ? 19 : (((int)((1UL) << 12))) & (1ULL << 18) ? 18 : (((int)((1UL) << 12))) & (1ULL << 17) ? 17 : (((int)((1UL) << 12))) & (1ULL << 16) ? 16 : (((int)((1UL) << 12))) & (1ULL << 15) ? 15 : (((int)((1UL) << 12))) & (1ULL << 14) ? 14 : (((int)((1UL) << 12))) & (1ULL << 13) ? 13 : (((int)((1UL) << 12))) & (1ULL << 12) ? 12 : (((int)((1UL) << 12))) & (1ULL << 11) ? 11 : (((int)((1UL) << 12))) & (1ULL << 10) ? 10 : (((int)((1UL) << 12))) & (1ULL << 9) ? 9 : (((int)((1UL) << 12))) & (1ULL << 8) ? 8 : (((int)((1UL) << 12))) & (1ULL << 7) ? 7 : (((int)((1UL) << 12))) & (1ULL << 6) ? 6 : (((int)((1UL) << 12))) & (1ULL << 5) ? 5 : (((int)((1UL) << 12))) & (1ULL << 4) ? 4 : (((int)((1UL) << 12))) & (1ULL << 3) ? 3 : (((int)((1UL) << 12))) & (1ULL << 2) ? 2 : (((int)((1UL) << 12))) & (1ULL << 1) ? 1 : (((int)((1UL) << 12))) & (1ULL << 0) ? 0 : Model1_____ilog2_NaN() ) : (sizeof(((int)((1UL) << 12))) <= 4) ? Model1___ilog2_u32(((int)((1UL) << 12))) : Model1___ilog2_u64(((int)((1UL) << 12))) );
}

static inline __attribute__((no_instrument_function)) bool Model1_sk_has_account(struct Model1_sock *Model1_sk)
{
 /* return true if protocol supports memory accounting */
 return !!Model1_sk->Model1___sk_common.Model1_skc_prot->Model1_memory_allocated;
}

static inline __attribute__((no_instrument_function)) bool Model1_sk_wmem_schedule(struct Model1_sock *Model1_sk, int Model1_size)
{
 if (!Model1_sk_has_account(Model1_sk))
  return true;
 return Model1_size <= Model1_sk->Model1_sk_forward_alloc ||
  Model1___sk_mem_schedule(Model1_sk, Model1_size, 0);
}

static inline __attribute__((no_instrument_function)) bool
Model1_sk_rmem_schedule(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb, int Model1_size)
{
 if (!Model1_sk_has_account(Model1_sk))
  return true;
 return Model1_size<= Model1_sk->Model1_sk_forward_alloc ||
  Model1___sk_mem_schedule(Model1_sk, Model1_size, 1) ||
  Model1_skb_pfmemalloc(Model1_skb);
}

static inline __attribute__((no_instrument_function)) void Model1_sk_mem_reclaim(struct Model1_sock *Model1_sk)
{
 if (!Model1_sk_has_account(Model1_sk))
  return;
 if (Model1_sk->Model1_sk_forward_alloc >= ((int)((1UL) << 12)))
  Model1___sk_mem_reclaim(Model1_sk, Model1_sk->Model1_sk_forward_alloc);
}

static inline __attribute__((no_instrument_function)) void Model1_sk_mem_reclaim_partial(struct Model1_sock *Model1_sk)
{
 if (!Model1_sk_has_account(Model1_sk))
  return;
 if (Model1_sk->Model1_sk_forward_alloc > ((int)((1UL) << 12)))
  Model1___sk_mem_reclaim(Model1_sk, Model1_sk->Model1_sk_forward_alloc - 1);
}

static inline __attribute__((no_instrument_function)) void Model1_sk_mem_charge(struct Model1_sock *Model1_sk, int Model1_size)
{
 if (!Model1_sk_has_account(Model1_sk))
  return;
 Model1_sk->Model1_sk_forward_alloc -= Model1_size;
}

static inline __attribute__((no_instrument_function)) void Model1_sk_mem_uncharge(struct Model1_sock *Model1_sk, int Model1_size)
{
 if (!Model1_sk_has_account(Model1_sk))
  return;
 Model1_sk->Model1_sk_forward_alloc += Model1_size;

 /* Avoid a possible overflow.
	 * TCP send queues can make this happen, if sk_mem_reclaim()
	 * is not called and more than 2 GBytes are released at once.
	 *
	 * If we reach 2 MBytes, reclaim 1 MBytes right now, there is
	 * no need to hold that much forward allocation anyway.
	 */
 if (__builtin_expect(!!(Model1_sk->Model1_sk_forward_alloc >= 1 << 21), 0))
  Model1___sk_mem_reclaim(Model1_sk, 1 << 20);
}

static inline __attribute__((no_instrument_function)) void Model1_sk_wmem_free_skb(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb)
{
 Model1_sock_set_flag(Model1_sk, Model1_SOCK_QUEUE_SHRUNK);
 Model1_sk->Model1_sk_wmem_queued -= Model1_skb->Model1_truesize;
 Model1_sk_mem_uncharge(Model1_sk, Model1_skb->Model1_truesize);
 Model1___kfree_skb(Model1_skb);
}

static inline __attribute__((no_instrument_function)) void Model1_sock_release_ownership(struct Model1_sock *Model1_sk)
{
 if (Model1_sk->Model1_sk_lock.Model1_owned) {
  Model1_sk->Model1_sk_lock.Model1_owned = 0;

  /* The sk_lock has mutex_unlock() semantics: */
  do { } while (0);
 }
}

/*
 * Macro so as to not evaluate some arguments when
 * lockdep is not enabled.
 *
 * Mark both the sk_lock and the sk_lock.slock as a
 * per-address-family lock class.
 */
void Model1_lock_sock_nested(struct Model1_sock *Model1_sk, int Model1_subclass);

static inline __attribute__((no_instrument_function)) void Model1_lock_sock(struct Model1_sock *Model1_sk)
{
 Model1_lock_sock_nested(Model1_sk, 0);
}

void Model1_release_sock(struct Model1_sock *Model1_sk);

/* BH context may only use the following locking interface. */






bool Model1_lock_sock_fast(struct Model1_sock *Model1_sk);
/**
 * unlock_sock_fast - complement of lock_sock_fast
 * @sk: socket
 * @slow: slow mode
 *
 * fast unlock socket for user context.
 * If slow mode is on, we call regular release_sock()
 */
static inline __attribute__((no_instrument_function)) void Model1_unlock_sock_fast(struct Model1_sock *Model1_sk, bool Model1_slow)
{
 if (Model1_slow)
  Model1_release_sock(Model1_sk);
 else
  Model1_spin_unlock_bh(&Model1_sk->Model1_sk_lock.Model1_slock);
}

/* Used by processes to "lock" a socket state, so that
 * interrupts and bottom half handlers won't change it
 * from under us. It essentially blocks any incoming
 * packets, so that we won't get any new data or any
 * packets that change the state of the socket.
 *
 * While locked, BH processing will add new packets to
 * the backlog queue.  This queue is processed by the
 * owner of the socket lock right before it is released.
 *
 * Since ~2.3.5 it is also exclusive sleep lock serializing
 * accesses from user process context.
 */

static inline __attribute__((no_instrument_function)) void Model1_sock_owned_by_me(const struct Model1_sock *Model1_sk)
{



}

static inline __attribute__((no_instrument_function)) bool Model1_sock_owned_by_user(const struct Model1_sock *Model1_sk)
{
 Model1_sock_owned_by_me(Model1_sk);
 return Model1_sk->Model1_sk_lock.Model1_owned;
}

/* no reclassification while locks are held */
static inline __attribute__((no_instrument_function)) bool Model1_sock_allow_reclassification(const struct Model1_sock *Model1_csk)
{
 struct Model1_sock *Model1_sk = (struct Model1_sock *)Model1_csk;

 return !Model1_sk->Model1_sk_lock.Model1_owned && !Model1_spin_is_locked(&Model1_sk->Model1_sk_lock.Model1_slock);
}

struct Model1_sock *Model1_sk_alloc(struct Model1_net *Model1_net, int Model1_family, Model1_gfp_t Model1_priority,
        struct Model1_proto *Model1_prot, int Model1_kern);
void Model1_sk_free(struct Model1_sock *Model1_sk);
void Model1_sk_destruct(struct Model1_sock *Model1_sk);
struct Model1_sock *Model1_sk_clone_lock(const struct Model1_sock *Model1_sk, const Model1_gfp_t Model1_priority);

struct Model1_sk_buff *Model1_sock_wmalloc(struct Model1_sock *Model1_sk, unsigned long Model1_size, int Model1_force,
        Model1_gfp_t Model1_priority);
void Model1___sock_wfree(struct Model1_sk_buff *Model1_skb);
void Model1_sock_wfree(struct Model1_sk_buff *Model1_skb);
void Model1_skb_orphan_partial(struct Model1_sk_buff *Model1_skb);
void Model1_sock_rfree(struct Model1_sk_buff *Model1_skb);
void Model1_sock_efree(struct Model1_sk_buff *Model1_skb);

void Model1_sock_edemux(struct Model1_sk_buff *Model1_skb);




int Model1_sock_setsockopt(struct Model1_socket *Model1_sock, int Model1_level, int Model1_op,
      char *Model1_optval, unsigned int Model1_optlen);

int Model1_sock_getsockopt(struct Model1_socket *Model1_sock, int Model1_level, int Model1_op,
      char *Model1_optval, int *Model1_optlen);
struct Model1_sk_buff *Model1_sock_alloc_send_skb(struct Model1_sock *Model1_sk, unsigned long Model1_size,
        int Model1_noblock, int *Model1_errcode);
struct Model1_sk_buff *Model1_sock_alloc_send_pskb(struct Model1_sock *Model1_sk, unsigned long Model1_header_len,
         unsigned long Model1_data_len, int Model1_noblock,
         int *Model1_errcode, int Model1_max_page_order);
void *Model1_sock_kmalloc(struct Model1_sock *Model1_sk, int Model1_size, Model1_gfp_t Model1_priority);
void Model1_sock_kfree_s(struct Model1_sock *Model1_sk, void *Model1_mem, int Model1_size);
void Model1_sock_kzfree_s(struct Model1_sock *Model1_sk, void *Model1_mem, int Model1_size);
void Model1_sk_send_sigurg(struct Model1_sock *Model1_sk);

struct Model1_sockcm_cookie {
 Model1_u32 Model1_mark;
 Model1_u16 Model1_tsflags;
};

int Model1___sock_cmsg_send(struct Model1_sock *Model1_sk, struct Model1_msghdr *Model1_msg, struct Model1_cmsghdr *Model1_cmsg,
       struct Model1_sockcm_cookie *Model1_sockc);
int Model1_sock_cmsg_send(struct Model1_sock *Model1_sk, struct Model1_msghdr *Model1_msg,
     struct Model1_sockcm_cookie *Model1_sockc);

/*
 * Functions to fill in entries in struct proto_ops when a protocol
 * does not implement a particular function.
 */
int Model1_sock_no_bind(struct Model1_socket *, struct Model1_sockaddr *, int);
int Model1_sock_no_connect(struct Model1_socket *, struct Model1_sockaddr *, int, int);
int Model1_sock_no_socketpair(struct Model1_socket *, struct Model1_socket *);
int Model1_sock_no_accept(struct Model1_socket *, struct Model1_socket *, int);
int Model1_sock_no_getname(struct Model1_socket *, struct Model1_sockaddr *, int *, int);
unsigned int Model1_sock_no_poll(struct Model1_file *, struct Model1_socket *,
     struct Model1_poll_table_struct *);
int Model1_sock_no_ioctl(struct Model1_socket *, unsigned int, unsigned long);
int Model1_sock_no_listen(struct Model1_socket *, int);
int Model1_sock_no_shutdown(struct Model1_socket *, int);
int Model1_sock_no_getsockopt(struct Model1_socket *, int , int, char *, int *);
int Model1_sock_no_setsockopt(struct Model1_socket *, int, int, char *, unsigned int);
int Model1_sock_no_sendmsg(struct Model1_socket *, struct Model1_msghdr *, Model1_size_t);
int Model1_sock_no_recvmsg(struct Model1_socket *, struct Model1_msghdr *, Model1_size_t, int);
int Model1_sock_no_mmap(struct Model1_file *Model1_file, struct Model1_socket *Model1_sock,
   struct Model1_vm_area_struct *Model1_vma);
Model1_ssize_t Model1_sock_no_sendpage(struct Model1_socket *Model1_sock, struct Model1_page *Model1_page, int Model1_offset,
    Model1_size_t Model1_size, int Model1_flags);

/*
 * Functions to fill in entries in struct proto_ops when a protocol
 * uses the inet style.
 */
int Model1_sock_common_getsockopt(struct Model1_socket *Model1_sock, int Model1_level, int Model1_optname,
      char *Model1_optval, int *Model1_optlen);
int Model1_sock_common_recvmsg(struct Model1_socket *Model1_sock, struct Model1_msghdr *Model1_msg, Model1_size_t Model1_size,
   int Model1_flags);
int Model1_sock_common_setsockopt(struct Model1_socket *Model1_sock, int Model1_level, int Model1_optname,
      char *Model1_optval, unsigned int Model1_optlen);
int Model1_compat_sock_common_getsockopt(struct Model1_socket *Model1_sock, int Model1_level,
  int Model1_optname, char *Model1_optval, int *Model1_optlen);
int Model1_compat_sock_common_setsockopt(struct Model1_socket *Model1_sock, int Model1_level,
  int Model1_optname, char *Model1_optval, unsigned int Model1_optlen);

void Model1_sk_common_release(struct Model1_sock *Model1_sk);

/*
 *	Default socket callbacks and setup code
 */

/* Initialise core socket variables */
void Model1_sock_init_data(struct Model1_socket *Model1_sock, struct Model1_sock *Model1_sk);

/*
 * Socket reference counting postulates.
 *
 * * Each user of socket SHOULD hold a reference count.
 * * Each access point to socket (an hash table bucket, reference from a list,
 *   running timer, skb in flight MUST hold a reference count.
 * * When reference count hits 0, it means it will never increase back.
 * * When reference count hits 0, it means that no references from
 *   outside exist to this socket and current process on current CPU
 *   is last user and may/should destroy this socket.
 * * sk_free is called from any context: process, BH, IRQ. When
 *   it is called, socket has no references from outside -> sk_free
 *   may release descendant resources allocated by the socket, but
 *   to the time when it is called, socket is NOT referenced by any
 *   hash tables, lists etc.
 * * Packets, delivered from outside (from network or from another process)
 *   and enqueued on receive/error queues SHOULD NOT grab reference count,
 *   when they sit in queue. Otherwise, packets will leak to hole, when
 *   socket is looked up by one cpu and unhasing is made by another CPU.
 *   It is true for udp/raw, netlink (leak to receive and error queues), tcp
 *   (leak to backlog). Packet socket does all the processing inside
 *   BR_NETPROTO_LOCK, so that it has not this race condition. UNIX sockets
 *   use separate SMP lock, so that they are prone too.
 */

/* Ungrab socket and destroy it, if it was the last reference. */
static inline __attribute__((no_instrument_function)) void Model1_sock_put(struct Model1_sock *Model1_sk)
{
 if (Model1_atomic_dec_and_test(&Model1_sk->Model1___sk_common.Model1_skc_refcnt))
  Model1_sk_free(Model1_sk);
}
/* Generic version of sock_put(), dealing with all sockets
 * (TCP_TIMEWAIT, TCP_NEW_SYN_RECV, ESTABLISHED...)
 */
void Model1_sock_gen_put(struct Model1_sock *Model1_sk);

int Model1___sk_receive_skb(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb, const int Model1_nested,
       unsigned int Model1_trim_cap);
static inline __attribute__((no_instrument_function)) int Model1_sk_receive_skb(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb,
     const int Model1_nested)
{
 return Model1___sk_receive_skb(Model1_sk, Model1_skb, Model1_nested, 1);
}

static inline __attribute__((no_instrument_function)) void Model1_sk_tx_queue_set(struct Model1_sock *Model1_sk, int Model1_tx_queue)
{
 Model1_sk->Model1___sk_common.Model1_skc_tx_queue_mapping = Model1_tx_queue;
}

static inline __attribute__((no_instrument_function)) void Model1_sk_tx_queue_clear(struct Model1_sock *Model1_sk)
{
 Model1_sk->Model1___sk_common.Model1_skc_tx_queue_mapping = -1;
}

static inline __attribute__((no_instrument_function)) int Model1_sk_tx_queue_get(const struct Model1_sock *Model1_sk)
{
 return Model1_sk ? Model1_sk->Model1___sk_common.Model1_skc_tx_queue_mapping : -1;
}

static inline __attribute__((no_instrument_function)) void Model1_sk_set_socket(struct Model1_sock *Model1_sk, struct Model1_socket *Model1_sock)
{
 Model1_sk_tx_queue_clear(Model1_sk);
 Model1_sk->Model1_sk_socket = Model1_sock;
}

static inline __attribute__((no_instrument_function)) Model1_wait_queue_head_t *Model1_sk_sleep(struct Model1_sock *Model1_sk)
{
 do { bool Model1___cond = !(!(__builtin_offsetof(struct Model1_socket_wq, Model1_wait) != 0)); extern void Model1___compiletime_assert_1620(void) ; if (Model1___cond) Model1___compiletime_assert_1620(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
 return &({ typeof(Model1_sk->Model1_sk_wq) Model1_________p1 = ({ typeof(Model1_sk->Model1_sk_wq) Model1__________p1 = ({ union { typeof(Model1_sk->Model1_sk_wq) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&(Model1_sk->Model1_sk_wq), Model1___u.Model1___c, sizeof(Model1_sk->Model1_sk_wq)); else Model1___read_once_size_nocheck(&(Model1_sk->Model1_sk_wq), Model1___u.Model1___c, sizeof(Model1_sk->Model1_sk_wq)); Model1___u.Model1___val; }); typeof(*(Model1_sk->Model1_sk_wq)) *Model1____typecheck_p __attribute__((unused)); do { } while (0); (Model1__________p1); }); ((typeof(*Model1_sk->Model1_sk_wq) *)(Model1_________p1)); })->Model1_wait;
}
/* Detach socket from process context.
 * Announce socket dead, detach it from wait queue and inode.
 * Note that parent inode held reference count on this struct sock,
 * we do not release it in this function, because protocol
 * probably wants some additional cleanups or even continuing
 * to work with this socket (TCP).
 */
static inline __attribute__((no_instrument_function)) void Model1_sock_orphan(struct Model1_sock *Model1_sk)
{
 Model1__raw_write_lock_bh(&Model1_sk->Model1_sk_callback_lock);
 Model1_sock_set_flag(Model1_sk, Model1_SOCK_DEAD);
 Model1_sk_set_socket(Model1_sk, ((void *)0));
 Model1_sk->Model1_sk_wq = ((void *)0);
 Model1__raw_write_unlock_bh(&Model1_sk->Model1_sk_callback_lock);
}

static inline __attribute__((no_instrument_function)) void Model1_sock_graft(struct Model1_sock *Model1_sk, struct Model1_socket *Model1_parent)
{
 Model1__raw_write_lock_bh(&Model1_sk->Model1_sk_callback_lock);
 Model1_sk->Model1_sk_wq = Model1_parent->Model1_wq;
 Model1_parent->Model1_sk = Model1_sk;
 Model1_sk_set_socket(Model1_sk, Model1_parent);
 Model1_security_sock_graft(Model1_sk, Model1_parent);
 Model1__raw_write_unlock_bh(&Model1_sk->Model1_sk_callback_lock);
}

Model1_kuid_t Model1_sock_i_uid(struct Model1_sock *Model1_sk);
unsigned long Model1_sock_i_ino(struct Model1_sock *Model1_sk);

static inline __attribute__((no_instrument_function)) Model1_u32 Model1_net_tx_rndhash(void)
{
 Model1_u32 Model1_v = Model1_prandom_u32();

 return Model1_v ?: 1;
}

static inline __attribute__((no_instrument_function)) void Model1_sk_set_txhash(struct Model1_sock *Model1_sk)
{
 Model1_sk->Model1_sk_txhash = Model1_net_tx_rndhash();
}

static inline __attribute__((no_instrument_function)) void Model1_sk_rethink_txhash(struct Model1_sock *Model1_sk)
{
 if (Model1_sk->Model1_sk_txhash)
  Model1_sk_set_txhash(Model1_sk);
}

static inline __attribute__((no_instrument_function)) struct Model1_dst_entry *
Model1___sk_dst_get(struct Model1_sock *Model1_sk)
{
 return ({ typeof(*(Model1_sk->Model1_sk_dst_cache)) *Model1_________p1 = (typeof(*(Model1_sk->Model1_sk_dst_cache)) *)({ typeof((Model1_sk->Model1_sk_dst_cache)) Model1__________p1 = ({ union { typeof((Model1_sk->Model1_sk_dst_cache)) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&((Model1_sk->Model1_sk_dst_cache)), Model1___u.Model1___c, sizeof((Model1_sk->Model1_sk_dst_cache))); else Model1___read_once_size_nocheck(&((Model1_sk->Model1_sk_dst_cache)), Model1___u.Model1___c, sizeof((Model1_sk->Model1_sk_dst_cache))); Model1___u.Model1___val; }); typeof(*((Model1_sk->Model1_sk_dst_cache))) *Model1____typecheck_p __attribute__((unused)); do { } while (0); (Model1__________p1); }); do { } while (0); ; ((typeof(*(Model1_sk->Model1_sk_dst_cache)) *)(Model1_________p1)); });

}

static inline __attribute__((no_instrument_function)) struct Model1_dst_entry *
Model1_sk_dst_get(struct Model1_sock *Model1_sk)
{
 struct Model1_dst_entry *Model1_dst;

 Model1_rcu_read_lock();
 Model1_dst = ({ typeof(*(Model1_sk->Model1_sk_dst_cache)) *Model1_________p1 = (typeof(*(Model1_sk->Model1_sk_dst_cache)) *)({ typeof((Model1_sk->Model1_sk_dst_cache)) Model1__________p1 = ({ union { typeof((Model1_sk->Model1_sk_dst_cache)) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&((Model1_sk->Model1_sk_dst_cache)), Model1___u.Model1___c, sizeof((Model1_sk->Model1_sk_dst_cache))); else Model1___read_once_size_nocheck(&((Model1_sk->Model1_sk_dst_cache)), Model1___u.Model1___c, sizeof((Model1_sk->Model1_sk_dst_cache))); Model1___u.Model1___val; }); typeof(*((Model1_sk->Model1_sk_dst_cache))) *Model1____typecheck_p __attribute__((unused)); do { } while (0); (Model1__________p1); }); do { } while (0); ; ((typeof(*(Model1_sk->Model1_sk_dst_cache)) *)(Model1_________p1)); });
 if (Model1_dst && !Model1_atomic_add_unless((&Model1_dst->Model1___refcnt), 1, 0))
  Model1_dst = ((void *)0);
 Model1_rcu_read_unlock();
 return Model1_dst;
}

static inline __attribute__((no_instrument_function)) void Model1_dst_negative_advice(struct Model1_sock *Model1_sk)
{
 struct Model1_dst_entry *Model1_ndst, *Model1_dst = Model1___sk_dst_get(Model1_sk);

 Model1_sk_rethink_txhash(Model1_sk);

 if (Model1_dst && Model1_dst->Model1_ops->Model1_negative_advice) {
  Model1_ndst = Model1_dst->Model1_ops->Model1_negative_advice(Model1_dst);

  if (Model1_ndst != Model1_dst) {
   ({ Model1_uintptr_t Model1__r_a_p__v = (Model1_uintptr_t)(Model1_ndst); if (__builtin_constant_p(Model1_ndst) && (Model1__r_a_p__v) == (Model1_uintptr_t)((void *)0)) ({ union { typeof((Model1_sk->Model1_sk_dst_cache)) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof((Model1_sk->Model1_sk_dst_cache))) ((typeof(Model1_sk->Model1_sk_dst_cache))(Model1__r_a_p__v)) }; Model1___write_once_size(&((Model1_sk->Model1_sk_dst_cache)), Model1___u.Model1___c, sizeof((Model1_sk->Model1_sk_dst_cache))); Model1___u.Model1___val; }); else do { do { bool Model1___cond = !((sizeof(*&Model1_sk->Model1_sk_dst_cache) == sizeof(char) || sizeof(*&Model1_sk->Model1_sk_dst_cache) == sizeof(short) || sizeof(*&Model1_sk->Model1_sk_dst_cache) == sizeof(int) || sizeof(*&Model1_sk->Model1_sk_dst_cache) == sizeof(long))); extern void Model1___compiletime_assert_1700(void) ; if (Model1___cond) Model1___compiletime_assert_1700(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&Model1_sk->Model1_sk_dst_cache) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(*&Model1_sk->Model1_sk_dst_cache)) ((typeof(*((typeof(Model1_sk->Model1_sk_dst_cache))Model1__r_a_p__v)) *)((typeof(Model1_sk->Model1_sk_dst_cache))Model1__r_a_p__v)) }; Model1___write_once_size(&(*&Model1_sk->Model1_sk_dst_cache), Model1___u.Model1___c, sizeof(*&Model1_sk->Model1_sk_dst_cache)); Model1___u.Model1___val; }); } while (0); Model1__r_a_p__v; });
   Model1_sk_tx_queue_clear(Model1_sk);
  }
 }
}

static inline __attribute__((no_instrument_function)) void
Model1___sk_dst_set(struct Model1_sock *Model1_sk, struct Model1_dst_entry *Model1_dst)
{
 struct Model1_dst_entry *Model1_old_dst;

 Model1_sk_tx_queue_clear(Model1_sk);
 /*
	 * This can be called while sk is owned by the caller only,
	 * with no state that can be checked in a rcu_dereference_check() cond
	 */
 Model1_old_dst = ({ typeof(Model1_sk->Model1_sk_dst_cache) Model1_________p1 = ({ typeof(Model1_sk->Model1_sk_dst_cache) Model1__________p1 = ({ union { typeof(Model1_sk->Model1_sk_dst_cache) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&(Model1_sk->Model1_sk_dst_cache), Model1___u.Model1___c, sizeof(Model1_sk->Model1_sk_dst_cache)); else Model1___read_once_size_nocheck(&(Model1_sk->Model1_sk_dst_cache), Model1___u.Model1___c, sizeof(Model1_sk->Model1_sk_dst_cache)); Model1___u.Model1___val; }); typeof(*(Model1_sk->Model1_sk_dst_cache)) *Model1____typecheck_p __attribute__((unused)); do { } while (0); (Model1__________p1); }); ((typeof(*Model1_sk->Model1_sk_dst_cache) *)(Model1_________p1)); });
 ({ Model1_uintptr_t Model1__r_a_p__v = (Model1_uintptr_t)(Model1_dst); if (__builtin_constant_p(Model1_dst) && (Model1__r_a_p__v) == (Model1_uintptr_t)((void *)0)) ({ union { typeof((Model1_sk->Model1_sk_dst_cache)) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof((Model1_sk->Model1_sk_dst_cache))) ((typeof(Model1_sk->Model1_sk_dst_cache))(Model1__r_a_p__v)) }; Model1___write_once_size(&((Model1_sk->Model1_sk_dst_cache)), Model1___u.Model1___c, sizeof((Model1_sk->Model1_sk_dst_cache))); Model1___u.Model1___val; }); else do { do { bool Model1___cond = !((sizeof(*&Model1_sk->Model1_sk_dst_cache) == sizeof(char) || sizeof(*&Model1_sk->Model1_sk_dst_cache) == sizeof(short) || sizeof(*&Model1_sk->Model1_sk_dst_cache) == sizeof(int) || sizeof(*&Model1_sk->Model1_sk_dst_cache) == sizeof(long))); extern void Model1___compiletime_assert_1717(void) ; if (Model1___cond) Model1___compiletime_assert_1717(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&Model1_sk->Model1_sk_dst_cache) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(*&Model1_sk->Model1_sk_dst_cache)) ((typeof(*((typeof(Model1_sk->Model1_sk_dst_cache))Model1__r_a_p__v)) *)((typeof(Model1_sk->Model1_sk_dst_cache))Model1__r_a_p__v)) }; Model1___write_once_size(&(*&Model1_sk->Model1_sk_dst_cache), Model1___u.Model1___c, sizeof(*&Model1_sk->Model1_sk_dst_cache)); Model1___u.Model1___val; }); } while (0); Model1__r_a_p__v; });
 Model1_dst_release(Model1_old_dst);
}

static inline __attribute__((no_instrument_function)) void
Model1_sk_dst_set(struct Model1_sock *Model1_sk, struct Model1_dst_entry *Model1_dst)
{
 struct Model1_dst_entry *Model1_old_dst;

 Model1_sk_tx_queue_clear(Model1_sk);
 Model1_old_dst = ({ __typeof__ (*((( struct Model1_dst_entry **)&Model1_sk->Model1_sk_dst_cache))) Model1___ret = ((Model1_dst)); switch (sizeof(*((( struct Model1_dst_entry **)&Model1_sk->Model1_sk_dst_cache)))) { case 1: asm volatile ("" "xchg" "b %b0, %1\n" : "+q" (Model1___ret), "+m" (*((( struct Model1_dst_entry **)&Model1_sk->Model1_sk_dst_cache))) : : "memory", "cc"); break; case 2: asm volatile ("" "xchg" "w %w0, %1\n" : "+r" (Model1___ret), "+m" (*((( struct Model1_dst_entry **)&Model1_sk->Model1_sk_dst_cache))) : : "memory", "cc"); break; case 4: asm volatile ("" "xchg" "l %0, %1\n" : "+r" (Model1___ret), "+m" (*((( struct Model1_dst_entry **)&Model1_sk->Model1_sk_dst_cache))) : : "memory", "cc"); break; case 8: asm volatile ("" "xchg" "q %q0, %1\n" : "+r" (Model1___ret), "+m" (*((( struct Model1_dst_entry **)&Model1_sk->Model1_sk_dst_cache))) : : "memory", "cc"); break; default: Model1___xchg_wrong_size(); } Model1___ret; });
 Model1_dst_release(Model1_old_dst);
}

static inline __attribute__((no_instrument_function)) void
Model1___sk_dst_reset(struct Model1_sock *Model1_sk)
{
 Model1___sk_dst_set(Model1_sk, ((void *)0));
}

static inline __attribute__((no_instrument_function)) void
Model1_sk_dst_reset(struct Model1_sock *Model1_sk)
{
 Model1_sk_dst_set(Model1_sk, ((void *)0));
}

struct Model1_dst_entry *Model1___sk_dst_check(struct Model1_sock *Model1_sk, Model1_u32 Model1_cookie);

struct Model1_dst_entry *Model1_sk_dst_check(struct Model1_sock *Model1_sk, Model1_u32 Model1_cookie);

bool Model1_sk_mc_loop(struct Model1_sock *Model1_sk);

static inline __attribute__((no_instrument_function)) bool Model1_sk_can_gso(const struct Model1_sock *Model1_sk)
{
 return Model1_net_gso_ok(Model1_sk->Model1_sk_route_caps, Model1_sk->Model1_sk_gso_type);
}

void Model1_sk_setup_caps(struct Model1_sock *Model1_sk, struct Model1_dst_entry *Model1_dst);

static inline __attribute__((no_instrument_function)) void Model1_sk_nocaps_add(struct Model1_sock *Model1_sk, Model1_netdev_features_t Model1_flags)
{
 Model1_sk->Model1_sk_route_nocaps |= Model1_flags;
 Model1_sk->Model1_sk_route_caps &= ~Model1_flags;
}

static inline __attribute__((no_instrument_function)) bool Model1_sk_check_csum_caps(struct Model1_sock *Model1_sk)
{
 return (Model1_sk->Model1_sk_route_caps & ((Model1_netdev_features_t)1 << (Model1_NETIF_F_HW_CSUM_BIT))) ||
        (Model1_sk->Model1___sk_common.Model1_skc_family == 2 &&
  (Model1_sk->Model1_sk_route_caps & ((Model1_netdev_features_t)1 << (Model1_NETIF_F_IP_CSUM_BIT)))) ||
        (Model1_sk->Model1___sk_common.Model1_skc_family == 10 &&
  (Model1_sk->Model1_sk_route_caps & ((Model1_netdev_features_t)1 << (Model1_NETIF_F_IPV6_CSUM_BIT))));
}

static inline __attribute__((no_instrument_function)) int Model1_skb_do_copy_data_nocache(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb,
        struct Model1_iov_iter *Model1_from, char *Model1_to,
        int Model1_copy, int Model1_offset)
{
 if (Model1_skb->Model1_ip_summed == 0) {
  Model1___wsum Model1_csum = 0;
  if (Model1_csum_and_copy_from_iter(Model1_to, Model1_copy, &Model1_csum, Model1_from) != Model1_copy)
   return -14;
  Model1_skb->Model1_csum = Model1_csum_block_add(Model1_skb->Model1_csum, Model1_csum, Model1_offset);
 } else if (Model1_sk->Model1_sk_route_caps & ((Model1_netdev_features_t)1 << (Model1_NETIF_F_NOCACHE_COPY_BIT))) {
  if (Model1_copy_from_iter_nocache(Model1_to, Model1_copy, Model1_from) != Model1_copy)
   return -14;
 } else if (Model1_copy_from_iter(Model1_to, Model1_copy, Model1_from) != Model1_copy)
  return -14;

 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_skb_add_data_nocache(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb,
           struct Model1_iov_iter *Model1_from, int Model1_copy)
{
 int err, Model1_offset = Model1_skb->Model1_len;

 err = Model1_skb_do_copy_data_nocache(Model1_sk, Model1_skb, Model1_from, Model1_skb_put(Model1_skb, Model1_copy),
           Model1_copy, Model1_offset);
 if (err)
  Model1___skb_trim(Model1_skb, Model1_offset);

 return err;
}

static inline __attribute__((no_instrument_function)) int Model1_skb_copy_to_page_nocache(struct Model1_sock *Model1_sk, struct Model1_iov_iter *Model1_from,
        struct Model1_sk_buff *Model1_skb,
        struct Model1_page *Model1_page,
        int Model1_off, int Model1_copy)
{
 int err;

 err = Model1_skb_do_copy_data_nocache(Model1_sk, Model1_skb, Model1_from, Model1_lowmem_page_address(Model1_page) + Model1_off,
           Model1_copy, Model1_skb->Model1_len);
 if (err)
  return err;

 Model1_skb->Model1_len += Model1_copy;
 Model1_skb->Model1_data_len += Model1_copy;
 Model1_skb->Model1_truesize += Model1_copy;
 Model1_sk->Model1_sk_wmem_queued += Model1_copy;
 Model1_sk_mem_charge(Model1_sk, Model1_copy);
 return 0;
}

/**
 * sk_wmem_alloc_get - returns write allocations
 * @sk: socket
 *
 * Returns sk_wmem_alloc minus initial offset of one
 */
static inline __attribute__((no_instrument_function)) int Model1_sk_wmem_alloc_get(const struct Model1_sock *Model1_sk)
{
 return Model1_atomic_read(&Model1_sk->Model1_sk_wmem_alloc) - 1;
}

/**
 * sk_rmem_alloc_get - returns read allocations
 * @sk: socket
 *
 * Returns sk_rmem_alloc
 */
static inline __attribute__((no_instrument_function)) int Model1_sk_rmem_alloc_get(const struct Model1_sock *Model1_sk)
{
 return Model1_atomic_read(&Model1_sk->Model1_sk_backlog.Model1_rmem_alloc);
}

/**
 * sk_has_allocations - check if allocations are outstanding
 * @sk: socket
 *
 * Returns true if socket has write or read allocations
 */
static inline __attribute__((no_instrument_function)) bool Model1_sk_has_allocations(const struct Model1_sock *Model1_sk)
{
 return Model1_sk_wmem_alloc_get(Model1_sk) || Model1_sk_rmem_alloc_get(Model1_sk);
}

/**
 * skwq_has_sleeper - check if there are any waiting processes
 * @wq: struct socket_wq
 *
 * Returns true if socket_wq has waiting processes
 *
 * The purpose of the skwq_has_sleeper and sock_poll_wait is to wrap the memory
 * barrier call. They were added due to the race found within the tcp code.
 *
 * Consider following tcp code paths:
 *
 * CPU1                  CPU2
 *
 * sys_select            receive packet
 *   ...                 ...
 *   __add_wait_queue    update tp->rcv_nxt
 *   ...                 ...
 *   tp->rcv_nxt check   sock_def_readable
 *   ...                 {
 *   schedule               rcu_read_lock();
 *                          wq = rcu_dereference(sk->sk_wq);
 *                          if (wq && waitqueue_active(&wq->wait))
 *                              wake_up_interruptible(&wq->wait)
 *                          ...
 *                       }
 *
 * The race for tcp fires when the __add_wait_queue changes done by CPU1 stay
 * in its cache, and so does the tp->rcv_nxt update on CPU2 side.  The CPU1
 * could then endup calling schedule and sleep forever if there are no more
 * data on the socket.
 *
 */
static inline __attribute__((no_instrument_function)) bool Model1_skwq_has_sleeper(struct Model1_socket_wq *Model1_wq)
{
 return Model1_wq && Model1_wq_has_sleeper(&Model1_wq->Model1_wait);
}

/**
 * sock_poll_wait - place memory barrier behind the poll_wait call.
 * @filp:           file
 * @wait_address:   socket wait queue
 * @p:              poll_table
 *
 * See the comments in the wq_has_sleeper function.
 */
static inline __attribute__((no_instrument_function)) void Model1_sock_poll_wait(struct Model1_file *Model1_filp,
  Model1_wait_queue_head_t *Model1_wait_address, Model1_poll_table *Model1_p)
{
 if (!Model1_poll_does_not_wait(Model1_p) && Model1_wait_address) {
  Model1_poll_wait(Model1_filp, Model1_wait_address, Model1_p);
  /* We need to be sure we are in sync with the
		 * socket flags modification.
		 *
		 * This memory barrier is paired in the wq_has_sleeper.
		 */
  asm volatile("mfence":::"memory");
 }
}

static inline __attribute__((no_instrument_function)) void Model1_skb_set_hash_from_sk(struct Model1_sk_buff *Model1_skb, struct Model1_sock *Model1_sk)
{
 if (Model1_sk->Model1_sk_txhash) {
  Model1_skb->Model1_l4_hash = 1;
  Model1_skb->Model1_hash = Model1_sk->Model1_sk_txhash;
 }
}

void Model1_skb_set_owner_w(struct Model1_sk_buff *Model1_skb, struct Model1_sock *Model1_sk);

/*
 *	Queue a received datagram if it will fit. Stream and sequenced
 *	protocols can't normally use this as they need to fit buffers in
 *	and play with them.
 *
 *	Inlined as it's very short and called for pretty much every
 *	packet ever received.
 */
static inline __attribute__((no_instrument_function)) void Model1_skb_set_owner_r(struct Model1_sk_buff *Model1_skb, struct Model1_sock *Model1_sk)
{
 Model1_skb_orphan(Model1_skb);
 Model1_skb->Model1_sk = Model1_sk;
 Model1_skb->Model1_destructor = Model1_sock_rfree;
 Model1_atomic_add(Model1_skb->Model1_truesize, &Model1_sk->Model1_sk_backlog.Model1_rmem_alloc);
 Model1_sk_mem_charge(Model1_sk, Model1_skb->Model1_truesize);
}

void Model1_sk_reset_timer(struct Model1_sock *Model1_sk, struct Model1_timer_list *Model1_timer,
      unsigned long Model1_expires);

void Model1_sk_stop_timer(struct Model1_sock *Model1_sk, struct Model1_timer_list *Model1_timer);

int Model1___sock_queue_rcv_skb(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);
int Model1_sock_queue_rcv_skb(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);

int Model1_sock_queue_err_skb(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);
struct Model1_sk_buff *Model1_sock_dequeue_err_skb(struct Model1_sock *Model1_sk);

/*
 *	Recover an error report and clear atomically
 */

static inline __attribute__((no_instrument_function)) int Model1_sock_error(struct Model1_sock *Model1_sk)
{
 int err;
 if (__builtin_expect(!!(!Model1_sk->Model1_sk_err), 1))
  return 0;
 err = ({ __typeof__ (*((&Model1_sk->Model1_sk_err))) Model1___ret = ((0)); switch (sizeof(*((&Model1_sk->Model1_sk_err)))) { case 1: asm volatile ("" "xchg" "b %b0, %1\n" : "+q" (Model1___ret), "+m" (*((&Model1_sk->Model1_sk_err))) : : "memory", "cc"); break; case 2: asm volatile ("" "xchg" "w %w0, %1\n" : "+r" (Model1___ret), "+m" (*((&Model1_sk->Model1_sk_err))) : : "memory", "cc"); break; case 4: asm volatile ("" "xchg" "l %0, %1\n" : "+r" (Model1___ret), "+m" (*((&Model1_sk->Model1_sk_err))) : : "memory", "cc"); break; case 8: asm volatile ("" "xchg" "q %q0, %1\n" : "+r" (Model1___ret), "+m" (*((&Model1_sk->Model1_sk_err))) : : "memory", "cc"); break; default: Model1___xchg_wrong_size(); } Model1___ret; });
 return -err;
}

static inline __attribute__((no_instrument_function)) unsigned long Model1_sock_wspace(struct Model1_sock *Model1_sk)
{
 int Model1_amt = 0;

 if (!(Model1_sk->Model1_sk_shutdown & 2)) {
  Model1_amt = Model1_sk->Model1_sk_sndbuf - Model1_atomic_read(&Model1_sk->Model1_sk_wmem_alloc);
  if (Model1_amt < 0)
   Model1_amt = 0;
 }
 return Model1_amt;
}

/* Note:
 *  We use sk->sk_wq_raw, from contexts knowing this
 *  pointer is not NULL and cannot disappear/change.
 */
static inline __attribute__((no_instrument_function)) void Model1_sk_set_bit(int Model1_nr, struct Model1_sock *Model1_sk)
{
 if ((Model1_nr == 0 || Model1_nr == 1) &&
     !Model1_sock_flag(Model1_sk, Model1_SOCK_FASYNC))
  return;

 Model1_set_bit(Model1_nr, &Model1_sk->Model1_sk_wq_raw->Model1_flags);
}

static inline __attribute__((no_instrument_function)) void Model1_sk_clear_bit(int Model1_nr, struct Model1_sock *Model1_sk)
{
 if ((Model1_nr == 0 || Model1_nr == 1) &&
     !Model1_sock_flag(Model1_sk, Model1_SOCK_FASYNC))
  return;

 Model1_clear_bit(Model1_nr, &Model1_sk->Model1_sk_wq_raw->Model1_flags);
}

static inline __attribute__((no_instrument_function)) void Model1_sk_wake_async(const struct Model1_sock *Model1_sk, int Model1_how, int Model1_band)
{
 if (Model1_sock_flag(Model1_sk, Model1_SOCK_FASYNC)) {
  Model1_rcu_read_lock();
  Model1_sock_wake_async(({ typeof(*(Model1_sk->Model1_sk_wq)) *Model1_________p1 = (typeof(*(Model1_sk->Model1_sk_wq)) *)({ typeof((Model1_sk->Model1_sk_wq)) Model1__________p1 = ({ union { typeof((Model1_sk->Model1_sk_wq)) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&((Model1_sk->Model1_sk_wq)), Model1___u.Model1___c, sizeof((Model1_sk->Model1_sk_wq))); else Model1___read_once_size_nocheck(&((Model1_sk->Model1_sk_wq)), Model1___u.Model1___c, sizeof((Model1_sk->Model1_sk_wq))); Model1___u.Model1___val; }); typeof(*((Model1_sk->Model1_sk_wq))) *Model1____typecheck_p __attribute__((unused)); do { } while (0); (Model1__________p1); }); do { } while (0); ; ((typeof(*(Model1_sk->Model1_sk_wq)) *)(Model1_________p1)); }), Model1_how, Model1_band);
  Model1_rcu_read_unlock();
 }
}

/* Since sk_{r,w}mem_alloc sums skb->truesize, even a small frame might
 * need sizeof(sk_buff) + MTU + padding, unless net driver perform copybreak.
 * Note: for send buffers, TCP works better if we can build two skbs at
 * minimum.
 */





static inline __attribute__((no_instrument_function)) void Model1_sk_stream_moderate_sndbuf(struct Model1_sock *Model1_sk)
{
 if (!(Model1_sk->Model1_sk_userlocks & 1)) {
  Model1_sk->Model1_sk_sndbuf = ({ typeof(Model1_sk->Model1_sk_sndbuf) Model1__min1 = (Model1_sk->Model1_sk_sndbuf); typeof(Model1_sk->Model1_sk_wmem_queued >> 1) Model1__min2 = (Model1_sk->Model1_sk_wmem_queued >> 1); (void) (&Model1__min1 == &Model1__min2); Model1__min1 < Model1__min2 ? Model1__min1 : Model1__min2; });
  Model1_sk->Model1_sk_sndbuf = ({ Model1_u32 Model1___max1 = (Model1_sk->Model1_sk_sndbuf); Model1_u32 Model1___max2 = (((2048 + ((((sizeof(struct Model1_sk_buff))) + ((typeof((sizeof(struct Model1_sk_buff))))(((1 << (6)))) - 1)) & ~((typeof((sizeof(struct Model1_sk_buff))))(((1 << (6)))) - 1))) * 2)); Model1___max1 > Model1___max2 ? Model1___max1: Model1___max2; });
 }
}

struct Model1_sk_buff *Model1_sk_stream_alloc_skb(struct Model1_sock *Model1_sk, int Model1_size, Model1_gfp_t Model1_gfp,
        bool Model1_force_schedule);

/**
 * sk_page_frag - return an appropriate page_frag
 * @sk: socket
 *
 * If socket allocation mode allows current thread to sleep, it means its
 * safe to use the per task page_frag instead of the per socket one.
 */
static inline __attribute__((no_instrument_function)) struct Model1_page_frag *Model1_sk_page_frag(struct Model1_sock *Model1_sk)
{
 if (Model1_gfpflags_allow_blocking(Model1_sk->Model1_sk_allocation))
  return &Model1_get_current()->Model1_task_frag;

 return &Model1_sk->Model1_sk_frag;
}

bool Model1_sk_page_frag_refill(struct Model1_sock *Model1_sk, struct Model1_page_frag *Model1_pfrag);

/*
 *	Default write policy as shown to user space via poll/select/SIGIO
 */
static inline __attribute__((no_instrument_function)) bool Model1_sock_writeable(const struct Model1_sock *Model1_sk)
{
 return Model1_atomic_read(&Model1_sk->Model1_sk_wmem_alloc) < (Model1_sk->Model1_sk_sndbuf >> 1);
}

static inline __attribute__((no_instrument_function)) Model1_gfp_t Model1_gfp_any(void)
{
 return ((Model1_preempt_count() & (((1UL << (8))-1) << (0 + 8)))) ? ((( Model1_gfp_t)0x20u)|(( Model1_gfp_t)0x80000u)|(( Model1_gfp_t)0x2000000u)) : ((( Model1_gfp_t)(0x400000u|0x2000000u)) | (( Model1_gfp_t)0x40u) | (( Model1_gfp_t)0x80u));
}

static inline __attribute__((no_instrument_function)) long Model1_sock_rcvtimeo(const struct Model1_sock *Model1_sk, bool Model1_noblock)
{
 return Model1_noblock ? 0 : Model1_sk->Model1_sk_rcvtimeo;
}

static inline __attribute__((no_instrument_function)) long Model1_sock_sndtimeo(const struct Model1_sock *Model1_sk, bool Model1_noblock)
{
 return Model1_noblock ? 0 : Model1_sk->Model1_sk_sndtimeo;
}

static inline __attribute__((no_instrument_function)) int Model1_sock_rcvlowat(const struct Model1_sock *Model1_sk, int Model1_waitall, int Model1_len)
{
 return (Model1_waitall ? Model1_len : ({ int Model1___min1 = (Model1_sk->Model1_sk_rcvlowat); int Model1___min2 = (Model1_len); Model1___min1 < Model1___min2 ? Model1___min1: Model1___min2; })) ? : 1;
}

/* Alas, with timeout socket operations are not restartable.
 * Compare this to poll().
 */
static inline __attribute__((no_instrument_function)) int Model1_sock_intr_errno(long Model1_timeo)
{
 return Model1_timeo == ((long)(~0UL>>1)) ? -512 : -4;
}

struct Model1_sock_skb_cb {
 Model1_u32 Model1_dropcount;
};

/* Store sock_skb_cb at the end of skb->cb[] so protocol families
 * using skb->cb[] would keep using it directly and utilize its
 * alignement guarantee.
 */
static inline __attribute__((no_instrument_function)) void
Model1_sock_skb_set_dropcount(const struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb)
{
 ((struct Model1_sock_skb_cb *)((Model1_skb)->Model1_cb + (((sizeof(((struct Model1_sk_buff*)0)->Model1_cb)) - sizeof(struct Model1_sock_skb_cb)))))->Model1_dropcount = Model1_atomic_read(&Model1_sk->Model1_sk_drops);
}

static inline __attribute__((no_instrument_function)) void Model1_sk_drops_add(struct Model1_sock *Model1_sk, const struct Model1_sk_buff *Model1_skb)
{
 int Model1_segs = ({ Model1_u16 Model1___max1 = (1); Model1_u16 Model1___max2 = (((struct Model1_skb_shared_info *)(Model1_skb_end_pointer(Model1_skb)))->Model1_gso_segs); Model1___max1 > Model1___max2 ? Model1___max1: Model1___max2; });

 Model1_atomic_add(Model1_segs, &Model1_sk->Model1_sk_drops);
}

void Model1___sock_recv_timestamp(struct Model1_msghdr *Model1_msg, struct Model1_sock *Model1_sk,
      struct Model1_sk_buff *Model1_skb);
void Model1___sock_recv_wifi_status(struct Model1_msghdr *Model1_msg, struct Model1_sock *Model1_sk,
        struct Model1_sk_buff *Model1_skb);

static inline __attribute__((no_instrument_function)) void
Model1_sock_recv_timestamp(struct Model1_msghdr *Model1_msg, struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb)
{
 Model1_ktime_t Model1_kt = Model1_skb->Model1_tstamp;
 struct Model1_skb_shared_hwtstamps *Model1_hwtstamps = Model1_skb_hwtstamps(Model1_skb);

 /*
	 * generate control messages if
	 * - receive time stamping in software requested
	 * - software time stamp available and wanted
	 * - hardware time stamps available and wanted
	 */
 if (Model1_sock_flag(Model1_sk, Model1_SOCK_RCVTSTAMP) ||
     (Model1_sk->Model1_sk_tsflags & Model1_SOF_TIMESTAMPING_RX_SOFTWARE) ||
     (Model1_kt.Model1_tv64 && Model1_sk->Model1_sk_tsflags & Model1_SOF_TIMESTAMPING_SOFTWARE) ||
     (Model1_hwtstamps->Model1_hwtstamp.Model1_tv64 &&
      (Model1_sk->Model1_sk_tsflags & Model1_SOF_TIMESTAMPING_RAW_HARDWARE)))
  Model1___sock_recv_timestamp(Model1_msg, Model1_sk, Model1_skb);
 else
  Model1_sk->Model1_sk_stamp = Model1_kt;

 if (Model1_sock_flag(Model1_sk, Model1_SOCK_WIFI_STATUS) && Model1_skb->Model1_wifi_acked_valid)
  Model1___sock_recv_wifi_status(Model1_msg, Model1_sk, Model1_skb);
}

void Model1___sock_recv_ts_and_drops(struct Model1_msghdr *Model1_msg, struct Model1_sock *Model1_sk,
         struct Model1_sk_buff *Model1_skb);

static inline __attribute__((no_instrument_function)) void Model1_sock_recv_ts_and_drops(struct Model1_msghdr *Model1_msg, struct Model1_sock *Model1_sk,
       struct Model1_sk_buff *Model1_skb)
{





 if (Model1_sk->Model1___sk_common.Model1_skc_flags & ((1UL << Model1_SOCK_RXQ_OVFL) | (1UL << Model1_SOCK_RCVTSTAMP)) || Model1_sk->Model1_sk_tsflags & (Model1_SOF_TIMESTAMPING_SOFTWARE | Model1_SOF_TIMESTAMPING_RAW_HARDWARE))
  Model1___sock_recv_ts_and_drops(Model1_msg, Model1_sk, Model1_skb);
 else
  Model1_sk->Model1_sk_stamp = Model1_skb->Model1_tstamp;
}

void Model1___sock_tx_timestamp(Model1___u16 Model1_tsflags, __u8 *Model1_tx_flags);

/**
 * sock_tx_timestamp - checks whether the outgoing packet is to be time stamped
 * @sk:		socket sending this packet
 * @tsflags:	timestamping flags to use
 * @tx_flags:	completed with instructions for time stamping
 *
 * Note : callers should take care of initial *tx_flags value (usually 0)
 */
static inline __attribute__((no_instrument_function)) void Model1_sock_tx_timestamp(const struct Model1_sock *Model1_sk, Model1___u16 Model1_tsflags,
         __u8 *Model1_tx_flags)
{
 if (__builtin_expect(!!(Model1_tsflags), 0))
  Model1___sock_tx_timestamp(Model1_tsflags, Model1_tx_flags);
 if (__builtin_expect(!!(Model1_sock_flag(Model1_sk, Model1_SOCK_WIFI_STATUS)), 0))
  *Model1_tx_flags |= Model1_SKBTX_WIFI_STATUS;
}

/**
 * sk_eat_skb - Release a skb if it is no longer needed
 * @sk: socket to eat this skb from
 * @skb: socket buffer to eat
 *
 * This routine must be called with interrupts disabled or with the socket
 * locked so that the sk_buff queue operation is ok.
*/
static inline __attribute__((no_instrument_function)) void Model1_sk_eat_skb(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb)
{
 Model1___skb_unlink(Model1_skb, &Model1_sk->Model1_sk_receive_queue);
 Model1___kfree_skb(Model1_skb);
}

static inline __attribute__((no_instrument_function))
struct Model1_net *Model1_sock_net(const struct Model1_sock *Model1_sk)
{
 return Model1_read_pnet(&Model1_sk->Model1___sk_common.Model1_skc_net);
}

static inline __attribute__((no_instrument_function))
void Model1_sock_net_set(struct Model1_sock *Model1_sk, struct Model1_net *Model1_net)
{
 Model1_write_pnet(&Model1_sk->Model1___sk_common.Model1_skc_net, Model1_net);
}

static inline __attribute__((no_instrument_function)) struct Model1_sock *Model1_skb_steal_sock(struct Model1_sk_buff *Model1_skb)
{
 if (Model1_skb->Model1_sk) {
  struct Model1_sock *Model1_sk = Model1_skb->Model1_sk;

  Model1_skb->Model1_destructor = ((void *)0);
  Model1_skb->Model1_sk = ((void *)0);
  return Model1_sk;
 }
 return ((void *)0);
}

/* This helper checks if a socket is a full socket,
 * ie _not_ a timewait or request socket.
 */
static inline __attribute__((no_instrument_function)) bool Model1_sk_fullsock(const struct Model1_sock *Model1_sk)
{
 return (1 << Model1_sk->Model1___sk_common.Model1_skc_state) & ~(Model1_TCPF_TIME_WAIT | Model1_TCPF_NEW_SYN_RECV);
}

/* This helper checks if a socket is a LISTEN or NEW_SYN_RECV
 * SYNACK messages can be attached to either ones (depending on SYNCOOKIE)
 */
static inline __attribute__((no_instrument_function)) bool Model1_sk_listener(const struct Model1_sock *Model1_sk)
{
 return (1 << Model1_sk->Model1___sk_common.Model1_skc_state) & (Model1_TCPF_LISTEN | Model1_TCPF_NEW_SYN_RECV);
}

/**
 * sk_state_load - read sk->sk_state for lockless contexts
 * @sk: socket pointer
 *
 * Paired with sk_state_store(). Used in places we do not hold socket lock :
 * tcp_diag_get_info(), tcp_get_info(), tcp_poll(), get_tcp4_sock() ...
 */
static inline __attribute__((no_instrument_function)) int Model1_sk_state_load(const struct Model1_sock *Model1_sk)
{
 return ({ typeof(*&Model1_sk->Model1___sk_common.Model1_skc_state) Model1____p1 = ({ union { typeof(*&Model1_sk->Model1___sk_common.Model1_skc_state) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&(*&Model1_sk->Model1___sk_common.Model1_skc_state), Model1___u.Model1___c, sizeof(*&Model1_sk->Model1___sk_common.Model1_skc_state)); else Model1___read_once_size_nocheck(&(*&Model1_sk->Model1___sk_common.Model1_skc_state), Model1___u.Model1___c, sizeof(*&Model1_sk->Model1___sk_common.Model1_skc_state)); Model1___u.Model1___val; }); do { bool Model1___cond = !((sizeof(*&Model1_sk->Model1___sk_common.Model1_skc_state) == sizeof(char) || sizeof(*&Model1_sk->Model1___sk_common.Model1_skc_state) == sizeof(short) || sizeof(*&Model1_sk->Model1___sk_common.Model1_skc_state) == sizeof(int) || sizeof(*&Model1_sk->Model1___sk_common.Model1_skc_state) == sizeof(long))); extern void Model1___compiletime_assert_2241(void) ; if (Model1___cond) Model1___compiletime_assert_2241(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); Model1____p1; });
}

/**
 * sk_state_store - update sk->sk_state
 * @sk: socket pointer
 * @newstate: new state
 *
 * Paired with sk_state_load(). Should be used in contexts where
 * state change might impact lockless readers.
 */
static inline __attribute__((no_instrument_function)) void Model1_sk_state_store(struct Model1_sock *Model1_sk, int Model1_newstate)
{
 do { do { bool Model1___cond = !((sizeof(*&Model1_sk->Model1___sk_common.Model1_skc_state) == sizeof(char) || sizeof(*&Model1_sk->Model1___sk_common.Model1_skc_state) == sizeof(short) || sizeof(*&Model1_sk->Model1___sk_common.Model1_skc_state) == sizeof(int) || sizeof(*&Model1_sk->Model1___sk_common.Model1_skc_state) == sizeof(long))); extern void Model1___compiletime_assert_2254(void) ; if (Model1___cond) Model1___compiletime_assert_2254(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0); __asm__ __volatile__("": : :"memory"); ({ union { typeof(*&Model1_sk->Model1___sk_common.Model1_skc_state) Model1___val; char Model1___c[1]; } Model1___u = { .Model1___val = ( typeof(*&Model1_sk->Model1___sk_common.Model1_skc_state)) (Model1_newstate) }; Model1___write_once_size(&(*&Model1_sk->Model1___sk_common.Model1_skc_state), Model1___u.Model1___c, sizeof(*&Model1_sk->Model1___sk_common.Model1_skc_state)); Model1___u.Model1___val; }); } while (0);
}

void Model1_sock_enable_timestamp(struct Model1_sock *Model1_sk, int Model1_flag);
int Model1_sock_get_timestamp(struct Model1_sock *, struct Model1_timeval *);
int Model1_sock_get_timestampns(struct Model1_sock *, struct Model1_timespec *);
int Model1_sock_recv_errqueue(struct Model1_sock *Model1_sk, struct Model1_msghdr *Model1_msg, int Model1_len, int Model1_level,
         int Model1_type);

bool Model1_sk_ns_capable(const struct Model1_sock *Model1_sk,
     struct Model1_user_namespace *Model1_user_ns, int Model1_cap);
bool Model1_sk_capable(const struct Model1_sock *Model1_sk, int Model1_cap);
bool Model1_sk_net_capable(const struct Model1_sock *Model1_sk, int Model1_cap);

extern __u32 Model1_sysctl_wmem_max;
extern __u32 Model1_sysctl_rmem_max;

extern int Model1_sysctl_tstamp_allow_data;
extern int Model1_sysctl_optmem_max;

extern __u32 Model1_sysctl_wmem_default;
extern __u32 Model1_sysctl_rmem_default;
/*
 * NET		Generic infrastructure for INET connection oriented protocols.
 *
 *		Definitions for inet_connection_sock 
 *
 * Authors:	Many people, see the TCP sources
 *
 * 		From code originally in TCP
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */
/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for inet_sock
 *
 * Authors:	Many, reorganised here by
 * 		Arnaldo Carvalho de Melo <acme@mandriva.com>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */











/* jhash.h: Jenkins hash support.
 *
 * Copyright (C) 2006. Bob Jenkins (bob_jenkins@burtleburtle.net)
 *
 * http://burtleburtle.net/bob/hash/
 *
 * These are the credits from Bob's sources:
 *
 * lookup3.c, by Bob Jenkins, May 2006, Public Domain.
 *
 * These are functions for producing 32-bit hashes for hash table lookup.
 * hashword(), hashlittle(), hashlittle2(), hashbig(), mix(), and final()
 * are externally useful functions.  Routines to test the hash are included
 * if SELF_TEST is defined.  You can use this free for any purpose.  It's in
 * the public domain.  It has no warranty.
 *
 * Copyright (C) 2009-2010 Jozsef Kadlecsik (kadlec@blackhole.kfki.hu)
 *
 * I've modified Bob's hash to be useful in the Linux kernel, and
 * any bugs present are my fault.
 * Jozsef
 */







struct Model1___una_u16 { Model1_u16 Model1_x; } __attribute__((packed));
struct Model1___una_u32 { Model1_u32 Model1_x; } __attribute__((packed));
struct Model1___una_u64 { Model1_u64 Model1_x; } __attribute__((packed));

static inline __attribute__((no_instrument_function)) Model1_u16 Model1___get_unaligned_cpu16(const void *Model1_p)
{
 const struct Model1___una_u16 *Model1_ptr = (const struct Model1___una_u16 *)Model1_p;
 return Model1_ptr->Model1_x;
}

static inline __attribute__((no_instrument_function)) Model1_u32 Model1___get_unaligned_cpu32(const void *Model1_p)
{
 const struct Model1___una_u32 *Model1_ptr = (const struct Model1___una_u32 *)Model1_p;
 return Model1_ptr->Model1_x;
}

static inline __attribute__((no_instrument_function)) Model1_u64 Model1___get_unaligned_cpu64(const void *Model1_p)
{
 const struct Model1___una_u64 *Model1_ptr = (const struct Model1___una_u64 *)Model1_p;
 return Model1_ptr->Model1_x;
}

static inline __attribute__((no_instrument_function)) void Model1___put_unaligned_cpu16(Model1_u16 Model1_val, void *Model1_p)
{
 struct Model1___una_u16 *Model1_ptr = (struct Model1___una_u16 *)Model1_p;
 Model1_ptr->Model1_x = Model1_val;
}

static inline __attribute__((no_instrument_function)) void Model1___put_unaligned_cpu32(Model1_u32 Model1_val, void *Model1_p)
{
 struct Model1___una_u32 *Model1_ptr = (struct Model1___una_u32 *)Model1_p;
 Model1_ptr->Model1_x = Model1_val;
}

static inline __attribute__((no_instrument_function)) void Model1___put_unaligned_cpu64(Model1_u64 Model1_val, void *Model1_p)
{
 struct Model1___una_u64 *Model1_ptr = (struct Model1___una_u64 *)Model1_p;
 Model1_ptr->Model1_x = Model1_val;
}

/* Best hash sizes are of power of two */

/* Mask the hash value, i.e (value & jhash_mask(n)) instead of (value % n) */


/* __jhash_mix -- mix 3 32-bit values reversibly. */
/* __jhash_final - final mixing of 3 32-bit values (a,b,c) into c */
/* An arbitrary initial parameter */


/* jhash - hash an arbitrary key
 * @k: sequence of bytes as key
 * @length: the length of the key
 * @initval: the previous hash, or an arbitray value
 *
 * The generic version, hashes an arbitrary sequence of bytes.
 * No alignment or length assumptions are made about the input key.
 *
 * Returns the hash value of the key. The result depends on endianness.
 */
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_jhash(const void *Model1_key, Model1_u32 Model1_length, Model1_u32 Model1_initval)
{
 Model1_u32 Model1_a, Model1_b, Model1_c;
 const Model1_u8 *Model1_k = Model1_key;

 /* Set up the internal state */
 Model1_a = Model1_b = Model1_c = 0xdeadbeef + Model1_length + Model1_initval;

 /* All but the last block: affect some 32 bits of (a,b,c) */
 while (Model1_length > 12) {
  Model1_a += Model1___get_unaligned_cpu32(Model1_k);
  Model1_b += Model1___get_unaligned_cpu32(Model1_k + 4);
  Model1_c += Model1___get_unaligned_cpu32(Model1_k + 8);
  { Model1_a -= Model1_c; Model1_a ^= Model1_rol32(Model1_c, 4); Model1_c += Model1_b; Model1_b -= Model1_a; Model1_b ^= Model1_rol32(Model1_a, 6); Model1_a += Model1_c; Model1_c -= Model1_b; Model1_c ^= Model1_rol32(Model1_b, 8); Model1_b += Model1_a; Model1_a -= Model1_c; Model1_a ^= Model1_rol32(Model1_c, 16); Model1_c += Model1_b; Model1_b -= Model1_a; Model1_b ^= Model1_rol32(Model1_a, 19); Model1_a += Model1_c; Model1_c -= Model1_b; Model1_c ^= Model1_rol32(Model1_b, 4); Model1_b += Model1_a; };
  Model1_length -= 12;
  Model1_k += 12;
 }
 /* Last block: affect all 32 bits of (c) */
 /* All the case statements fall through */
 switch (Model1_length) {
 case 12: Model1_c += (Model1_u32)Model1_k[11]<<24;
 case 11: Model1_c += (Model1_u32)Model1_k[10]<<16;
 case 10: Model1_c += (Model1_u32)Model1_k[9]<<8;
 case 9: Model1_c += Model1_k[8];
 case 8: Model1_b += (Model1_u32)Model1_k[7]<<24;
 case 7: Model1_b += (Model1_u32)Model1_k[6]<<16;
 case 6: Model1_b += (Model1_u32)Model1_k[5]<<8;
 case 5: Model1_b += Model1_k[4];
 case 4: Model1_a += (Model1_u32)Model1_k[3]<<24;
 case 3: Model1_a += (Model1_u32)Model1_k[2]<<16;
 case 2: Model1_a += (Model1_u32)Model1_k[1]<<8;
 case 1: Model1_a += Model1_k[0];
   { Model1_c ^= Model1_b; Model1_c -= Model1_rol32(Model1_b, 14); Model1_a ^= Model1_c; Model1_a -= Model1_rol32(Model1_c, 11); Model1_b ^= Model1_a; Model1_b -= Model1_rol32(Model1_a, 25); Model1_c ^= Model1_b; Model1_c -= Model1_rol32(Model1_b, 16); Model1_a ^= Model1_c; Model1_a -= Model1_rol32(Model1_c, 4); Model1_b ^= Model1_a; Model1_b -= Model1_rol32(Model1_a, 14); Model1_c ^= Model1_b; Model1_c -= Model1_rol32(Model1_b, 24); };
 case 0: /* Nothing left to add */
  break;
 }

 return Model1_c;
}

/* jhash2 - hash an array of u32's
 * @k: the key which must be an array of u32's
 * @length: the number of u32's in the key
 * @initval: the previous hash, or an arbitray value
 *
 * Returns the hash value of the key.
 */
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_jhash2(const Model1_u32 *Model1_k, Model1_u32 Model1_length, Model1_u32 Model1_initval)
{
 Model1_u32 Model1_a, Model1_b, Model1_c;

 /* Set up the internal state */
 Model1_a = Model1_b = Model1_c = 0xdeadbeef + (Model1_length<<2) + Model1_initval;

 /* Handle most of the key */
 while (Model1_length > 3) {
  Model1_a += Model1_k[0];
  Model1_b += Model1_k[1];
  Model1_c += Model1_k[2];
  { Model1_a -= Model1_c; Model1_a ^= Model1_rol32(Model1_c, 4); Model1_c += Model1_b; Model1_b -= Model1_a; Model1_b ^= Model1_rol32(Model1_a, 6); Model1_a += Model1_c; Model1_c -= Model1_b; Model1_c ^= Model1_rol32(Model1_b, 8); Model1_b += Model1_a; Model1_a -= Model1_c; Model1_a ^= Model1_rol32(Model1_c, 16); Model1_c += Model1_b; Model1_b -= Model1_a; Model1_b ^= Model1_rol32(Model1_a, 19); Model1_a += Model1_c; Model1_c -= Model1_b; Model1_c ^= Model1_rol32(Model1_b, 4); Model1_b += Model1_a; };
  Model1_length -= 3;
  Model1_k += 3;
 }

 /* Handle the last 3 u32's: all the case statements fall through */
 switch (Model1_length) {
 case 3: Model1_c += Model1_k[2];
 case 2: Model1_b += Model1_k[1];
 case 1: Model1_a += Model1_k[0];
  { Model1_c ^= Model1_b; Model1_c -= Model1_rol32(Model1_b, 14); Model1_a ^= Model1_c; Model1_a -= Model1_rol32(Model1_c, 11); Model1_b ^= Model1_a; Model1_b -= Model1_rol32(Model1_a, 25); Model1_c ^= Model1_b; Model1_c -= Model1_rol32(Model1_b, 16); Model1_a ^= Model1_c; Model1_a -= Model1_rol32(Model1_c, 4); Model1_b ^= Model1_a; Model1_b -= Model1_rol32(Model1_a, 14); Model1_c ^= Model1_b; Model1_c -= Model1_rol32(Model1_b, 24); };
 case 0: /* Nothing left to add */
  break;
 }

 return Model1_c;
}


/* __jhash_nwords - hash exactly 3, 2 or 1 word(s) */
static inline __attribute__((no_instrument_function)) Model1_u32 Model1___jhash_nwords(Model1_u32 Model1_a, Model1_u32 Model1_b, Model1_u32 Model1_c, Model1_u32 Model1_initval)
{
 Model1_a += Model1_initval;
 Model1_b += Model1_initval;
 Model1_c += Model1_initval;

 { Model1_c ^= Model1_b; Model1_c -= Model1_rol32(Model1_b, 14); Model1_a ^= Model1_c; Model1_a -= Model1_rol32(Model1_c, 11); Model1_b ^= Model1_a; Model1_b -= Model1_rol32(Model1_a, 25); Model1_c ^= Model1_b; Model1_c -= Model1_rol32(Model1_b, 16); Model1_a ^= Model1_c; Model1_a -= Model1_rol32(Model1_c, 4); Model1_b ^= Model1_a; Model1_b -= Model1_rol32(Model1_a, 14); Model1_c ^= Model1_b; Model1_c -= Model1_rol32(Model1_b, 24); };

 return Model1_c;
}

static inline __attribute__((no_instrument_function)) Model1_u32 Model1_jhash_3words(Model1_u32 Model1_a, Model1_u32 Model1_b, Model1_u32 Model1_c, Model1_u32 Model1_initval)
{
 return Model1___jhash_nwords(Model1_a, Model1_b, Model1_c, Model1_initval + 0xdeadbeef + (3 << 2));
}

static inline __attribute__((no_instrument_function)) Model1_u32 Model1_jhash_2words(Model1_u32 Model1_a, Model1_u32 Model1_b, Model1_u32 Model1_initval)
{
 return Model1___jhash_nwords(Model1_a, Model1_b, 0, Model1_initval + 0xdeadbeef + (2 << 2));
}

static inline __attribute__((no_instrument_function)) Model1_u32 Model1_jhash_1word(Model1_u32 Model1_a, Model1_u32 Model1_initval)
{
 return Model1___jhash_nwords(Model1_a, 0, 0, Model1_initval + 0xdeadbeef + (1 << 2));
}




/*
 * NET		Generic infrastructure for Network protocols.
 *
 *		Definitions for request_sock 
 *
 * Authors:	Arnaldo Carvalho de Melo <acme@conectiva.com.br>
 *
 * 		From code originally in include/net/tcp.h
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */
struct Model1_request_sock;
struct Model1_sk_buff;
struct Model1_dst_entry;
struct Model1_proto;

struct Model1_request_sock_ops {
 int Model1_family;
 int Model1_obj_size;
 struct Model1_kmem_cache *Model1_slab;
 char *Model1_slab_name;
 int (*Model1_rtx_syn_ack)(const struct Model1_sock *Model1_sk,
           struct Model1_request_sock *Model1_req);
 void (*Model1_send_ack)(const struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb,
        struct Model1_request_sock *Model1_req);
 void (*Model1_send_reset)(const struct Model1_sock *Model1_sk,
          struct Model1_sk_buff *Model1_skb);
 void (*Model1_destructor)(struct Model1_request_sock *Model1_req);
 void (*Model1_syn_ack_timeout)(const struct Model1_request_sock *Model1_req);
};

int Model1_inet_rtx_syn_ack(const struct Model1_sock *Model1_parent, struct Model1_request_sock *Model1_req);

/* struct request_sock - mini sock to represent a connection request
 */
struct Model1_request_sock {
 struct Model1_sock_common Model1___req_common;






 struct Model1_request_sock *Model1_dl_next;
 Model1_u16 Model1_mss;
 Model1_u8 Model1_num_retrans; /* number of retransmits */
 Model1_u8 Model1_cookie_ts:1; /* syncookie: encode tcpopts in timestamp */
 Model1_u8 Model1_num_timeout:7; /* number of timeouts */
 Model1_u32 Model1_ts_recent;
 struct Model1_timer_list Model1_rsk_timer;
 const struct Model1_request_sock_ops *Model1_rsk_ops;
 struct Model1_sock *Model1_sk;
 Model1_u32 *Model1_saved_syn;
 Model1_u32 Model1_secid;
 Model1_u32 Model1_peer_secid;
};

static inline __attribute__((no_instrument_function)) struct Model1_request_sock *Model1_inet_reqsk(const struct Model1_sock *Model1_sk)
{
 return (struct Model1_request_sock *)Model1_sk;
}

static inline __attribute__((no_instrument_function)) struct Model1_sock *Model1_req_to_sk(struct Model1_request_sock *Model1_req)
{
 return (struct Model1_sock *)Model1_req;
}
#if CY_ABSTRACT1
void *Model1_cy_kmem_cache_alloc(int Model1_memory_type);
extern bool Model1_Src_sk;
#endif
extern struct Model1_tcp_sock Model1_Server_L;
static inline __attribute__((no_instrument_function)) struct Model1_request_sock *
Model1_reqsk_alloc(const struct Model1_request_sock_ops *Model1_ops, struct Model1_sock *Model1_sk_listener,
     bool Model1_attach_listener)
{
 struct Model1_request_sock *Model1_req;

#if CY_ABSTRACT1
 if (Model1_Src_sk){ //from attacke
    Model1_req = Model1_cy_kmem_cache_alloc(1); //Model1_Req_A
 }
 else{
    Model1_req = Model1_cy_kmem_cache_alloc(2); //Model1_Req
 }
#else
 Model1_req = Model1_kmem_cache_alloc(Model1_ops->Model1_slab, ((( Model1_gfp_t)0x20u)|(( Model1_gfp_t)0x80000u)|(( Model1_gfp_t)0x2000000u)) | (( Model1_gfp_t)0x200u));
#endif
 if (!Model1_req)
  return ((void *)0);
 Model1_req->Model1___req_common.Model1_skc_listener = ((void *)0);
 if (Model1_attach_listener) {
  if (__builtin_expect(!!(!Model1_atomic_add_unless((&Model1_sk_listener->Model1___sk_common.Model1_skc_refcnt), 1, 0)), 0)) {
   Model1_kmem_cache_free(Model1_ops->Model1_slab, Model1_req);
   return ((void *)0);
  }
  Model1_req->Model1___req_common.Model1_skc_listener = Model1_sk_listener;
 }
 Model1_req->Model1_rsk_ops = Model1_ops;
 Model1_req_to_sk(Model1_req)->Model1___sk_common.Model1_skc_prot = Model1_sk_listener->Model1___sk_common.Model1_skc_prot;
 Model1_sk_node_init(&Model1_req_to_sk(Model1_req)->Model1___sk_common.Model1_skc_node);
 Model1_sk_tx_queue_clear(Model1_req_to_sk(Model1_req));
 Model1_req->Model1_saved_syn = ((void *)0);
 Model1_atomic_set(&Model1_req->Model1___req_common.Model1_skc_refcnt, 0);

 return Model1_req;
}

#if CY_ABSTRACT7
static void Model1_tcp_v4_reqsk_destructor(struct Model1_request_sock *Model1_req);
#endif

static inline __attribute__((no_instrument_function)) void Model1_reqsk_free(struct Model1_request_sock *Model1_req)
{
 /* temporary debugging */
 ({ static bool __attribute__ ((__section__(".data.unlikely"))) Model1___warned; int Model1___ret_warn_once = !!(Model1_atomic_read(&Model1_req->Model1___req_common.Model1_skc_refcnt) != 0); if (__builtin_expect(!!(Model1___ret_warn_once && !Model1___warned), 0)) { Model1___warned = true; ({ int Model1___ret_warn_on = !!(1); if (__builtin_expect(!!(Model1___ret_warn_on), 0)) Model1_warn_slowpath_null("./include/net/request_sock.h", 111); __builtin_expect(!!(Model1___ret_warn_on), 0); }); } __builtin_expect(!!(Model1___ret_warn_once), 0); });
#if CY_ABSTRACT7
 Model1_tcp_v4_reqsk_destructor(Model1_req);
#else
 Model1_req->Model1_rsk_ops->Model1_destructor(Model1_req);
#endif
 if (Model1_req->Model1___req_common.Model1_skc_listener)
  Model1_sock_put(Model1_req->Model1___req_common.Model1_skc_listener);
 Model1_kfree(Model1_req->Model1_saved_syn);
 Model1_kmem_cache_free(Model1_req->Model1_rsk_ops->Model1_slab, Model1_req);
}

static inline __attribute__((no_instrument_function)) void Model1_reqsk_put(struct Model1_request_sock *Model1_req)
{
 if (Model1_atomic_dec_and_test(&Model1_req->Model1___req_common.Model1_skc_refcnt))
  Model1_reqsk_free(Model1_req);
}

extern int Model1_sysctl_max_syn_backlog;

/*
 * For a TCP Fast Open listener -
 *	lock - protects the access to all the reqsk, which is co-owned by
 *		the listener and the child socket.
 *	qlen - pending TFO requests (still in TCP_SYN_RECV).
 *	max_qlen - max TFO reqs allowed before TFO is disabled.
 *
 *	XXX (TFO) - ideally these fields can be made as part of "listen_sock"
 *	structure above. But there is some implementation difficulty due to
 *	listen_sock being part of request_sock_queue hence will be freed when
 *	a listener is stopped. But TFO related fields may continue to be
 *	accessed even after a listener is closed, until its sk_refcnt drops
 *	to 0 implying no more outstanding TFO reqs. One solution is to keep
 *	listen_opt around until	sk_refcnt drops to 0. But there is some other
 *	complexity that needs to be resolved. E.g., a listener can be disabled
 *	temporarily through shutdown()->tcp_disconnect(), and re-enabled later.
 */
struct Model1_fastopen_queue {
 struct Model1_request_sock *Model1_rskq_rst_head; /* Keep track of past TFO */
 struct Model1_request_sock *Model1_rskq_rst_tail; /* requests that caused RST.
						 * This is part of the defense
						 * against spoofing attack.
						 */
 Model1_spinlock_t Model1_lock;
 int Model1_qlen; /* # of pending (TCP_SYN_RECV) reqs */
 int Model1_max_qlen; /* != 0 iff TFO is currently enabled */
};

/** struct request_sock_queue - queue of request_socks
 *
 * @rskq_accept_head - FIFO head of established children
 * @rskq_accept_tail - FIFO tail of established children
 * @rskq_defer_accept - User waits for some data after accept()
 *
 */
struct Model1_request_sock_queue {
 Model1_spinlock_t Model1_rskq_lock;
 Model1_u8 Model1_rskq_defer_accept;

 Model1_u32 Model1_synflood_warned;
 Model1_atomic_t Model1_qlen;
 Model1_atomic_t Model1_young;

 struct Model1_request_sock *Model1_rskq_accept_head;
 struct Model1_request_sock *Model1_rskq_accept_tail;
 struct Model1_fastopen_queue Model1_fastopenq; /* Check max_qlen != 0 to determine
					     * if TFO is enabled.
					     */
};

void Model1_reqsk_queue_alloc(struct Model1_request_sock_queue *Model1_queue);

void Model1_reqsk_fastopen_remove(struct Model1_sock *Model1_sk, struct Model1_request_sock *Model1_req,
      bool Model1_reset);

static inline __attribute__((no_instrument_function)) bool Model1_reqsk_queue_empty(const struct Model1_request_sock_queue *Model1_queue)
{
 return Model1_queue->Model1_rskq_accept_head == ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model1_request_sock *Model1_reqsk_queue_remove(struct Model1_request_sock_queue *Model1_queue,
            struct Model1_sock *Model1_parent)
{
 struct Model1_request_sock *Model1_req;

 Model1_spin_lock_bh(&Model1_queue->Model1_rskq_lock);
 Model1_req = Model1_queue->Model1_rskq_accept_head;
 if (Model1_req) {
  Model1_sk_acceptq_removed(Model1_parent);
  Model1_queue->Model1_rskq_accept_head = Model1_req->Model1_dl_next;
  if (Model1_queue->Model1_rskq_accept_head == ((void *)0))
   Model1_queue->Model1_rskq_accept_tail = ((void *)0);
 }
 Model1_spin_unlock_bh(&Model1_queue->Model1_rskq_lock);
 return Model1_req;
}

static inline __attribute__((no_instrument_function)) void Model1_reqsk_queue_removed(struct Model1_request_sock_queue *Model1_queue,
           const struct Model1_request_sock *Model1_req)
{
 if (Model1_req->Model1_num_timeout == 0)
  Model1_atomic_dec(&Model1_queue->Model1_young);
 Model1_atomic_dec(&Model1_queue->Model1_qlen);
}

static inline __attribute__((no_instrument_function)) void Model1_reqsk_queue_added(struct Model1_request_sock_queue *Model1_queue)
{
 Model1_atomic_inc(&Model1_queue->Model1_young);
 Model1_atomic_inc(&Model1_queue->Model1_qlen);
}

static inline __attribute__((no_instrument_function)) int Model1_reqsk_queue_len(const struct Model1_request_sock_queue *Model1_queue)
{
 return Model1_atomic_read(&Model1_queue->Model1_qlen);
}

static inline __attribute__((no_instrument_function)) int Model1_reqsk_queue_len_young(const struct Model1_request_sock_queue *Model1_queue)
{
 return Model1_atomic_read(&Model1_queue->Model1_young);
}





struct Model1_net;

static inline __attribute__((no_instrument_function)) Model1_u32 Model1_net_hash_mix(const struct Model1_net *Model1_net)
{

 /*
	 * shift this right to eliminate bits, that are
	 * always zeroed
	 */

 return (Model1_u32)(((unsigned long)Model1_net) >> (6));



}

/*
 * include/net/l3mdev.h - L3 master device API
 * Copyright (c) 2015 Cumulus Networks
 * Copyright (c) 2015 David Ahern <dsa@cumulusnetworks.com>
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 */
















/* rule is permanent, and cannot be deleted */







/* try to find source address in routing lookups */


struct Model1_fib_rule_hdr {
 __u8 Model1_family;
 __u8 Model1_dst_len;
 __u8 Model1_src_len;
 __u8 Model1_tos;

 __u8 Model1_table;
 __u8 Model1_res1; /* reserved */
 __u8 Model1_res2; /* reserved */
 __u8 Model1_action;

 __u32 Model1_flags;
};

enum {
 Model1_FRA_UNSPEC,
 Model1_FRA_DST, /* destination address */
 Model1_FRA_SRC, /* source address */
 Model1_FRA_IIFNAME, /* interface name */

 Model1_FRA_GOTO, /* target to jump to (FR_ACT_GOTO) */
 Model1_FRA_UNUSED2,
 Model1_FRA_PRIORITY, /* priority/preference */
 Model1_FRA_UNUSED3,
 Model1_FRA_UNUSED4,
 Model1_FRA_UNUSED5,
 Model1_FRA_FWMARK, /* mark */
 Model1_FRA_FLOW, /* flow/class id */
 Model1_FRA_TUN_ID,
 Model1_FRA_SUPPRESS_IFGROUP,
 Model1_FRA_SUPPRESS_PREFIXLEN,
 Model1_FRA_TABLE, /* Extended table id */
 Model1_FRA_FWMASK, /* mask for netfilter mark */
 Model1_FRA_OIFNAME,
 Model1_FRA_PAD,
 Model1_FRA_L3MDEV, /* iif or oif is l3mdev goto its table */
 Model1___FRA_MAX
};



enum {
 Model1_FR_ACT_UNSPEC,
 Model1_FR_ACT_TO_TBL, /* Pass to fixed table */
 Model1_FR_ACT_GOTO, /* Jump to another rule */
 Model1_FR_ACT_NOP, /* No operation */
 Model1_FR_ACT_RES3,
 Model1_FR_ACT_RES4,
 Model1_FR_ACT_BLACKHOLE, /* Drop without notification */
 Model1_FR_ACT_UNREACHABLE, /* Drop with ENETUNREACH */
 Model1_FR_ACT_PROHIBIT, /* Drop with EACCES */
 Model1___FR_ACT_MAX,
};



struct Model1_fib_rule {
 struct Model1_list_head Model1_list;
 int Model1_iifindex;
 int Model1_oifindex;
 Model1_u32 Model1_mark;
 Model1_u32 Model1_mark_mask;
 Model1_u32 Model1_flags;
 Model1_u32 Model1_table;
 Model1_u8 Model1_action;
 Model1_u8 Model1_l3mdev;
 /* 2 bytes hole, try to use */
 Model1_u32 Model1_target;
 Model1___be64 Model1_tun_id;
 struct Model1_fib_rule *Model1_ctarget;
 struct Model1_net *Model1_fr_net;

 Model1_atomic_t Model1_refcnt;
 Model1_u32 Model1_pref;
 int Model1_suppress_ifgroup;
 int Model1_suppress_prefixlen;
 char Model1_iifname[16];
 char Model1_oifname[16];
 struct Model1_callback_head Model1_rcu;
};

struct Model1_fib_lookup_arg {
 void *Model1_lookup_ptr;
 void *Model1_result;
 struct Model1_fib_rule *Model1_rule;
 Model1_u32 Model1_table;
 int Model1_flags;


};

struct Model1_fib_rules_ops {
 int Model1_family;
 struct Model1_list_head Model1_list;
 int Model1_rule_size;
 int Model1_addr_size;
 int Model1_unresolved_rules;
 int Model1_nr_goto_rules;

 int (*Model1_action)(struct Model1_fib_rule *,
       struct Model1_flowi *, int,
       struct Model1_fib_lookup_arg *);
 bool (*Model1_suppress)(struct Model1_fib_rule *,
         struct Model1_fib_lookup_arg *);
 int (*Model1_match)(struct Model1_fib_rule *,
      struct Model1_flowi *, int);
 int (*Model1_configure)(struct Model1_fib_rule *,
          struct Model1_sk_buff *,
          struct Model1_fib_rule_hdr *,
          struct Model1_nlattr **);
 int (*Model1_delete)(struct Model1_fib_rule *);
 int (*Model1_compare)(struct Model1_fib_rule *,
        struct Model1_fib_rule_hdr *,
        struct Model1_nlattr **);
 int (*Model1_fill)(struct Model1_fib_rule *, struct Model1_sk_buff *,
     struct Model1_fib_rule_hdr *);
 Model1_size_t (*Model1_nlmsg_payload)(struct Model1_fib_rule *);

 /* Called after modifications to the rules set, must flush
	 * the route cache if one exists. */
 void (*Model1_flush_cache)(struct Model1_fib_rules_ops *Model1_ops);

 int Model1_nlgroup;
 const struct Model1_nla_policy *Model1_policy;
 struct Model1_list_head Model1_rules_list;
 struct Model1_module *Model1_owner;
 struct Model1_net *Model1_fro_net;
 struct Model1_callback_head Model1_rcu;
};
static inline __attribute__((no_instrument_function)) void Model1_fib_rule_get(struct Model1_fib_rule *Model1_rule)
{
 Model1_atomic_inc(&Model1_rule->Model1_refcnt);
}

static inline __attribute__((no_instrument_function)) void Model1_fib_rule_put(struct Model1_fib_rule *Model1_rule)
{
 if (Model1_atomic_dec_and_test(&Model1_rule->Model1_refcnt))
  do { do { bool Model1___cond = !(!(!((__builtin_offsetof(typeof(*(Model1_rule)), Model1_rcu)) < 4096))); extern void Model1___compiletime_assert_105(void) ; if (Model1___cond) Model1___compiletime_assert_105(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0); Model1_kfree_call_rcu(&((Model1_rule)->Model1_rcu), (Model1_rcu_callback_t)(unsigned long)(__builtin_offsetof(typeof(*(Model1_rule)), Model1_rcu))); } while (0);
}
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_fib_rule_get_table(struct Model1_fib_rule *Model1_rule,
         struct Model1_fib_lookup_arg *Model1_arg)
{
 return Model1_rule->Model1_table;
}


static inline __attribute__((no_instrument_function)) Model1_u32 Model1_frh_get_table(struct Model1_fib_rule_hdr *Model1_frh, struct Model1_nlattr **Model1_nla)
{
 if (Model1_nla[Model1_FRA_TABLE])
  return Model1_nla_get_u32(Model1_nla[Model1_FRA_TABLE]);
 return Model1_frh->Model1_table;
}

struct Model1_fib_rules_ops *Model1_fib_rules_register(const struct Model1_fib_rules_ops *,
      struct Model1_net *);
void Model1_fib_rules_unregister(struct Model1_fib_rules_ops *);

int Model1_fib_rules_lookup(struct Model1_fib_rules_ops *, struct Model1_flowi *, int Model1_flags,
       struct Model1_fib_lookup_arg *);
int Model1_fib_default_rule_add(struct Model1_fib_rules_ops *, Model1_u32 Model1_pref, Model1_u32 Model1_table,
    Model1_u32 Model1_flags);

int Model1_fib_nl_newrule(struct Model1_sk_buff *Model1_skb, struct Model1_nlmsghdr *Model1_nlh);
int Model1_fib_nl_delrule(struct Model1_sk_buff *Model1_skb, struct Model1_nlmsghdr *Model1_nlh);

/**
 * struct l3mdev_ops - l3mdev operations
 *
 * @l3mdev_fib_table: Get FIB table id to use for lookups
 *
 * @l3mdev_get_rtable: Get cached IPv4 rtable (dst_entry) for device
 *
 * @l3mdev_get_saddr: Get source address for a flow
 *
 * @l3mdev_get_rt6_dst: Get cached IPv6 rt6_info (dst_entry) for device
 */

struct Model1_l3mdev_ops {
 Model1_u32 (*Model1_l3mdev_fib_table)(const struct Model1_net_device *Model1_dev);
 struct Model1_sk_buff * (*Model1_l3mdev_l3_rcv)(struct Model1_net_device *Model1_dev,
       struct Model1_sk_buff *Model1_skb, Model1_u16 Model1_proto);

 /* IPv4 ops */
 struct Model1_rtable * (*Model1_l3mdev_get_rtable)(const struct Model1_net_device *Model1_dev,
          const struct Model1_flowi4 *Model1_fl4);
 int (*Model1_l3mdev_get_saddr)(struct Model1_net_device *Model1_dev,
         struct Model1_flowi4 *Model1_fl4);

 /* IPv6 ops */
 struct Model1_dst_entry * (*Model1_l3mdev_get_rt6_dst)(const struct Model1_net_device *Model1_dev,
       struct Model1_flowi6 *Model1_fl6);
 int (*Model1_l3mdev_get_saddr6)(struct Model1_net_device *Model1_dev,
      const struct Model1_sock *Model1_sk,
      struct Model1_flowi6 *Model1_fl6);
};
static inline __attribute__((no_instrument_function)) int Model1_l3mdev_master_ifindex_rcu(const struct Model1_net_device *Model1_dev)
{
 return 0;
}
static inline __attribute__((no_instrument_function)) int Model1_l3mdev_master_ifindex(struct Model1_net_device *Model1_dev)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_l3mdev_master_ifindex_by_index(struct Model1_net *Model1_net, int Model1_ifindex)
{
 return 0;
}

static inline __attribute__((no_instrument_function))
const struct Model1_net_device *Model1_l3mdev_master_dev_rcu(const struct Model1_net_device *Model1_dev)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) int Model1_l3mdev_fib_oif_rcu(struct Model1_net_device *Model1_dev)
{
 return Model1_dev ? Model1_dev->Model1_ifindex : 0;
}
static inline __attribute__((no_instrument_function)) int Model1_l3mdev_fib_oif(struct Model1_net_device *Model1_dev)
{
 return Model1_dev ? Model1_dev->Model1_ifindex : 0;
}

static inline __attribute__((no_instrument_function)) Model1_u32 Model1_l3mdev_fib_table_rcu(const struct Model1_net_device *Model1_dev)
{
 return 0;
}
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_l3mdev_fib_table(const struct Model1_net_device *Model1_dev)
{
 return 0;
}
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_l3mdev_fib_table_by_index(struct Model1_net *Model1_net, int Model1_ifindex)
{
 return 0;
}

static inline __attribute__((no_instrument_function)) struct Model1_rtable *Model1_l3mdev_get_rtable(const struct Model1_net_device *Model1_dev,
            const struct Model1_flowi4 *Model1_fl4)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) bool Model1_netif_index_is_l3_master(struct Model1_net *Model1_net, int Model1_ifindex)
{
 return false;
}

static inline __attribute__((no_instrument_function)) int Model1_l3mdev_get_saddr(struct Model1_net *Model1_net, int Model1_ifindex,
       struct Model1_flowi4 *Model1_fl4)
{
 return 0;
}

static inline __attribute__((no_instrument_function))
struct Model1_dst_entry *Model1_l3mdev_get_rt6_dst(struct Model1_net *Model1_net, struct Model1_flowi6 *Model1_fl6)
{
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) int Model1_l3mdev_get_saddr6(struct Model1_net *Model1_net, const struct Model1_sock *Model1_sk,
        struct Model1_flowi6 *Model1_fl6)
{
 return 0;
}

static inline __attribute__((no_instrument_function))
struct Model1_sk_buff *Model1_l3mdev_ip_rcv(struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb;
}

static inline __attribute__((no_instrument_function))
struct Model1_sk_buff *Model1_l3mdev_ip6_rcv(struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb;
}

static inline __attribute__((no_instrument_function))
int Model1_l3mdev_fib_rule_match(struct Model1_net *Model1_net, struct Model1_flowi *Model1_fl,
     struct Model1_fib_lookup_arg *Model1_arg)
{
 return 1;
}

/** struct ip_options - IP Options
 *
 * @faddr - Saved first hop address
 * @nexthop - Saved nexthop address in LSRR and SSRR
 * @is_strictroute - Strict source route
 * @srr_is_hit - Packet destination addr was our one
 * @is_changed - IP checksum more not valid
 * @rr_needaddr - Need to record addr of outgoing dev
 * @ts_needtime - Need to record timestamp
 * @ts_needaddr - Need to record addr of outgoing dev
 */
struct Model1_ip_options {
 Model1___be32 Model1_faddr;
 Model1___be32 Model1_nexthop;
 unsigned char Model1_optlen;
 unsigned char Model1_srr;
 unsigned char Model1_rr;
 unsigned char Model1_ts;
 unsigned char Model1_is_strictroute:1,
   Model1_srr_is_hit:1,
   Model1_is_changed:1,
   Model1_rr_needaddr:1,
   Model1_ts_needtime:1,
   Model1_ts_needaddr:1;
 unsigned char Model1_router_alert;
 unsigned char Model1_cipso;
 unsigned char Model1___pad2;
 unsigned char Model1___data[0];
};

struct Model1_ip_options_rcu {
 struct Model1_callback_head Model1_rcu;
 struct Model1_ip_options Model1_opt;
};

struct Model1_ip_options_data {
 struct Model1_ip_options_rcu Model1_opt;
 char Model1_data[40];
};

struct Model1_inet_request_sock {
 struct Model1_request_sock Model1_req;
                                ;
 Model1_u16 Model1_snd_wscale : 4,
    Model1_rcv_wscale : 4,
    Model1_tstamp_ok : 1,
    Model1_sack_ok : 1,
    Model1_wscale_ok : 1,
    Model1_ecn_ok : 1,
    Model1_acked : 1,
    Model1_no_srccheck: 1;
                              ;
 Model1_u32 Model1_ir_mark;
 union {
  struct Model1_ip_options_rcu *Model1_opt;

  struct {
   struct Model1_ipv6_txoptions *Model1_ipv6_opt;
   struct Model1_sk_buff *Model1_pktopts;
  };

 };
};

static inline __attribute__((no_instrument_function)) struct Model1_inet_request_sock *Model1_inet_rsk(const struct Model1_request_sock *Model1_sk)
{
 return (struct Model1_inet_request_sock *)Model1_sk;
}

static inline __attribute__((no_instrument_function)) Model1_u32 Model1_inet_request_mark(const struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb)
{
 if (!Model1_sk->Model1_sk_mark && Model1_sock_net(Model1_sk)->Model1_ipv4.Model1_sysctl_tcp_fwmark_accept)
  return Model1_skb->Model1_mark;

 return Model1_sk->Model1_sk_mark;
}

static inline __attribute__((no_instrument_function)) int Model1_inet_request_bound_dev_if(const struct Model1_sock *Model1_sk,
         struct Model1_sk_buff *Model1_skb)
{







 return Model1_sk->Model1___sk_common.Model1_skc_bound_dev_if;
}

struct Model1_inet_cork {
 unsigned int Model1_flags;
 Model1___be32 Model1_addr;
 struct Model1_ip_options *Model1_opt;
 unsigned int Model1_fragsize;
 int Model1_length; /* Total length of all frames */
 struct Model1_dst_entry *Model1_dst;
 Model1_u8 Model1_tx_flags;
 __u8 Model1_ttl;
 Model1___s16 Model1_tos;
 char Model1_priority;
};

struct Model1_inet_cork_full {
 struct Model1_inet_cork Model1_base;
 struct Model1_flowi Model1_fl;
};

struct Model1_ip_mc_socklist;
struct Model1_ipv6_pinfo;
struct Model1_rtable;

/** struct inet_sock - representation of INET sockets
 *
 * @sk - ancestor class
 * @pinet6 - pointer to IPv6 control block
 * @inet_daddr - Foreign IPv4 addr
 * @inet_rcv_saddr - Bound local IPv4 addr
 * @inet_dport - Destination port
 * @inet_num - Local port
 * @inet_saddr - Sending source
 * @uc_ttl - Unicast TTL
 * @inet_sport - Source port
 * @inet_id - ID counter for DF pkts
 * @tos - TOS
 * @mc_ttl - Multicasting TTL
 * @is_icsk - is this an inet_connection_sock?
 * @uc_index - Unicast outgoing device index
 * @mc_index - Multicast device index
 * @mc_list - Group array
 * @cork - info to build ip hdr on each ip frag while socket is corked
 */
struct Model1_inet_sock {
 /* sk and pinet6 has to be the first two members of inet_sock */
 struct Model1_sock Model1_sk;

 struct Model1_ipv6_pinfo *Model1_pinet6;

 /* Socket demultiplex comparisons on incoming packets. */





 Model1___be32 Model1_inet_saddr;
 Model1___s16 Model1_uc_ttl;
 Model1___u16 Model1_cmsg_flags;
 Model1___be16 Model1_inet_sport;
 Model1___u16 Model1_inet_id;

 struct Model1_ip_options_rcu *Model1_inet_opt;
 int Model1_rx_dst_ifindex;
 __u8 Model1_tos;
 __u8 Model1_min_ttl;
 __u8 Model1_mc_ttl;
 __u8 Model1_pmtudisc;
 __u8 Model1_recverr:1,
    Model1_is_icsk:1,
    Model1_freebind:1,
    Model1_hdrincl:1,
    Model1_mc_loop:1,
    Model1_transparent:1,
    Model1_mc_all:1,
    Model1_nodefrag:1;
 __u8 Model1_bind_address_no_port:1;
 __u8 Model1_rcv_tos;
 __u8 Model1_convert_csum;
 int Model1_uc_index;
 int Model1_mc_index;
 Model1___be32 Model1_mc_addr;
 struct Model1_ip_mc_socklist *Model1_mc_list;
 struct Model1_inet_cork_full Model1_cork;
};




/* cmsg flags for inet */
/**
 * sk_to_full_sk - Access to a full socket
 * @sk: pointer to a socket
 *
 * SYNACK messages might be attached to request sockets.
 * Some places want to reach the listener in this case.
 */
static inline __attribute__((no_instrument_function)) struct Model1_sock *Model1_sk_to_full_sk(struct Model1_sock *Model1_sk)
{

 if (Model1_sk && Model1_sk->Model1___sk_common.Model1_skc_state == Model1_TCP_NEW_SYN_RECV)
  Model1_sk = Model1_inet_reqsk(Model1_sk)->Model1___req_common.Model1_skc_listener;

 return Model1_sk;
}

/* sk_to_full_sk() variant with a const argument */
static inline __attribute__((no_instrument_function)) const struct Model1_sock *Model1_sk_const_to_full_sk(const struct Model1_sock *Model1_sk)
{

 if (Model1_sk && Model1_sk->Model1___sk_common.Model1_skc_state == Model1_TCP_NEW_SYN_RECV)
  Model1_sk = ((const struct Model1_request_sock *)Model1_sk)->Model1___req_common.Model1_skc_listener;

 return Model1_sk;
}

static inline __attribute__((no_instrument_function)) struct Model1_sock *Model1_skb_to_full_sk(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_sk_to_full_sk(Model1_skb->Model1_sk);
}

static inline __attribute__((no_instrument_function)) struct Model1_inet_sock *Model1_inet_sk(const struct Model1_sock *Model1_sk)
{
 return (struct Model1_inet_sock *)Model1_sk;
}

static inline __attribute__((no_instrument_function)) void Model1___inet_sk_copy_descendant(struct Model1_sock *Model1_sk_to,
          const struct Model1_sock *Model1_sk_from,
          const int Model1_ancestor_size)
{
 ({ Model1_size_t Model1___len = (Model1_sk_from->Model1___sk_common.Model1_skc_prot->Model1_obj_size - Model1_ancestor_size); void *Model1___ret; if (__builtin_constant_p(Model1_sk_from->Model1___sk_common.Model1_skc_prot->Model1_obj_size - Model1_ancestor_size) && Model1___len >= 64) Model1___ret = Model1___memcpy((Model1_inet_sk(Model1_sk_to) + 1), (Model1_inet_sk(Model1_sk_from) + 1), Model1___len); else Model1___ret = __builtin_memcpy((Model1_inet_sk(Model1_sk_to) + 1), (Model1_inet_sk(Model1_sk_from) + 1), Model1___len); Model1___ret; });

}
int Model1_inet_sk_rebuild_header(struct Model1_sock *Model1_sk);

static inline __attribute__((no_instrument_function)) unsigned int Model1___inet_ehashfn(const Model1___be32 Model1_laddr,
       const Model1___u16 Model1_lport,
       const Model1___be32 Model1_faddr,
       const Model1___be16 Model1_fport,
       Model1_u32 Model1_initval)
{
 return Model1_jhash_3words(( __u32) Model1_laddr,
       ( __u32) Model1_faddr,
       ((__u32) Model1_lport) << 16 | ( __u32)Model1_fport,
       Model1_initval);
}

struct Model1_request_sock *Model1_inet_reqsk_alloc(const struct Model1_request_sock_ops *Model1_ops,
          struct Model1_sock *Model1_sk_listener,
          bool Model1_attach_listener);

static inline __attribute__((no_instrument_function)) __u8 Model1_inet_sk_flowi_flags(const struct Model1_sock *Model1_sk)
{
 __u8 Model1_flags = 0;

 if (Model1_inet_sk(Model1_sk)->Model1_transparent || Model1_inet_sk(Model1_sk)->Model1_hdrincl)
  Model1_flags |= 0x01;
 return Model1_flags;
}

static inline __attribute__((no_instrument_function)) void Model1_inet_inc_convert_csum(struct Model1_sock *Model1_sk)
{
 Model1_inet_sk(Model1_sk)->Model1_convert_csum++;
}

static inline __attribute__((no_instrument_function)) void Model1_inet_dec_convert_csum(struct Model1_sock *Model1_sk)
{
 if (Model1_inet_sk(Model1_sk)->Model1_convert_csum > 0)
  Model1_inet_sk(Model1_sk)->Model1_convert_csum--;
}

static inline __attribute__((no_instrument_function)) bool Model1_inet_get_convert_csum(struct Model1_sock *Model1_sk)
{
 return !!Model1_inet_sk(Model1_sk)->Model1_convert_csum;
}




/* Cancel timers, when they are not required. */


struct Model1_inet_bind_bucket;
struct Model1_tcp_congestion_ops;

/*
 * Pointers to address related TCP functions
 * (i.e. things that depend on the address family)
 */
struct Model1_inet_connection_sock_af_ops {
 int (*Model1_queue_xmit)(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb, struct Model1_flowi *Model1_fl);
 void (*Model1_send_check)(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);
 int (*Model1_rebuild_header)(struct Model1_sock *Model1_sk);
 void (*Model1_sk_rx_dst_set)(struct Model1_sock *Model1_sk, const struct Model1_sk_buff *Model1_skb);
 int (*Model1_conn_request)(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);
 struct Model1_sock *(*Model1_syn_recv_sock)(const struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb,
          struct Model1_request_sock *Model1_req,
          struct Model1_dst_entry *Model1_dst,
          struct Model1_request_sock *Model1_req_unhash,
          bool *Model1_own_req);
 Model1_u16 Model1_net_header_len;
 Model1_u16 Model1_net_frag_header_len;
 Model1_u16 Model1_sockaddr_len;
 int (*Model1_setsockopt)(struct Model1_sock *Model1_sk, int Model1_level, int Model1_optname,
      char *Model1_optval, unsigned int Model1_optlen);
 int (*Model1_getsockopt)(struct Model1_sock *Model1_sk, int Model1_level, int Model1_optname,
      char *Model1_optval, int *Model1_optlen);

 int (*Model1_compat_setsockopt)(struct Model1_sock *Model1_sk,
    int Model1_level, int Model1_optname,
    char *Model1_optval, unsigned int Model1_optlen);
 int (*Model1_compat_getsockopt)(struct Model1_sock *Model1_sk,
    int Model1_level, int Model1_optname,
    char *Model1_optval, int *Model1_optlen);

 void (*Model1_addr2sockaddr)(struct Model1_sock *Model1_sk, struct Model1_sockaddr *);
 int (*Model1_bind_conflict)(const struct Model1_sock *Model1_sk,
         const struct Model1_inet_bind_bucket *Model1_tb, bool Model1_relax);
 void (*Model1_mtu_reduced)(struct Model1_sock *Model1_sk);
};

/** inet_connection_sock - INET connection oriented sock
 *
 * @icsk_accept_queue:	   FIFO of established children 
 * @icsk_bind_hash:	   Bind node
 * @icsk_timeout:	   Timeout
 * @icsk_retransmit_timer: Resend (no ack)
 * @icsk_rto:		   Retransmit timeout
 * @icsk_pmtu_cookie	   Last pmtu seen by socket
 * @icsk_ca_ops		   Pluggable congestion control hook
 * @icsk_af_ops		   Operations which are AF_INET{4,6} specific
 * @icsk_ca_state:	   Congestion control state
 * @icsk_retransmits:	   Number of unrecovered [RTO] timeouts
 * @icsk_pending:	   Scheduled timer event
 * @icsk_backoff:	   Backoff
 * @icsk_syn_retries:      Number of allowed SYN (or equivalent) retries
 * @icsk_probes_out:	   unanswered 0 window probes
 * @icsk_ext_hdr_len:	   Network protocol overhead (IP/IPv6 options)
 * @icsk_ack:		   Delayed ACK control data
 * @icsk_mtup;		   MTU probing control data
 */
struct Model1_inet_connection_sock {
 /* inet_sock has to be the first member! */
 struct Model1_inet_sock Model1_icsk_inet;
 struct Model1_request_sock_queue Model1_icsk_accept_queue;
 struct Model1_inet_bind_bucket *Model1_icsk_bind_hash;
 unsigned long Model1_icsk_timeout;
  struct Model1_timer_list Model1_icsk_retransmit_timer;
  struct Model1_timer_list Model1_icsk_delack_timer;
 __u32 Model1_icsk_rto;
 __u32 Model1_icsk_pmtu_cookie;
 const struct Model1_tcp_congestion_ops *Model1_icsk_ca_ops;
 const struct Model1_inet_connection_sock_af_ops *Model1_icsk_af_ops;
 unsigned int (*Model1_icsk_sync_mss)(struct Model1_sock *Model1_sk, Model1_u32 Model1_pmtu);
 __u8 Model1_icsk_ca_state:6,
      Model1_icsk_ca_setsockopt:1,
      Model1_icsk_ca_dst_locked:1;
 __u8 Model1_icsk_retransmits;
 __u8 Model1_icsk_pending;
 __u8 Model1_icsk_backoff;
 __u8 Model1_icsk_syn_retries;
 __u8 Model1_icsk_probes_out;
 Model1___u16 Model1_icsk_ext_hdr_len;
 struct {
  __u8 Model1_pending; /* ACK is pending			   */
  __u8 Model1_quick; /* Scheduled number of quick acks	   */
  __u8 Model1_pingpong; /* The session is interactive		   */
  __u8 Model1_blocked; /* Delayed ACK was blocked by socket lock */
  __u32 Model1_ato; /* Predicted tick of soft clock	   */
  unsigned long Model1_timeout; /* Currently scheduled timeout		   */
  __u32 Model1_lrcvtime; /* timestamp of last received data packet */
  Model1___u16 Model1_last_seg_size; /* Size of last incoming segment	   */
  Model1___u16 Model1_rcv_mss; /* MSS used for delayed ACK decisions	   */
 } Model1_icsk_ack;
 struct {
  int Model1_enabled;

  /* Range of MTUs to search */
  int Model1_search_high;
  int Model1_search_low;

  /* Information on the current probe. */
  int Model1_probe_size;

  Model1_u32 Model1_probe_timestamp;
 } Model1_icsk_mtup;
 Model1_u32 Model1_icsk_user_timeout;

 Model1_u64 Model1_icsk_ca_priv[64 / sizeof(Model1_u64)];

};







static inline __attribute__((no_instrument_function)) struct Model1_inet_connection_sock *Model1_inet_csk(const struct Model1_sock *Model1_sk)
{
 return (struct Model1_inet_connection_sock *)Model1_sk;
}

static inline __attribute__((no_instrument_function)) void *Model1_inet_csk_ca(const struct Model1_sock *Model1_sk)
{
 return (void *)Model1_inet_csk(Model1_sk)->Model1_icsk_ca_priv;
}

struct Model1_sock *Model1_inet_csk_clone_lock(const struct Model1_sock *Model1_sk,
     const struct Model1_request_sock *Model1_req,
     const Model1_gfp_t Model1_priority);

enum Model1_inet_csk_ack_state_t {
 Model1_ICSK_ACK_SCHED = 1,
 Model1_ICSK_ACK_TIMER = 2,
 Model1_ICSK_ACK_PUSHED = 4,
 Model1_ICSK_ACK_PUSHED2 = 8
};

void Model1_inet_csk_init_xmit_timers(struct Model1_sock *Model1_sk,
          void (*Model1_retransmit_handler)(unsigned long),
          void (*Model1_delack_handler)(unsigned long),
          void (*Model1_keepalive_handler)(unsigned long));
void Model1_inet_csk_clear_xmit_timers(struct Model1_sock *Model1_sk);

static inline __attribute__((no_instrument_function)) void Model1_inet_csk_schedule_ack(struct Model1_sock *Model1_sk)
{
 Model1_inet_csk(Model1_sk)->Model1_icsk_ack.Model1_pending |= Model1_ICSK_ACK_SCHED;
}

static inline __attribute__((no_instrument_function)) int Model1_inet_csk_ack_scheduled(const struct Model1_sock *Model1_sk)
{
 return Model1_inet_csk(Model1_sk)->Model1_icsk_ack.Model1_pending & Model1_ICSK_ACK_SCHED;
}

static inline __attribute__((no_instrument_function)) void Model1_inet_csk_delack_init(struct Model1_sock *Model1_sk)
{
 memset(&Model1_inet_csk(Model1_sk)->Model1_icsk_ack, 0, sizeof(Model1_inet_csk(Model1_sk)->Model1_icsk_ack));
}

void Model1_inet_csk_delete_keepalive_timer(struct Model1_sock *Model1_sk);
void Model1_inet_csk_reset_keepalive_timer(struct Model1_sock *Model1_sk, unsigned long Model1_timeout);


extern const char Model1_inet_csk_timer_bug_msg[];


static inline __attribute__((no_instrument_function)) void Model1_inet_csk_clear_xmit_timer(struct Model1_sock *Model1_sk, const int Model1_what)
{
 struct Model1_inet_connection_sock *Model1_icsk = Model1_inet_csk(Model1_sk);

 if (Model1_what == 1 || Model1_what == 3) {
  Model1_icsk->Model1_icsk_pending = 0;



 } else if (Model1_what == 2) {
  Model1_icsk->Model1_icsk_ack.Model1_blocked = Model1_icsk->Model1_icsk_ack.Model1_pending = 0;



 }

 else {
  ({ do { if (0) Model1_printk("\001" "7" "TCP: " "%s", Model1_inet_csk_timer_bug_msg); } while (0); 0; });
 }

}

/*
 *	Reset the retransmission timer
 */
static inline __attribute__((no_instrument_function)) void Model1_inet_csk_reset_xmit_timer(struct Model1_sock *Model1_sk, const int Model1_what,
          unsigned long Model1_when,
          const unsigned long Model1_max_when)
{
 struct Model1_inet_connection_sock *Model1_icsk = Model1_inet_csk(Model1_sk);

 if (Model1_when > Model1_max_when) {

  ({ do { if (0) Model1_printk("\001" "7" "TCP: " "reset_xmit_timer: sk=%p %d when=0x%lx, caller=%p\n", Model1_sk, Model1_what, Model1_when, Model1_current_text_addr()); } while (0); 0; });


  Model1_when = Model1_max_when;
 }

 if (Model1_what == 1 || Model1_what == 3 ||
     Model1_what == 4 || Model1_what == 5) {
  Model1_icsk->Model1_icsk_pending = Model1_what;
  Model1_icsk->Model1_icsk_timeout = Model1_jiffies + Model1_when;
  Model1_sk_reset_timer(Model1_sk, &Model1_icsk->Model1_icsk_retransmit_timer, Model1_icsk->Model1_icsk_timeout);
 } else if (Model1_what == 2) {
  Model1_icsk->Model1_icsk_ack.Model1_pending |= Model1_ICSK_ACK_TIMER;
  Model1_icsk->Model1_icsk_ack.Model1_timeout = Model1_jiffies + Model1_when;
  Model1_sk_reset_timer(Model1_sk, &Model1_icsk->Model1_icsk_delack_timer, Model1_icsk->Model1_icsk_ack.Model1_timeout);
 }

 else {
  ({ do { if (0) Model1_printk("\001" "7" "TCP: " "%s", Model1_inet_csk_timer_bug_msg); } while (0); 0; });
 }

}

static inline __attribute__((no_instrument_function)) unsigned long
Model1_inet_csk_rto_backoff(const struct Model1_inet_connection_sock *Model1_icsk,
       unsigned long Model1_max_when)
{
        Model1_u64 Model1_when = (Model1_u64)Model1_icsk->Model1_icsk_rto << Model1_icsk->Model1_icsk_backoff;

        return (unsigned long)({ Model1_u64 Model1___min1 = (Model1_when); Model1_u64 Model1___min2 = (Model1_max_when); Model1___min1 < Model1___min2 ? Model1___min1: Model1___min2; });
}

struct Model1_sock *Model1_inet_csk_accept(struct Model1_sock *Model1_sk, int Model1_flags, int *err);

int Model1_inet_csk_bind_conflict(const struct Model1_sock *Model1_sk,
      const struct Model1_inet_bind_bucket *Model1_tb, bool Model1_relax);
int Model1_inet_csk_get_port(struct Model1_sock *Model1_sk, unsigned short Model1_snum);

struct Model1_dst_entry *Model1_inet_csk_route_req(const struct Model1_sock *Model1_sk, struct Model1_flowi4 *Model1_fl4,
         const struct Model1_request_sock *Model1_req);
struct Model1_dst_entry *Model1_inet_csk_route_child_sock(const struct Model1_sock *Model1_sk,
         struct Model1_sock *Model1_newsk,
         const struct Model1_request_sock *Model1_req);

struct Model1_sock *Model1_inet_csk_reqsk_queue_add(struct Model1_sock *Model1_sk,
          struct Model1_request_sock *Model1_req,
          struct Model1_sock *Model1_child);
void Model1_inet_csk_reqsk_queue_hash_add(struct Model1_sock *Model1_sk, struct Model1_request_sock *Model1_req,
       unsigned long Model1_timeout);
struct Model1_sock *Model1_inet_csk_complete_hashdance(struct Model1_sock *Model1_sk, struct Model1_sock *Model1_child,
      struct Model1_request_sock *Model1_req,
      bool Model1_own_req);

static inline __attribute__((no_instrument_function)) void Model1_inet_csk_reqsk_queue_added(struct Model1_sock *Model1_sk)
{
 Model1_reqsk_queue_added(&Model1_inet_csk(Model1_sk)->Model1_icsk_accept_queue);
}

static inline __attribute__((no_instrument_function)) int Model1_inet_csk_reqsk_queue_len(const struct Model1_sock *Model1_sk)
{
 return Model1_reqsk_queue_len(&Model1_inet_csk(Model1_sk)->Model1_icsk_accept_queue);
}

static inline __attribute__((no_instrument_function)) int Model1_inet_csk_reqsk_queue_young(const struct Model1_sock *Model1_sk)
{
 return Model1_reqsk_queue_len_young(&Model1_inet_csk(Model1_sk)->Model1_icsk_accept_queue);
}

static inline __attribute__((no_instrument_function)) int Model1_inet_csk_reqsk_queue_is_full(const struct Model1_sock *Model1_sk)
{
 return Model1_inet_csk_reqsk_queue_len(Model1_sk) >= Model1_sk->Model1_sk_max_ack_backlog;
}

void Model1_inet_csk_reqsk_queue_drop(struct Model1_sock *Model1_sk, struct Model1_request_sock *Model1_req);
void Model1_inet_csk_reqsk_queue_drop_and_put(struct Model1_sock *Model1_sk, struct Model1_request_sock *Model1_req);

void Model1_inet_csk_destroy_sock(struct Model1_sock *Model1_sk);
void Model1_inet_csk_prepare_forced_close(struct Model1_sock *Model1_sk);

/*
 * LISTEN is a special case for poll..
 */
static inline __attribute__((no_instrument_function)) unsigned int Model1_inet_csk_listen_poll(const struct Model1_sock *Model1_sk)
{
 return !Model1_reqsk_queue_empty(&Model1_inet_csk(Model1_sk)->Model1_icsk_accept_queue) ?
   (0x0001 | 0x0040) : 0;
}

int Model1_inet_csk_listen_start(struct Model1_sock *Model1_sk, int Model1_backlog);
void Model1_inet_csk_listen_stop(struct Model1_sock *Model1_sk);

void Model1_inet_csk_addr2sockaddr(struct Model1_sock *Model1_sk, struct Model1_sockaddr *Model1_uaddr);

int Model1_inet_csk_compat_getsockopt(struct Model1_sock *Model1_sk, int Model1_level, int Model1_optname,
          char *Model1_optval, int *Model1_optlen);
int Model1_inet_csk_compat_setsockopt(struct Model1_sock *Model1_sk, int Model1_level, int Model1_optname,
          char *Model1_optval, unsigned int Model1_optlen);

struct Model1_dst_entry *Model1_inet_csk_update_pmtu(struct Model1_sock *Model1_sk, Model1_u32 Model1_mtu);
/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for a generic INET TIMEWAIT sock
 *
 *		From code originally in net/tcp.h
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */
/*
 * NET		Generic infrastructure for Network protocols.
 *
 * Authors:	Arnaldo Carvalho de Melo <acme@conectiva.com.br>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */







struct Model1_timewait_sock_ops {
 struct Model1_kmem_cache *Model1_twsk_slab;
 char *Model1_twsk_slab_name;
 unsigned int Model1_twsk_obj_size;
 int (*Model1_twsk_unique)(struct Model1_sock *Model1_sk,
           struct Model1_sock *Model1_sktw, void *Model1_twp);
 void (*Model1_twsk_destructor)(struct Model1_sock *Model1_sk);
};

static inline __attribute__((no_instrument_function)) int Model1_twsk_unique(struct Model1_sock *Model1_sk, struct Model1_sock *Model1_sktw, void *Model1_twp)
{
 if (Model1_sk->Model1___sk_common.Model1_skc_prot->Model1_twsk_prot->Model1_twsk_unique != ((void *)0))
  return Model1_sk->Model1___sk_common.Model1_skc_prot->Model1_twsk_prot->Model1_twsk_unique(Model1_sk, Model1_sktw, Model1_twp);
 return 0;
}

static inline __attribute__((no_instrument_function)) void Model1_twsk_destructor(struct Model1_sock *Model1_sk)
{
 if (Model1_sk->Model1___sk_common.Model1_skc_prot->Model1_twsk_prot->Model1_twsk_destructor != ((void *)0))
  Model1_sk->Model1___sk_common.Model1_skc_prot->Model1_twsk_prot->Model1_twsk_destructor(Model1_sk);
}



struct Model1_inet_hashinfo;

struct Model1_inet_timewait_death_row {
 Model1_atomic_t Model1_tw_count;

 struct Model1_inet_hashinfo *Model1_hashinfo __attribute__((__aligned__((1 << (6)))));
 int Model1_sysctl_tw_recycle;
 int Model1_sysctl_max_tw_buckets;
};

struct Model1_inet_bind_bucket;

/*
 * This is a TIME_WAIT sock. It works around the memory consumption
 * problems of sockets in such a state on heavily loaded servers, but
 * without violating the protocol specification.
 */
struct Model1_inet_timewait_sock {
 /*
	 * Now struct sock also uses sock_common, so please just
	 * don't add nothing before this first member (__tw_common) --acme
	 */
 struct Model1_sock_common Model1___tw_common;
 int Model1_tw_timeout;
 volatile unsigned char Model1_tw_substate;
 unsigned char Model1_tw_rcv_wscale;

 /* Socket demultiplex comparisons on incoming packets. */
 /* these three are in inet_sock */
 Model1___be16 Model1_tw_sport;
                                ;
 /* And these are ours. */
 unsigned int Model1_tw_kill : 1,
    Model1_tw_transparent : 1,
    Model1_tw_flowlabel : 20,
    Model1_tw_pad : 2, /* 2 bits hole */
    Model1_tw_tos : 8;
                              ;
 struct Model1_timer_list Model1_tw_timer;
 struct Model1_inet_bind_bucket *Model1_tw_tb;
};


static inline __attribute__((no_instrument_function)) struct Model1_inet_timewait_sock *Model1_inet_twsk(const struct Model1_sock *Model1_sk)
{
 return (struct Model1_inet_timewait_sock *)Model1_sk;
}

void Model1_inet_twsk_free(struct Model1_inet_timewait_sock *Model1_tw);
void Model1_inet_twsk_put(struct Model1_inet_timewait_sock *Model1_tw);

void Model1_inet_twsk_bind_unhash(struct Model1_inet_timewait_sock *Model1_tw,
      struct Model1_inet_hashinfo *Model1_hashinfo);

struct Model1_inet_timewait_sock *Model1_inet_twsk_alloc(const struct Model1_sock *Model1_sk,
        struct Model1_inet_timewait_death_row *Model1_dr,
        const int Model1_state);

void Model1___inet_twsk_hashdance(struct Model1_inet_timewait_sock *Model1_tw, struct Model1_sock *Model1_sk,
      struct Model1_inet_hashinfo *Model1_hashinfo);

void Model1___inet_twsk_schedule(struct Model1_inet_timewait_sock *Model1_tw, int Model1_timeo,
     bool Model1_rearm);

static inline __attribute__((no_instrument_function)) void Model1_inet_twsk_schedule(struct Model1_inet_timewait_sock *Model1_tw, int Model1_timeo)
{
 Model1___inet_twsk_schedule(Model1_tw, Model1_timeo, false);
}

static inline __attribute__((no_instrument_function)) void Model1_inet_twsk_reschedule(struct Model1_inet_timewait_sock *Model1_tw, int Model1_timeo)
{
 Model1___inet_twsk_schedule(Model1_tw, Model1_timeo, true);
}

void Model1_inet_twsk_deschedule_put(struct Model1_inet_timewait_sock *Model1_tw);

void Model1_inet_twsk_purge(struct Model1_inet_hashinfo *Model1_hashinfo,
       struct Model1_inet_timewait_death_row *Model1_twdr, int Model1_family);

static inline __attribute__((no_instrument_function))
struct Model1_net *Model1_twsk_net(const struct Model1_inet_timewait_sock *Model1_twsk)
{
 return Model1_read_pnet(&Model1_twsk->Model1___tw_common.Model1_skc_net);
}

static inline __attribute__((no_instrument_function))
void Model1_twsk_net_set(struct Model1_inet_timewait_sock *Model1_twsk, struct Model1_net *Model1_net)
{
 Model1_write_pnet(&Model1_twsk->Model1___tw_common.Model1_skc_net, Model1_net);
}
/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for the TCP protocol.
 *
 * Version:	@(#)tcp.h	1.0.2	04/28/93
 *
 * Author:	Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */







struct Model1_tcphdr {
 Model1___be16 Model1_source;
 Model1___be16 Model1_dest;
 Model1___be32 Model1_seq;
 Model1___be32 Model1_ack_seq;

 Model1___u16 Model1_res1:4,
  Model1_doff:4,
  Model1_fin:1,
  Model1_syn:1,
  Model1_rst:1,
  Model1_psh:1,
  Model1_ack:1,
  Model1_urg:1,
  Model1_ece:1,
  Model1_cwr:1;
 Model1___be16 Model1_window;
 Model1___sum16 Model1_check;
 Model1___be16 Model1_urg_ptr;
};

/*
 *	The union cast uses a gcc extension to avoid aliasing problems
 *  (union is compatible to any of its members)
 *  This means this part of the code is -fstrict-aliasing safe now.
 */
union Model1_tcp_word_hdr {
 struct Model1_tcphdr Model1_hdr;
 Model1___be32 Model1_words[5];
};



enum {
 Model1_TCP_FLAG_CWR = (( Model1___be32)((__u32)( (((__u32)((0x00800000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x00800000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x00800000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x00800000)) & (__u32)0xff000000UL) >> 24)))),
 Model1_TCP_FLAG_ECE = (( Model1___be32)((__u32)( (((__u32)((0x00400000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x00400000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x00400000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x00400000)) & (__u32)0xff000000UL) >> 24)))),
 Model1_TCP_FLAG_URG = (( Model1___be32)((__u32)( (((__u32)((0x00200000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x00200000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x00200000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x00200000)) & (__u32)0xff000000UL) >> 24)))),
 Model1_TCP_FLAG_ACK = (( Model1___be32)((__u32)( (((__u32)((0x00100000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x00100000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x00100000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x00100000)) & (__u32)0xff000000UL) >> 24)))),
 Model1_TCP_FLAG_PSH = (( Model1___be32)((__u32)( (((__u32)((0x00080000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x00080000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x00080000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x00080000)) & (__u32)0xff000000UL) >> 24)))),
 Model1_TCP_FLAG_RST = (( Model1___be32)((__u32)( (((__u32)((0x00040000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x00040000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x00040000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x00040000)) & (__u32)0xff000000UL) >> 24)))),
 Model1_TCP_FLAG_SYN = (( Model1___be32)((__u32)( (((__u32)((0x00020000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x00020000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x00020000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x00020000)) & (__u32)0xff000000UL) >> 24)))),
 Model1_TCP_FLAG_FIN = (( Model1___be32)((__u32)( (((__u32)((0x00010000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x00010000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x00010000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x00010000)) & (__u32)0xff000000UL) >> 24)))),
 Model1_TCP_RESERVED_BITS = (( Model1___be32)((__u32)( (((__u32)((0x0F000000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x0F000000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x0F000000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x0F000000)) & (__u32)0xff000000UL) >> 24)))),
 Model1_TCP_DATA_OFFSET = (( Model1___be32)((__u32)( (((__u32)((0xF0000000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xF0000000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xF0000000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xF0000000)) & (__u32)0xff000000UL) >> 24))))
};

/*
 * TCP general constants
 */



/* TCP socket options */
struct Model1_tcp_repair_opt {
 __u32 Model1_opt_code;
 __u32 Model1_opt_val;
};

struct Model1_tcp_repair_window {
 __u32 Model1_snd_wl1;
 __u32 Model1_snd_wnd;
 __u32 Model1_max_window;

 __u32 Model1_rcv_wnd;
 __u32 Model1_rcv_wup;
};

enum {
 Model1_TCP_NO_QUEUE,
 Model1_TCP_RECV_QUEUE,
 Model1_TCP_SEND_QUEUE,
 Model1_TCP_QUEUES_NR,
};

/* for TCP_INFO socket option */







enum Model1_tcp_ca_state {
 Model1_TCP_CA_Open = 0,

 Model1_TCP_CA_Disorder = 1,

 Model1_TCP_CA_CWR = 2,

 Model1_TCP_CA_Recovery = 3,

 Model1_TCP_CA_Loss = 4

};

struct Model1_tcp_info {
 __u8 Model1_tcpi_state;
 __u8 Model1_tcpi_ca_state;
 __u8 Model1_tcpi_retransmits;
 __u8 Model1_tcpi_probes;
 __u8 Model1_tcpi_backoff;
 __u8 Model1_tcpi_options;
 __u8 Model1_tcpi_snd_wscale : 4, Model1_tcpi_rcv_wscale : 4;

 __u32 Model1_tcpi_rto;
 __u32 Model1_tcpi_ato;
 __u32 Model1_tcpi_snd_mss;
 __u32 Model1_tcpi_rcv_mss;

 __u32 Model1_tcpi_unacked;
 __u32 Model1_tcpi_sacked;
 __u32 Model1_tcpi_lost;
 __u32 Model1_tcpi_retrans;
 __u32 Model1_tcpi_fackets;

 /* Times. */
 __u32 Model1_tcpi_last_data_sent;
 __u32 Model1_tcpi_last_ack_sent; /* Not remembered, sorry. */
 __u32 Model1_tcpi_last_data_recv;
 __u32 Model1_tcpi_last_ack_recv;

 /* Metrics. */
 __u32 Model1_tcpi_pmtu;
 __u32 Model1_tcpi_rcv_ssthresh;
 __u32 Model1_tcpi_rtt;
 __u32 Model1_tcpi_rttvar;
 __u32 Model1_tcpi_snd_ssthresh;
 __u32 Model1_tcpi_snd_cwnd;
 __u32 Model1_tcpi_advmss;
 __u32 Model1_tcpi_reordering;

 __u32 Model1_tcpi_rcv_rtt;
 __u32 Model1_tcpi_rcv_space;

 __u32 Model1_tcpi_total_retrans;

 __u64 Model1_tcpi_pacing_rate;
 __u64 Model1_tcpi_max_pacing_rate;
 __u64 Model1_tcpi_bytes_acked; /* RFC4898 tcpEStatsAppHCThruOctetsAcked */
 __u64 Model1_tcpi_bytes_received; /* RFC4898 tcpEStatsAppHCThruOctetsReceived */
 __u32 Model1_tcpi_segs_out; /* RFC4898 tcpEStatsPerfSegsOut */
 __u32 Model1_tcpi_segs_in; /* RFC4898 tcpEStatsPerfSegsIn */

 __u32 Model1_tcpi_notsent_bytes;
 __u32 Model1_tcpi_min_rtt;
 __u32 Model1_tcpi_data_segs_in; /* RFC4898 tcpEStatsDataSegsIn */
 __u32 Model1_tcpi_data_segs_out; /* RFC4898 tcpEStatsDataSegsOut */
};

/* for TCP_MD5SIG socket option */


struct Model1_tcp_md5sig {
 struct Model1___kernel_sockaddr_storage Model1_tcpm_addr; /* address associated */
 Model1___u16 Model1___tcpm_pad1; /* zero */
 Model1___u16 Model1_tcpm_keylen; /* key length */
 __u32 Model1___tcpm_pad2; /* zero */
 __u8 Model1_tcpm_key[80]; /* key (binary) */
};

static inline __attribute__((no_instrument_function)) struct Model1_tcphdr *Model1_tcp_hdr(const struct Model1_sk_buff *Model1_skb)
{
 return (struct Model1_tcphdr *)Model1_skb_transport_header(Model1_skb);
}

static inline __attribute__((no_instrument_function)) unsigned int Model1___tcp_hdrlen(const struct Model1_tcphdr *Model1_th)
{
 return Model1_th->Model1_doff * 4;
}

static inline __attribute__((no_instrument_function)) unsigned int Model1_tcp_hdrlen(const struct Model1_sk_buff *Model1_skb)
{
 return Model1___tcp_hdrlen(Model1_tcp_hdr(Model1_skb));
}

static inline __attribute__((no_instrument_function)) struct Model1_tcphdr *Model1_inner_tcp_hdr(const struct Model1_sk_buff *Model1_skb)
{
 return (struct Model1_tcphdr *)Model1_skb_inner_transport_header(Model1_skb);
}

static inline __attribute__((no_instrument_function)) unsigned int Model1_inner_tcp_hdrlen(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_inner_tcp_hdr(Model1_skb)->Model1_doff * 4;
}

static inline __attribute__((no_instrument_function)) unsigned int Model1_tcp_optlen(const struct Model1_sk_buff *Model1_skb)
{
 return (Model1_tcp_hdr(Model1_skb)->Model1_doff - 5) * 4;
}

/* TCP Fast Open */




/* TCP Fast Open Cookie as stored in memory */
struct Model1_tcp_fastopen_cookie {
 Model1_s8 Model1_len;
 Model1_u8 Model1_val[16];
 bool Model1_exp; /* In RFC6994 experimental option format */
};

/* This defines a selective acknowledgement block. */
struct Model1_tcp_sack_block_wire {
 Model1___be32 Model1_start_seq;
 Model1___be32 Model1_end_seq;
};

struct Model1_tcp_sack_block {
 Model1_u32 Model1_start_seq;
 Model1_u32 Model1_end_seq;
};

/*These are used to set the sack_ok field in struct tcp_options_received */




struct Model1_tcp_options_received {
/*	PAWS/RTTM data	*/
 long Model1_ts_recent_stamp;/* Time we stored ts_recent (for aging) */
 Model1_u32 Model1_ts_recent; /* Time stamp to echo next		*/
 Model1_u32 Model1_rcv_tsval; /* Time stamp value             	*/
 Model1_u32 Model1_rcv_tsecr; /* Time stamp echo reply        	*/
 Model1_u16 Model1_saw_tstamp : 1, /* Saw TIMESTAMP on last packet		*/
  Model1_tstamp_ok : 1, /* TIMESTAMP seen on SYN packet		*/
  Model1_dsack : 1, /* D-SACK is scheduled			*/
  Model1_wscale_ok : 1, /* Wscale seen on SYN packet		*/
  Model1_sack_ok : 4, /* SACK seen on SYN packet		*/
  Model1_snd_wscale : 4, /* Window scaling received from sender	*/
  Model1_rcv_wscale : 4; /* Window scaling to send to receiver	*/
 Model1_u8 Model1_num_sacks; /* Number of SACK blocks		*/
 Model1_u16 Model1_user_mss; /* mss requested by user in ioctl	*/
 Model1_u16 Model1_mss_clamp; /* Maximal mss, negotiated at connection setup */
};

static inline __attribute__((no_instrument_function)) void Model1_tcp_clear_options(struct Model1_tcp_options_received *Model1_rx_opt)
{
 Model1_rx_opt->Model1_tstamp_ok = Model1_rx_opt->Model1_sack_ok = 0;
 Model1_rx_opt->Model1_wscale_ok = Model1_rx_opt->Model1_snd_wscale = 0;
}

/* This is the max number of SACKS that we'll generate and process. It's safe
 * to increase this, although since:
 *   size = TCPOLEN_SACK_BASE_ALIGNED (4) + n * TCPOLEN_SACK_PERBLOCK (8)
 * only four options will fit in a standard TCP header */


struct Model1_tcp_request_sock_ops;

struct Model1_tcp_request_sock {
 struct Model1_inet_request_sock Model1_req;
 const struct Model1_tcp_request_sock_ops *Model1_af_specific;
 struct Model1_skb_mstamp Model1_snt_synack; /* first SYNACK sent time */
 bool Model1_tfo_listener;
 Model1_u32 Model1_txhash;
 Model1_u32 Model1_rcv_isn;
 Model1_u32 Model1_snt_isn;
 Model1_u32 Model1_last_oow_ack_time; /* last SYNACK */
 Model1_u32 Model1_rcv_nxt; /* the ack # by SYNACK. For
						  * FastOpen it's the seq#
						  * after data-in-SYN.
						  */
};

static inline __attribute__((no_instrument_function)) struct Model1_tcp_request_sock *Model1_tcp_rsk(const struct Model1_request_sock *Model1_req)
{
 return (struct Model1_tcp_request_sock *)Model1_req;
}

struct Model1_tcp_sock {
 /* inet_connection_sock has to be the first member of tcp_sock */
 struct Model1_inet_connection_sock Model1_inet_conn;
 Model1_u16 Model1_tcp_header_len; /* Bytes of tcp header to send		*/
 Model1_u16 Model1_gso_segs; /* Max number of segs per GSO packet	*/

/*
 *	Header prediction flags
 *	0x5?10 << 16 + snd_wnd in net byte order
 */
 Model1___be32 Model1_pred_flags;

/*
 *	RFC793 variables by their proper names. This means you can
 *	read the code and the spec side by side (and laugh ...)
 *	See RFC793 and RFC1122. The RFC writes these in capitals.
 */
 Model1_u64 Model1_bytes_received; /* RFC4898 tcpEStatsAppHCThruOctetsReceived
				 * sum(delta(rcv_nxt)), or how many bytes
				 * were acked.
				 */
 Model1_u32 Model1_segs_in; /* RFC4898 tcpEStatsPerfSegsIn
				 * total number of segments in.
				 */
 Model1_u32 Model1_data_segs_in; /* RFC4898 tcpEStatsPerfDataSegsIn
				 * total number of data segments in.
				 */
  Model1_u32 Model1_rcv_nxt; /* What we want to receive next 	*/
 Model1_u32 Model1_copied_seq; /* Head of yet unread data		*/
 Model1_u32 Model1_rcv_wup; /* rcv_nxt on last window update sent	*/
  Model1_u32 Model1_snd_nxt; /* Next sequence we send		*/
 Model1_u32 Model1_segs_out; /* RFC4898 tcpEStatsPerfSegsOut
				 * The total number of segments sent.
				 */
 Model1_u32 Model1_data_segs_out; /* RFC4898 tcpEStatsPerfDataSegsOut
				 * total number of data segments sent.
				 */
 Model1_u64 Model1_bytes_acked; /* RFC4898 tcpEStatsAppHCThruOctetsAcked
				 * sum(delta(snd_una)), or how many bytes
				 * were acked.
				 */
 struct Model1_u64_stats_sync Model1_syncp; /* protects 64bit vars (cf tcp_get_info()) */

  Model1_u32 Model1_snd_una; /* First byte we want an ack for	*/
  Model1_u32 Model1_snd_sml; /* Last byte of the most recently transmitted small packet */
 Model1_u32 Model1_rcv_tstamp; /* timestamp of last received ACK (for keepalives) */
 Model1_u32 Model1_lsndtime; /* timestamp of last sent data packet (for restart window) */
 Model1_u32 Model1_last_oow_ack_time; /* timestamp of last out-of-window ACK */

 Model1_u32 Model1_tsoffset; /* timestamp offset */

 struct Model1_list_head Model1_tsq_node; /* anchor in tsq_tasklet.head list */
 unsigned long Model1_tsq_flags;

 /* Data for direct copy to user */
 struct {
  struct Model1_sk_buff_head Model1_prequeue;
  struct Model1_task_struct *Model1_task;
  struct Model1_msghdr *Model1_msg;
  int Model1_memory;
  int Model1_len;
 } Model1_ucopy;

 Model1_u32 Model1_snd_wl1; /* Sequence for window update		*/
 Model1_u32 Model1_snd_wnd; /* The window we expect to receive	*/
 Model1_u32 Model1_max_window; /* Maximal window ever seen from peer	*/
 Model1_u32 Model1_mss_cache; /* Cached effective mss, not including SACKS */

 Model1_u32 Model1_window_clamp; /* Maximal window to advertise		*/
 Model1_u32 Model1_rcv_ssthresh; /* Current window clamp			*/

 /* Information of the most recently (s)acked skb */
 struct Model1_tcp_rack {
  struct Model1_skb_mstamp Model1_mstamp; /* (Re)sent time of the skb */
  Model1_u8 Model1_advanced; /* mstamp advanced since last lost marking */
  Model1_u8 Model1_reord; /* reordering detected */
 } Model1_rack;
 Model1_u16 Model1_advmss; /* Advertised MSS			*/
 Model1_u8 unused;
 Model1_u8 Model1_nonagle : 4,/* Disable Nagle algorithm?             */
  Model1_thin_lto : 1,/* Use linear timeouts for thin streams */
  Model1_thin_dupack : 1,/* Fast retransmit on first dupack      */
  Model1_repair : 1,
  Model1_frto : 1;/* F-RTO (RFC5682) activated in CA_Loss */
 Model1_u8 Model1_repair_queue;
 Model1_u8 Model1_do_early_retrans:1,/* Enable RFC5827 early-retransmit  */
  Model1_syn_data:1, /* SYN includes data */
  Model1_syn_fastopen:1, /* SYN includes Fast Open option */
  Model1_syn_fastopen_exp:1,/* SYN includes Fast Open exp. option */
  Model1_syn_data_acked:1,/* data in SYN is acked by SYN-ACK */
  Model1_save_syn:1, /* Save headers of SYN packet */
  Model1_is_cwnd_limited:1;/* forward progress limited by snd_cwnd? */
 Model1_u32 Model1_tlp_high_seq; /* snd_nxt at the time of TLP retransmit. */

/* RTT measurement */
 Model1_u32 Model1_srtt_us; /* smoothed round trip time << 3 in usecs */
 Model1_u32 Model1_mdev_us; /* medium deviation			*/
 Model1_u32 Model1_mdev_max_us; /* maximal mdev for the last rtt period	*/
 Model1_u32 Model1_rttvar_us; /* smoothed mdev_max			*/
 Model1_u32 Model1_rtt_seq; /* sequence number to update rttvar	*/
 struct Model1_rtt_meas {
  Model1_u32 Model1_rtt, Model1_ts; /* RTT in usec and sampling time in jiffies. */
 } Model1_rtt_min[3];

 Model1_u32 Model1_packets_out; /* Packets which are "in flight"	*/
 Model1_u32 Model1_retrans_out; /* Retransmitted packets out		*/
 Model1_u32 Model1_max_packets_out; /* max packets_out in last window */
 Model1_u32 Model1_max_packets_seq; /* right edge of max_packets_out flight */

 Model1_u16 Model1_urg_data; /* Saved octet of OOB data and control flags */
 Model1_u8 Model1_ecn_flags; /* ECN status bits.			*/
 Model1_u8 Model1_keepalive_probes; /* num of allowed keep alive probes	*/
 Model1_u32 Model1_reordering; /* Packet reordering metric.		*/
 Model1_u32 Model1_snd_up; /* Urgent pointer		*/

/*
 *      Options received (usually on last packet, some only on SYN packets).
 */
 struct Model1_tcp_options_received Model1_rx_opt;

/*
 *	Slow start and congestion control (see also Nagle, and Karn & Partridge)
 */
  Model1_u32 Model1_snd_ssthresh; /* Slow start size threshold		*/
  Model1_u32 Model1_snd_cwnd; /* Sending congestion window		*/
 Model1_u32 Model1_snd_cwnd_cnt; /* Linear increase counter		*/
 Model1_u32 Model1_snd_cwnd_clamp; /* Do not allow snd_cwnd to grow above this */
 Model1_u32 Model1_snd_cwnd_used;
 Model1_u32 Model1_snd_cwnd_stamp;
 Model1_u32 Model1_prior_cwnd; /* Congestion window at start of Recovery. */
 Model1_u32 Model1_prr_delivered; /* Number of newly delivered packets to
				 * receiver in Recovery. */
 Model1_u32 Model1_prr_out; /* Total number of pkts sent during Recovery. */
 Model1_u32 Model1_delivered; /* Total data packets delivered incl. rexmits */

  Model1_u32 Model1_rcv_wnd; /* Current receiver window		*/
 Model1_u32 Model1_write_seq; /* Tail(+1) of data held in tcp send buffer */
 Model1_u32 Model1_notsent_lowat; /* TCP_NOTSENT_LOWAT */
 Model1_u32 Model1_pushed_seq; /* Last pushed seq, required to talk to windows */
 Model1_u32 Model1_lost_out; /* Lost packets			*/
 Model1_u32 Model1_sacked_out; /* SACK'd packets			*/
 Model1_u32 Model1_fackets_out; /* FACK'd packets			*/

 /* from STCP, retrans queue hinting */
 struct Model1_sk_buff* Model1_lost_skb_hint;
 struct Model1_sk_buff *Model1_retransmit_skb_hint;

 /* OOO segments go in this list. Note that socket lock must be held,
	 * as we do not use sk_buff_head lock.
	 */
 struct Model1_sk_buff_head Model1_out_of_order_queue;

 /* SACKs data, these 2 need to be together (see tcp_options_write) */
 struct Model1_tcp_sack_block Model1_duplicate_sack[1]; /* D-SACK block */
 struct Model1_tcp_sack_block Model1_selective_acks[4]; /* The SACKS themselves*/

 struct Model1_tcp_sack_block Model1_recv_sack_cache[4];

 struct Model1_sk_buff *Model1_highest_sack; /* skb just after the highest
					 * skb with SACKed bit set
					 * (validity guaranteed only if
					 * sacked_out > 0)
					 */

 int Model1_lost_cnt_hint;
 Model1_u32 Model1_retransmit_high; /* L-bits may be on up to this seqno */

 Model1_u32 Model1_prior_ssthresh; /* ssthresh saved at recovery start	*/
 Model1_u32 Model1_high_seq; /* snd_nxt at onset of congestion	*/

 Model1_u32 Model1_retrans_stamp; /* Timestamp of the last retransmit,
				 * also used in SYN-SENT to remember stamp of
				 * the first SYN. */
 Model1_u32 Model1_undo_marker; /* snd_una upon a new recovery episode. */
 int Model1_undo_retrans; /* number of undoable retransmissions. */
 Model1_u32 Model1_total_retrans; /* Total retransmits for entire connection */

 Model1_u32 Model1_urg_seq; /* Seq of received urgent pointer */
 unsigned int Model1_keepalive_time; /* time before keep alive takes place */
 unsigned int Model1_keepalive_intvl; /* time interval between keep alive probes */

 int Model1_linger2;

/* Receiver side RTT estimation */
 struct {
  Model1_u32 Model1_rtt;
  Model1_u32 Model1_seq;
  Model1_u32 Model1_time;
 } Model1_rcv_rtt_est;

/* Receiver queue space */
 struct {
  int Model1_space;
  Model1_u32 Model1_seq;
  Model1_u32 Model1_time;
 } Model1_rcvq_space;

/* TCP-specific MTU probe information. */
 struct {
  Model1_u32 Model1_probe_seq_start;
  Model1_u32 Model1_probe_seq_end;
 } Model1_mtu_probe;
 Model1_u32 Model1_mtu_info; /* We received an ICMP_FRAG_NEEDED / ICMPV6_PKT_TOOBIG
			   * while socket was owned by user.
			   */


/* TCP AF-Specific parts; only used by MD5 Signature support so far */
 const struct Model1_tcp_sock_af_ops *Model1_af_specific;

/* TCP MD5 Signature Option information */
 struct Model1_tcp_md5sig_info *Model1_md5sig_info;


/* TCP fastopen related information */
 struct Model1_tcp_fastopen_request *Model1_fastopen_req;
 /* fastopen_rsk points to request_sock that resulted in this big
	 * socket. Used to retransmit SYNACKs etc.
	 */
 struct Model1_request_sock *Model1_fastopen_rsk;
 Model1_u32 *Model1_saved_syn;
};

enum Model1_tsq_flags {
 Model1_TSQ_THROTTLED,
 Model1_TSQ_QUEUED,
 Model1_TCP_TSQ_DEFERRED, /* tcp_tasklet_func() found socket was owned */
 Model1_TCP_WRITE_TIMER_DEFERRED, /* tcp_write_timer() found socket was owned */
 Model1_TCP_DELACK_TIMER_DEFERRED, /* tcp_delack_timer() found socket was owned */
 Model1_TCP_MTU_REDUCED_DEFERRED, /* tcp_v{4|6}_err() could not call
				    * tcp_v{4|6}_mtu_reduced()
				    */
};

static inline __attribute__((no_instrument_function)) struct Model1_tcp_sock *Model1_tcp_sk(const struct Model1_sock *Model1_sk)
{
 return (struct Model1_tcp_sock *)Model1_sk;
}

struct Model1_tcp_timewait_sock {
 struct Model1_inet_timewait_sock Model1_tw_sk;


 Model1_u32 Model1_tw_rcv_wnd;
 Model1_u32 Model1_tw_ts_offset;
 Model1_u32 Model1_tw_ts_recent;

 /* The time we sent the last out-of-window ACK: */
 Model1_u32 Model1_tw_last_oow_ack_time;

 long Model1_tw_ts_recent_stamp;

 struct Model1_tcp_md5sig_key *Model1_tw_md5_key;

};

static inline __attribute__((no_instrument_function)) struct Model1_tcp_timewait_sock *Model1_tcp_twsk(const struct Model1_sock *Model1_sk)
{
 return (struct Model1_tcp_timewait_sock *)Model1_sk;
}

static inline __attribute__((no_instrument_function)) bool Model1_tcp_passive_fastopen(const struct Model1_sock *Model1_sk)
{
 return (Model1_sk->Model1___sk_common.Model1_skc_state == Model1_TCP_SYN_RECV &&
  Model1_tcp_sk(Model1_sk)->Model1_fastopen_rsk != ((void *)0));
}

static inline __attribute__((no_instrument_function)) void Model1_fastopen_queue_tune(struct Model1_sock *Model1_sk, int Model1_backlog)
{
 struct Model1_request_sock_queue *Model1_queue = &Model1_inet_csk(Model1_sk)->Model1_icsk_accept_queue;
 int Model1_somaxconn = ({ union { typeof(Model1_sock_net(Model1_sk)->Model1_core.Model1_sysctl_somaxconn) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&(Model1_sock_net(Model1_sk)->Model1_core.Model1_sysctl_somaxconn), Model1___u.Model1___c, sizeof(Model1_sock_net(Model1_sk)->Model1_core.Model1_sysctl_somaxconn)); else Model1___read_once_size_nocheck(&(Model1_sock_net(Model1_sk)->Model1_core.Model1_sysctl_somaxconn), Model1___u.Model1___c, sizeof(Model1_sock_net(Model1_sk)->Model1_core.Model1_sysctl_somaxconn)); Model1___u.Model1___val; });

 Model1_queue->Model1_fastopenq.Model1_max_qlen = ({ unsigned int Model1___min1 = (Model1_backlog); unsigned int Model1___min2 = (Model1_somaxconn); Model1___min1 < Model1___min2 ? Model1___min1: Model1___min2; });
}

static inline __attribute__((no_instrument_function)) void Model1_tcp_move_syn(struct Model1_tcp_sock *Model1_tp,
    struct Model1_request_sock *Model1_req)
{
 Model1_tp->Model1_saved_syn = Model1_req->Model1_saved_syn;
 Model1_req->Model1_saved_syn = ((void *)0);
}

static inline __attribute__((no_instrument_function)) void Model1_tcp_saved_syn_free(struct Model1_tcp_sock *Model1_tp)
{
 Model1_kfree(Model1_tp->Model1_saved_syn);
 Model1_tp->Model1_saved_syn = ((void *)0);
}





void Model1_sha_init(__u32 *Model1_buf);
void Model1_sha_transform(__u32 *Model1_digest, const char *Model1_data, __u32 *Model1_W);




void Model1_md5_transform(__u32 *Model1_hash, __u32 const *Model1_in);

__u32 Model1_half_md4_transform(__u32 Model1_buf[4], __u32 const Model1_in[8]);





/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the BSD Socket
 *		interface as the means of communication with the user level.
 *
 * Authors:	Lotsa people, from code originally in tcp
 *
 *	This program is free software; you can redistribute it and/or
 *      modify it under the terms of the GNU General Public License
 *      as published by the Free Software Foundation; either version
 *      2 of the License, or (at your option) any later version.
 */







/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for the IP protocol.
 *
 * Version:	@(#)ip.h	1.0.2	04/28/93
 *
 * Authors:	Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */





/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for the IP protocol.
 *
 * Version:	@(#)ip.h	1.0.2	04/28/93
 *
 * Authors:	Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */
/* IP options */
struct Model1_iphdr {

 __u8 Model1_ihl:4,
  Model1_version:4;






 __u8 Model1_tos;
 Model1___be16 Model1_tot_len;
 Model1___be16 Model1_id;
 Model1___be16 Model1_frag_off;
 __u8 Model1_ttl;
 __u8 Model1_protocol;
 Model1___sum16 Model1_check;
 Model1___be32 Model1_saddr;
 Model1___be32 Model1_daddr;
 /*The options start here. */
};


struct Model1_ip_auth_hdr {
 __u8 Model1_nexthdr;
 __u8 Model1_hdrlen; /* This one is measured in 32 bit units! */
 Model1___be16 Model1_reserved;
 Model1___be32 Model1_spi;
 Model1___be32 Model1_seq_no; /* Sequence number */
 __u8 Model1_auth_data[0]; /* Variable len but >=4. Mind the 64 bit alignment! */
};

struct Model1_ip_esp_hdr {
 Model1___be32 Model1_spi;
 Model1___be32 Model1_seq_no; /* Sequence number */
 __u8 Model1_enc_data[0]; /* Variable len but >=8. Mind the 64 bit alignment! */
};

struct Model1_ip_comp_hdr {
 __u8 Model1_nexthdr;
 __u8 Model1_flags;
 Model1___be16 Model1_cpi;
};

struct Model1_ip_beet_phdr {
 __u8 Model1_nexthdr;
 __u8 Model1_hdrlen;
 __u8 Model1_padlen;
 __u8 Model1_reserved;
};

/* index values for the variables in ipv4_devconf */
enum
{
 Model1_IPV4_DEVCONF_FORWARDING=1,
 Model1_IPV4_DEVCONF_MC_FORWARDING,
 Model1_IPV4_DEVCONF_PROXY_ARP,
 Model1_IPV4_DEVCONF_ACCEPT_REDIRECTS,
 Model1_IPV4_DEVCONF_SECURE_REDIRECTS,
 Model1_IPV4_DEVCONF_SEND_REDIRECTS,
 Model1_IPV4_DEVCONF_SHARED_MEDIA,
 Model1_IPV4_DEVCONF_RP_FILTER,
 Model1_IPV4_DEVCONF_ACCEPT_SOURCE_ROUTE,
 Model1_IPV4_DEVCONF_BOOTP_RELAY,
 Model1_IPV4_DEVCONF_LOG_MARTIANS,
 Model1_IPV4_DEVCONF_TAG,
 Model1_IPV4_DEVCONF_ARPFILTER,
 Model1_IPV4_DEVCONF_MEDIUM_ID,
 Model1_IPV4_DEVCONF_NOXFRM,
 Model1_IPV4_DEVCONF_NOPOLICY,
 Model1_IPV4_DEVCONF_FORCE_IGMP_VERSION,
 Model1_IPV4_DEVCONF_ARP_ANNOUNCE,
 Model1_IPV4_DEVCONF_ARP_IGNORE,
 Model1_IPV4_DEVCONF_PROMOTE_SECONDARIES,
 Model1_IPV4_DEVCONF_ARP_ACCEPT,
 Model1_IPV4_DEVCONF_ARP_NOTIFY,
 Model1_IPV4_DEVCONF_ACCEPT_LOCAL,
 Model1_IPV4_DEVCONF_SRC_VMARK,
 Model1_IPV4_DEVCONF_PROXY_ARP_PVLAN,
 Model1_IPV4_DEVCONF_ROUTE_LOCALNET,
 Model1_IPV4_DEVCONF_IGMPV2_UNSOLICITED_REPORT_INTERVAL,
 Model1_IPV4_DEVCONF_IGMPV3_UNSOLICITED_REPORT_INTERVAL,
 Model1_IPV4_DEVCONF_IGNORE_ROUTES_WITH_LINKDOWN,
 Model1_IPV4_DEVCONF_DROP_UNICAST_IN_L2_MULTICAST,
 Model1_IPV4_DEVCONF_DROP_GRATUITOUS_ARP,
 Model1___IPV4_DEVCONF_MAX
};

static inline __attribute__((no_instrument_function)) struct Model1_iphdr *Model1_ip_hdr(const struct Model1_sk_buff *Model1_skb)
{
 return (struct Model1_iphdr *)Model1_skb_network_header(Model1_skb);
}

static inline __attribute__((no_instrument_function)) struct Model1_iphdr *Model1_inner_ip_hdr(const struct Model1_sk_buff *Model1_skb)
{
 return (struct Model1_iphdr *)Model1_skb_inner_network_header(Model1_skb);
}

static inline __attribute__((no_instrument_function)) struct Model1_iphdr *Model1_ipip_hdr(const struct Model1_sk_buff *Model1_skb)
{
 return (struct Model1_iphdr *)Model1_skb_transport_header(Model1_skb);
}











/* The latest drafts declared increase in minimal mtu up to 1280. */



/*
 *	Advanced API
 *	source interface/address selection, source routing, etc...
 *	*under construction*
 */


struct Model1_in6_pktinfo {
 struct Model1_in6_addr Model1_ipi6_addr;
 int Model1_ipi6_ifindex;
};



struct Model1_ip6_mtuinfo {
 struct Model1_sockaddr_in6 Model1_ip6m_addr;
 __u32 Model1_ip6m_mtu;
};


struct Model1_in6_ifreq {
 struct Model1_in6_addr Model1_ifr6_addr;
 __u32 Model1_ifr6_prefixlen;
 int Model1_ifr6_ifindex;
};





/*
 *	routing header
 */
struct Model1_ipv6_rt_hdr {
 __u8 Model1_nexthdr;
 __u8 Model1_hdrlen;
 __u8 Model1_type;
 __u8 Model1_segments_left;

 /*
	 *	type specific data
	 *	variable length field
	 */
};


struct Model1_ipv6_opt_hdr {
 __u8 Model1_nexthdr;
 __u8 Model1_hdrlen;
 /* 
	 * TLV encoded option data follows.
	 */
} __attribute__((packed)); /* required for some archs */




/* Router Alert option values (RFC2711) */


/*
 *	routing header type 0 (used in cmsghdr struct)
 */

struct Model1_rt0_hdr {
 struct Model1_ipv6_rt_hdr Model1_rt_hdr;
 __u32 Model1_reserved;
 struct Model1_in6_addr Model1_addr[0];


};

/*
 *	routing header type 2
 */

struct Model1_rt2_hdr {
 struct Model1_ipv6_rt_hdr Model1_rt_hdr;
 __u32 Model1_reserved;
 struct Model1_in6_addr Model1_addr;


};

/*
 *	home address option in destination options header
 */

struct Model1_ipv6_destopt_hao {
 __u8 Model1_type;
 __u8 Model1_length;
 struct Model1_in6_addr Model1_addr;
} __attribute__((packed));

/*
 *	IPv6 fixed header
 *
 *	BEWARE, it is incorrect. The first 4 bits of flow_lbl
 *	are glued to priority now, forming "class".
 */

struct Model1_ipv6hdr {

 __u8 Model1_priority:4,
    Model1_version:4;






 __u8 Model1_flow_lbl[3];

 Model1___be16 Model1_payload_len;
 __u8 Model1_nexthdr;
 __u8 Model1_hop_limit;

 struct Model1_in6_addr Model1_saddr;
 struct Model1_in6_addr Model1_daddr;
};


/* index values for the variables in ipv6_devconf */
enum {
 Model1_DEVCONF_FORWARDING = 0,
 Model1_DEVCONF_HOPLIMIT,
 Model1_DEVCONF_MTU6,
 Model1_DEVCONF_ACCEPT_RA,
 Model1_DEVCONF_ACCEPT_REDIRECTS,
 Model1_DEVCONF_AUTOCONF,
 Model1_DEVCONF_DAD_TRANSMITS,
 Model1_DEVCONF_RTR_SOLICITS,
 Model1_DEVCONF_RTR_SOLICIT_INTERVAL,
 Model1_DEVCONF_RTR_SOLICIT_DELAY,
 Model1_DEVCONF_USE_TEMPADDR,
 Model1_DEVCONF_TEMP_VALID_LFT,
 Model1_DEVCONF_TEMP_PREFERED_LFT,
 Model1_DEVCONF_REGEN_MAX_RETRY,
 Model1_DEVCONF_MAX_DESYNC_FACTOR,
 Model1_DEVCONF_MAX_ADDRESSES,
 Model1_DEVCONF_FORCE_MLD_VERSION,
 Model1_DEVCONF_ACCEPT_RA_DEFRTR,
 Model1_DEVCONF_ACCEPT_RA_PINFO,
 Model1_DEVCONF_ACCEPT_RA_RTR_PREF,
 Model1_DEVCONF_RTR_PROBE_INTERVAL,
 Model1_DEVCONF_ACCEPT_RA_RT_INFO_MAX_PLEN,
 Model1_DEVCONF_PROXY_NDP,
 Model1_DEVCONF_OPTIMISTIC_DAD,
 Model1_DEVCONF_ACCEPT_SOURCE_ROUTE,
 Model1_DEVCONF_MC_FORWARDING,
 Model1_DEVCONF_DISABLE_IPV6,
 Model1_DEVCONF_ACCEPT_DAD,
 Model1_DEVCONF_FORCE_TLLAO,
 Model1_DEVCONF_NDISC_NOTIFY,
 Model1_DEVCONF_MLDV1_UNSOLICITED_REPORT_INTERVAL,
 Model1_DEVCONF_MLDV2_UNSOLICITED_REPORT_INTERVAL,
 Model1_DEVCONF_SUPPRESS_FRAG_NDISC,
 Model1_DEVCONF_ACCEPT_RA_FROM_LOCAL,
 Model1_DEVCONF_USE_OPTIMISTIC,
 Model1_DEVCONF_ACCEPT_RA_MTU,
 Model1_DEVCONF_STABLE_SECRET,
 Model1_DEVCONF_USE_OIF_ADDRS_ONLY,
 Model1_DEVCONF_ACCEPT_RA_MIN_HOP_LIMIT,
 Model1_DEVCONF_IGNORE_ROUTES_WITH_LINKDOWN,
 Model1_DEVCONF_DROP_UNICAST_IN_L2_MULTICAST,
 Model1_DEVCONF_DROP_UNSOLICITED_NA,
 Model1_DEVCONF_KEEP_ADDR_ON_DOWN,
 Model1_DEVCONF_MAX
};



/*
 * This structure contains configuration options per IPv6 link.
 */
struct Model1_ipv6_devconf {
 Model1___s32 Model1_forwarding;
 Model1___s32 Model1_hop_limit;
 Model1___s32 Model1_mtu6;
 Model1___s32 Model1_accept_ra;
 Model1___s32 Model1_accept_redirects;
 Model1___s32 Model1_autoconf;
 Model1___s32 Model1_dad_transmits;
 Model1___s32 Model1_rtr_solicits;
 Model1___s32 Model1_rtr_solicit_interval;
 Model1___s32 Model1_rtr_solicit_delay;
 Model1___s32 Model1_force_mld_version;
 Model1___s32 Model1_mldv1_unsolicited_report_interval;
 Model1___s32 Model1_mldv2_unsolicited_report_interval;
 Model1___s32 Model1_use_tempaddr;
 Model1___s32 Model1_temp_valid_lft;
 Model1___s32 Model1_temp_prefered_lft;
 Model1___s32 Model1_regen_max_retry;
 Model1___s32 Model1_max_desync_factor;
 Model1___s32 Model1_max_addresses;
 Model1___s32 Model1_accept_ra_defrtr;
 Model1___s32 Model1_accept_ra_min_hop_limit;
 Model1___s32 Model1_accept_ra_pinfo;
 Model1___s32 Model1_ignore_routes_with_linkdown;







 Model1___s32 Model1_proxy_ndp;
 Model1___s32 Model1_accept_source_route;
 Model1___s32 Model1_accept_ra_from_local;







 Model1___s32 Model1_disable_ipv6;
 Model1___s32 Model1_drop_unicast_in_l2_multicast;
 Model1___s32 Model1_accept_dad;
 Model1___s32 Model1_force_tllao;
 Model1___s32 Model1_ndisc_notify;
 Model1___s32 Model1_suppress_frag_ndisc;
 Model1___s32 Model1_accept_ra_mtu;
 Model1___s32 Model1_drop_unsolicited_na;
 struct Model1_ipv6_stable_secret {
  bool Model1_initialized;
  struct Model1_in6_addr Model1_secret;
 } Model1_stable_secret;
 Model1___s32 Model1_use_oif_addrs_only;
 Model1___s32 Model1_keep_addr_on_down;

 struct Model1_ctl_table_header *Model1_sysctl_header;
};

struct Model1_ipv6_params {
 Model1___s32 Model1_disable_ipv6;
 Model1___s32 Model1_autoconf;
};
extern struct Model1_ipv6_params Model1_ipv6_defaults;











struct Model1_icmp6hdr {

 __u8 Model1_icmp6_type;
 __u8 Model1_icmp6_code;
 Model1___sum16 Model1_icmp6_cksum;


 union {
  Model1___be32 Model1_un_data32[1];
  Model1___be16 Model1_un_data16[2];
  __u8 Model1_un_data8[4];

  struct Model1_icmpv6_echo {
   Model1___be16 identifier;
   Model1___be16 Model1_sequence;
  } Model1_u_echo;

                struct Model1_icmpv6_nd_advt {

                        __u32 Model1_reserved:5,
                          Model1_override:1,
                          Model1_solicited:1,
                          Model1_router:1,
     Model1_reserved2:24;
                } Model1_u_nd_advt;

                struct Model1_icmpv6_nd_ra {
   __u8 Model1_hop_limit;

   __u8 Model1_reserved:3,
     Model1_router_pref:2,
     Model1_home_agent:1,
     Model1_other:1,
     Model1_managed:1;
   Model1___be16 Model1_rt_lifetime;
                } Model1_u_nd_ra;

 } Model1_icmp6_dataun;
};
/*
 *	Codes for Destination Unreachable
 */
/*
 *	Codes for Time Exceeded
 */



/*
 *	Codes for Parameter Problem
 */




/*
 *	constants for (set|get)sockopt
 */



/*
 *	ICMPV6 filter
 */






struct Model1_icmp6_filter {
 __u32 Model1_data[8];
};

/*
 *	Definitions for MLDv2
 */

static inline __attribute__((no_instrument_function)) struct Model1_icmp6hdr *Model1_icmp6_hdr(const struct Model1_sk_buff *Model1_skb)
{
 return (struct Model1_icmp6hdr *)Model1_skb_transport_header(Model1_skb);
}




extern void Model1_icmpv6_send(struct Model1_sk_buff *Model1_skb, Model1_u8 Model1_type, Model1_u8 Model1_code, __u32 Model1_info);

typedef void Model1_ip6_icmp_send_t(struct Model1_sk_buff *Model1_skb, Model1_u8 Model1_type, Model1_u8 Model1_code, __u32 Model1_info,
        const struct Model1_in6_addr *Model1_force_saddr);
extern int Model1_inet6_register_icmp_sender(Model1_ip6_icmp_send_t *Model1_fn);
extern int Model1_inet6_unregister_icmp_sender(Model1_ip6_icmp_send_t *Model1_fn);
int Model1_ip6_err_gen_icmpv6_unreach(struct Model1_sk_buff *Model1_skb, int Model1_nhs, int Model1_type,
          unsigned int Model1_data_len);
extern int Model1_icmpv6_init(void);
extern int Model1_icmpv6_err_convert(Model1_u8 Model1_type, Model1_u8 Model1_code,
          int *err);
extern void Model1_icmpv6_cleanup(void);
extern void Model1_icmpv6_param_prob(struct Model1_sk_buff *Model1_skb,
         Model1_u8 Model1_code, int Model1_pos);

struct Model1_flowi6;
struct Model1_in6_addr;
extern void Model1_icmpv6_flow_init(struct Model1_sock *Model1_sk,
        struct Model1_flowi6 *Model1_fl6,
        Model1_u8 Model1_type,
        const struct Model1_in6_addr *Model1_saddr,
        const struct Model1_in6_addr *Model1_daddr,
        int Model1_oif);

/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for the UDP protocol.
 *
 * Version:	@(#)udp.h	1.0.2	04/28/93
 *
 * Author:	Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */







/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for the UDP protocol.
 *
 * Version:	@(#)udp.h	1.0.2	04/28/93
 *
 * Author:	Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */





struct Model1_udphdr {
 Model1___be16 Model1_source;
 Model1___be16 Model1_dest;
 Model1___be16 Model1_len;
 Model1___sum16 Model1_check;
};

/* UDP socket options */





/* UDP encapsulation types */

static inline __attribute__((no_instrument_function)) struct Model1_udphdr *Model1_udp_hdr(const struct Model1_sk_buff *Model1_skb)
{
 return (struct Model1_udphdr *)Model1_skb_transport_header(Model1_skb);
}

static inline __attribute__((no_instrument_function)) struct Model1_udphdr *Model1_inner_udp_hdr(const struct Model1_sk_buff *Model1_skb)
{
 return (struct Model1_udphdr *)Model1_skb_inner_transport_header(Model1_skb);
}



static inline __attribute__((no_instrument_function)) Model1_u32 Model1_udp_hashfn(const struct Model1_net *Model1_net, Model1_u32 Model1_num, Model1_u32 Model1_mask)
{
 return (Model1_num + Model1_net_hash_mix(Model1_net)) & Model1_mask;
}

struct Model1_udp_sock {
 /* inet_sock has to be the first member */
 struct Model1_inet_sock Model1_inet;



 int Model1_pending; /* Any pending frames ? */
 unsigned int Model1_corkflag; /* Cork is required */
 __u8 Model1_encap_type; /* Is this an Encapsulation socket? */
 unsigned char Model1_no_check6_tx:1,/* Send zero UDP6 checksums on TX? */
    Model1_no_check6_rx:1;/* Allow zero UDP6 checksums on RX? */
 /*
	 * Following member retains the information to create a UDP header
	 * when the socket is uncorked.
	 */
 Model1___u16 Model1_len; /* total length of pending frames */
 /*
	 * Fields specific to UDP-Lite.
	 */
 Model1___u16 Model1_pcslen;
 Model1___u16 Model1_pcrlen;
/* indicator bits used by pcflag: */



 __u8 Model1_pcflag; /* marks socket as UDP-Lite if > 0    */
 __u8 unused[3];
 /*
	 * For encapsulation sockets.
	 */
 int (*Model1_encap_rcv)(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);
 void (*Model1_encap_destroy)(struct Model1_sock *Model1_sk);

 /* GRO functions for UDP socket */
 struct Model1_sk_buff ** (*Model1_gro_receive)(struct Model1_sock *Model1_sk,
            struct Model1_sk_buff **Model1_head,
            struct Model1_sk_buff *Model1_skb);
 int (*Model1_gro_complete)(struct Model1_sock *Model1_sk,
      struct Model1_sk_buff *Model1_skb,
      int Model1_nhoff);
};

static inline __attribute__((no_instrument_function)) struct Model1_udp_sock *Model1_udp_sk(const struct Model1_sock *Model1_sk)
{
 return (struct Model1_udp_sock *)Model1_sk;
}

static inline __attribute__((no_instrument_function)) void Model1_udp_set_no_check6_tx(struct Model1_sock *Model1_sk, bool Model1_val)
{
 Model1_udp_sk(Model1_sk)->Model1_no_check6_tx = Model1_val;
}

static inline __attribute__((no_instrument_function)) void Model1_udp_set_no_check6_rx(struct Model1_sock *Model1_sk, bool Model1_val)
{
 Model1_udp_sk(Model1_sk)->Model1_no_check6_rx = Model1_val;
}

static inline __attribute__((no_instrument_function)) bool Model1_udp_get_no_check6_tx(struct Model1_sock *Model1_sk)
{
 return Model1_udp_sk(Model1_sk)->Model1_no_check6_tx;
}

static inline __attribute__((no_instrument_function)) bool Model1_udp_get_no_check6_rx(struct Model1_sock *Model1_sk)
{
 return Model1_udp_sk(Model1_sk)->Model1_no_check6_rx;
}



static inline __attribute__((no_instrument_function)) struct Model1_ipv6hdr *Model1_ipv6_hdr(const struct Model1_sk_buff *Model1_skb)
{
 return (struct Model1_ipv6hdr *)Model1_skb_network_header(Model1_skb);
}

static inline __attribute__((no_instrument_function)) struct Model1_ipv6hdr *Model1_inner_ipv6_hdr(const struct Model1_sk_buff *Model1_skb)
{
 return (struct Model1_ipv6hdr *)Model1_skb_inner_network_header(Model1_skb);
}

static inline __attribute__((no_instrument_function)) struct Model1_ipv6hdr *Model1_ipipv6_hdr(const struct Model1_sk_buff *Model1_skb)
{
 return (struct Model1_ipv6hdr *)Model1_skb_transport_header(Model1_skb);
}

/* 
   This structure contains results of exthdrs parsing
   as offsets from skb->nh.
 */

struct Model1_inet6_skb_parm {
 int Model1_iif;
 Model1___be16 Model1_ra;
 Model1___u16 Model1_dst0;
 Model1___u16 Model1_srcrt;
 Model1___u16 Model1_dst1;
 Model1___u16 Model1_lastopt;
 Model1___u16 Model1_nhoff;
 Model1___u16 Model1_flags;



 Model1___u16 Model1_frag_max_size;
};







static inline __attribute__((no_instrument_function)) bool Model1_skb_l3mdev_slave(Model1___u16 Model1_flags)
{
 return false;
}





static inline __attribute__((no_instrument_function)) int Model1_inet6_iif(const struct Model1_sk_buff *Model1_skb)
{
 bool Model1_l3_slave = Model1_skb_l3mdev_slave(((struct Model1_inet6_skb_parm*)((Model1_skb)->Model1_cb))->Model1_flags);

 return Model1_l3_slave ? Model1_skb->Model1_skb_iif : ((struct Model1_inet6_skb_parm*)((Model1_skb)->Model1_cb))->Model1_iif;
}

struct Model1_tcp6_request_sock {
 struct Model1_tcp_request_sock Model1_tcp6rsk_tcp;
};

struct Model1_ipv6_mc_socklist;
struct Model1_ipv6_ac_socklist;
struct Model1_ipv6_fl_socklist;

struct Model1_inet6_cork {
 struct Model1_ipv6_txoptions *Model1_opt;
 Model1_u8 Model1_hop_limit;
 Model1_u8 Model1_tclass;
};

/**
 * struct ipv6_pinfo - ipv6 private area
 *
 * In the struct sock hierarchy (tcp6_sock, upd6_sock, etc)
 * this _must_ be the last member, so that inet6_sk_generic
 * is able to calculate its offset from the base struct sock
 * by using the struct proto->slab_obj_size member. -acme
 */
struct Model1_ipv6_pinfo {
 struct Model1_in6_addr Model1_saddr;
 struct Model1_in6_pktinfo Model1_sticky_pktinfo;
 const struct Model1_in6_addr *Model1_daddr_cache;




 Model1___be32 Model1_flow_label;
 __u32 Model1_frag_size;

 /*
	 * Packed in 16bits.
	 * Omit one shift by by putting the signed field at MSB.
	 */




 Model1___u16 Model1___unused_1:7;
 Model1___s16 Model1_hop_limit:9;
 Model1___u16 Model1_mc_loop:1,
    Model1___unused_2:6;
 Model1___s16 Model1_mcast_hops:9;

 int Model1_ucast_oif;
 int Model1_mcast_oif;

 /* pktoption flags */
 union {
  struct {
   Model1___u16 Model1_srcrt:1,
    Model1_osrcrt:1,
           Model1_rxinfo:1,
           Model1_rxoinfo:1,
    Model1_rxhlim:1,
    Model1_rxohlim:1,
    Model1_hopopts:1,
    Model1_ohopopts:1,
    Model1_dstopts:1,
    Model1_odstopts:1,
                                Model1_rxflow:1,
    Model1_rxtclass:1,
    Model1_rxpmtu:1,
    Model1_rxorigdstaddr:1;
    /* 2 bits hole */
  } Model1_bits;
  Model1___u16 Model1_all;
 } Model1_rxopt;

 /* sockopt flags */
 Model1___u16 Model1_recverr:1,
                         Model1_sndflow:1,
    Model1_repflow:1,
    Model1_pmtudisc:3,
    Model1_padding:1, /* 1 bit hole */
    Model1_srcprefs:3, /* 001: prefer temporary address
						 * 010: prefer public address
						 * 100: prefer care-of address
						 */
    Model1_dontfrag:1,
    Model1_autoflowlabel:1;
 __u8 Model1_min_hopcount;
 __u8 Model1_tclass;
 Model1___be32 Model1_rcv_flowinfo;

 __u32 Model1_dst_cookie;
 __u32 Model1_rx_dst_cookie;

 struct Model1_ipv6_mc_socklist *Model1_ipv6_mc_list;
 struct Model1_ipv6_ac_socklist *Model1_ipv6_ac_list;
 struct Model1_ipv6_fl_socklist *Model1_ipv6_fl_list;

 struct Model1_ipv6_txoptions *Model1_opt;
 struct Model1_sk_buff *Model1_pktoptions;
 struct Model1_sk_buff *Model1_rxpmtu;
 struct Model1_inet6_cork Model1_cork;
};

/* WARNING: don't change the layout of the members in {raw,udp,tcp}6_sock! */
struct Model1_raw6_sock {
 /* inet_sock has to be the first member of raw6_sock */
 struct Model1_inet_sock Model1_inet;
 __u32 Model1_checksum; /* perform checksum */
 __u32 Model1_offset; /* checksum offset  */
 struct Model1_icmp6_filter Model1_filter;
 __u32 Model1_ip6mr_table;
 /* ipv6_pinfo has to be the last member of raw6_sock, see inet6_sk_generic */
 struct Model1_ipv6_pinfo Model1_inet6;
};

struct Model1_udp6_sock {
 struct Model1_udp_sock Model1_udp;
 /* ipv6_pinfo has to be the last member of udp6_sock, see inet6_sk_generic */
 struct Model1_ipv6_pinfo Model1_inet6;
};

struct Model1_tcp6_sock {
 struct Model1_tcp_sock Model1_tcp;
 /* ipv6_pinfo has to be the last member of tcp6_sock, see inet6_sk_generic */
 struct Model1_ipv6_pinfo Model1_inet6;
};

extern int Model1_inet6_sk_rebuild_header(struct Model1_sock *Model1_sk);

struct Model1_tcp6_timewait_sock {
 struct Model1_tcp_timewait_sock Model1_tcp6tw_tcp;
};


bool Model1_ipv6_mod_enabled(void);

static inline __attribute__((no_instrument_function)) struct Model1_ipv6_pinfo *Model1_inet6_sk(const struct Model1_sock *Model1___sk)
{
 return Model1_sk_fullsock(Model1___sk) ? Model1_inet_sk(Model1___sk)->Model1_pinet6 : ((void *)0);
}

static inline __attribute__((no_instrument_function)) struct Model1_raw6_sock *Model1_raw6_sk(const struct Model1_sock *Model1_sk)
{
 return (struct Model1_raw6_sock *)Model1_sk;
}

static inline __attribute__((no_instrument_function)) void Model1_inet_sk_copy_descendant(struct Model1_sock *Model1_sk_to,
        const struct Model1_sock *Model1_sk_from)
{
 int Model1_ancestor_size = sizeof(struct Model1_inet_sock);

 if (Model1_sk_from->Model1___sk_common.Model1_skc_family == 10)
  Model1_ancestor_size += sizeof(struct Model1_ipv6_pinfo);

 Model1___inet_sk_copy_descendant(Model1_sk_to, Model1_sk_from, Model1_ancestor_size);
}






static inline __attribute__((no_instrument_function)) const struct Model1_in6_addr *Model1_inet6_rcv_saddr(const struct Model1_sock *Model1_sk)
{
 if (Model1_sk->Model1___sk_common.Model1_skc_family == 10)
  return &Model1_sk->Model1___sk_common.Model1_skc_v6_rcv_saddr;
 return ((void *)0);
}

static inline __attribute__((no_instrument_function)) int Model1_inet_v6_ipv6only(const struct Model1_sock *Model1_sk)
{
 /* ipv6only field is at same position for timewait and other sockets */
 return ((Model1_sk->Model1___sk_common.Model1_skc_ipv6only));
}
/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET  is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for the IP router.
 *
 * Version:	@(#)route.h	1.0.4	05/27/93
 *
 * Authors:	Ross Biro
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 * Fixes:
 *		Alan Cox	:	Reformatted. Added ip_rt_local()
 *		Alan Cox	:	Support for TCP parameters.
 *		Alexey Kuznetsov:	Major changes for new routing code.
 *		Mike McLagan    :	Routing by source
 *		Robert Olsson   :	Added rt_cache statistics
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */





/*
 *		INETPEER - A storage for permanent information about peers
 *
 *  Authors:	Andrey V. Savochkin <saw@msu.ru>
 */
/*
 *	Linux INET6 implementation
 *
 *	Authors:
 *	Pedro Roque		<roque@di.fc.ul.pt>
 *
 *	This program is free software; you can redistribute it and/or
 *      modify it under the terms of the GNU General Public License
 *      as published by the Free Software Foundation; either version
 *      2 of the License, or (at your option) any later version.
 */








/*
 *	inet6 interface/address list definitions
 *	Linux INET6 implementation 
 *
 *	Authors:
 *	Pedro Roque		<roque@di.fc.ul.pt>	
 *
 *
 *	This program is free software; you can redistribute it and/or
 *      modify it under the terms of the GNU General Public License
 *      as published by the Free Software Foundation; either version
 *      2 of the License, or (at your option) any later version.
 */







/* inet6_dev.if_flags */







/* prefix flags */



enum {
 Model1_INET6_IFADDR_STATE_PREDAD,
 Model1_INET6_IFADDR_STATE_DAD,
 Model1_INET6_IFADDR_STATE_POSTDAD,
 Model1_INET6_IFADDR_STATE_ERRDAD,
 Model1_INET6_IFADDR_STATE_DEAD,
};

struct Model1_inet6_ifaddr {
 struct Model1_in6_addr Model1_addr;
 __u32 Model1_prefix_len;

 /* In seconds, relative to tstamp. Expiry is at tstamp + HZ * lft. */
 __u32 Model1_valid_lft;
 __u32 Model1_prefered_lft;
 Model1_atomic_t Model1_refcnt;
 Model1_spinlock_t Model1_lock;

 int Model1_state;

 __u32 Model1_flags;
 __u8 Model1_dad_probes;
 __u8 Model1_stable_privacy_retry;

 Model1___u16 Model1_scope;

 unsigned long Model1_cstamp; /* created timestamp */
 unsigned long Model1_tstamp; /* updated timestamp */

 struct Model1_delayed_work Model1_dad_work;

 struct Model1_inet6_dev *Model1_idev;
 struct Model1_rt6_info *Model1_rt;

 struct Model1_hlist_node Model1_addr_lst;
 struct Model1_list_head Model1_if_list;

 struct Model1_list_head Model1_tmp_list;
 struct Model1_inet6_ifaddr *Model1_ifpub;
 int Model1_regen_count;

 bool Model1_tokenized;

 struct Model1_callback_head Model1_rcu;
 struct Model1_in6_addr Model1_peer_addr;
};

struct Model1_ip6_sf_socklist {
 unsigned int Model1_sl_max;
 unsigned int Model1_sl_count;
 struct Model1_in6_addr Model1_sl_addr[0];
};






struct Model1_ipv6_mc_socklist {
 struct Model1_in6_addr Model1_addr;
 int Model1_ifindex;
 struct Model1_ipv6_mc_socklist *Model1_next;
 Model1_rwlock_t Model1_sflock;
 unsigned int Model1_sfmode; /* MCAST_{INCLUDE,EXCLUDE} */
 struct Model1_ip6_sf_socklist *Model1_sflist;
 struct Model1_callback_head Model1_rcu;
};

struct Model1_ip6_sf_list {
 struct Model1_ip6_sf_list *Model1_sf_next;
 struct Model1_in6_addr Model1_sf_addr;
 unsigned long Model1_sf_count[2]; /* include/exclude counts */
 unsigned char Model1_sf_gsresp; /* include in g & s response? */
 unsigned char Model1_sf_oldin; /* change state */
 unsigned char Model1_sf_crcount; /* retrans. left to send */
};







struct Model1_ifmcaddr6 {
 struct Model1_in6_addr Model1_mca_addr;
 struct Model1_inet6_dev *Model1_idev;
 struct Model1_ifmcaddr6 *Model1_next;
 struct Model1_ip6_sf_list *Model1_mca_sources;
 struct Model1_ip6_sf_list *Model1_mca_tomb;
 unsigned int Model1_mca_sfmode;
 unsigned char Model1_mca_crcount;
 unsigned long Model1_mca_sfcount[2];
 struct Model1_timer_list Model1_mca_timer;
 unsigned int Model1_mca_flags;
 int Model1_mca_users;
 Model1_atomic_t Model1_mca_refcnt;
 Model1_spinlock_t Model1_mca_lock;
 unsigned long Model1_mca_cstamp;
 unsigned long Model1_mca_tstamp;
};

/* Anycast stuff */

struct Model1_ipv6_ac_socklist {
 struct Model1_in6_addr Model1_acl_addr;
 int Model1_acl_ifindex;
 struct Model1_ipv6_ac_socklist *Model1_acl_next;
};

struct Model1_ifacaddr6 {
 struct Model1_in6_addr Model1_aca_addr;
 struct Model1_inet6_dev *Model1_aca_idev;
 struct Model1_rt6_info *Model1_aca_rt;
 struct Model1_ifacaddr6 *Model1_aca_next;
 int Model1_aca_users;
 Model1_atomic_t Model1_aca_refcnt;
 unsigned long Model1_aca_cstamp;
 unsigned long Model1_aca_tstamp;
};





struct Model1_ipv6_devstat {
 struct Model1_proc_dir_entry *Model1_proc_dir_entry;
 __typeof__(struct Model1_ipstats_mib) *Model1_ipv6;
 __typeof__(struct Model1_icmpv6_mib_device) *Model1_icmpv6dev;
 __typeof__(struct Model1_icmpv6msg_mib_device) *Model1_icmpv6msgdev;
};

struct Model1_inet6_dev {
 struct Model1_net_device *Model1_dev;

 struct Model1_list_head Model1_addr_list;

 struct Model1_ifmcaddr6 *Model1_mc_list;
 struct Model1_ifmcaddr6 *Model1_mc_tomb;
 Model1_spinlock_t Model1_mc_lock;

 unsigned char Model1_mc_qrv; /* Query Robustness Variable */
 unsigned char Model1_mc_gq_running;
 unsigned char Model1_mc_ifc_count;
 unsigned char Model1_mc_dad_count;

 unsigned long Model1_mc_v1_seen; /* Max time we stay in MLDv1 mode */
 unsigned long Model1_mc_qi; /* Query Interval */
 unsigned long Model1_mc_qri; /* Query Response Interval */
 unsigned long Model1_mc_maxdelay;

 struct Model1_timer_list Model1_mc_gq_timer; /* general query timer */
 struct Model1_timer_list Model1_mc_ifc_timer; /* interface change timer */
 struct Model1_timer_list Model1_mc_dad_timer; /* dad complete mc timer */

 struct Model1_ifacaddr6 *Model1_ac_list;
 Model1_rwlock_t Model1_lock;
 Model1_atomic_t Model1_refcnt;
 __u32 Model1_if_flags;
 int Model1_dead;

 Model1_u8 Model1_rndid[8];
 struct Model1_timer_list Model1_regen_timer;
 struct Model1_list_head Model1_tempaddr_list;

 struct Model1_in6_addr Model1_token;

 struct Model1_neigh_parms *Model1_nd_parms;
 struct Model1_ipv6_devconf Model1_cnf;
 struct Model1_ipv6_devstat Model1_stats;

 struct Model1_timer_list Model1_rs_timer;
 __u8 Model1_rs_probes;

 __u8 Model1_addr_gen_mode;
 unsigned long Model1_tstamp; /* ipv6InterfaceTable update timestamp */
 struct Model1_callback_head Model1_rcu;
};

static inline __attribute__((no_instrument_function)) void Model1_ipv6_eth_mc_map(const struct Model1_in6_addr *Model1_addr, char *Model1_buf)
{
 /*
	 *	+-------+-------+-------+-------+-------+-------+
	 *      |   33  |   33  | DST13 | DST14 | DST15 | DST16 |
	 *      +-------+-------+-------+-------+-------+-------+
	 */

 Model1_buf[0]= 0x33;
 Model1_buf[1]= 0x33;

 ({ Model1_size_t Model1___len = (sizeof(__u32)); void *Model1___ret; if (__builtin_constant_p(sizeof(__u32)) && Model1___len >= 64) Model1___ret = Model1___memcpy((Model1_buf + 2), (&Model1_addr->Model1_in6_u.Model1_u6_addr32[3]), Model1___len); else Model1___ret = __builtin_memcpy((Model1_buf + 2), (&Model1_addr->Model1_in6_u.Model1_u6_addr32[3]), Model1___len); Model1___ret; });
}

static inline __attribute__((no_instrument_function)) void Model1_ipv6_arcnet_mc_map(const struct Model1_in6_addr *Model1_addr, char *Model1_buf)
{
 Model1_buf[0] = 0x00;
}

static inline __attribute__((no_instrument_function)) void Model1_ipv6_ib_mc_map(const struct Model1_in6_addr *Model1_addr,
      const unsigned char *Model1_broadcast, char *Model1_buf)
{
 unsigned char Model1_scope = Model1_broadcast[5] & 0xF;

 Model1_buf[0] = 0; /* Reserved */
 Model1_buf[1] = 0xff; /* Multicast QPN */
 Model1_buf[2] = 0xff;
 Model1_buf[3] = 0xff;
 Model1_buf[4] = 0xff;
 Model1_buf[5] = 0x10 | Model1_scope; /* scope from broadcast address */
 Model1_buf[6] = 0x60; /* IPv6 signature */
 Model1_buf[7] = 0x1b;
 Model1_buf[8] = Model1_broadcast[8]; /* P_Key */
 Model1_buf[9] = Model1_broadcast[9];
 ({ Model1_size_t Model1___len = (10); void *Model1___ret; if (__builtin_constant_p(10) && Model1___len >= 64) Model1___ret = Model1___memcpy((Model1_buf + 10), (Model1_addr->Model1_in6_u.Model1_u6_addr8 + 6), Model1___len); else Model1___ret = __builtin_memcpy((Model1_buf + 10), (Model1_addr->Model1_in6_u.Model1_u6_addr8 + 6), Model1___len); Model1___ret; });
}

static inline __attribute__((no_instrument_function)) int Model1_ipv6_ipgre_mc_map(const struct Model1_in6_addr *Model1_addr,
        const unsigned char *Model1_broadcast, char *Model1_buf)
{
 if ((Model1_broadcast[0] | Model1_broadcast[1] | Model1_broadcast[2] | Model1_broadcast[3]) != 0) {
  ({ Model1_size_t Model1___len = (4); void *Model1___ret; if (__builtin_constant_p(4) && Model1___len >= 64) Model1___ret = Model1___memcpy((Model1_buf), (Model1_broadcast), Model1___len); else Model1___ret = __builtin_memcpy((Model1_buf), (Model1_broadcast), Model1___len); Model1___ret; });
 } else {
  /* v4mapped? */
  if ((Model1_addr->Model1_in6_u.Model1_u6_addr32[0] | Model1_addr->Model1_in6_u.Model1_u6_addr32[1] |
       (Model1_addr->Model1_in6_u.Model1_u6_addr32[2] ^ (( Model1___be32)(__builtin_constant_p((__u32)((0x0000ffff))) ? ((__u32)( (((__u32)((0x0000ffff)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x0000ffff)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x0000ffff)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x0000ffff)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0x0000ffff)))))) != 0)
   return -22;
  ({ Model1_size_t Model1___len = (4); void *Model1___ret; if (__builtin_constant_p(4) && Model1___len >= 64) Model1___ret = Model1___memcpy((Model1_buf), (&Model1_addr->Model1_in6_u.Model1_u6_addr32[3]), Model1___len); else Model1___ret = __builtin_memcpy((Model1_buf), (&Model1_addr->Model1_in6_u.Model1_u6_addr32[3]), Model1___len); Model1___ret; });
 }
 return 0;
}



/*
 *	ICMP codes for neighbour discovery messages
 */







/*
 * Router type: cross-layer information from link-layer to
 * IPv6 layer reported by certain link types (e.g., RFC4214).
 */





/*
 *	ndisc options
 */

enum {
 Model1___ND_OPT_PREFIX_INFO_END = 0,
 Model1_ND_OPT_SOURCE_LL_ADDR = 1, /* RFC2461 */
 Model1_ND_OPT_TARGET_LL_ADDR = 2, /* RFC2461 */
 Model1_ND_OPT_PREFIX_INFO = 3, /* RFC2461 */
 Model1_ND_OPT_REDIRECT_HDR = 4, /* RFC2461 */
 Model1_ND_OPT_MTU = 5, /* RFC2461 */
 Model1___ND_OPT_ARRAY_MAX,
 Model1_ND_OPT_ROUTE_INFO = 24, /* RFC4191 */
 Model1_ND_OPT_RDNSS = 25, /* RFC5006 */
 Model1_ND_OPT_DNSSL = 31, /* RFC6106 */
 Model1_ND_OPT_6CO = 34, /* RFC6775 */
 Model1___ND_OPT_MAX
};
/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Global definitions for the ARP (RFC 826) protocol.
 *
 * Version:	@(#)if_arp.h	1.0.1	04/16/93
 *
 * Authors:	Original taken from Berkeley UNIX 4.3, (c) UCB 1986-1988
 *		Portions taken from the KA9Q/NOS (v2.00m PA0GRI) source.
 *		Ross Biro
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *		Florian La Roche,
 *		Jonathan Layes <layes@loran.com>
 *		Arnaldo Carvalho de Melo <acme@conectiva.com.br> ARPHRD_HWX25
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */





/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Global definitions for the ARP (RFC 826) protocol.
 *
 * Version:	@(#)if_arp.h	1.0.1	04/16/93
 *
 * Authors:	Original taken from Berkeley UNIX 4.3, (c) UCB 1986-1988
 *		Portions taken from the KA9Q/NOS (v2.00m PA0GRI) source.
 *		Ross Biro
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *		Florian La Roche,
 *		Jonathan Layes <layes@loran.com>
 *		Arnaldo Carvalho de Melo <acme@conectiva.com.br> ARPHRD_HWX25
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */





/* ARP protocol HARDWARE identifiers. */
/* Dummy types for non ARP hardware */
/* ARP works differently on different FC media .. so  */




 /* 787->799 reserved for fibrechannel media types */
/* ARP protocol opcodes. */
/* ARP ioctl request. */
struct Model1_arpreq {
  struct Model1_sockaddr Model1_arp_pa; /* protocol address		*/
  struct Model1_sockaddr Model1_arp_ha; /* hardware address		*/
  int Model1_arp_flags; /* flags			*/
  struct Model1_sockaddr Model1_arp_netmask; /* netmask (only for proxy arps) */
  char Model1_arp_dev[16];
};

struct Model1_arpreq_old {
  struct Model1_sockaddr Model1_arp_pa; /* protocol address		*/
  struct Model1_sockaddr Model1_arp_ha; /* hardware address		*/
  int Model1_arp_flags; /* flags			*/
  struct Model1_sockaddr Model1_arp_netmask; /* netmask (only for proxy arps) */
};

/* ARP Flag values. */
/*
 *	This structure defines an ethernet arp header.
 */

struct Model1_arphdr {
 Model1___be16 Model1_ar_hrd; /* format of hardware address	*/
 Model1___be16 Model1_ar_pro; /* format of protocol address	*/
 unsigned char Model1_ar_hln; /* length of hardware address	*/
 unsigned char Model1_ar_pln; /* length of protocol address	*/
 Model1___be16 Model1_ar_op; /* ARP opcode (command)		*/
};

static inline __attribute__((no_instrument_function)) struct Model1_arphdr *Model1_arp_hdr(const struct Model1_sk_buff *Model1_skb)
{
 return (struct Model1_arphdr *)Model1_skb_network_header(Model1_skb);
}

static inline __attribute__((no_instrument_function)) int Model1_arp_hdr_len(struct Model1_net_device *Model1_dev)
{
 switch (Model1_dev->Model1_type) {





 default:
  /* ARP header, plus 2 device addresses, plus 2 IP addresses. */
  return sizeof(struct Model1_arphdr) + (Model1_dev->Model1_addr_len + sizeof(Model1_u32)) * 2;
 }
}





/* Set to 3 to get tracing... */
struct Model1_ctl_table;
struct Model1_inet6_dev;
struct Model1_net_device;
struct Model1_net_proto_family;
struct Model1_sk_buff;
struct Model1_prefix_info;

extern struct Model1_neigh_table Model1_nd_tbl;

struct Model1_nd_msg {
        struct Model1_icmp6hdr Model1_icmph;
        struct Model1_in6_addr Model1_target;
 __u8 Model1_opt[0];
};

struct Model1_rs_msg {
 struct Model1_icmp6hdr Model1_icmph;
 __u8 Model1_opt[0];
};

struct Model1_ra_msg {
        struct Model1_icmp6hdr Model1_icmph;
 Model1___be32 Model1_reachable_time;
 Model1___be32 Model1_retrans_timer;
};

struct Model1_rd_msg {
 struct Model1_icmp6hdr Model1_icmph;
 struct Model1_in6_addr Model1_target;
 struct Model1_in6_addr Model1_dest;
 __u8 Model1_opt[0];
};

struct Model1_nd_opt_hdr {
 __u8 Model1_nd_opt_type;
 __u8 Model1_nd_opt_len;
} __attribute__((packed));

/* ND options */
struct Model1_ndisc_options {
 struct Model1_nd_opt_hdr *Model1_nd_opt_array[Model1___ND_OPT_ARRAY_MAX];




 struct Model1_nd_opt_hdr *Model1_nd_useropts;
 struct Model1_nd_opt_hdr *Model1_nd_useropts_end;



};
struct Model1_ndisc_options *Model1_ndisc_parse_options(const struct Model1_net_device *Model1_dev,
       Model1_u8 *Model1_opt, int Model1_opt_len,
       struct Model1_ndisc_options *Model1_ndopts);

void Model1___ndisc_fill_addr_option(struct Model1_sk_buff *Model1_skb, int Model1_type, void *Model1_data,
         int Model1_data_len, int Model1_pad);



/*
 * This structure defines the hooks for IPv6 neighbour discovery.
 * The following hooks can be defined; unless noted otherwise, they are
 * optional and can be filled with a null pointer.
 *
 * int (*is_useropt)(u8 nd_opt_type):
 *     This function is called when IPv6 decide RA userspace options. if
 *     this function returns 1 then the option given by nd_opt_type will
 *     be handled as userspace option additional to the IPv6 options.
 *
 * int (*parse_options)(const struct net_device *dev,
 *			struct nd_opt_hdr *nd_opt,
 *			struct ndisc_options *ndopts):
 *     This function is called while parsing ndisc ops and put each position
 *     as pointer into ndopts. If this function return unequal 0, then this
 *     function took care about the ndisc option, if 0 then the IPv6 ndisc
 *     option parser will take care about that option.
 *
 * void (*update)(const struct net_device *dev, struct neighbour *n,
 *		  u32 flags, u8 icmp6_type,
 *		  const struct ndisc_options *ndopts):
 *     This function is called when IPv6 ndisc updates the neighbour cache
 *     entry. Additional options which can be updated may be previously
 *     parsed by parse_opts callback and accessible over ndopts parameter.
 *
 * int (*opt_addr_space)(const struct net_device *dev, u8 icmp6_type,
 *			 struct neighbour *neigh, u8 *ha_buf,
 *			 u8 **ha):
 *     This function is called when the necessary option space will be
 *     calculated before allocating a skb. The parameters neigh, ha_buf
 *     abd ha are available on NDISC_REDIRECT messages only.
 *
 * void (*fill_addr_option)(const struct net_device *dev,
 *			    struct sk_buff *skb, u8 icmp6_type,
 *			    const u8 *ha):
 *     This function is called when the skb will finally fill the option
 *     fields inside skb. NOTE: this callback should fill the option
 *     fields to the skb which are previously indicated by opt_space
 *     parameter. That means the decision to add such option should
 *     not lost between these two callbacks, e.g. protected by interface
 *     up state.
 *
 * void (*prefix_rcv_add_addr)(struct net *net, struct net_device *dev,
 *			       const struct prefix_info *pinfo,
 *			       struct inet6_dev *in6_dev,
 *			       struct in6_addr *addr,
 *			       int addr_type, u32 addr_flags,
 *			       bool sllao, bool tokenized,
 *			       __u32 valid_lft, u32 prefered_lft,
 *			       bool dev_addr_generated):
 *     This function is called when a RA messages is received with valid
 *     PIO option fields and an IPv6 address will be added to the interface
 *     for autoconfiguration. The parameter dev_addr_generated reports about
 *     if the address was based on dev->dev_addr or not. This can be used
 *     to add a second address if link-layer operates with two link layer
 *     addresses. E.g. 802.15.4 6LoWPAN.
 */
struct Model1_ndisc_ops {
 int (*Model1_is_useropt)(Model1_u8 Model1_nd_opt_type);
 int (*Model1_parse_options)(const struct Model1_net_device *Model1_dev,
     struct Model1_nd_opt_hdr *Model1_nd_opt,
     struct Model1_ndisc_options *Model1_ndopts);
 void (*Model1_update)(const struct Model1_net_device *Model1_dev, struct Model1_neighbour *Model1_n,
     Model1_u32 Model1_flags, Model1_u8 Model1_icmp6_type,
     const struct Model1_ndisc_options *Model1_ndopts);
 int (*Model1_opt_addr_space)(const struct Model1_net_device *Model1_dev, Model1_u8 Model1_icmp6_type,
      struct Model1_neighbour *Model1_neigh, Model1_u8 *Model1_ha_buf,
      Model1_u8 **Model1_ha);
 void (*Model1_fill_addr_option)(const struct Model1_net_device *Model1_dev,
        struct Model1_sk_buff *Model1_skb, Model1_u8 Model1_icmp6_type,
        const Model1_u8 *Model1_ha);
 void (*Model1_prefix_rcv_add_addr)(struct Model1_net *Model1_net, struct Model1_net_device *Model1_dev,
           const struct Model1_prefix_info *Model1_pinfo,
           struct Model1_inet6_dev *Model1_in6_dev,
           struct Model1_in6_addr *Model1_addr,
           int Model1_addr_type, Model1_u32 Model1_addr_flags,
           bool Model1_sllao, bool Model1_tokenized,
           __u32 Model1_valid_lft, Model1_u32 Model1_prefered_lft,
           bool Model1_dev_addr_generated);
};


static inline __attribute__((no_instrument_function)) int Model1_ndisc_ops_is_useropt(const struct Model1_net_device *Model1_dev,
           Model1_u8 Model1_nd_opt_type)
{
 if (Model1_dev->Model1_ndisc_ops && Model1_dev->Model1_ndisc_ops->Model1_is_useropt)
  return Model1_dev->Model1_ndisc_ops->Model1_is_useropt(Model1_nd_opt_type);
 else
  return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_ndisc_ops_parse_options(const struct Model1_net_device *Model1_dev,
       struct Model1_nd_opt_hdr *Model1_nd_opt,
       struct Model1_ndisc_options *Model1_ndopts)
{
 if (Model1_dev->Model1_ndisc_ops && Model1_dev->Model1_ndisc_ops->Model1_parse_options)
  return Model1_dev->Model1_ndisc_ops->Model1_parse_options(Model1_dev, Model1_nd_opt, Model1_ndopts);
 else
  return 0;
}

static inline __attribute__((no_instrument_function)) void Model1_ndisc_ops_update(const struct Model1_net_device *Model1_dev,
       struct Model1_neighbour *Model1_n, Model1_u32 Model1_flags,
       Model1_u8 Model1_icmp6_type,
       const struct Model1_ndisc_options *Model1_ndopts)
{
 if (Model1_dev->Model1_ndisc_ops && Model1_dev->Model1_ndisc_ops->Model1_update)
  Model1_dev->Model1_ndisc_ops->Model1_update(Model1_dev, Model1_n, Model1_flags, Model1_icmp6_type, Model1_ndopts);
}

static inline __attribute__((no_instrument_function)) int Model1_ndisc_ops_opt_addr_space(const struct Model1_net_device *Model1_dev,
        Model1_u8 Model1_icmp6_type)
{
 if (Model1_dev->Model1_ndisc_ops && Model1_dev->Model1_ndisc_ops->Model1_opt_addr_space &&
     Model1_icmp6_type != 137)
  return Model1_dev->Model1_ndisc_ops->Model1_opt_addr_space(Model1_dev, Model1_icmp6_type, ((void *)0),
            ((void *)0), ((void *)0));
 else
  return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_ndisc_ops_redirect_opt_addr_space(const struct Model1_net_device *Model1_dev,
          struct Model1_neighbour *Model1_neigh,
          Model1_u8 *Model1_ha_buf, Model1_u8 **Model1_ha)
{
 if (Model1_dev->Model1_ndisc_ops && Model1_dev->Model1_ndisc_ops->Model1_opt_addr_space)
  return Model1_dev->Model1_ndisc_ops->Model1_opt_addr_space(Model1_dev, 137,
            Model1_neigh, Model1_ha_buf, Model1_ha);
 else
  return 0;
}

static inline __attribute__((no_instrument_function)) void Model1_ndisc_ops_fill_addr_option(const struct Model1_net_device *Model1_dev,
           struct Model1_sk_buff *Model1_skb,
           Model1_u8 Model1_icmp6_type)
{
 if (Model1_dev->Model1_ndisc_ops && Model1_dev->Model1_ndisc_ops->Model1_fill_addr_option &&
     Model1_icmp6_type != 137)
  Model1_dev->Model1_ndisc_ops->Model1_fill_addr_option(Model1_dev, Model1_skb, Model1_icmp6_type, ((void *)0));
}

static inline __attribute__((no_instrument_function)) void Model1_ndisc_ops_fill_redirect_addr_option(const struct Model1_net_device *Model1_dev,
             struct Model1_sk_buff *Model1_skb,
             const Model1_u8 *Model1_ha)
{
 if (Model1_dev->Model1_ndisc_ops && Model1_dev->Model1_ndisc_ops->Model1_fill_addr_option)
  Model1_dev->Model1_ndisc_ops->Model1_fill_addr_option(Model1_dev, Model1_skb, 137, Model1_ha);
}

static inline __attribute__((no_instrument_function)) void Model1_ndisc_ops_prefix_rcv_add_addr(struct Model1_net *Model1_net,
       struct Model1_net_device *Model1_dev,
       const struct Model1_prefix_info *Model1_pinfo,
       struct Model1_inet6_dev *Model1_in6_dev,
       struct Model1_in6_addr *Model1_addr,
       int Model1_addr_type, Model1_u32 Model1_addr_flags,
       bool Model1_sllao, bool Model1_tokenized,
       __u32 Model1_valid_lft,
       Model1_u32 Model1_prefered_lft,
       bool Model1_dev_addr_generated)
{
 if (Model1_dev->Model1_ndisc_ops && Model1_dev->Model1_ndisc_ops->Model1_prefix_rcv_add_addr)
  Model1_dev->Model1_ndisc_ops->Model1_prefix_rcv_add_addr(Model1_net, Model1_dev, Model1_pinfo, Model1_in6_dev,
          Model1_addr, Model1_addr_type,
          Model1_addr_flags, Model1_sllao,
          Model1_tokenized, Model1_valid_lft,
          Model1_prefered_lft,
          Model1_dev_addr_generated);
}


/*
 * Return the padding between the option length and the start of the
 * link addr.  Currently only IP-over-InfiniBand needs this, although
 * if RFC 3831 IPv6-over-Fibre Channel is ever implemented it may
 * also need a pad of 2.
 */
static inline __attribute__((no_instrument_function)) int Model1_ndisc_addr_option_pad(unsigned short Model1_type)
{
 switch (Model1_type) {
 case 32: return 2;
 default: return 0;
 }
}

static inline __attribute__((no_instrument_function)) int Model1___ndisc_opt_addr_space(unsigned char Model1_addr_len, int Model1_pad)
{
 return (((Model1_addr_len + Model1_pad)+2+7)&~7);
}


static inline __attribute__((no_instrument_function)) int Model1_ndisc_opt_addr_space(struct Model1_net_device *Model1_dev, Model1_u8 Model1_icmp6_type)
{
 return Model1___ndisc_opt_addr_space(Model1_dev->Model1_addr_len,
          Model1_ndisc_addr_option_pad(Model1_dev->Model1_type)) +
  Model1_ndisc_ops_opt_addr_space(Model1_dev, Model1_icmp6_type);
}

static inline __attribute__((no_instrument_function)) int Model1_ndisc_redirect_opt_addr_space(struct Model1_net_device *Model1_dev,
      struct Model1_neighbour *Model1_neigh,
      Model1_u8 *Model1_ops_data_buf,
      Model1_u8 **Model1_ops_data)
{
 return Model1___ndisc_opt_addr_space(Model1_dev->Model1_addr_len,
          Model1_ndisc_addr_option_pad(Model1_dev->Model1_type)) +
  Model1_ndisc_ops_redirect_opt_addr_space(Model1_dev, Model1_neigh, Model1_ops_data_buf,
        Model1_ops_data);
}


static inline __attribute__((no_instrument_function)) Model1_u8 *Model1___ndisc_opt_addr_data(struct Model1_nd_opt_hdr *Model1_p,
     unsigned char Model1_addr_len, int Model1_prepad)
{
 Model1_u8 *Model1_lladdr = (Model1_u8 *)(Model1_p + 1);
 int Model1_lladdrlen = Model1_p->Model1_nd_opt_len << 3;
 if (Model1_lladdrlen != Model1___ndisc_opt_addr_space(Model1_addr_len, Model1_prepad))
  return ((void *)0);
 return Model1_lladdr + Model1_prepad;
}

static inline __attribute__((no_instrument_function)) Model1_u8 *Model1_ndisc_opt_addr_data(struct Model1_nd_opt_hdr *Model1_p,
          struct Model1_net_device *Model1_dev)
{
 return Model1___ndisc_opt_addr_data(Model1_p, Model1_dev->Model1_addr_len,
         Model1_ndisc_addr_option_pad(Model1_dev->Model1_type));
}

static inline __attribute__((no_instrument_function)) Model1_u32 Model1_ndisc_hashfn(const void *Model1_pkey, const struct Model1_net_device *Model1_dev, __u32 *Model1_hash_rnd)
{
 const Model1_u32 *Model1_p32 = Model1_pkey;

 return (((Model1_p32[0] ^ Model1_hash32_ptr(Model1_dev)) * Model1_hash_rnd[0]) +
  (Model1_p32[1] * Model1_hash_rnd[1]) +
  (Model1_p32[2] * Model1_hash_rnd[2]) +
  (Model1_p32[3] * Model1_hash_rnd[3]));
}

static inline __attribute__((no_instrument_function)) struct Model1_neighbour *Model1___ipv6_neigh_lookup_noref(struct Model1_net_device *Model1_dev, const void *Model1_pkey)
{
 return Model1____neigh_lookup_noref(&Model1_nd_tbl, Model1_neigh_key_eq128, Model1_ndisc_hashfn, Model1_pkey, Model1_dev);
}

static inline __attribute__((no_instrument_function)) struct Model1_neighbour *Model1___ipv6_neigh_lookup(struct Model1_net_device *Model1_dev, const void *Model1_pkey)
{
 struct Model1_neighbour *Model1_n;

 Model1_rcu_read_lock_bh();
 Model1_n = Model1___ipv6_neigh_lookup_noref(Model1_dev, Model1_pkey);
 if (Model1_n && !Model1_atomic_add_unless((&Model1_n->Model1_refcnt), 1, 0))
  Model1_n = ((void *)0);
 Model1_rcu_read_unlock_bh();

 return Model1_n;
}

int Model1_ndisc_init(void);
int Model1_ndisc_late_init(void);

void Model1_ndisc_late_cleanup(void);
void Model1_ndisc_cleanup(void);

int Model1_ndisc_rcv(struct Model1_sk_buff *Model1_skb);

void Model1_ndisc_send_ns(struct Model1_net_device *Model1_dev, const struct Model1_in6_addr *Model1_solicit,
     const struct Model1_in6_addr *Model1_daddr, const struct Model1_in6_addr *Model1_saddr);

void Model1_ndisc_send_rs(struct Model1_net_device *Model1_dev,
     const struct Model1_in6_addr *Model1_saddr, const struct Model1_in6_addr *Model1_daddr);
void Model1_ndisc_send_na(struct Model1_net_device *Model1_dev, const struct Model1_in6_addr *Model1_daddr,
     const struct Model1_in6_addr *Model1_solicited_addr,
     bool Model1_router, bool Model1_solicited, bool Model1_override, bool Model1_inc_opt);

void Model1_ndisc_send_redirect(struct Model1_sk_buff *Model1_skb, const struct Model1_in6_addr *Model1_target);

int Model1_ndisc_mc_map(const struct Model1_in6_addr *Model1_addr, char *Model1_buf, struct Model1_net_device *Model1_dev,
   int Model1_dir);

void Model1_ndisc_update(const struct Model1_net_device *Model1_dev, struct Model1_neighbour *Model1_neigh,
    const Model1_u8 *Model1_lladdr, Model1_u8 Model1_new, Model1_u32 Model1_flags, Model1_u8 Model1_icmp6_type,
    struct Model1_ndisc_options *Model1_ndopts);

/*
 *	IGMP
 */
int Model1_igmp6_init(void);

void Model1_igmp6_cleanup(void);

int Model1_igmp6_event_query(struct Model1_sk_buff *Model1_skb);

int Model1_igmp6_event_report(struct Model1_sk_buff *Model1_skb);



int Model1_ndisc_ifinfo_sysctl_change(struct Model1_ctl_table *Model1_ctl, int Model1_write,
          void *Model1_buffer, Model1_size_t *Model1_lenp, Model1_loff_t *Model1_ppos);
int Model1_ndisc_ifinfo_sysctl_strategy(struct Model1_ctl_table *Model1_ctl,
     void *Model1_oldval, Model1_size_t *Model1_oldlenp,
     void *Model1_newval, Model1_size_t Model1_newlen);


void Model1_inet6_ifinfo_notify(int Model1_event, struct Model1_inet6_dev *Model1_idev);








/*
 *	NextHeader field of IPv6 header
 */
/*
 *	Addr type
 *	
 *	type	-	unicast | multicast
 *	scope	-	local	| site	    | global
 *	v4	-	compat
 *	v4mapped
 *	any
 *	loopback
 */
/*
 *	Addr scopes
 */
/*
 *	Addr flags
 */







/*
 *	fragmentation header
 */

struct Model1_frag_hdr {
 __u8 Model1_nexthdr;
 __u8 Model1_reserved;
 Model1___be16 Model1_frag_off;
 Model1___be32 Model1_identification;
};
/* sysctls */
extern int Model1_sysctl_mld_max_msf;
extern int Model1_sysctl_mld_qrv;
/* per device counters are atomic_long_t */
/* per device and per net counters are atomic_long_t */
/* MIBs */
struct Model1_ip6_ra_chain {
 struct Model1_ip6_ra_chain *Model1_next;
 struct Model1_sock *Model1_sk;
 int Model1_sel;
 void (*Model1_destructor)(struct Model1_sock *);
};

extern struct Model1_ip6_ra_chain *Model1_ip6_ra_chain;
extern Model1_rwlock_t Model1_ip6_ra_lock;

/*
   This structure is prepared by protocol, when parsing
   ancillary data and passed to IPv6.
 */

struct Model1_ipv6_txoptions {
 Model1_atomic_t Model1_refcnt;
 /* Length of this structure */
 int Model1_tot_len;

 /* length of extension headers   */

 Model1___u16 Model1_opt_flen; /* after fragment hdr */
 Model1___u16 Model1_opt_nflen; /* before fragment hdr */

 struct Model1_ipv6_opt_hdr *Model1_hopopt;
 struct Model1_ipv6_opt_hdr *Model1_dst0opt;
 struct Model1_ipv6_rt_hdr *Model1_srcrt; /* Routing Header */
 struct Model1_ipv6_opt_hdr *Model1_dst1opt;
 struct Model1_callback_head Model1_rcu;
 /* Option buffer, as read by IPV6_PKTOPTIONS, starts here. */
};

struct Model1_ip6_flowlabel {
 struct Model1_ip6_flowlabel *Model1_next;
 Model1___be32 Model1_label;
 Model1_atomic_t Model1_users;
 struct Model1_in6_addr Model1_dst;
 struct Model1_ipv6_txoptions *Model1_opt;
 unsigned long Model1_linger;
 struct Model1_callback_head Model1_rcu;
 Model1_u8 Model1_share;
 union {
  struct Model1_pid *Model1_pid;
  Model1_kuid_t Model1_uid;
 } Model1_owner;
 unsigned long Model1_lastuse;
 unsigned long Model1_expires;
 struct Model1_net *Model1_fl_net;
};
struct Model1_ipv6_fl_socklist {
 struct Model1_ipv6_fl_socklist *Model1_next;
 struct Model1_ip6_flowlabel *Model1_fl;
 struct Model1_callback_head Model1_rcu;
};

struct Model1_ipcm6_cookie {
 Model1___s16 Model1_hlimit;
 Model1___s16 Model1_tclass;
 Model1___s8 Model1_dontfrag;
 struct Model1_ipv6_txoptions *Model1_opt;
};

static inline __attribute__((no_instrument_function)) struct Model1_ipv6_txoptions *Model1_txopt_get(const struct Model1_ipv6_pinfo *Model1_np)
{
 struct Model1_ipv6_txoptions *Model1_opt;

 Model1_rcu_read_lock();
 Model1_opt = ({ typeof(*(Model1_np->Model1_opt)) *Model1_________p1 = (typeof(*(Model1_np->Model1_opt)) *)({ typeof((Model1_np->Model1_opt)) Model1__________p1 = ({ union { typeof((Model1_np->Model1_opt)) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&((Model1_np->Model1_opt)), Model1___u.Model1___c, sizeof((Model1_np->Model1_opt))); else Model1___read_once_size_nocheck(&((Model1_np->Model1_opt)), Model1___u.Model1___c, sizeof((Model1_np->Model1_opt))); Model1___u.Model1___val; }); typeof(*((Model1_np->Model1_opt))) *Model1____typecheck_p __attribute__((unused)); do { } while (0); (Model1__________p1); }); do { } while (0); ; ((typeof(*(Model1_np->Model1_opt)) *)(Model1_________p1)); });
 if (Model1_opt) {
  if (!Model1_atomic_add_unless((&Model1_opt->Model1_refcnt), 1, 0))
   Model1_opt = ((void *)0);
  else
   Model1_opt = (Model1_opt);
 }
 Model1_rcu_read_unlock();
 return Model1_opt;
}

static inline __attribute__((no_instrument_function)) void Model1_txopt_put(struct Model1_ipv6_txoptions *Model1_opt)
{
 if (Model1_opt && Model1_atomic_dec_and_test(&Model1_opt->Model1_refcnt))
  do { do { bool Model1___cond = !(!(!((__builtin_offsetof(typeof(*(Model1_opt)), Model1_rcu)) < 4096))); extern void Model1___compiletime_assert_280(void) ; if (Model1___cond) Model1___compiletime_assert_280(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0); Model1_kfree_call_rcu(&((Model1_opt)->Model1_rcu), (Model1_rcu_callback_t)(unsigned long)(__builtin_offsetof(typeof(*(Model1_opt)), Model1_rcu))); } while (0);
}

struct Model1_ip6_flowlabel *Model1_fl6_sock_lookup(struct Model1_sock *Model1_sk, Model1___be32 Model1_label);
struct Model1_ipv6_txoptions *Model1_fl6_merge_options(struct Model1_ipv6_txoptions *Model1_opt_space,
      struct Model1_ip6_flowlabel *Model1_fl,
      struct Model1_ipv6_txoptions *Model1_fopt);
void Model1_fl6_free_socklist(struct Model1_sock *Model1_sk);
int Model1_ipv6_flowlabel_opt(struct Model1_sock *Model1_sk, char *Model1_optval, int Model1_optlen);
int Model1_ipv6_flowlabel_opt_get(struct Model1_sock *Model1_sk, struct Model1_in6_flowlabel_req *Model1_freq,
      int Model1_flags);
int Model1_ip6_flowlabel_init(void);
void Model1_ip6_flowlabel_cleanup(void);

static inline __attribute__((no_instrument_function)) void Model1_fl6_sock_release(struct Model1_ip6_flowlabel *Model1_fl)
{
 if (Model1_fl)
  Model1_atomic_dec(&Model1_fl->Model1_users);
}

void Model1_icmpv6_notify(struct Model1_sk_buff *Model1_skb, Model1_u8 Model1_type, Model1_u8 Model1_code, Model1___be32 Model1_info);

int Model1_icmpv6_push_pending_frames(struct Model1_sock *Model1_sk, struct Model1_flowi6 *Model1_fl6,
          struct Model1_icmp6hdr *Model1_thdr, int Model1_len);

int Model1_ip6_ra_control(struct Model1_sock *Model1_sk, int Model1_sel);

int Model1_ipv6_parse_hopopts(struct Model1_sk_buff *Model1_skb);

struct Model1_ipv6_txoptions *Model1_ipv6_dup_options(struct Model1_sock *Model1_sk,
     struct Model1_ipv6_txoptions *Model1_opt);
struct Model1_ipv6_txoptions *Model1_ipv6_renew_options(struct Model1_sock *Model1_sk,
       struct Model1_ipv6_txoptions *Model1_opt,
       int Model1_newtype,
       struct Model1_ipv6_opt_hdr *Model1_newopt,
       int Model1_newoptlen);
struct Model1_ipv6_txoptions *
Model1_ipv6_renew_options_kern(struct Model1_sock *Model1_sk,
   struct Model1_ipv6_txoptions *Model1_opt,
   int Model1_newtype,
   struct Model1_ipv6_opt_hdr *Model1_newopt,
   int Model1_newoptlen);
struct Model1_ipv6_txoptions *Model1_ipv6_fixup_options(struct Model1_ipv6_txoptions *Model1_opt_space,
       struct Model1_ipv6_txoptions *Model1_opt);

bool Model1_ipv6_opt_accepted(const struct Model1_sock *Model1_sk, const struct Model1_sk_buff *Model1_skb,
         const struct Model1_inet6_skb_parm *Model1_opt);
struct Model1_ipv6_txoptions *Model1_ipv6_update_options(struct Model1_sock *Model1_sk,
        struct Model1_ipv6_txoptions *Model1_opt);

static inline __attribute__((no_instrument_function)) bool Model1_ipv6_accept_ra(struct Model1_inet6_dev *Model1_idev)
{
 /* If forwarding is enabled, RA are not accepted unless the special
	 * hybrid mode (accept_ra=2) is enabled.
	 */
 return Model1_idev->Model1_cnf.Model1_forwarding ? Model1_idev->Model1_cnf.Model1_accept_ra == 2 :
     Model1_idev->Model1_cnf.Model1_accept_ra;
}


static inline __attribute__((no_instrument_function)) int Model1_ip6_frag_mem(struct Model1_net *Model1_net)
{
 return Model1_sum_frag_mem_limit(&Model1_net->Model1_ipv6.Model1_frags);
}






int Model1___ipv6_addr_type(const struct Model1_in6_addr *Model1_addr);
static inline __attribute__((no_instrument_function)) int Model1_ipv6_addr_type(const struct Model1_in6_addr *Model1_addr)
{
 return Model1___ipv6_addr_type(Model1_addr) & 0xffff;
}

static inline __attribute__((no_instrument_function)) int Model1_ipv6_addr_scope(const struct Model1_in6_addr *Model1_addr)
{
 return Model1___ipv6_addr_type(Model1_addr) & 0x00f0U;
}

static inline __attribute__((no_instrument_function)) int Model1___ipv6_addr_src_scope(int Model1_type)
{
 return (Model1_type == 0x0000U) ? -1 : (Model1_type >> 16);
}

static inline __attribute__((no_instrument_function)) int Model1_ipv6_addr_src_scope(const struct Model1_in6_addr *Model1_addr)
{
 return Model1___ipv6_addr_src_scope(Model1___ipv6_addr_type(Model1_addr));
}

static inline __attribute__((no_instrument_function)) bool Model1___ipv6_addr_needs_scope_id(int Model1_type)
{
 return Model1_type & 0x0020U ||
        (Model1_type & 0x0002U &&
  (Model1_type & (0x0010U|0x0020U)));
}

static inline __attribute__((no_instrument_function)) __u32 Model1_ipv6_iface_scope_id(const struct Model1_in6_addr *Model1_addr, int Model1_iface)
{
 return Model1___ipv6_addr_needs_scope_id(Model1___ipv6_addr_type(Model1_addr)) ? Model1_iface : 0;
}

static inline __attribute__((no_instrument_function)) int Model1_ipv6_addr_cmp(const struct Model1_in6_addr *Model1_a1, const struct Model1_in6_addr *Model1_a2)
{
 return Model1_memcmp(Model1_a1, Model1_a2, sizeof(struct Model1_in6_addr));
}

static inline __attribute__((no_instrument_function)) bool
Model1_ipv6_masked_addr_cmp(const struct Model1_in6_addr *Model1_a1, const struct Model1_in6_addr *Model1_m,
       const struct Model1_in6_addr *Model1_a2)
{

 const unsigned long *Model1_ul1 = (const unsigned long *)Model1_a1;
 const unsigned long *Model1_ulm = (const unsigned long *)Model1_m;
 const unsigned long *Model1_ul2 = (const unsigned long *)Model1_a2;

 return !!(((Model1_ul1[0] ^ Model1_ul2[0]) & Model1_ulm[0]) |
    ((Model1_ul1[1] ^ Model1_ul2[1]) & Model1_ulm[1]));






}

static inline __attribute__((no_instrument_function)) void Model1_ipv6_addr_prefix(struct Model1_in6_addr *Model1_pfx,
        const struct Model1_in6_addr *Model1_addr,
        int Model1_plen)
{
 /* caller must guarantee 0 <= plen <= 128 */
 int Model1_o = Model1_plen >> 3,
     Model1_b = Model1_plen & 0x7;

 memset(Model1_pfx->Model1_in6_u.Model1_u6_addr8, 0, sizeof(Model1_pfx->Model1_in6_u.Model1_u6_addr8));
 ({ Model1_size_t Model1___len = (Model1_o); void *Model1___ret; if (__builtin_constant_p(Model1_o) && Model1___len >= 64) Model1___ret = Model1___memcpy((Model1_pfx->Model1_in6_u.Model1_u6_addr8), (Model1_addr), Model1___len); else Model1___ret = __builtin_memcpy((Model1_pfx->Model1_in6_u.Model1_u6_addr8), (Model1_addr), Model1___len); Model1___ret; });
 if (Model1_b != 0)
  Model1_pfx->Model1_in6_u.Model1_u6_addr8[Model1_o] = Model1_addr->Model1_in6_u.Model1_u6_addr8[Model1_o] & (0xff00 >> Model1_b);
}

static inline __attribute__((no_instrument_function)) void Model1_ipv6_addr_prefix_copy(struct Model1_in6_addr *Model1_addr,
      const struct Model1_in6_addr *Model1_pfx,
      int Model1_plen)
{
 /* caller must guarantee 0 <= plen <= 128 */
 int Model1_o = Model1_plen >> 3,
     Model1_b = Model1_plen & 0x7;

 ({ Model1_size_t Model1___len = (Model1_o); void *Model1___ret; if (__builtin_constant_p(Model1_o) && Model1___len >= 64) Model1___ret = Model1___memcpy((Model1_addr->Model1_in6_u.Model1_u6_addr8), (Model1_pfx), Model1___len); else Model1___ret = __builtin_memcpy((Model1_addr->Model1_in6_u.Model1_u6_addr8), (Model1_pfx), Model1___len); Model1___ret; });
 if (Model1_b != 0) {
  Model1_addr->Model1_in6_u.Model1_u6_addr8[Model1_o] &= ~(0xff00 >> Model1_b);
  Model1_addr->Model1_in6_u.Model1_u6_addr8[Model1_o] |= (Model1_pfx->Model1_in6_u.Model1_u6_addr8[Model1_o] & (0xff00 >> Model1_b));
 }
}

static inline __attribute__((no_instrument_function)) void Model1___ipv6_addr_set_half(Model1___be32 *Model1_addr,
     Model1___be32 Model1_wh, Model1___be32 Model1_wl)
{







 if (__builtin_constant_p(Model1_wl) && __builtin_constant_p(Model1_wh)) {
  *( Model1_u64 *)Model1_addr = (( Model1_u64)(Model1_wl) << 32 | ( Model1_u64)(Model1_wh));
  return;
 }


 Model1_addr[0] = Model1_wh;
 Model1_addr[1] = Model1_wl;
}

static inline __attribute__((no_instrument_function)) void Model1_ipv6_addr_set(struct Model1_in6_addr *Model1_addr,
         Model1___be32 Model1_w1, Model1___be32 Model1_w2,
         Model1___be32 Model1_w3, Model1___be32 Model1_w4)
{
 Model1___ipv6_addr_set_half(&Model1_addr->Model1_in6_u.Model1_u6_addr32[0], Model1_w1, Model1_w2);
 Model1___ipv6_addr_set_half(&Model1_addr->Model1_in6_u.Model1_u6_addr32[2], Model1_w3, Model1_w4);
}

static inline __attribute__((no_instrument_function)) bool Model1_ipv6_addr_equal(const struct Model1_in6_addr *Model1_a1,
       const struct Model1_in6_addr *Model1_a2)
{

 const unsigned long *Model1_ul1 = (const unsigned long *)Model1_a1;
 const unsigned long *Model1_ul2 = (const unsigned long *)Model1_a2;

 return ((Model1_ul1[0] ^ Model1_ul2[0]) | (Model1_ul1[1] ^ Model1_ul2[1])) == 0UL;






}


static inline __attribute__((no_instrument_function)) bool Model1___ipv6_prefix_equal64_half(const Model1___be64 *Model1_a1,
           const Model1___be64 *Model1_a2,
           unsigned int Model1_len)
{
 if (Model1_len && ((*Model1_a1 ^ *Model1_a2) & (( Model1___be64)(__builtin_constant_p((__u64)(((~0UL) << (64 - Model1_len)))) ? ((__u64)( (((__u64)(((~0UL) << (64 - Model1_len))) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(((~0UL) << (64 - Model1_len))) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(((~0UL) << (64 - Model1_len))) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(((~0UL) << (64 - Model1_len))) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(((~0UL) << (64 - Model1_len))) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(((~0UL) << (64 - Model1_len))) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(((~0UL) << (64 - Model1_len))) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(((~0UL) << (64 - Model1_len))) & (__u64)0xff00000000000000ULL) >> 56))) : Model1___fswab64(((~0UL) << (64 - Model1_len)))))))
  return false;
 return true;
}

static inline __attribute__((no_instrument_function)) bool Model1_ipv6_prefix_equal(const struct Model1_in6_addr *Model1_addr1,
         const struct Model1_in6_addr *Model1_addr2,
         unsigned int Model1_prefixlen)
{
 const Model1___be64 *Model1_a1 = (const Model1___be64 *)Model1_addr1;
 const Model1___be64 *Model1_a2 = (const Model1___be64 *)Model1_addr2;

 if (Model1_prefixlen >= 64) {
  if (Model1_a1[0] ^ Model1_a2[0])
   return false;
  return Model1___ipv6_prefix_equal64_half(Model1_a1 + 1, Model1_a2 + 1, Model1_prefixlen - 64);
 }
 return Model1___ipv6_prefix_equal64_half(Model1_a1, Model1_a2, Model1_prefixlen);
}
struct Model1_inet_frag_queue;

enum Model1_ip6_defrag_users {
 Model1_IP6_DEFRAG_LOCAL_DELIVER,
 Model1_IP6_DEFRAG_CONNTRACK_IN,
 Model1___IP6_DEFRAG_CONNTRACK_IN = Model1_IP6_DEFRAG_CONNTRACK_IN + ((Model1_u16)(~0U)),
 Model1_IP6_DEFRAG_CONNTRACK_OUT,
 Model1___IP6_DEFRAG_CONNTRACK_OUT = Model1_IP6_DEFRAG_CONNTRACK_OUT + ((Model1_u16)(~0U)),
 Model1_IP6_DEFRAG_CONNTRACK_BRIDGE_IN,
 Model1___IP6_DEFRAG_CONNTRACK_BRIDGE_IN = Model1_IP6_DEFRAG_CONNTRACK_BRIDGE_IN + ((Model1_u16)(~0U)),
};

struct Model1_ip6_create_arg {
 Model1___be32 Model1_id;
 Model1_u32 Model1_user;
 const struct Model1_in6_addr *Model1_src;
 const struct Model1_in6_addr *Model1_dst;
 int Model1_iif;
 Model1_u8 Model1_ecn;
};

void Model1_ip6_frag_init(struct Model1_inet_frag_queue *Model1_q, const void *Model1_a);
bool Model1_ip6_frag_match(const struct Model1_inet_frag_queue *Model1_q, const void *Model1_a);

/*
 *	Equivalent of ipv4 struct ip
 */
struct Model1_frag_queue {
 struct Model1_inet_frag_queue Model1_q;

 Model1___be32 Model1_id; /* fragment id		*/
 Model1_u32 Model1_user;
 struct Model1_in6_addr Model1_saddr;
 struct Model1_in6_addr Model1_daddr;

 int Model1_iif;
 unsigned int Model1_csum;
 Model1___u16 Model1_nhoffset;
 Model1_u8 Model1_ecn;
};

void Model1_ip6_expire_frag_queue(struct Model1_net *Model1_net, struct Model1_frag_queue *Model1_fq,
      struct Model1_inet_frags *Model1_frags);

static inline __attribute__((no_instrument_function)) bool Model1_ipv6_addr_any(const struct Model1_in6_addr *Model1_a)
{

 const unsigned long *Model1_ul = (const unsigned long *)Model1_a;

 return (Model1_ul[0] | Model1_ul[1]) == 0UL;




}

static inline __attribute__((no_instrument_function)) Model1_u32 Model1_ipv6_addr_hash(const struct Model1_in6_addr *Model1_a)
{

 const unsigned long *Model1_ul = (const unsigned long *)Model1_a;
 unsigned long Model1_x = Model1_ul[0] ^ Model1_ul[1];

 return (Model1_u32)(Model1_x ^ (Model1_x >> 32));




}

/* more secured version of ipv6_addr_hash() */
static inline __attribute__((no_instrument_function)) Model1_u32 Model1___ipv6_addr_jhash(const struct Model1_in6_addr *Model1_a, const Model1_u32 Model1_initval)
{
 Model1_u32 Model1_v = ( Model1_u32)Model1_a->Model1_in6_u.Model1_u6_addr32[0] ^ ( Model1_u32)Model1_a->Model1_in6_u.Model1_u6_addr32[1];

 return Model1_jhash_3words(Model1_v,
       ( Model1_u32)Model1_a->Model1_in6_u.Model1_u6_addr32[2],
       ( Model1_u32)Model1_a->Model1_in6_u.Model1_u6_addr32[3],
       Model1_initval);
}

static inline __attribute__((no_instrument_function)) bool Model1_ipv6_addr_loopback(const struct Model1_in6_addr *Model1_a)
{

 const Model1___be64 *Model1_be = (const Model1___be64 *)Model1_a;

 return (Model1_be[0] | (Model1_be[1] ^ (( Model1___be64)(__builtin_constant_p((__u64)((1))) ? ((__u64)( (((__u64)((1)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)((1)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)((1)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)((1)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)((1)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)((1)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)((1)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)((1)) & (__u64)0xff00000000000000ULL) >> 56))) : Model1___fswab64((1)))))) == 0UL;




}

/*
 * Note that we must __force cast these to unsigned long to make sparse happy,
 * since all of the endian-annotated types are fixed size regardless of arch.
 */
static inline __attribute__((no_instrument_function)) bool Model1_ipv6_addr_v4mapped(const struct Model1_in6_addr *Model1_a)
{
 return (

  *(unsigned long *)Model1_a |



  ( unsigned long)(Model1_a->Model1_in6_u.Model1_u6_addr32[2] ^
     (( Model1___be32)(__builtin_constant_p((__u32)((0x0000ffff))) ? ((__u32)( (((__u32)((0x0000ffff)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x0000ffff)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x0000ffff)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x0000ffff)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0x0000ffff)))))) == 0UL;
}

/*
 * Check for a RFC 4843 ORCHID address
 * (Overlay Routable Cryptographic Hash Identifiers)
 */
static inline __attribute__((no_instrument_function)) bool Model1_ipv6_addr_orchid(const struct Model1_in6_addr *Model1_a)
{
 return (Model1_a->Model1_in6_u.Model1_u6_addr32[0] & (( Model1___be32)(__builtin_constant_p((__u32)((0xfffffff0))) ? ((__u32)( (((__u32)((0xfffffff0)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xfffffff0)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xfffffff0)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xfffffff0)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0xfffffff0))))) == (( Model1___be32)(__builtin_constant_p((__u32)((0x20010010))) ? ((__u32)( (((__u32)((0x20010010)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x20010010)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x20010010)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x20010010)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0x20010010))));
}

static inline __attribute__((no_instrument_function)) bool Model1_ipv6_addr_is_multicast(const struct Model1_in6_addr *Model1_addr)
{
 return (Model1_addr->Model1_in6_u.Model1_u6_addr32[0] & (( Model1___be32)(__builtin_constant_p((__u32)((0xFF000000))) ? ((__u32)( (((__u32)((0xFF000000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xFF000000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xFF000000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xFF000000)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0xFF000000))))) == (( Model1___be32)(__builtin_constant_p((__u32)((0xFF000000))) ? ((__u32)( (((__u32)((0xFF000000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0xFF000000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0xFF000000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0xFF000000)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0xFF000000))));
}

static inline __attribute__((no_instrument_function)) void Model1_ipv6_addr_set_v4mapped(const Model1___be32 Model1_addr,
       struct Model1_in6_addr *Model1_v4mapped)
{
 Model1_ipv6_addr_set(Model1_v4mapped,
   0, 0,
   (( Model1___be32)(__builtin_constant_p((__u32)((0x0000FFFF))) ? ((__u32)( (((__u32)((0x0000FFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x0000FFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x0000FFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x0000FFFF)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0x0000FFFF)))),
   Model1_addr);
}

/*
 * find the first different bit between two addresses
 * length of address must be a multiple of 32bits
 */
static inline __attribute__((no_instrument_function)) int Model1___ipv6_addr_diff32(const void *Model1_token1, const void *Model1_token2, int Model1_addrlen)
{
 const Model1___be32 *Model1_a1 = Model1_token1, *Model1_a2 = Model1_token2;
 int Model1_i;

 Model1_addrlen >>= 2;

 for (Model1_i = 0; Model1_i < Model1_addrlen; Model1_i++) {
  Model1___be32 Model1_xb = Model1_a1[Model1_i] ^ Model1_a2[Model1_i];
  if (Model1_xb)
   return Model1_i * 32 + 31 - Model1___fls((__builtin_constant_p((__u32)(( __u32)(Model1___be32)(Model1_xb))) ? ((__u32)( (((__u32)(( __u32)(Model1___be32)(Model1_xb)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model1___be32)(Model1_xb)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model1___be32)(Model1_xb)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model1___be32)(Model1_xb)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32(( __u32)(Model1___be32)(Model1_xb))));
 }

 /*
	 *	we should *never* get to this point since that 
	 *	would mean the addrs are equal
	 *
	 *	However, we do get to it 8) And exacly, when
	 *	addresses are equal 8)
	 *
	 *	ip route add 1111::/128 via ...
	 *	ip route add 1111::/64 via ...
	 *	and we are here.
	 *
	 *	Ideally, this function should stop comparison
	 *	at prefix length. It does not, but it is still OK,
	 *	if returned value is greater than prefix length.
	 *					--ANK (980803)
	 */
 return Model1_addrlen << 5;
}


static inline __attribute__((no_instrument_function)) int Model1___ipv6_addr_diff64(const void *Model1_token1, const void *Model1_token2, int Model1_addrlen)
{
 const Model1___be64 *Model1_a1 = Model1_token1, *Model1_a2 = Model1_token2;
 int Model1_i;

 Model1_addrlen >>= 3;

 for (Model1_i = 0; Model1_i < Model1_addrlen; Model1_i++) {
  Model1___be64 Model1_xb = Model1_a1[Model1_i] ^ Model1_a2[Model1_i];
  if (Model1_xb)
   return Model1_i * 64 + 63 - Model1___fls((__builtin_constant_p((__u64)(( __u64)(Model1___be64)(Model1_xb))) ? ((__u64)( (((__u64)(( __u64)(Model1___be64)(Model1_xb)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(( __u64)(Model1___be64)(Model1_xb)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(( __u64)(Model1___be64)(Model1_xb)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(( __u64)(Model1___be64)(Model1_xb)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(( __u64)(Model1___be64)(Model1_xb)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(( __u64)(Model1___be64)(Model1_xb)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(( __u64)(Model1___be64)(Model1_xb)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(( __u64)(Model1___be64)(Model1_xb)) & (__u64)0xff00000000000000ULL) >> 56))) : Model1___fswab64(( __u64)(Model1___be64)(Model1_xb))));
 }

 return Model1_addrlen << 6;
}


static inline __attribute__((no_instrument_function)) int Model1___ipv6_addr_diff(const void *Model1_token1, const void *Model1_token2, int Model1_addrlen)
{

 if (__builtin_constant_p(Model1_addrlen) && !(Model1_addrlen & 7))
  return Model1___ipv6_addr_diff64(Model1_token1, Model1_token2, Model1_addrlen);

 return Model1___ipv6_addr_diff32(Model1_token1, Model1_token2, Model1_addrlen);
}

static inline __attribute__((no_instrument_function)) int Model1_ipv6_addr_diff(const struct Model1_in6_addr *Model1_a1, const struct Model1_in6_addr *Model1_a2)
{
 return Model1___ipv6_addr_diff(Model1_a1, Model1_a2, sizeof(struct Model1_in6_addr));
}

Model1___be32 Model1_ipv6_select_ident(struct Model1_net *Model1_net,
    const struct Model1_in6_addr *Model1_daddr,
    const struct Model1_in6_addr *Model1_saddr);
void Model1_ipv6_proxy_select_ident(struct Model1_net *Model1_net, struct Model1_sk_buff *Model1_skb);

int Model1_ip6_dst_hoplimit(struct Model1_dst_entry *Model1_dst);

static inline __attribute__((no_instrument_function)) int Model1_ip6_sk_dst_hoplimit(struct Model1_ipv6_pinfo *Model1_np, struct Model1_flowi6 *Model1_fl6,
          struct Model1_dst_entry *Model1_dst)
{
 int Model1_hlimit;

 if (Model1_ipv6_addr_is_multicast(&Model1_fl6->Model1_daddr))
  Model1_hlimit = Model1_np->Model1_mcast_hops;
 else
  Model1_hlimit = Model1_np->Model1_hop_limit;
 if (Model1_hlimit < 0)
  Model1_hlimit = Model1_ip6_dst_hoplimit(Model1_dst);
 return Model1_hlimit;
}

/* copy IPv6 saddr & daddr to flow_keys, possibly using 64bit load/store
 * Equivalent to :	flow->v6addrs.src = iph->saddr;
 *			flow->v6addrs.dst = iph->daddr;
 */
static inline __attribute__((no_instrument_function)) void Model1_iph_to_flow_copy_v6addrs(struct Model1_flow_keys *Model1_flow,
         const struct Model1_ipv6hdr *Model1_iph)
{
 do { bool Model1___cond = !(!(__builtin_offsetof(typeof(Model1_flow->Model1_addrs), Model1_v6addrs.Model1_dst) != __builtin_offsetof(typeof(Model1_flow->Model1_addrs), Model1_v6addrs.Model1_src) + sizeof(Model1_flow->Model1_addrs.Model1_v6addrs.Model1_src))); extern void Model1___compiletime_assert_756(void) ; if (Model1___cond) Model1___compiletime_assert_756(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);


 ({ Model1_size_t Model1___len = (sizeof(Model1_flow->Model1_addrs.Model1_v6addrs)); void *Model1___ret; if (__builtin_constant_p(sizeof(Model1_flow->Model1_addrs.Model1_v6addrs)) && Model1___len >= 64) Model1___ret = Model1___memcpy((&Model1_flow->Model1_addrs.Model1_v6addrs), (&Model1_iph->Model1_saddr), Model1___len); else Model1___ret = __builtin_memcpy((&Model1_flow->Model1_addrs.Model1_v6addrs), (&Model1_iph->Model1_saddr), Model1___len); Model1___ret; });
 Model1_flow->Model1_control.Model1_addr_type = Model1_FLOW_DISSECTOR_KEY_IPV6_ADDRS;
}



/* Sysctl settings for net ipv6.auto_flowlabels */
static inline __attribute__((no_instrument_function)) Model1___be32 Model1_ip6_make_flowlabel(struct Model1_net *Model1_net, struct Model1_sk_buff *Model1_skb,
     Model1___be32 Model1_flowlabel, bool Model1_autolabel,
     struct Model1_flowi6 *Model1_fl6)
{
 Model1_u32 Model1_hash;

 if (Model1_flowlabel ||
     Model1_net->Model1_ipv6.Model1_sysctl.Model1_auto_flowlabels == 0 ||
     (!Model1_autolabel &&
      Model1_net->Model1_ipv6.Model1_sysctl.Model1_auto_flowlabels != 3))
  return Model1_flowlabel;

 Model1_hash = Model1_skb_get_hash_flowi6(Model1_skb, Model1_fl6);

 /* Since this is being sent on the wire obfuscate hash a bit
	 * to minimize possbility that any useful information to an
	 * attacker is leaked. Only lower 20 bits are relevant.
	 */
 Model1_rol32(Model1_hash, 16);

 Model1_flowlabel = ( Model1___be32)Model1_hash & (( Model1___be32)(__builtin_constant_p((__u32)((0x000FFFFF))) ? ((__u32)( (((__u32)((0x000FFFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x000FFFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x000FFFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x000FFFFF)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0x000FFFFF))));

 if (Model1_net->Model1_ipv6.Model1_sysctl.Model1_flowlabel_state_ranges)
  Model1_flowlabel |= (( Model1___be32)(__builtin_constant_p((__u32)((0x00080000))) ? ((__u32)( (((__u32)((0x00080000)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x00080000)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x00080000)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x00080000)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0x00080000))));

 return Model1_flowlabel;
}

static inline __attribute__((no_instrument_function)) int Model1_ip6_default_np_autolabel(struct Model1_net *Model1_net)
{
 switch (Model1_net->Model1_ipv6.Model1_sysctl.Model1_auto_flowlabels) {
 case 0:
 case 2:
 default:
  return 0;
 case 1:
 case 3:
  return 1;
 }
}
/*
 *	Header manipulation
 */
static inline __attribute__((no_instrument_function)) void Model1_ip6_flow_hdr(struct Model1_ipv6hdr *Model1_hdr, unsigned int Model1_tclass,
    Model1___be32 Model1_flowlabel)
{
 *(Model1___be32 *)Model1_hdr = (( Model1___be32)(__builtin_constant_p((__u32)((0x60000000 | (Model1_tclass << 20)))) ? ((__u32)( (((__u32)((0x60000000 | (Model1_tclass << 20))) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x60000000 | (Model1_tclass << 20))) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x60000000 | (Model1_tclass << 20))) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x60000000 | (Model1_tclass << 20))) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0x60000000 | (Model1_tclass << 20))))) | Model1_flowlabel;
}

static inline __attribute__((no_instrument_function)) Model1___be32 Model1_ip6_flowinfo(const struct Model1_ipv6hdr *Model1_hdr)
{
 return *(Model1___be32 *)Model1_hdr & (( Model1___be32)(__builtin_constant_p((__u32)((0x0FFFFFFF))) ? ((__u32)( (((__u32)((0x0FFFFFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x0FFFFFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x0FFFFFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x0FFFFFFF)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0x0FFFFFFF))));
}

static inline __attribute__((no_instrument_function)) Model1___be32 Model1_ip6_flowlabel(const struct Model1_ipv6hdr *Model1_hdr)
{
 return *(Model1___be32 *)Model1_hdr & (( Model1___be32)(__builtin_constant_p((__u32)((0x000FFFFF))) ? ((__u32)( (((__u32)((0x000FFFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x000FFFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x000FFFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x000FFFFF)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0x000FFFFF))));
}

static inline __attribute__((no_instrument_function)) Model1_u8 Model1_ip6_tclass(Model1___be32 Model1_flowinfo)
{
 return (__builtin_constant_p((__u32)(( __u32)(Model1___be32)(Model1_flowinfo & ((( Model1___be32)(__builtin_constant_p((__u32)((0x0FFFFFFF))) ? ((__u32)( (((__u32)((0x0FFFFFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x0FFFFFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x0FFFFFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x0FFFFFFF)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0x0FFFFFFF)))) & ~(( Model1___be32)(__builtin_constant_p((__u32)((0x000FFFFF))) ? ((__u32)( (((__u32)((0x000FFFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x000FFFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x000FFFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x000FFFFF)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0x000FFFFF)))))))) ? ((__u32)( (((__u32)(( __u32)(Model1___be32)(Model1_flowinfo & ((( Model1___be32)(__builtin_constant_p((__u32)((0x0FFFFFFF))) ? ((__u32)( (((__u32)((0x0FFFFFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x0FFFFFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x0FFFFFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x0FFFFFFF)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0x0FFFFFFF)))) & ~(( Model1___be32)(__builtin_constant_p((__u32)((0x000FFFFF))) ? ((__u32)( (((__u32)((0x000FFFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x000FFFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x000FFFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x000FFFFF)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0x000FFFFF))))))) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model1___be32)(Model1_flowinfo & ((( Model1___be32)(__builtin_constant_p((__u32)((0x0FFFFFFF))) ? ((__u32)( (((__u32)((0x0FFFFFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x0FFFFFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x0FFFFFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x0FFFFFFF)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0x0FFFFFFF)))) & ~(( Model1___be32)(__builtin_constant_p((__u32)((0x000FFFFF))) ? ((__u32)( (((__u32)((0x000FFFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x000FFFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x000FFFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x000FFFFF)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0x000FFFFF))))))) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model1___be32)(Model1_flowinfo & ((( Model1___be32)(__builtin_constant_p((__u32)((0x0FFFFFFF))) ? ((__u32)( (((__u32)((0x0FFFFFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x0FFFFFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x0FFFFFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x0FFFFFFF)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0x0FFFFFFF)))) & ~(( Model1___be32)(__builtin_constant_p((__u32)((0x000FFFFF))) ? ((__u32)( (((__u32)((0x000FFFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x000FFFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x000FFFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x000FFFFF)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0x000FFFFF))))))) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model1___be32)(Model1_flowinfo & ((( Model1___be32)(__builtin_constant_p((__u32)((0x0FFFFFFF))) ? ((__u32)( (((__u32)((0x0FFFFFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x0FFFFFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x0FFFFFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x0FFFFFFF)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0x0FFFFFFF)))) & ~(( Model1___be32)(__builtin_constant_p((__u32)((0x000FFFFF))) ? ((__u32)( (((__u32)((0x000FFFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x000FFFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x000FFFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x000FFFFF)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0x000FFFFF))))))) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32(( __u32)(Model1___be32)(Model1_flowinfo & ((( Model1___be32)(__builtin_constant_p((__u32)((0x0FFFFFFF))) ? ((__u32)( (((__u32)((0x0FFFFFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x0FFFFFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x0FFFFFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x0FFFFFFF)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0x0FFFFFFF)))) & ~(( Model1___be32)(__builtin_constant_p((__u32)((0x000FFFFF))) ? ((__u32)( (((__u32)((0x000FFFFF)) & (__u32)0x000000ffUL) << 24) | (((__u32)((0x000FFFFF)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((0x000FFFFF)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((0x000FFFFF)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((0x000FFFFF)))))))) >> 20;
}

static inline __attribute__((no_instrument_function)) Model1___be32 Model1_ip6_make_flowinfo(unsigned int Model1_tclass, Model1___be32 Model1_flowlabel)
{
 return (( Model1___be32)(__builtin_constant_p((__u32)((Model1_tclass << 20))) ? ((__u32)( (((__u32)((Model1_tclass << 20)) & (__u32)0x000000ffUL) << 24) | (((__u32)((Model1_tclass << 20)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((Model1_tclass << 20)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((Model1_tclass << 20)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((Model1_tclass << 20)))) | Model1_flowlabel;
}

/*
 *	Prototypes exported by ipv6
 */

/*
 *	rcv function (called from netdevice level)
 */

int Model1_ipv6_rcv(struct Model1_sk_buff *Model1_skb, struct Model1_net_device *Model1_dev,
      struct Model1_packet_type *Model1_pt, struct Model1_net_device *Model1_orig_dev);

int Model1_ip6_rcv_finish(struct Model1_net *Model1_net, struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);

/*
 *	upper-layer output functions
 */
int Model1_ip6_xmit(const struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb, struct Model1_flowi6 *Model1_fl6,
      struct Model1_ipv6_txoptions *Model1_opt, int Model1_tclass);

int Model1_ip6_find_1stfragopt(struct Model1_sk_buff *Model1_skb, Model1_u8 **Model1_nexthdr);

int Model1_ip6_append_data(struct Model1_sock *Model1_sk,
      int Model1_getfrag(void *Model1_from, char *Model1_to, int Model1_offset, int Model1_len,
    int Model1_odd, struct Model1_sk_buff *Model1_skb),
      void *Model1_from, int Model1_length, int Model1_transhdrlen,
      struct Model1_ipcm6_cookie *Model1_ipc6, struct Model1_flowi6 *Model1_fl6,
      struct Model1_rt6_info *Model1_rt, unsigned int Model1_flags,
      const struct Model1_sockcm_cookie *Model1_sockc);

int Model1_ip6_push_pending_frames(struct Model1_sock *Model1_sk);

void Model1_ip6_flush_pending_frames(struct Model1_sock *Model1_sk);

int Model1_ip6_send_skb(struct Model1_sk_buff *Model1_skb);

struct Model1_sk_buff *Model1___ip6_make_skb(struct Model1_sock *Model1_sk, struct Model1_sk_buff_head *Model1_queue,
          struct Model1_inet_cork_full *Model1_cork,
          struct Model1_inet6_cork *Model1_v6_cork);
struct Model1_sk_buff *Model1_ip6_make_skb(struct Model1_sock *Model1_sk,
        int Model1_getfrag(void *Model1_from, char *Model1_to, int Model1_offset,
      int Model1_len, int Model1_odd, struct Model1_sk_buff *Model1_skb),
        void *Model1_from, int Model1_length, int Model1_transhdrlen,
        struct Model1_ipcm6_cookie *Model1_ipc6, struct Model1_flowi6 *Model1_fl6,
        struct Model1_rt6_info *Model1_rt, unsigned int Model1_flags,
        const struct Model1_sockcm_cookie *Model1_sockc);

static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1_ip6_finish_skb(struct Model1_sock *Model1_sk)
{
 return Model1___ip6_make_skb(Model1_sk, &Model1_sk->Model1_sk_write_queue, &Model1_inet_sk(Model1_sk)->Model1_cork,
         &Model1_inet6_sk(Model1_sk)->Model1_cork);
}

int Model1_ip6_dst_lookup(struct Model1_net *Model1_net, struct Model1_sock *Model1_sk, struct Model1_dst_entry **Model1_dst,
     struct Model1_flowi6 *Model1_fl6);
struct Model1_dst_entry *Model1_ip6_dst_lookup_flow(const struct Model1_sock *Model1_sk, struct Model1_flowi6 *Model1_fl6,
          const struct Model1_in6_addr *Model1_final_dst);
struct Model1_dst_entry *Model1_ip6_sk_dst_lookup_flow(struct Model1_sock *Model1_sk, struct Model1_flowi6 *Model1_fl6,
      const struct Model1_in6_addr *Model1_final_dst);
struct Model1_dst_entry *Model1_ip6_blackhole_route(struct Model1_net *Model1_net,
          struct Model1_dst_entry *Model1_orig_dst);

/*
 *	skb processing functions
 */

int Model1_ip6_output(struct Model1_net *Model1_net, struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);
int Model1_ip6_forward(struct Model1_sk_buff *Model1_skb);
int Model1_ip6_input(struct Model1_sk_buff *Model1_skb);
int Model1_ip6_mc_input(struct Model1_sk_buff *Model1_skb);

int Model1___ip6_local_out(struct Model1_net *Model1_net, struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);
int Model1_ip6_local_out(struct Model1_net *Model1_net, struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);

/*
 *	Extension header (options) processing
 */

void Model1_ipv6_push_nfrag_opts(struct Model1_sk_buff *Model1_skb, struct Model1_ipv6_txoptions *Model1_opt,
     Model1_u8 *Model1_proto, struct Model1_in6_addr **Model1_daddr_p);
void Model1_ipv6_push_frag_opts(struct Model1_sk_buff *Model1_skb, struct Model1_ipv6_txoptions *Model1_opt,
    Model1_u8 *Model1_proto);

int Model1_ipv6_skip_exthdr(const struct Model1_sk_buff *, int Model1_start, Model1_u8 *Model1_nexthdrp,
       Model1___be16 *Model1_frag_offp);

bool Model1_ipv6_ext_hdr(Model1_u8 Model1_nexthdr);

enum {
 Model1_IP6_FH_F_FRAG = (1 << 0),
 Model1_IP6_FH_F_AUTH = (1 << 1),
 Model1_IP6_FH_F_SKIP_RH = (1 << 2),
};

/* find specified header and get offset to it */
int Model1_ipv6_find_hdr(const struct Model1_sk_buff *Model1_skb, unsigned int *Model1_offset, int Model1_target,
    unsigned short *Model1_fragoff, int *Model1_fragflg);

int Model1_ipv6_find_tlv(const struct Model1_sk_buff *Model1_skb, int Model1_offset, int Model1_type);

struct Model1_in6_addr *Model1_fl6_update_dst(struct Model1_flowi6 *Model1_fl6,
    const struct Model1_ipv6_txoptions *Model1_opt,
    struct Model1_in6_addr *Model1_orig);

/*
 *	socket options (ipv6_sockglue.c)
 */

int Model1_ipv6_setsockopt(struct Model1_sock *Model1_sk, int Model1_level, int Model1_optname,
      char *Model1_optval, unsigned int Model1_optlen);
int Model1_ipv6_getsockopt(struct Model1_sock *Model1_sk, int Model1_level, int Model1_optname,
      char *Model1_optval, int *Model1_optlen);
int Model1_compat_ipv6_setsockopt(struct Model1_sock *Model1_sk, int Model1_level, int Model1_optname,
      char *Model1_optval, unsigned int Model1_optlen);
int Model1_compat_ipv6_getsockopt(struct Model1_sock *Model1_sk, int Model1_level, int Model1_optname,
      char *Model1_optval, int *Model1_optlen);

int Model1_ip6_datagram_connect(struct Model1_sock *Model1_sk, struct Model1_sockaddr *Model1_addr, int Model1_addr_len);
int Model1_ip6_datagram_connect_v6_only(struct Model1_sock *Model1_sk, struct Model1_sockaddr *Model1_addr,
     int Model1_addr_len);
int Model1_ip6_datagram_dst_update(struct Model1_sock *Model1_sk, bool Model1_fix_sk_saddr);
void Model1_ip6_datagram_release_cb(struct Model1_sock *Model1_sk);

int Model1_ipv6_recv_error(struct Model1_sock *Model1_sk, struct Model1_msghdr *Model1_msg, int Model1_len,
      int *Model1_addr_len);
int Model1_ipv6_recv_rxpmtu(struct Model1_sock *Model1_sk, struct Model1_msghdr *Model1_msg, int Model1_len,
       int *Model1_addr_len);
void Model1_ipv6_icmp_error(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb, int err, Model1___be16 Model1_port,
       Model1_u32 Model1_info, Model1_u8 *Model1_payload);
void Model1_ipv6_local_error(struct Model1_sock *Model1_sk, int err, struct Model1_flowi6 *Model1_fl6, Model1_u32 Model1_info);
void Model1_ipv6_local_rxpmtu(struct Model1_sock *Model1_sk, struct Model1_flowi6 *Model1_fl6, Model1_u32 Model1_mtu);

int Model1_inet6_release(struct Model1_socket *Model1_sock);
int Model1_inet6_bind(struct Model1_socket *Model1_sock, struct Model1_sockaddr *Model1_uaddr, int Model1_addr_len);
int Model1_inet6_getname(struct Model1_socket *Model1_sock, struct Model1_sockaddr *Model1_uaddr, int *Model1_uaddr_len,
    int Model1_peer);
int Model1_inet6_ioctl(struct Model1_socket *Model1_sock, unsigned int Model1_cmd, unsigned long Model1_arg);

int Model1_inet6_hash_connect(struct Model1_inet_timewait_death_row *Model1_death_row,
         struct Model1_sock *Model1_sk);

/*
 * reassembly.c
 */
extern const struct Model1_proto_ops Model1_inet6_stream_ops;
extern const struct Model1_proto_ops Model1_inet6_dgram_ops;

struct Model1_group_source_req;
struct Model1_group_filter;

int Model1_ip6_mc_source(int Model1_add, int Model1_omode, struct Model1_sock *Model1_sk,
    struct Model1_group_source_req *Model1_pgsr);
int Model1_ip6_mc_msfilter(struct Model1_sock *Model1_sk, struct Model1_group_filter *Model1_gsf);
int Model1_ip6_mc_msfget(struct Model1_sock *Model1_sk, struct Model1_group_filter *Model1_gsf,
    struct Model1_group_filter *Model1_optval, int *Model1_optlen);


int Model1_ac6_proc_init(struct Model1_net *Model1_net);
void Model1_ac6_proc_exit(struct Model1_net *Model1_net);
int Model1_raw6_proc_init(void);
void Model1_raw6_proc_exit(void);
int Model1_tcp6_proc_init(struct Model1_net *Model1_net);
void Model1_tcp6_proc_exit(struct Model1_net *Model1_net);
int Model1_udp6_proc_init(struct Model1_net *Model1_net);
void Model1_udp6_proc_exit(struct Model1_net *Model1_net);
int Model1_udplite6_proc_init(void);
void Model1_udplite6_proc_exit(void);
int Model1_ipv6_misc_proc_init(void);
void Model1_ipv6_misc_proc_exit(void);
int Model1_snmp6_register_dev(struct Model1_inet6_dev *Model1_idev);
int Model1_snmp6_unregister_dev(struct Model1_inet6_dev *Model1_idev);
extern struct Model1_ctl_table Model1_ipv6_route_table_template[];

struct Model1_ctl_table *Model1_ipv6_icmp_sysctl_init(struct Model1_net *Model1_net);
struct Model1_ctl_table *Model1_ipv6_route_sysctl_init(struct Model1_net *Model1_net);
int Model1_ipv6_sysctl_register(void);
void Model1_ipv6_sysctl_unregister(void);


int Model1_ipv6_sock_mc_join(struct Model1_sock *Model1_sk, int Model1_ifindex,
        const struct Model1_in6_addr *Model1_addr);
int Model1_ipv6_sock_mc_drop(struct Model1_sock *Model1_sk, int Model1_ifindex,
        const struct Model1_in6_addr *Model1_addr);


/* IPv4 address key for cache lookups */
struct Model1_ipv4_addr_key {
 Model1___be32 Model1_addr;
 int Model1_vif;
};



struct Model1_inetpeer_addr {
 union {
  struct Model1_ipv4_addr_key Model1_a4;
  struct Model1_in6_addr Model1_a6;
  Model1_u32 Model1_key[(sizeof(struct Model1_in6_addr) / sizeof(Model1_u32))];
 };
 Model1___u16 Model1_family;
};

struct Model1_inet_peer {
 /* group together avl_left,avl_right,v4daddr to speedup lookups */
 struct Model1_inet_peer *Model1_avl_left, *Model1_avl_right;
 struct Model1_inetpeer_addr Model1_daddr;
 __u32 Model1_avl_height;

 Model1_u32 Model1_metrics[(Model1___RTAX_MAX - 1)];
 Model1_u32 Model1_rate_tokens; /* rate limiting for ICMP */
 unsigned long Model1_rate_last;
 union {
  struct Model1_list_head Model1_gc_list;
  struct Model1_callback_head Model1_gc_rcu;
 };
 /*
	 * Once inet_peer is queued for deletion (refcnt == -1), following field
	 * is not available: rid
	 * We can share memory with rcu_head to help keep inet_peer small.
	 */
 union {
  struct {
   Model1_atomic_t Model1_rid; /* Frag reception counter */
  };
  struct Model1_callback_head Model1_rcu;
  struct Model1_inet_peer *Model1_gc_next;
 };

 /* following fields might be frequently dirtied */
 __u32 Model1_dtime; /* the time of last use of not referenced entries */
 Model1_atomic_t Model1_refcnt;
};

struct Model1_inet_peer_base {
 struct Model1_inet_peer *Model1_root;
 Model1_seqlock_t Model1_lock;
 int Model1_total;
};

void Model1_inet_peer_base_init(struct Model1_inet_peer_base *);

void Model1_inet_initpeers(void) __attribute__ ((__section__(".init.text"))) __attribute__((no_instrument_function));



static inline __attribute__((no_instrument_function)) void Model1_inetpeer_set_addr_v4(struct Model1_inetpeer_addr *Model1_iaddr, Model1___be32 Model1_ip)
{
 Model1_iaddr->Model1_a4.Model1_addr = Model1_ip;
 Model1_iaddr->Model1_a4.Model1_vif = 0;
 Model1_iaddr->Model1_family = 2;
}

static inline __attribute__((no_instrument_function)) Model1___be32 Model1_inetpeer_get_addr_v4(struct Model1_inetpeer_addr *Model1_iaddr)
{
 return Model1_iaddr->Model1_a4.Model1_addr;
}

static inline __attribute__((no_instrument_function)) void Model1_inetpeer_set_addr_v6(struct Model1_inetpeer_addr *Model1_iaddr,
     struct Model1_in6_addr *Model1_in6)
{
 Model1_iaddr->Model1_a6 = *Model1_in6;
 Model1_iaddr->Model1_family = 10;
}

static inline __attribute__((no_instrument_function)) struct Model1_in6_addr *Model1_inetpeer_get_addr_v6(struct Model1_inetpeer_addr *Model1_iaddr)
{
 return &Model1_iaddr->Model1_a6;
}

/* can be called with or without local BH being disabled */
struct Model1_inet_peer *Model1_inet_getpeer(struct Model1_inet_peer_base *Model1_base,
          const struct Model1_inetpeer_addr *Model1_daddr,
          int Model1_create);

static inline __attribute__((no_instrument_function)) struct Model1_inet_peer *Model1_inet_getpeer_v4(struct Model1_inet_peer_base *Model1_base,
      Model1___be32 Model1_v4daddr,
      int Model1_vif, int Model1_create)
{
 struct Model1_inetpeer_addr Model1_daddr;

 Model1_daddr.Model1_a4.Model1_addr = Model1_v4daddr;
 Model1_daddr.Model1_a4.Model1_vif = Model1_vif;
 Model1_daddr.Model1_family = 2;
 return Model1_inet_getpeer(Model1_base, &Model1_daddr, Model1_create);
}

static inline __attribute__((no_instrument_function)) struct Model1_inet_peer *Model1_inet_getpeer_v6(struct Model1_inet_peer_base *Model1_base,
      const struct Model1_in6_addr *Model1_v6daddr,
      int Model1_create)
{
 struct Model1_inetpeer_addr Model1_daddr;

 Model1_daddr.Model1_a6 = *Model1_v6daddr;
 Model1_daddr.Model1_family = 10;
 return Model1_inet_getpeer(Model1_base, &Model1_daddr, Model1_create);
}

static inline __attribute__((no_instrument_function)) int Model1_inetpeer_addr_cmp(const struct Model1_inetpeer_addr *Model1_a,
        const struct Model1_inetpeer_addr *Model1_b)
{
 int Model1_i, Model1_n;

 if (Model1_a->Model1_family == 2)
  Model1_n = sizeof(Model1_a->Model1_a4) / sizeof(Model1_u32);
 else
  Model1_n = sizeof(Model1_a->Model1_a6) / sizeof(Model1_u32);

 for (Model1_i = 0; Model1_i < Model1_n; Model1_i++) {
  if (Model1_a->Model1_key[Model1_i] == Model1_b->Model1_key[Model1_i])
   continue;
  if (Model1_a->Model1_key[Model1_i] < Model1_b->Model1_key[Model1_i])
   return -1;
  return 1;
 }

 return 0;
}

/* can be called from BH context or outside */
void Model1_inet_putpeer(struct Model1_inet_peer *Model1_p);
bool Model1_inet_peer_xrlim_allow(struct Model1_inet_peer *Model1_peer, int Model1_timeout);

void Model1_inetpeer_invalidate_tree(struct Model1_inet_peer_base *);


/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET  is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for the Forwarding Information Base.
 *
 * Authors:	A.N.Kuznetsov, <kuznet@ms2.inr.ac.ru>
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */
struct Model1_fib_config {
 Model1_u8 Model1_fc_dst_len;
 Model1_u8 Model1_fc_tos;
 Model1_u8 Model1_fc_protocol;
 Model1_u8 Model1_fc_scope;
 Model1_u8 Model1_fc_type;
 /* 3 bytes unused */
 Model1_u32 Model1_fc_table;
 Model1___be32 Model1_fc_dst;
 Model1___be32 Model1_fc_gw;
 int Model1_fc_oif;
 Model1_u32 Model1_fc_flags;
 Model1_u32 Model1_fc_priority;
 Model1___be32 Model1_fc_prefsrc;
 struct Model1_nlattr *Model1_fc_mx;
 struct Model1_rtnexthop *Model1_fc_mp;
 int Model1_fc_mx_len;
 int Model1_fc_mp_len;
 Model1_u32 Model1_fc_flow;
 Model1_u32 Model1_fc_nlflags;
 struct Model1_nl_info Model1_fc_nlinfo;
 struct Model1_nlattr *Model1_fc_encap;
 Model1_u16 Model1_fc_encap_type;
};

struct Model1_fib_info;
struct Model1_rtable;

struct Model1_fib_nh_exception {
 struct Model1_fib_nh_exception *Model1_fnhe_next;
 int Model1_fnhe_genid;
 Model1___be32 Model1_fnhe_daddr;
 Model1_u32 Model1_fnhe_pmtu;
 Model1___be32 Model1_fnhe_gw;
 unsigned long Model1_fnhe_expires;
 struct Model1_rtable *Model1_fnhe_rth_input;
 struct Model1_rtable *Model1_fnhe_rth_output;
 unsigned long Model1_fnhe_stamp;
 struct Model1_callback_head Model1_rcu;
};

struct Model1_fnhe_hash_bucket {
 struct Model1_fib_nh_exception *Model1_chain;
};





struct Model1_fib_nh {
 struct Model1_net_device *Model1_nh_dev;
 struct Model1_hlist_node Model1_nh_hash;
 struct Model1_fib_info *Model1_nh_parent;
 unsigned int Model1_nh_flags;
 unsigned char Model1_nh_scope;

 int Model1_nh_weight;
 Model1_atomic_t Model1_nh_upper_bound;




 int Model1_nh_oif;
 Model1___be32 Model1_nh_gw;
 Model1___be32 Model1_nh_saddr;
 int Model1_nh_saddr_genid;
 struct Model1_rtable * *Model1_nh_pcpu_rth_output;
 struct Model1_rtable *Model1_nh_rth_input;
 struct Model1_fnhe_hash_bucket *Model1_nh_exceptions;
 struct Model1_lwtunnel_state *Model1_nh_lwtstate;
};

/*
 * This structure contains data shared by many of routes.
 */

struct Model1_fib_info {
 struct Model1_hlist_node Model1_fib_hash;
 struct Model1_hlist_node Model1_fib_lhash;
 struct Model1_net *Model1_fib_net;
 int Model1_fib_treeref;
 Model1_atomic_t Model1_fib_clntref;
 unsigned int Model1_fib_flags;
 unsigned char Model1_fib_dead;
 unsigned char Model1_fib_protocol;
 unsigned char Model1_fib_scope;
 unsigned char Model1_fib_type;
 Model1___be32 Model1_fib_prefsrc;
 Model1_u32 Model1_fib_tb_id;
 Model1_u32 Model1_fib_priority;
 Model1_u32 *Model1_fib_metrics;




 int Model1_fib_nhs;

 int Model1_fib_weight;

 struct Model1_callback_head Model1_rcu;
 struct Model1_fib_nh Model1_fib_nh[0];

};



struct Model1_fib_rule;


struct Model1_fib_table;
struct Model1_fib_result {
 unsigned char Model1_prefixlen;
 unsigned char Model1_nh_sel;
 unsigned char Model1_type;
 unsigned char Model1_scope;
 Model1_u32 Model1_tclassid;
 struct Model1_fib_info *Model1_fi;
 struct Model1_fib_table *Model1_table;
 struct Model1_hlist_head *Model1_fa_head;
};

struct Model1_fib_result_nl {
 Model1___be32 Model1_fl_addr; /* To be looked up*/
 Model1_u32 Model1_fl_mark;
 unsigned char Model1_fl_tos;
 unsigned char Model1_fl_scope;
 unsigned char Model1_tb_id_in;

 unsigned char Model1_tb_id; /* Results */
 unsigned char Model1_prefixlen;
 unsigned char Model1_nh_sel;
 unsigned char Model1_type;
 unsigned char Model1_scope;
 int err;
};
Model1___be32 Model1_fib_info_update_nh_saddr(struct Model1_net *Model1_net, struct Model1_fib_nh *Model1_nh);
struct Model1_fib_table {
 struct Model1_hlist_node Model1_tb_hlist;
 Model1_u32 Model1_tb_id;
 int Model1_tb_num_default;
 struct Model1_callback_head Model1_rcu;
 unsigned long *Model1_tb_data;
 unsigned long Model1___data[0];
};

int Model1_fib_table_lookup(struct Model1_fib_table *Model1_tb, const struct Model1_flowi4 *Model1_flp,
       struct Model1_fib_result *Model1_res, int Model1_fib_flags);
int Model1_fib_table_insert(struct Model1_fib_table *, struct Model1_fib_config *);
int Model1_fib_table_delete(struct Model1_fib_table *, struct Model1_fib_config *);
int Model1_fib_table_dump(struct Model1_fib_table *Model1_table, struct Model1_sk_buff *Model1_skb,
     struct Model1_netlink_callback *Model1_cb);
int Model1_fib_table_flush(struct Model1_fib_table *Model1_table);
struct Model1_fib_table *Model1_fib_trie_unmerge(struct Model1_fib_table *Model1_main_tb);
void Model1_fib_table_flush_external(struct Model1_fib_table *Model1_table);
void Model1_fib_free_table(struct Model1_fib_table *Model1_tb);
int Model1_fib4_rules_init(struct Model1_net *Model1_net);
void Model1_fib4_rules_exit(struct Model1_net *Model1_net);

struct Model1_fib_table *Model1_fib_new_table(struct Model1_net *Model1_net, Model1_u32 Model1_id);
struct Model1_fib_table *Model1_fib_get_table(struct Model1_net *Model1_net, Model1_u32 Model1_id);

int Model1___fib_lookup(struct Model1_net *Model1_net, struct Model1_flowi4 *Model1_flp,
   struct Model1_fib_result *Model1_res, unsigned int Model1_flags);

static inline __attribute__((no_instrument_function)) int Model1_fib_lookup(struct Model1_net *Model1_net, struct Model1_flowi4 *Model1_flp,
        struct Model1_fib_result *Model1_res, unsigned int Model1_flags)
{
 struct Model1_fib_table *Model1_tb;
 int err = -101;

 Model1_flags |= 1;
 if (Model1_net->Model1_ipv4.Model1_fib_has_custom_rules)
  return Model1___fib_lookup(Model1_net, Model1_flp, Model1_res, Model1_flags);

 Model1_rcu_read_lock();

 Model1_res->Model1_tclassid = 0;

 Model1_tb = ({ typeof(*(Model1_net->Model1_ipv4.Model1_fib_main)) *Model1_________p1 = (typeof(*(Model1_net->Model1_ipv4.Model1_fib_main)) *)({ typeof((Model1_net->Model1_ipv4.Model1_fib_main)) Model1__________p1 = ({ union { typeof((Model1_net->Model1_ipv4.Model1_fib_main)) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&((Model1_net->Model1_ipv4.Model1_fib_main)), Model1___u.Model1___c, sizeof((Model1_net->Model1_ipv4.Model1_fib_main))); else Model1___read_once_size_nocheck(&((Model1_net->Model1_ipv4.Model1_fib_main)), Model1___u.Model1___c, sizeof((Model1_net->Model1_ipv4.Model1_fib_main))); Model1___u.Model1___val; }); typeof(*((Model1_net->Model1_ipv4.Model1_fib_main))) *Model1____typecheck_p __attribute__((unused)); do { } while (0); (Model1__________p1); }); do { } while (0); ; ((typeof(*(Model1_net->Model1_ipv4.Model1_fib_main)) *)(Model1_________p1)); });
 if (Model1_tb)
  err = Model1_fib_table_lookup(Model1_tb, Model1_flp, Model1_res, Model1_flags);

 if (!err)
  goto Model1_out;

 Model1_tb = ({ typeof(*(Model1_net->Model1_ipv4.Model1_fib_default)) *Model1_________p1 = (typeof(*(Model1_net->Model1_ipv4.Model1_fib_default)) *)({ typeof((Model1_net->Model1_ipv4.Model1_fib_default)) Model1__________p1 = ({ union { typeof((Model1_net->Model1_ipv4.Model1_fib_default)) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&((Model1_net->Model1_ipv4.Model1_fib_default)), Model1___u.Model1___c, sizeof((Model1_net->Model1_ipv4.Model1_fib_default))); else Model1___read_once_size_nocheck(&((Model1_net->Model1_ipv4.Model1_fib_default)), Model1___u.Model1___c, sizeof((Model1_net->Model1_ipv4.Model1_fib_default))); Model1___u.Model1___val; }); typeof(*((Model1_net->Model1_ipv4.Model1_fib_default))) *Model1____typecheck_p __attribute__((unused)); do { } while (0); (Model1__________p1); }); do { } while (0); ; ((typeof(*(Model1_net->Model1_ipv4.Model1_fib_default)) *)(Model1_________p1)); });
 if (Model1_tb)
  err = Model1_fib_table_lookup(Model1_tb, Model1_flp, Model1_res, Model1_flags);

Model1_out:
 if (err == -11)
  err = -101;

 Model1_rcu_read_unlock();

 return err;
}



/* Exported by fib_frontend.c */
extern const struct Model1_nla_policy Model1_rtm_ipv4_policy[];
void Model1_ip_fib_init(void);
Model1___be32 Model1_fib_compute_spec_dst(struct Model1_sk_buff *Model1_skb);
int Model1_fib_validate_source(struct Model1_sk_buff *Model1_skb, Model1___be32 Model1_src, Model1___be32 Model1_dst,
   Model1_u8 Model1_tos, int Model1_oif, struct Model1_net_device *Model1_dev,
   struct Model1_in_device *Model1_idev, Model1_u32 *Model1_itag);
void Model1_fib_select_default(const struct Model1_flowi4 *Model1_flp, struct Model1_fib_result *Model1_res);






static inline __attribute__((no_instrument_function)) int Model1_fib_num_tclassid_users(struct Model1_net *Model1_net)
{
 return 0;
}

int Model1_fib_unmerge(struct Model1_net *Model1_net);
void Model1_fib_flush_external(struct Model1_net *Model1_net);

/* Exported by fib_semantics.c */
int Model1_ip_fib_check_default(Model1___be32 Model1_gw, struct Model1_net_device *Model1_dev);
int Model1_fib_sync_down_dev(struct Model1_net_device *Model1_dev, unsigned long Model1_event, bool Model1_force);
int Model1_fib_sync_down_addr(struct Model1_net_device *Model1_dev, Model1___be32 Model1_local);
int Model1_fib_sync_up(struct Model1_net_device *Model1_dev, unsigned int Model1_nh_flags);

extern Model1_u32 Model1_fib_multipath_secret __attribute__((__section__(".data..read_mostly")));

static inline __attribute__((no_instrument_function)) int Model1_fib_multipath_hash(Model1___be32 Model1_saddr, Model1___be32 Model1_daddr)
{
 return Model1_jhash_2words(( Model1_u32)Model1_saddr, ( Model1_u32)Model1_daddr,
       Model1_fib_multipath_secret) >> 1;
}

void Model1_fib_select_multipath(struct Model1_fib_result *Model1_res, int Model1_hash);
void Model1_fib_select_path(struct Model1_net *Model1_net, struct Model1_fib_result *Model1_res,
       struct Model1_flowi4 *Model1_fl4, int Model1_mp_hash);

/* Exported by fib_trie.c */
void Model1_fib_trie_init(void);
struct Model1_fib_table *Model1_fib_trie_table(Model1_u32 Model1_id, struct Model1_fib_table *Model1_alias);

static inline __attribute__((no_instrument_function)) void Model1_fib_combine_itag(Model1_u32 *Model1_itag, const struct Model1_fib_result *Model1_res)
{
}

void Model1_free_fib_info(struct Model1_fib_info *Model1_fi);

static inline __attribute__((no_instrument_function)) void Model1_fib_info_put(struct Model1_fib_info *Model1_fi)
{
 if (Model1_atomic_dec_and_test(&Model1_fi->Model1_fib_clntref))
  Model1_free_fib_info(Model1_fi);
}


int Model1_fib_proc_init(struct Model1_net *Model1_net);
void Model1_fib_proc_exit(struct Model1_net *Model1_net);




/* IPv4 routing cache flags */




/* Obsolete flag. About to be deleted */


/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Global definitions for the IP router interface.
 *
 * Version:	@(#)route.h	1.0.3	05/27/93
 *
 * Authors:	Original taken from Berkeley UNIX 4.3, (c) UCB 1986-1988
 *		for the purposes of compatibility only.
 *
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *
 * Changes:
 *              Mike McLagan    :       Routing by source
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */






/* This structure gets passed by the SIOCADDRT and SIOCDELRT calls. */
struct Model1_rtentry {
 unsigned long Model1_rt_pad1;
 struct Model1_sockaddr Model1_rt_dst; /* target address		*/
 struct Model1_sockaddr Model1_rt_gateway; /* gateway addr (RTF_GATEWAY)	*/
 struct Model1_sockaddr Model1_rt_genmask; /* target network mask (IP)	*/
 unsigned short Model1_rt_flags;
 short Model1_rt_pad2;
 unsigned long Model1_rt_pad3;
 void *Model1_rt_pad4;
 short Model1_rt_metric; /* +1 for binary compatibility!	*/
 char *Model1_rt_dev; /* forcing the device at add	*/
 unsigned long Model1_rt_mtu; /* per route MTU/Window 	*/



 unsigned long Model1_rt_window; /* Window clamping 		*/
 unsigned short Model1_rt_irtt; /* Initial RTT			*/
};
/*
 *	<linux/ipv6_route.h> uses RTF values >= 64k
 */




/* IPv4 datagram length is stored into 16bit field (tot_len) */







struct Model1_fib_nh;
struct Model1_fib_info;
struct Model1_uncached_list;
struct Model1_rtable {
 struct Model1_dst_entry Model1_dst;

 int Model1_rt_genid;
 unsigned int Model1_rt_flags;
 Model1___u16 Model1_rt_type;
 __u8 Model1_rt_is_input;
 __u8 Model1_rt_uses_gateway;

 int Model1_rt_iif;

 /* Info on neighbour */
 Model1___be32 Model1_rt_gateway;

 /* Miscellaneous cached information */
 Model1_u32 Model1_rt_pmtu;

 Model1_u32 Model1_rt_table_id;

 struct Model1_list_head Model1_rt_uncached;
 struct Model1_uncached_list *Model1_rt_uncached_list;
};

static inline __attribute__((no_instrument_function)) bool Model1_rt_is_input_route(const struct Model1_rtable *Model1_rt)
{
 return Model1_rt->Model1_rt_is_input != 0;
}

static inline __attribute__((no_instrument_function)) bool Model1_rt_is_output_route(const struct Model1_rtable *Model1_rt)
{
 return Model1_rt->Model1_rt_is_input == 0;
}

static inline __attribute__((no_instrument_function)) Model1___be32 Model1_rt_nexthop(const struct Model1_rtable *Model1_rt, Model1___be32 Model1_daddr)
{
 if (Model1_rt->Model1_rt_gateway)
  return Model1_rt->Model1_rt_gateway;
 return Model1_daddr;
}

struct Model1_ip_rt_acct {
 __u32 Model1_o_bytes;
 __u32 Model1_o_packets;
 __u32 Model1_i_bytes;
 __u32 Model1_i_packets;
};

struct Model1_rt_cache_stat {
        unsigned int Model1_in_slow_tot;
        unsigned int Model1_in_slow_mc;
        unsigned int Model1_in_no_route;
        unsigned int Model1_in_brd;
        unsigned int Model1_in_martian_dst;
        unsigned int Model1_in_martian_src;
        unsigned int Model1_out_slow_tot;
        unsigned int Model1_out_slow_mc;
};

extern struct Model1_ip_rt_acct *Model1_ip_rt_acct;

struct Model1_in_device;

int Model1_ip_rt_init(void);
void Model1_rt_cache_flush(struct Model1_net *Model1_net);
void Model1_rt_flush_dev(struct Model1_net_device *Model1_dev);
struct Model1_rtable *Model1___ip_route_output_key_hash(struct Model1_net *, struct Model1_flowi4 *Model1_flp,
       int Model1_mp_hash);

static inline __attribute__((no_instrument_function)) struct Model1_rtable *Model1___ip_route_output_key(struct Model1_net *Model1_net,
         struct Model1_flowi4 *Model1_flp)
{
 return Model1___ip_route_output_key_hash(Model1_net, Model1_flp, -1);
}

struct Model1_rtable *Model1_ip_route_output_flow(struct Model1_net *, struct Model1_flowi4 *Model1_flp,
        const struct Model1_sock *Model1_sk);
struct Model1_dst_entry *Model1_ipv4_blackhole_route(struct Model1_net *Model1_net,
           struct Model1_dst_entry *Model1_dst_orig);

static inline __attribute__((no_instrument_function)) struct Model1_rtable *Model1_ip_route_output_key(struct Model1_net *Model1_net, struct Model1_flowi4 *Model1_flp)
{
 return Model1_ip_route_output_flow(Model1_net, Model1_flp, ((void *)0));
}

static inline __attribute__((no_instrument_function)) struct Model1_rtable *Model1_ip_route_output(struct Model1_net *Model1_net, Model1___be32 Model1_daddr,
          Model1___be32 Model1_saddr, Model1_u8 Model1_tos, int Model1_oif)
{
 struct Model1_flowi4 Model1_fl4 = {
  .Model1___fl_common.Model1_flowic_oif = Model1_oif,
  .Model1___fl_common.Model1_flowic_tos = Model1_tos,
  .Model1_daddr = Model1_daddr,
  .Model1_saddr = Model1_saddr,
 };
 return Model1_ip_route_output_key(Model1_net, &Model1_fl4);
}

static inline __attribute__((no_instrument_function)) struct Model1_rtable *Model1_ip_route_output_ports(struct Model1_net *Model1_net, struct Model1_flowi4 *Model1_fl4,
         struct Model1_sock *Model1_sk,
         Model1___be32 Model1_daddr, Model1___be32 Model1_saddr,
         Model1___be16 Model1_dport, Model1___be16 Model1_sport,
         __u8 Model1_proto, __u8 Model1_tos, int Model1_oif)
{
 Model1_flowi4_init_output(Model1_fl4, Model1_oif, Model1_sk ? Model1_sk->Model1_sk_mark : 0, Model1_tos,
      Model1_RT_SCOPE_UNIVERSE, Model1_proto,
      Model1_sk ? Model1_inet_sk_flowi_flags(Model1_sk) : 0,
      Model1_daddr, Model1_saddr, Model1_dport, Model1_sport);
 if (Model1_sk)
  Model1_security_sk_classify_flow(Model1_sk, Model1_flowi4_to_flowi(Model1_fl4));
 return Model1_ip_route_output_flow(Model1_net, Model1_fl4, Model1_sk);
}

static inline __attribute__((no_instrument_function)) struct Model1_rtable *Model1_ip_route_output_gre(struct Model1_net *Model1_net, struct Model1_flowi4 *Model1_fl4,
       Model1___be32 Model1_daddr, Model1___be32 Model1_saddr,
       Model1___be32 Model1_gre_key, __u8 Model1_tos, int Model1_oif)
{
 memset(Model1_fl4, 0, sizeof(*Model1_fl4));
 Model1_fl4->Model1___fl_common.Model1_flowic_oif = Model1_oif;
 Model1_fl4->Model1_daddr = Model1_daddr;
 Model1_fl4->Model1_saddr = Model1_saddr;
 Model1_fl4->Model1___fl_common.Model1_flowic_tos = Model1_tos;
 Model1_fl4->Model1___fl_common.Model1_flowic_proto = Model1_IPPROTO_GRE;
 Model1_fl4->Model1_uli.Model1_gre_key = Model1_gre_key;
 return Model1_ip_route_output_key(Model1_net, Model1_fl4);
}

int Model1_ip_route_input_noref(struct Model1_sk_buff *Model1_skb, Model1___be32 Model1_dst, Model1___be32 Model1_src,
    Model1_u8 Model1_tos, struct Model1_net_device *Model1_devin);

static inline __attribute__((no_instrument_function)) int Model1_ip_route_input(struct Model1_sk_buff *Model1_skb, Model1___be32 Model1_dst, Model1___be32 Model1_src,
     Model1_u8 Model1_tos, struct Model1_net_device *Model1_devin)
{
 int err;

 Model1_rcu_read_lock();
 err = Model1_ip_route_input_noref(Model1_skb, Model1_dst, Model1_src, Model1_tos, Model1_devin);
 if (!err)
  Model1_skb_dst_force(Model1_skb);
 Model1_rcu_read_unlock();

 return err;
}

void Model1_ipv4_update_pmtu(struct Model1_sk_buff *Model1_skb, struct Model1_net *Model1_net, Model1_u32 Model1_mtu, int Model1_oif,
        Model1_u32 Model1_mark, Model1_u8 Model1_protocol, int Model1_flow_flags);
void Model1_ipv4_sk_update_pmtu(struct Model1_sk_buff *Model1_skb, struct Model1_sock *Model1_sk, Model1_u32 Model1_mtu);
void Model1_ipv4_redirect(struct Model1_sk_buff *Model1_skb, struct Model1_net *Model1_net, int Model1_oif, Model1_u32 Model1_mark,
     Model1_u8 Model1_protocol, int Model1_flow_flags);
void Model1_ipv4_sk_redirect(struct Model1_sk_buff *Model1_skb, struct Model1_sock *Model1_sk);
void Model1_ip_rt_send_redirect(struct Model1_sk_buff *Model1_skb);

unsigned int Model1_inet_addr_type(struct Model1_net *Model1_net, Model1___be32 Model1_addr);
unsigned int Model1_inet_addr_type_table(struct Model1_net *Model1_net, Model1___be32 Model1_addr, Model1_u32 Model1_tb_id);
unsigned int Model1_inet_dev_addr_type(struct Model1_net *Model1_net, const struct Model1_net_device *Model1_dev,
    Model1___be32 Model1_addr);
unsigned int Model1_inet_addr_type_dev_table(struct Model1_net *Model1_net,
          const struct Model1_net_device *Model1_dev,
          Model1___be32 Model1_addr);
void Model1_ip_rt_multicast_event(struct Model1_in_device *);
int Model1_ip_rt_ioctl(struct Model1_net *, unsigned int Model1_cmd, void *Model1_arg);
void Model1_ip_rt_get_source(Model1_u8 *Model1_src, struct Model1_sk_buff *Model1_skb, struct Model1_rtable *Model1_rt);
struct Model1_rtable *Model1_rt_dst_alloc(struct Model1_net_device *Model1_dev,
        unsigned int Model1_flags, Model1_u16 Model1_type,
        bool Model1_nopolicy, bool Model1_noxfrm, bool Model1_will_cache);

struct Model1_in_ifaddr;
void Model1_fib_add_ifaddr(struct Model1_in_ifaddr *);
void Model1_fib_del_ifaddr(struct Model1_in_ifaddr *, struct Model1_in_ifaddr *);

static inline __attribute__((no_instrument_function)) void Model1_ip_rt_put(struct Model1_rtable *Model1_rt)
{
 /* dst_release() accepts a NULL parameter.
	 * We rely on dst being first structure in struct rtable
	 */
 do { bool Model1___cond = !(!(__builtin_offsetof(struct Model1_rtable, Model1_dst) != 0)); extern void Model1___compiletime_assert_225(void) ; if (Model1___cond) Model1___compiletime_assert_225(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);
 Model1_dst_release(&Model1_rt->Model1_dst);
}



extern const __u8 Model1_ip_tos2prio[16];

static inline __attribute__((no_instrument_function)) char Model1_rt_tos2priority(Model1_u8 Model1_tos)
{
 return Model1_ip_tos2prio[((Model1_tos)&0x1E)>>1];
}

/* ip_route_connect() and ip_route_newports() work in tandem whilst
 * binding a socket for a new outgoing connection.
 *
 * In order to use IPSEC properly, we must, in the end, have a
 * route that was looked up using all available keys including source
 * and destination ports.
 *
 * However, if a source port needs to be allocated (the user specified
 * a wildcard source port) we need to obtain addressing information
 * in order to perform that allocation.
 *
 * So ip_route_connect() looks up a route using wildcarded source and
 * destination ports in the key, simply so that we can get a pair of
 * addresses to use for port allocation.
 *
 * Later, once the ports are allocated, ip_route_newports() will make
 * another route lookup if needed to make sure we catch any IPSEC
 * rules keyed on the port information.
 *
 * The callers allocate the flow key on their stack, and must pass in
 * the same flowi4 object to both the ip_route_connect() and the
 * ip_route_newports() calls.
 */

static inline __attribute__((no_instrument_function)) void Model1_ip_route_connect_init(struct Model1_flowi4 *Model1_fl4, Model1___be32 Model1_dst, Model1___be32 Model1_src,
      Model1_u32 Model1_tos, int Model1_oif, Model1_u8 Model1_protocol,
      Model1___be16 Model1_sport, Model1___be16 Model1_dport,
      struct Model1_sock *Model1_sk)
{
 __u8 Model1_flow_flags = 0;

 if (Model1_inet_sk(Model1_sk)->Model1_transparent)
  Model1_flow_flags |= 0x01;

 Model1_flowi4_init_output(Model1_fl4, Model1_oif, Model1_sk->Model1_sk_mark, Model1_tos, Model1_RT_SCOPE_UNIVERSE,
      Model1_protocol, Model1_flow_flags, Model1_dst, Model1_src, Model1_dport, Model1_sport);
}

static inline __attribute__((no_instrument_function)) struct Model1_rtable *Model1_ip_route_connect(struct Model1_flowi4 *Model1_fl4,
           Model1___be32 Model1_dst, Model1___be32 Model1_src, Model1_u32 Model1_tos,
           int Model1_oif, Model1_u8 Model1_protocol,
           Model1___be16 Model1_sport, Model1___be16 Model1_dport,
           struct Model1_sock *Model1_sk)
{
 struct Model1_net *Model1_net = Model1_sock_net(Model1_sk);
 struct Model1_rtable *Model1_rt;

 Model1_ip_route_connect_init(Model1_fl4, Model1_dst, Model1_src, Model1_tos, Model1_oif, Model1_protocol,
         Model1_sport, Model1_dport, Model1_sk);

 if (!Model1_src && Model1_oif) {
  int Model1_rc;

  Model1_rc = Model1_l3mdev_get_saddr(Model1_net, Model1_oif, Model1_fl4);
  if (Model1_rc < 0)
   return Model1_ERR_PTR(Model1_rc);

  Model1_src = Model1_fl4->Model1_saddr;
 }
 if (!Model1_dst || !Model1_src) {
  Model1_rt = Model1___ip_route_output_key(Model1_net, Model1_fl4);
  if (Model1_IS_ERR(Model1_rt))
   return Model1_rt;
  Model1_ip_rt_put(Model1_rt);
  Model1_flowi4_update_output(Model1_fl4, Model1_oif, Model1_tos, Model1_fl4->Model1_daddr, Model1_fl4->Model1_saddr);
 }
 Model1_security_sk_classify_flow(Model1_sk, Model1_flowi4_to_flowi(Model1_fl4));
 return Model1_ip_route_output_flow(Model1_net, Model1_fl4, Model1_sk);
}

static inline __attribute__((no_instrument_function)) struct Model1_rtable *Model1_ip_route_newports(struct Model1_flowi4 *Model1_fl4, struct Model1_rtable *Model1_rt,
            Model1___be16 Model1_orig_sport, Model1___be16 Model1_orig_dport,
            Model1___be16 Model1_sport, Model1___be16 Model1_dport,
            struct Model1_sock *Model1_sk)
{
 if (Model1_sport != Model1_orig_sport || Model1_dport != Model1_orig_dport) {
  Model1_fl4->Model1_uli.Model1_ports.Model1_dport = Model1_dport;
  Model1_fl4->Model1_uli.Model1_ports.Model1_sport = Model1_sport;
  Model1_ip_rt_put(Model1_rt);
  Model1_flowi4_update_output(Model1_fl4, Model1_sk->Model1___sk_common.Model1_skc_bound_dev_if,
         (((Model1_inet_sk(Model1_sk)->Model1_tos)&0x1E) | Model1_sock_flag(Model1_sk, Model1_SOCK_LOCALROUTE)), Model1_fl4->Model1_daddr,
         Model1_fl4->Model1_saddr);
  Model1_security_sk_classify_flow(Model1_sk, Model1_flowi4_to_flowi(Model1_fl4));
  return Model1_ip_route_output_flow(Model1_sock_net(Model1_sk), Model1_fl4, Model1_sk);
 }
 return Model1_rt;
}

static inline __attribute__((no_instrument_function)) int Model1_inet_iif(const struct Model1_sk_buff *Model1_skb)
{
 struct Model1_rtable *Model1_rt = Model1_skb_rtable(Model1_skb);

 if (Model1_rt && Model1_rt->Model1_rt_iif)
  return Model1_rt->Model1_rt_iif;

 return Model1_skb->Model1_skb_iif;
}

static inline __attribute__((no_instrument_function)) int Model1_ip4_dst_hoplimit(const struct Model1_dst_entry *Model1_dst)
{
 int Model1_hoplimit = Model1_dst_metric_raw(Model1_dst, Model1_RTAX_HOPLIMIT);
 struct Model1_net *Model1_net = Model1_dev_net(Model1_dst->Model1_dev);

 if (Model1_hoplimit == 0)
  Model1_hoplimit = Model1_net->Model1_ipv4.Model1_sysctl_ip_default_ttl;
 return Model1_hoplimit;
}






/* This is for all connections with a full identity, no wildcards.
 * The 'e' prefix stands for Establish, but we really put all sockets
 * but LISTEN ones.
 */
struct Model1_inet_ehash_bucket {
 struct Model1_hlist_nulls_head Model1_chain;
};

/* There are a few simple rules, which allow for local port reuse by
 * an application.  In essence:
 *
 *	1) Sockets bound to different interfaces may share a local port.
 *	   Failing that, goto test 2.
 *	2) If all sockets have sk->sk_reuse set, and none of them are in
 *	   TCP_LISTEN state, the port may be shared.
 *	   Failing that, goto test 3.
 *	3) If all sockets are bound to a specific inet_sk(sk)->rcv_saddr local
 *	   address, and none of them are the same, the port may be
 *	   shared.
 *	   Failing this, the port cannot be shared.
 *
 * The interesting point, is test #2.  This is what an FTP server does
 * all day.  To optimize this case we use a specific flag bit defined
 * below.  As we add sockets to a bind bucket list, we perform a
 * check of: (newsk->sk_reuse && (newsk->sk_state != TCP_LISTEN))
 * As long as all sockets added to a bind bucket pass this test,
 * the flag bit will be set.
 * The resulting situation is that tcp_v[46]_verify_bind() can just check
 * for this flag bit, if it is set and the socket trying to bind has
 * sk->sk_reuse set, we don't even have to walk the owners list at all,
 * we return that it is ok to bind this socket to the requested local port.
 *
 * Sounds like a lot of work, but it is worth it.  In a more naive
 * implementation (ie. current FreeBSD etc.) the entire list of ports
 * must be walked for each data port opened by an ftp server.  Needless
 * to say, this does not scale at all.  With a couple thousand FTP
 * users logged onto your box, isn't it nice to know that new data
 * ports are created in O(1) time?  I thought so. ;-)	-DaveM
 */
struct Model1_inet_bind_bucket {
 Model1_possible_net_t Model1_ib_net;
 unsigned short Model1_port;
 signed char Model1_fastreuse;
 signed char Model1_fastreuseport;
 Model1_kuid_t Model1_fastuid;
 int Model1_num_owners;
 struct Model1_hlist_node Model1_node;
 struct Model1_hlist_head Model1_owners;
};

static inline __attribute__((no_instrument_function)) struct Model1_net *Model1_ib_net(struct Model1_inet_bind_bucket *Model1_ib)
{
 return Model1_read_pnet(&Model1_ib->Model1_ib_net);
}




struct Model1_inet_bind_hashbucket {
 Model1_spinlock_t Model1_lock;
 struct Model1_hlist_head Model1_chain;
};

/*
 * Sockets can be hashed in established or listening table
 */
struct Model1_inet_listen_hashbucket {
 Model1_spinlock_t Model1_lock;
 struct Model1_hlist_head Model1_head;
};

/* This is for listening sockets, thus all sockets which possess wildcards. */


struct Model1_inet_hashinfo {
 /* This is for sockets with full identity only.  Sockets here will
	 * always be without wildcards and will have the following invariant:
	 *
	 *          TCP_ESTABLISHED <= sk->sk_state < TCP_CLOSE
	 *
	 */
 struct Model1_inet_ehash_bucket *Model1_ehash;
 Model1_spinlock_t *Model1_ehash_locks;
 unsigned int Model1_ehash_mask;
 unsigned int Model1_ehash_locks_mask;

 /* Ok, let's try this, I give up, we do need a local binding
	 * TCP hash as well as the others for fast bind/connect.
	 */
 struct Model1_inet_bind_hashbucket *Model1_bhash;

 unsigned int Model1_bhash_size;
 /* 4 bytes hole on 64 bit */

 struct Model1_kmem_cache *Model1_bind_bucket_cachep;

 /* All the above members are written once at bootup and
	 * never written again _or_ are predominantly read-access.
	 *
	 * Now align to a new cache line as all the following members
	 * might be often dirty.
	 */
 /* All sockets in TCP_LISTEN state will be in here.  This is the only
	 * table where wildcard'd TCP sockets can exist.  Hash function here
	 * is just local port number.
	 */
 struct Model1_inet_listen_hashbucket Model1_listening_hash[32]
     __attribute__((__aligned__((1 << (6)))));
};

static inline __attribute__((no_instrument_function)) struct Model1_inet_ehash_bucket *Model1_inet_ehash_bucket(
 struct Model1_inet_hashinfo *Model1_hashinfo,
 unsigned int Model1_hash)
{
 return &Model1_hashinfo->Model1_ehash[Model1_hash & Model1_hashinfo->Model1_ehash_mask];
}

static inline __attribute__((no_instrument_function)) Model1_spinlock_t *Model1_inet_ehash_lockp(
 struct Model1_inet_hashinfo *Model1_hashinfo,
 unsigned int Model1_hash)
{
 return &Model1_hashinfo->Model1_ehash_locks[Model1_hash & Model1_hashinfo->Model1_ehash_locks_mask];
}

int Model1_inet_ehash_locks_alloc(struct Model1_inet_hashinfo *Model1_hashinfo);

static inline __attribute__((no_instrument_function)) void Model1_inet_ehash_locks_free(struct Model1_inet_hashinfo *Model1_hashinfo)
{
 Model1_kvfree(Model1_hashinfo->Model1_ehash_locks);
 Model1_hashinfo->Model1_ehash_locks = ((void *)0);
}

struct Model1_inet_bind_bucket *
Model1_inet_bind_bucket_create(struct Model1_kmem_cache *Model1_cachep, struct Model1_net *Model1_net,
   struct Model1_inet_bind_hashbucket *Model1_head,
   const unsigned short Model1_snum);
void Model1_inet_bind_bucket_destroy(struct Model1_kmem_cache *Model1_cachep,
         struct Model1_inet_bind_bucket *Model1_tb);

static inline __attribute__((no_instrument_function)) Model1_u32 Model1_inet_bhashfn(const struct Model1_net *Model1_net, const Model1___u16 Model1_lport,
          const Model1_u32 Model1_bhash_size)
{
 return (Model1_lport + Model1_net_hash_mix(Model1_net)) & (Model1_bhash_size - 1);
}

void Model1_inet_bind_hash(struct Model1_sock *Model1_sk, struct Model1_inet_bind_bucket *Model1_tb,
      const unsigned short Model1_snum);

/* These can have wildcards, don't try too hard. */
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_inet_lhashfn(const struct Model1_net *Model1_net, const unsigned short Model1_num)
{
 return (Model1_num + Model1_net_hash_mix(Model1_net)) & (32 - 1);
}

static inline __attribute__((no_instrument_function)) int Model1_inet_sk_listen_hashfn(const struct Model1_sock *Model1_sk)
{
 return Model1_inet_lhashfn(Model1_sock_net(Model1_sk), Model1_inet_sk(Model1_sk)->Model1_sk.Model1___sk_common.Model1_skc_num);
}

/* Caller must disable local BH processing. */
int Model1___inet_inherit_port(const struct Model1_sock *Model1_sk, struct Model1_sock *Model1_child);

void Model1_inet_put_port(struct Model1_sock *Model1_sk);

void Model1_inet_hashinfo_init(struct Model1_inet_hashinfo *Model1_h);

bool Model1_inet_ehash_insert(struct Model1_sock *Model1_sk, struct Model1_sock *Model1_osk);
bool Model1_inet_ehash_nolisten(struct Model1_sock *Model1_sk, struct Model1_sock *Model1_osk);
int Model1___inet_hash(struct Model1_sock *Model1_sk, struct Model1_sock *Model1_osk,
  int (*Model1_saddr_same)(const struct Model1_sock *Model1_sk1,
      const struct Model1_sock *Model1_sk2,
      bool Model1_match_wildcard));
int Model1_inet_hash(struct Model1_sock *Model1_sk);
void Model1_inet_unhash(struct Model1_sock *Model1_sk);

struct Model1_sock *Model1___inet_lookup_listener(struct Model1_net *Model1_net,
        struct Model1_inet_hashinfo *Model1_hashinfo,
        struct Model1_sk_buff *Model1_skb, int Model1_doff,
        const Model1___be32 Model1_saddr, const Model1___be16 Model1_sport,
        const Model1___be32 Model1_daddr,
        const unsigned short Model1_hnum,
        const int Model1_dif);

static inline __attribute__((no_instrument_function)) struct Model1_sock *Model1_inet_lookup_listener(struct Model1_net *Model1_net,
  struct Model1_inet_hashinfo *Model1_hashinfo,
  struct Model1_sk_buff *Model1_skb, int Model1_doff,
  Model1___be32 Model1_saddr, Model1___be16 Model1_sport,
  Model1___be32 Model1_daddr, Model1___be16 Model1_dport, int Model1_dif)
{
 return Model1___inet_lookup_listener(Model1_net, Model1_hashinfo, Model1_skb, Model1_doff, Model1_saddr, Model1_sport,
          Model1_daddr, (__builtin_constant_p((Model1___u16)(( Model1___u16)(Model1___be16)(Model1_dport))) ? ((Model1___u16)( (((Model1___u16)(( Model1___u16)(Model1___be16)(Model1_dport)) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)(( Model1___u16)(Model1___be16)(Model1_dport)) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16(( Model1___u16)(Model1___be16)(Model1_dport))), Model1_dif);
}

/* Socket demux engine toys. */
/* What happens here is ugly; there's a pair of adjacent fields in
   struct inet_sock; __be16 dport followed by __u16 num.  We want to
   search by pair, so we combine the keys into a single 32bit value
   and compare with 32bit value read from &...->dport.  Let's at least
   make sure that it's not mixed with anything else...
   On 64bit targets we combine comparisons with pair of adjacent __be32
   fields in the same way.
*/
/* Sockets in TCP_CLOSE state are _always_ taken out of the hash, so we need
 * not check it for lookups anymore, thanks Alexey. -DaveM
 */
struct Model1_sock *Model1___inet_lookup_established(struct Model1_net *Model1_net,
           struct Model1_inet_hashinfo *Model1_hashinfo,
           const Model1___be32 Model1_saddr, const Model1___be16 Model1_sport,
           const Model1___be32 Model1_daddr, const Model1_u16 Model1_hnum,
           const int Model1_dif);

static inline __attribute__((no_instrument_function)) struct Model1_sock *
 Model1_inet_lookup_established(struct Model1_net *Model1_net, struct Model1_inet_hashinfo *Model1_hashinfo,
    const Model1___be32 Model1_saddr, const Model1___be16 Model1_sport,
    const Model1___be32 Model1_daddr, const Model1___be16 Model1_dport,
    const int Model1_dif)
{
 return Model1___inet_lookup_established(Model1_net, Model1_hashinfo, Model1_saddr, Model1_sport, Model1_daddr,
      (__builtin_constant_p((Model1___u16)(( Model1___u16)(Model1___be16)(Model1_dport))) ? ((Model1___u16)( (((Model1___u16)(( Model1___u16)(Model1___be16)(Model1_dport)) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)(( Model1___u16)(Model1___be16)(Model1_dport)) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16(( Model1___u16)(Model1___be16)(Model1_dport))), Model1_dif);
}

static inline __attribute__((no_instrument_function)) struct Model1_sock *Model1___inet_lookup(struct Model1_net *Model1_net,
      struct Model1_inet_hashinfo *Model1_hashinfo,
      struct Model1_sk_buff *Model1_skb, int Model1_doff,
      const Model1___be32 Model1_saddr, const Model1___be16 Model1_sport,
      const Model1___be32 Model1_daddr, const Model1___be16 Model1_dport,
      const int Model1_dif,
      bool *Model1_refcounted)
{
 Model1_u16 Model1_hnum = (__builtin_constant_p((Model1___u16)(( Model1___u16)(Model1___be16)(Model1_dport))) ? ((Model1___u16)( (((Model1___u16)(( Model1___u16)(Model1___be16)(Model1_dport)) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)(( Model1___u16)(Model1___be16)(Model1_dport)) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16(( Model1___u16)(Model1___be16)(Model1_dport)));
 struct Model1_sock *Model1_sk;

 Model1_sk = Model1___inet_lookup_established(Model1_net, Model1_hashinfo, Model1_saddr, Model1_sport,
           Model1_daddr, Model1_hnum, Model1_dif);
 *Model1_refcounted = true;
 if (Model1_sk)
  return Model1_sk;
 *Model1_refcounted = false;
 return Model1___inet_lookup_listener(Model1_net, Model1_hashinfo, Model1_skb, Model1_doff, Model1_saddr,
          Model1_sport, Model1_daddr, Model1_hnum, Model1_dif);
}

static inline __attribute__((no_instrument_function)) struct Model1_sock *Model1_inet_lookup(struct Model1_net *Model1_net,
           struct Model1_inet_hashinfo *Model1_hashinfo,
           struct Model1_sk_buff *Model1_skb, int Model1_doff,
           const Model1___be32 Model1_saddr, const Model1___be16 Model1_sport,
           const Model1___be32 Model1_daddr, const Model1___be16 Model1_dport,
           const int Model1_dif)
{
 struct Model1_sock *Model1_sk;
 bool Model1_refcounted;

 Model1_sk = Model1___inet_lookup(Model1_net, Model1_hashinfo, Model1_skb, Model1_doff, Model1_saddr, Model1_sport, Model1_daddr,
      Model1_dport, Model1_dif, &Model1_refcounted);

 if (Model1_sk && !Model1_refcounted && !Model1_atomic_add_unless((&Model1_sk->Model1___sk_common.Model1_skc_refcnt), 1, 0))
  Model1_sk = ((void *)0);
 return Model1_sk;
}

#if CY_ABSTRACT1
extern struct Model1_sock *Model1_Dst_sk;
extern struct Model1_tcp_sock Model1_Server_L, Model1_Server, Model1_Server_A;
extern struct Model1_tcp_request_sock Model1_Req, Model1_Req_A;
extern bool Model1_Port_guessed, Model1_Src_sk;
#endif

static inline __attribute__((no_instrument_function)) struct Model1_sock *Model1___inet_lookup_skb(struct Model1_inet_hashinfo *Model1_hashinfo,
          struct Model1_sk_buff *Model1_skb,
          int Model1_doff,
          const Model1___be16 Model1_sport,
          const Model1___be16 Model1_dport,
          bool *Model1_refcounted)
{
 struct Model1_sock *Model1_sk = Model1_skb_steal_sock(Model1_skb);
 const struct Model1_iphdr *Model1_iph = Model1_ip_hdr(Model1_skb);

 *Model1_refcounted = true;
 if (Model1_sk)
  return Model1_sk;

 return Model1___inet_lookup(Model1_dev_net(Model1_skb_dst(Model1_skb)->Model1_dev), Model1_hashinfo, Model1_skb,
        Model1_doff, Model1_iph->Model1_saddr, Model1_sport,
        Model1_iph->Model1_daddr, Model1_dport, Model1_inet_iif(Model1_skb),
        Model1_refcounted);
}

Model1_u32 Model1_sk_ehashfn(const struct Model1_sock *Model1_sk);
Model1_u32 Model1_inet6_ehashfn(const struct Model1_net *Model1_net,
    const struct Model1_in6_addr *Model1_laddr, const Model1_u16 Model1_lport,
    const struct Model1_in6_addr *Model1_faddr, const Model1___be16 Model1_fport);

static inline __attribute__((no_instrument_function)) void Model1_sk_daddr_set(struct Model1_sock *Model1_sk, Model1___be32 Model1_addr)
{
 Model1_sk->Model1___sk_common.Model1_skc_daddr = Model1_addr; /* alias of inet_daddr */

 Model1_ipv6_addr_set_v4mapped(Model1_addr, &Model1_sk->Model1___sk_common.Model1_skc_v6_daddr);

}

static inline __attribute__((no_instrument_function)) void Model1_sk_rcv_saddr_set(struct Model1_sock *Model1_sk, Model1___be32 Model1_addr)
{
 Model1_sk->Model1___sk_common.Model1_skc_rcv_saddr = Model1_addr; /* alias of inet_rcv_saddr */

 Model1_ipv6_addr_set_v4mapped(Model1_addr, &Model1_sk->Model1___sk_common.Model1_skc_v6_rcv_saddr);

}

int Model1___inet_hash_connect(struct Model1_inet_timewait_death_row *Model1_death_row,
   struct Model1_sock *Model1_sk, Model1_u32 Model1_port_offset,
   int (*Model1_check_established)(struct Model1_inet_timewait_death_row *,
       struct Model1_sock *, Model1___u16,
       struct Model1_inet_timewait_sock **));

int Model1_inet_hash_connect(struct Model1_inet_timewait_death_row *Model1_death_row,
        struct Model1_sock *Model1_sk);




/*
 * INET		An implementation of the TCP/IP protocol suite for the LINUX
 *		operating system.  INET is implemented using the  BSD Socket
 *		interface as the means of communication with the user level.
 *
 *		Definitions for the IP module.
 *
 * Version:	@(#)ip.h	1.0.2	05/07/93
 *
 * Authors:	Ross Biro
 *		Fred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>
 *		Alan Cox, <gw4pts@gw4pts.ampr.org>
 *
 * Changes:
 *		Mike McLagan    :       Routing by source
 *
 *		This program is free software; you can redistribute it and/or
 *		modify it under the terms of the GNU General Public License
 *		as published by the Free Software Foundation; either version
 *		2 of the License, or (at your option) any later version.
 */
struct Model1_sock;

struct Model1_inet_skb_parm {
 int Model1_iif;
 struct Model1_ip_options Model1_opt; /* Compiled IP options		*/
 unsigned char Model1_flags;
 Model1_u16 Model1_frag_max_size;
};

static inline __attribute__((no_instrument_function)) unsigned int Model1_ip_hdrlen(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_ip_hdr(Model1_skb)->Model1_ihl * 4;
}

struct Model1_ipcm_cookie {
 struct Model1_sockcm_cookie Model1_sockc;
 Model1___be32 Model1_addr;
 int Model1_oif;
 struct Model1_ip_options_rcu *Model1_opt;
 __u8 Model1_tx_flags;
 __u8 Model1_ttl;
 Model1___s16 Model1_tos;
 char Model1_priority;
};




struct Model1_ip_ra_chain {
 struct Model1_ip_ra_chain *Model1_next;
 struct Model1_sock *Model1_sk;
 union {
  void (*Model1_destructor)(struct Model1_sock *);
  struct Model1_sock *Model1_saved_sk;
 };
 struct Model1_callback_head Model1_rcu;
};

extern struct Model1_ip_ra_chain *Model1_ip_ra_chain;

/* IP flags. */







struct Model1_msghdr;
struct Model1_net_device;
struct Model1_packet_type;
struct Model1_rtable;
struct Model1_sockaddr;

int Model1_igmp_mc_init(void);

/*
 *	Functions provided by ip.c
 */

int Model1_ip_build_and_send_pkt(struct Model1_sk_buff *Model1_skb, const struct Model1_sock *Model1_sk,
     Model1___be32 Model1_saddr, Model1___be32 Model1_daddr,
     struct Model1_ip_options_rcu *Model1_opt);
int Model1_ip_rcv(struct Model1_sk_buff *Model1_skb, struct Model1_net_device *Model1_dev, struct Model1_packet_type *Model1_pt,
    struct Model1_net_device *Model1_orig_dev);
int Model1_ip_local_deliver(struct Model1_sk_buff *Model1_skb);
int Model1_ip_mr_input(struct Model1_sk_buff *Model1_skb);
int Model1_ip_output(struct Model1_net *Model1_net, struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);
int Model1_ip_mc_output(struct Model1_net *Model1_net, struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);
int Model1_ip_do_fragment(struct Model1_net *Model1_net, struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb,
     int (*Model1_output)(struct Model1_net *, struct Model1_sock *, struct Model1_sk_buff *));
void Model1_ip_send_check(struct Model1_iphdr *Model1_ip);
int Model1___ip_local_out(struct Model1_net *Model1_net, struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);
int Model1_ip_local_out(struct Model1_net *Model1_net, struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);

int Model1_ip_queue_xmit(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb, struct Model1_flowi *Model1_fl);
void Model1_ip_init(void);
int Model1_ip_append_data(struct Model1_sock *Model1_sk, struct Model1_flowi4 *Model1_fl4,
     int Model1_getfrag(void *Model1_from, char *Model1_to, int Model1_offset, int Model1_len,
          int Model1_odd, struct Model1_sk_buff *Model1_skb),
     void *Model1_from, int Model1_len, int Model1_protolen,
     struct Model1_ipcm_cookie *Model1_ipc,
     struct Model1_rtable **Model1_rt,
     unsigned int Model1_flags);
int Model1_ip_generic_getfrag(void *Model1_from, char *Model1_to, int Model1_offset, int Model1_len, int Model1_odd,
         struct Model1_sk_buff *Model1_skb);
Model1_ssize_t Model1_ip_append_page(struct Model1_sock *Model1_sk, struct Model1_flowi4 *Model1_fl4, struct Model1_page *Model1_page,
         int Model1_offset, Model1_size_t Model1_size, int Model1_flags);
struct Model1_sk_buff *Model1___ip_make_skb(struct Model1_sock *Model1_sk, struct Model1_flowi4 *Model1_fl4,
         struct Model1_sk_buff_head *Model1_queue,
         struct Model1_inet_cork *Model1_cork);
int Model1_ip_send_skb(struct Model1_net *Model1_net, struct Model1_sk_buff *Model1_skb);
int Model1_ip_push_pending_frames(struct Model1_sock *Model1_sk, struct Model1_flowi4 *Model1_fl4);
void Model1_ip_flush_pending_frames(struct Model1_sock *Model1_sk);
struct Model1_sk_buff *Model1_ip_make_skb(struct Model1_sock *Model1_sk, struct Model1_flowi4 *Model1_fl4,
       int Model1_getfrag(void *Model1_from, char *Model1_to, int Model1_offset,
     int Model1_len, int Model1_odd, struct Model1_sk_buff *Model1_skb),
       void *Model1_from, int Model1_length, int Model1_transhdrlen,
       struct Model1_ipcm_cookie *Model1_ipc, struct Model1_rtable **Model1_rtp,
       unsigned int Model1_flags);

static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1_ip_finish_skb(struct Model1_sock *Model1_sk, struct Model1_flowi4 *Model1_fl4)
{
 return Model1___ip_make_skb(Model1_sk, Model1_fl4, &Model1_sk->Model1_sk_write_queue, &Model1_inet_sk(Model1_sk)->Model1_cork.Model1_base);
}

static inline __attribute__((no_instrument_function)) __u8 Model1_get_rttos(struct Model1_ipcm_cookie* Model1_ipc, struct Model1_inet_sock *Model1_inet)
{
 return (Model1_ipc->Model1_tos != -1) ? ((Model1_ipc->Model1_tos)&0x1E) : ((Model1_inet->Model1_tos)&0x1E);
}

static inline __attribute__((no_instrument_function)) __u8 Model1_get_rtconn_flags(struct Model1_ipcm_cookie* Model1_ipc, struct Model1_sock* Model1_sk)
{
 return (Model1_ipc->Model1_tos != -1) ? (((Model1_ipc->Model1_tos)&0x1E) | Model1_sock_flag(Model1_sk, Model1_SOCK_LOCALROUTE)) : (((Model1_inet_sk(Model1_sk)->Model1_tos)&0x1E) | Model1_sock_flag(Model1_sk, Model1_SOCK_LOCALROUTE));
}

/* datagram.c */
int Model1___ip4_datagram_connect(struct Model1_sock *Model1_sk, struct Model1_sockaddr *Model1_uaddr, int Model1_addr_len);
int Model1_ip4_datagram_connect(struct Model1_sock *Model1_sk, struct Model1_sockaddr *Model1_uaddr, int Model1_addr_len);

void Model1_ip4_datagram_release_cb(struct Model1_sock *Model1_sk);

struct Model1_ip_reply_arg {
 struct Model1_kvec Model1_iov[1];
 int Model1_flags;
 Model1___wsum Model1_csum;
 int Model1_csumoffset; /* u16 offset of csum in iov[0].iov_base */
    /* -1 if not needed */
 int Model1_bound_dev_if;
 Model1_u8 Model1_tos;
};



static inline __attribute__((no_instrument_function)) __u8 Model1_ip_reply_arg_flowi_flags(const struct Model1_ip_reply_arg *Model1_arg)
{
 return (Model1_arg->Model1_flags & 1) ? 0x01 : 0;
}

void Model1_ip_send_unicast_reply(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb,
      const struct Model1_ip_options *Model1_sopt,
      Model1___be32 Model1_daddr, Model1___be32 Model1_saddr,
      const struct Model1_ip_reply_arg *Model1_arg,
      unsigned int Model1_len);

extern struct Model1_linux_mib Model1_cy_linux_mib;






//#define NET_INC_STATS(net, field)	SNMP_INC_STATS((net)->mib.net_statistics, field)
//#define __NET_INC_STATS(net, field)	__SNMP_INC_STATS((net)->mib.net_statistics, field)
//#define NET_ADD_STATS(net, field, adnd)	SNMP_ADD_STATS((net)->mib.net_statistics, field, adnd)





Model1_u64 Model1_snmp_get_cpu_field(void *Model1_mib, int Model1_cpu, int Model1_offct);
unsigned long Model1_snmp_fold_field(void *Model1_mib, int Model1_offt);





static inline __attribute__((no_instrument_function)) Model1_u64 Model1_snmp_get_cpu_field64(void *Model1_mib, int Model1_cpu, int Model1_offct,
     Model1_size_t Model1_syncp_offset)
{
 return Model1_snmp_get_cpu_field(Model1_mib, Model1_cpu, Model1_offct);

}

static inline __attribute__((no_instrument_function)) Model1_u64 Model1_snmp_fold_field64(void *Model1_mib, int Model1_offt, Model1_size_t Model1_syncp_off)
{
 return Model1_snmp_fold_field(Model1_mib, Model1_offt);
}


void Model1_inet_get_local_port_range(struct Model1_net *Model1_net, int *Model1_low, int *Model1_high);


static inline __attribute__((no_instrument_function)) int Model1_inet_is_local_reserved_port(struct Model1_net *Model1_net, int Model1_port)
{
 if (!Model1_net->Model1_ipv4.Model1_sysctl_local_reserved_ports)
  return 0;
 return (__builtin_constant_p((Model1_port)) ? Model1_constant_test_bit((Model1_port), (Model1_net->Model1_ipv4.Model1_sysctl_local_reserved_ports)) : Model1_variable_test_bit((Model1_port), (Model1_net->Model1_ipv4.Model1_sysctl_local_reserved_ports)));
}

static inline __attribute__((no_instrument_function)) bool Model1_sysctl_dev_name_is_allowed(const char *Model1_name)
{
 return Model1_strcmp(Model1_name, "default") != 0 && Model1_strcmp(Model1_name, "all") != 0;
}
Model1___be32 Model1_inet_current_timestamp(void);

/* From inetpeer.c */
extern int Model1_inet_peer_threshold;
extern int Model1_inet_peer_minttl;
extern int Model1_inet_peer_maxttl;

void Model1_ipfrag_init(void);

void Model1_ip_static_sysctl_init(void);




static inline __attribute__((no_instrument_function)) bool Model1_ip_is_fragment(const struct Model1_iphdr *Model1_iph)
{
 return (Model1_iph->Model1_frag_off & (( Model1___be16)(__builtin_constant_p((Model1___u16)((0x2000 | 0x1FFF))) ? ((Model1___u16)( (((Model1___u16)((0x2000 | 0x1FFF)) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)((0x2000 | 0x1FFF)) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16((0x2000 | 0x1FFF))))) != 0;
}




/* The function in 2.2 was invalid, producing wrong result for
 * check=0xFEFF. It was noticed by Arthur Skawina _year_ ago. --ANK(000625) */
static inline __attribute__((no_instrument_function))
int Model1_ip_decrease_ttl(struct Model1_iphdr *Model1_iph)
{
 Model1_u32 Model1_check = ( Model1_u32)Model1_iph->Model1_check;
 Model1_check += ( Model1_u32)(( Model1___be16)(__builtin_constant_p((Model1___u16)((0x0100))) ? ((Model1___u16)( (((Model1___u16)((0x0100)) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)((0x0100)) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16((0x0100))));
 Model1_iph->Model1_check = ( Model1___sum16)(Model1_check + (Model1_check>=0xFFFF));
 return --Model1_iph->Model1_ttl;
}

static inline __attribute__((no_instrument_function))
int Model1_ip_dont_fragment(const struct Model1_sock *Model1_sk, const struct Model1_dst_entry *Model1_dst)
{
 Model1_u8 Model1_pmtudisc = ({ union { typeof(Model1_inet_sk(Model1_sk)->Model1_pmtudisc) Model1___val; char Model1___c[1]; } Model1___u; if (1) Model1___read_once_size(&(Model1_inet_sk(Model1_sk)->Model1_pmtudisc), Model1___u.Model1___c, sizeof(Model1_inet_sk(Model1_sk)->Model1_pmtudisc)); else Model1___read_once_size_nocheck(&(Model1_inet_sk(Model1_sk)->Model1_pmtudisc), Model1___u.Model1___c, sizeof(Model1_inet_sk(Model1_sk)->Model1_pmtudisc)); Model1___u.Model1___val; });

 return Model1_pmtudisc == 2 ||
  (Model1_pmtudisc == 1 &&
   !(Model1_dst_metric_locked(Model1_dst, Model1_RTAX_MTU)));
}

static inline __attribute__((no_instrument_function)) bool Model1_ip_sk_accept_pmtu(const struct Model1_sock *Model1_sk)
{
 return Model1_inet_sk(Model1_sk)->Model1_pmtudisc != 4 &&
        Model1_inet_sk(Model1_sk)->Model1_pmtudisc != 5;
}

static inline __attribute__((no_instrument_function)) bool Model1_ip_sk_use_pmtu(const struct Model1_sock *Model1_sk)
{
 return Model1_inet_sk(Model1_sk)->Model1_pmtudisc < 3;
}

static inline __attribute__((no_instrument_function)) bool Model1_ip_sk_ignore_df(const struct Model1_sock *Model1_sk)
{
 return Model1_inet_sk(Model1_sk)->Model1_pmtudisc < 2 ||
        Model1_inet_sk(Model1_sk)->Model1_pmtudisc == 5;
}

static inline __attribute__((no_instrument_function)) unsigned int Model1_ip_dst_mtu_maybe_forward(const struct Model1_dst_entry *Model1_dst,
          bool Model1_forwarding)
{
 struct Model1_net *Model1_net = Model1_dev_net(Model1_dst->Model1_dev);

 if (Model1_net->Model1_ipv4.Model1_sysctl_ip_fwd_use_pmtu ||
     Model1_dst_metric_locked(Model1_dst, Model1_RTAX_MTU) ||
     !Model1_forwarding)
  return Model1_dst_mtu(Model1_dst);

 return ({ typeof(Model1_dst->Model1_dev->Model1_mtu) Model1__min1 = (Model1_dst->Model1_dev->Model1_mtu); typeof(0xFFFFU) Model1__min2 = (0xFFFFU); (void) (&Model1__min1 == &Model1__min2); Model1__min1 < Model1__min2 ? Model1__min1 : Model1__min2; });
}

static inline __attribute__((no_instrument_function)) unsigned int Model1_ip_skb_dst_mtu(struct Model1_sock *Model1_sk,
       const struct Model1_sk_buff *Model1_skb)
{
 if (!Model1_sk || !Model1_sk_fullsock(Model1_sk) || Model1_ip_sk_use_pmtu(Model1_sk)) {
  bool Model1_forwarding = ((struct Model1_inet_skb_parm*)((Model1_skb)->Model1_cb))->Model1_flags & (1UL << (0));

  return Model1_ip_dst_mtu_maybe_forward(Model1_skb_dst(Model1_skb), Model1_forwarding);
 }

 return ({ typeof(Model1_skb_dst(Model1_skb)->Model1_dev->Model1_mtu) Model1__min1 = (Model1_skb_dst(Model1_skb)->Model1_dev->Model1_mtu); typeof(0xFFFFU) Model1__min2 = (0xFFFFU); (void) (&Model1__min1 == &Model1__min2); Model1__min1 < Model1__min2 ? Model1__min1 : Model1__min2; });
}

Model1_u32 Model1_ip_idents_reserve(Model1_u32 Model1_hash, int Model1_segs);
void Model1___ip_select_ident(struct Model1_net *Model1_net, struct Model1_iphdr *Model1_iph, int Model1_segs);

static inline __attribute__((no_instrument_function)) void Model1_ip_select_ident_segs(struct Model1_net *Model1_net, struct Model1_sk_buff *Model1_skb,
     struct Model1_sock *Model1_sk, int Model1_segs)
{
 struct Model1_iphdr *Model1_iph = Model1_ip_hdr(Model1_skb);

 if ((Model1_iph->Model1_frag_off & (( Model1___be16)(__builtin_constant_p((Model1___u16)((0x4000))) ? ((Model1___u16)( (((Model1___u16)((0x4000)) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)((0x4000)) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16((0x4000))))) && !Model1_skb->Model1_ignore_df) {
  /* This is only to work around buggy Windows95/2000
		 * VJ compression implementations.  If the ID field
		 * does not change, they drop every other packet in
		 * a TCP stream using header compression.
		 */
  if (Model1_sk && Model1_inet_sk(Model1_sk)->Model1_sk.Model1___sk_common.Model1_skc_daddr) {
   Model1_iph->Model1_id = (( Model1___be16)(__builtin_constant_p((Model1___u16)((Model1_inet_sk(Model1_sk)->Model1_inet_id))) ? ((Model1___u16)( (((Model1___u16)((Model1_inet_sk(Model1_sk)->Model1_inet_id)) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)((Model1_inet_sk(Model1_sk)->Model1_inet_id)) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16((Model1_inet_sk(Model1_sk)->Model1_inet_id))));
   Model1_inet_sk(Model1_sk)->Model1_inet_id += Model1_segs;
  } else {
   Model1_iph->Model1_id = 0;
  }
 } else {
  Model1___ip_select_ident(Model1_net, Model1_iph, Model1_segs);
 }
}

static inline __attribute__((no_instrument_function)) void Model1_ip_select_ident(struct Model1_net *Model1_net, struct Model1_sk_buff *Model1_skb,
       struct Model1_sock *Model1_sk)
{
 Model1_ip_select_ident_segs(Model1_net, Model1_skb, Model1_sk, 1);
}

static inline __attribute__((no_instrument_function)) Model1___wsum Model1_inet_compute_pseudo(struct Model1_sk_buff *Model1_skb, int Model1_proto)
{
 return Model1_csum_tcpudp_nofold(Model1_ip_hdr(Model1_skb)->Model1_saddr, Model1_ip_hdr(Model1_skb)->Model1_daddr,
      Model1_skb->Model1_len, Model1_proto, 0);
}

/* copy IPv4 saddr & daddr to flow_keys, possibly using 64bit load/store
 * Equivalent to :	flow->v4addrs.src = iph->saddr;
 *			flow->v4addrs.dst = iph->daddr;
 */
static inline __attribute__((no_instrument_function)) void Model1_iph_to_flow_copy_v4addrs(struct Model1_flow_keys *Model1_flow,
         const struct Model1_iphdr *Model1_iph)
{
 do { bool Model1___cond = !(!(__builtin_offsetof(typeof(Model1_flow->Model1_addrs), Model1_v4addrs.Model1_dst) != __builtin_offsetof(typeof(Model1_flow->Model1_addrs), Model1_v4addrs.Model1_src) + sizeof(Model1_flow->Model1_addrs.Model1_v4addrs.Model1_src))); extern void Model1___compiletime_assert_379(void) ; if (Model1___cond) Model1___compiletime_assert_379(); do { ((void)sizeof(char[1 - 2 * Model1___cond])); } while (0); } while (0);


 ({ Model1_size_t Model1___len = (sizeof(Model1_flow->Model1_addrs.Model1_v4addrs)); void *Model1___ret; if (__builtin_constant_p(sizeof(Model1_flow->Model1_addrs.Model1_v4addrs)) && Model1___len >= 64) Model1___ret = Model1___memcpy((&Model1_flow->Model1_addrs.Model1_v4addrs), (&Model1_iph->Model1_saddr), Model1___len); else Model1___ret = __builtin_memcpy((&Model1_flow->Model1_addrs.Model1_v4addrs), (&Model1_iph->Model1_saddr), Model1___len); Model1___ret; });
 Model1_flow->Model1_control.Model1_addr_type = Model1_FLOW_DISSECTOR_KEY_IPV4_ADDRS;
}

static inline __attribute__((no_instrument_function)) Model1___wsum Model1_inet_gro_compute_pseudo(struct Model1_sk_buff *Model1_skb, int Model1_proto)
{
 const struct Model1_iphdr *Model1_iph = Model1_skb_gro_network_header(Model1_skb);

 return Model1_csum_tcpudp_nofold(Model1_iph->Model1_saddr, Model1_iph->Model1_daddr,
      Model1_skb_gro_len(Model1_skb), Model1_proto, 0);
}

/*
 *	Map a multicast IP onto multicast MAC for type ethernet.
 */

static inline __attribute__((no_instrument_function)) void Model1_ip_eth_mc_map(Model1___be32 Model1_naddr, char *Model1_buf)
{
 __u32 Model1_addr=(__builtin_constant_p((__u32)(( __u32)(Model1___be32)(Model1_naddr))) ? ((__u32)( (((__u32)(( __u32)(Model1___be32)(Model1_naddr)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model1___be32)(Model1_naddr)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model1___be32)(Model1_naddr)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model1___be32)(Model1_naddr)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32(( __u32)(Model1___be32)(Model1_naddr)));
 Model1_buf[0]=0x01;
 Model1_buf[1]=0x00;
 Model1_buf[2]=0x5e;
 Model1_buf[5]=Model1_addr&0xFF;
 Model1_addr>>=8;
 Model1_buf[4]=Model1_addr&0xFF;
 Model1_addr>>=8;
 Model1_buf[3]=Model1_addr&0x7F;
}

/*
 *	Map a multicast IP onto multicast MAC for type IP-over-InfiniBand.
 *	Leave P_Key as 0 to be filled in by driver.
 */

static inline __attribute__((no_instrument_function)) void Model1_ip_ib_mc_map(Model1___be32 Model1_naddr, const unsigned char *Model1_broadcast, char *Model1_buf)
{
 __u32 Model1_addr;
 unsigned char Model1_scope = Model1_broadcast[5] & 0xF;

 Model1_buf[0] = 0; /* Reserved */
 Model1_buf[1] = 0xff; /* Multicast QPN */
 Model1_buf[2] = 0xff;
 Model1_buf[3] = 0xff;
 Model1_addr = (__builtin_constant_p((__u32)(( __u32)(Model1___be32)(Model1_naddr))) ? ((__u32)( (((__u32)(( __u32)(Model1___be32)(Model1_naddr)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model1___be32)(Model1_naddr)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model1___be32)(Model1_naddr)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model1___be32)(Model1_naddr)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32(( __u32)(Model1___be32)(Model1_naddr)));
 Model1_buf[4] = 0xff;
 Model1_buf[5] = 0x10 | Model1_scope; /* scope from broadcast address */
 Model1_buf[6] = 0x40; /* IPv4 signature */
 Model1_buf[7] = 0x1b;
 Model1_buf[8] = Model1_broadcast[8]; /* P_Key */
 Model1_buf[9] = Model1_broadcast[9];
 Model1_buf[10] = 0;
 Model1_buf[11] = 0;
 Model1_buf[12] = 0;
 Model1_buf[13] = 0;
 Model1_buf[14] = 0;
 Model1_buf[15] = 0;
 Model1_buf[19] = Model1_addr & 0xff;
 Model1_addr >>= 8;
 Model1_buf[18] = Model1_addr & 0xff;
 Model1_addr >>= 8;
 Model1_buf[17] = Model1_addr & 0xff;
 Model1_addr >>= 8;
 Model1_buf[16] = Model1_addr & 0x0f;
}

static inline __attribute__((no_instrument_function)) void Model1_ip_ipgre_mc_map(Model1___be32 Model1_naddr, const unsigned char *Model1_broadcast, char *Model1_buf)
{
 if ((Model1_broadcast[0] | Model1_broadcast[1] | Model1_broadcast[2] | Model1_broadcast[3]) != 0)
  ({ Model1_size_t Model1___len = (4); void *Model1___ret; if (__builtin_constant_p(4) && Model1___len >= 64) Model1___ret = Model1___memcpy((Model1_buf), (Model1_broadcast), Model1___len); else Model1___ret = __builtin_memcpy((Model1_buf), (Model1_broadcast), Model1___len); Model1___ret; });
 else
  ({ Model1_size_t Model1___len = (sizeof(Model1_naddr)); void *Model1___ret; if (__builtin_constant_p(sizeof(Model1_naddr)) && Model1___len >= 64) Model1___ret = Model1___memcpy((Model1_buf), (&Model1_naddr), Model1___len); else Model1___ret = __builtin_memcpy((Model1_buf), (&Model1_naddr), Model1___len); Model1___ret; });
}





static __inline__ __attribute__((no_instrument_function)) void Model1_inet_reset_saddr(struct Model1_sock *Model1_sk)
{
 Model1_inet_sk(Model1_sk)->Model1_sk.Model1___sk_common.Model1_skc_rcv_saddr = Model1_inet_sk(Model1_sk)->Model1_inet_saddr = 0;

 if (Model1_sk->Model1___sk_common.Model1_skc_family == 10) {
  struct Model1_ipv6_pinfo *Model1_np = Model1_inet6_sk(Model1_sk);

  memset(&Model1_np->Model1_saddr, 0, sizeof(Model1_np->Model1_saddr));
  memset(&Model1_sk->Model1___sk_common.Model1_skc_v6_rcv_saddr, 0, sizeof(Model1_sk->Model1___sk_common.Model1_skc_v6_rcv_saddr));
 }

}



static inline __attribute__((no_instrument_function)) unsigned int Model1_ipv4_addr_hash(Model1___be32 Model1_ip)
{
 return ( unsigned int) Model1_ip;
}

bool Model1_ip_call_ra_chain(struct Model1_sk_buff *Model1_skb);

/*
 *	Functions provided by ip_fragment.c
 */

enum Model1_ip_defrag_users {
 Model1_IP_DEFRAG_LOCAL_DELIVER,
 Model1_IP_DEFRAG_CALL_RA_CHAIN,
 Model1_IP_DEFRAG_CONNTRACK_IN,
 Model1___IP_DEFRAG_CONNTRACK_IN_END = Model1_IP_DEFRAG_CONNTRACK_IN + ((Model1_u16)(~0U)),
 Model1_IP_DEFRAG_CONNTRACK_OUT,
 Model1___IP_DEFRAG_CONNTRACK_OUT_END = Model1_IP_DEFRAG_CONNTRACK_OUT + ((Model1_u16)(~0U)),
 Model1_IP_DEFRAG_CONNTRACK_BRIDGE_IN,
 Model1___IP_DEFRAG_CONNTRACK_BRIDGE_IN = Model1_IP_DEFRAG_CONNTRACK_BRIDGE_IN + ((Model1_u16)(~0U)),
 Model1_IP_DEFRAG_VS_IN,
 Model1_IP_DEFRAG_VS_OUT,
 Model1_IP_DEFRAG_VS_FWD,
 Model1_IP_DEFRAG_AF_PACKET,
 Model1_IP_DEFRAG_MACVLAN,
};

/* Return true if the value of 'user' is between 'lower_bond'
 * and 'upper_bond' inclusively.
 */
static inline __attribute__((no_instrument_function)) bool Model1_ip_defrag_user_in_between(Model1_u32 Model1_user,
          enum Model1_ip_defrag_users Model1_lower_bond,
          enum Model1_ip_defrag_users Model1_upper_bond)
{
 return Model1_user >= Model1_lower_bond && Model1_user <= Model1_upper_bond;
}

int Model1_ip_defrag(struct Model1_net *Model1_net, struct Model1_sk_buff *Model1_skb, Model1_u32 Model1_user);

struct Model1_sk_buff *Model1_ip_check_defrag(struct Model1_net *Model1_net, struct Model1_sk_buff *Model1_skb, Model1_u32 Model1_user);






int Model1_ip_frag_mem(struct Model1_net *Model1_net);

/*
 *	Functions provided by ip_forward.c
 */

int Model1_ip_forward(struct Model1_sk_buff *Model1_skb);

/*
 *	Functions provided by ip_options.c
 */

void Model1_ip_options_build(struct Model1_sk_buff *Model1_skb, struct Model1_ip_options *Model1_opt,
        Model1___be32 Model1_daddr, struct Model1_rtable *Model1_rt, int Model1_is_frag);

int Model1___ip_options_echo(struct Model1_ip_options *Model1_dopt, struct Model1_sk_buff *Model1_skb,
        const struct Model1_ip_options *Model1_sopt);
static inline __attribute__((no_instrument_function)) int Model1_ip_options_echo(struct Model1_ip_options *Model1_dopt, struct Model1_sk_buff *Model1_skb)
{
 return Model1___ip_options_echo(Model1_dopt, Model1_skb, &((struct Model1_inet_skb_parm*)((Model1_skb)->Model1_cb))->Model1_opt);
}

void Model1_ip_options_fragment(struct Model1_sk_buff *Model1_skb);
int Model1_ip_options_compile(struct Model1_net *Model1_net, struct Model1_ip_options *Model1_opt,
         struct Model1_sk_buff *Model1_skb);
int Model1_ip_options_get(struct Model1_net *Model1_net, struct Model1_ip_options_rcu **Model1_optp,
     unsigned char *Model1_data, int Model1_optlen);
int Model1_ip_options_get_from_user(struct Model1_net *Model1_net, struct Model1_ip_options_rcu **Model1_optp,
        unsigned char *Model1_data, int Model1_optlen);
void Model1_ip_options_undo(struct Model1_ip_options *Model1_opt);
void Model1_ip_forward_options(struct Model1_sk_buff *Model1_skb);
int Model1_ip_options_rcv_srr(struct Model1_sk_buff *Model1_skb);

/*
 *	Functions provided by ip_sockglue.c
 */

void Model1_ipv4_pktinfo_prepare(const struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);
void Model1_ip_cmsg_recv_offset(struct Model1_msghdr *Model1_msg, struct Model1_sk_buff *Model1_skb, int Model1_offset);
int Model1_ip_cmsg_send(struct Model1_sock *Model1_sk, struct Model1_msghdr *Model1_msg,
   struct Model1_ipcm_cookie *Model1_ipc, bool Model1_allow_ipv6);
int Model1_ip_setsockopt(struct Model1_sock *Model1_sk, int Model1_level, int Model1_optname, char *Model1_optval,
    unsigned int Model1_optlen);
int Model1_ip_getsockopt(struct Model1_sock *Model1_sk, int Model1_level, int Model1_optname, char *Model1_optval,
    int *Model1_optlen);
int Model1_compat_ip_setsockopt(struct Model1_sock *Model1_sk, int Model1_level, int Model1_optname,
    char *Model1_optval, unsigned int Model1_optlen);
int Model1_compat_ip_getsockopt(struct Model1_sock *Model1_sk, int Model1_level, int Model1_optname,
    char *Model1_optval, int *Model1_optlen);
int Model1_ip_ra_control(struct Model1_sock *Model1_sk, unsigned char Model1_on,
    void (*Model1_destructor)(struct Model1_sock *));

int Model1_ip_recv_error(struct Model1_sock *Model1_sk, struct Model1_msghdr *Model1_msg, int Model1_len, int *Model1_addr_len);
void Model1_ip_icmp_error(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb, int err, Model1___be16 Model1_port,
     Model1_u32 Model1_info, Model1_u8 *Model1_payload);
void Model1_ip_local_error(struct Model1_sock *Model1_sk, int err, Model1___be32 Model1_daddr, Model1___be16 Model1_dport,
      Model1_u32 Model1_info);

static inline __attribute__((no_instrument_function)) void Model1_ip_cmsg_recv(struct Model1_msghdr *Model1_msg, struct Model1_sk_buff *Model1_skb)
{
 Model1_ip_cmsg_recv_offset(Model1_msg, Model1_skb, 0);
}

bool Model1_icmp_global_allow(void);
extern int Model1_sysctl_icmp_msgs_per_sec;
extern int Model1_sysctl_icmp_msgs_burst;


int Model1_ip_misc_proc_init(void);








/* include/net/dsfield.h - Manipulation of the Differentiated Services field */

/* Written 1998-2000 by Werner Almesberger, EPFL ICA */
static inline __attribute__((no_instrument_function)) __u8 Model1_ipv4_get_dsfield(const struct Model1_iphdr *Model1_iph)
{
 return Model1_iph->Model1_tos;
}


static inline __attribute__((no_instrument_function)) __u8 Model1_ipv6_get_dsfield(const struct Model1_ipv6hdr *Model1_ipv6h)
{
 return (__builtin_constant_p((Model1___u16)(( Model1___u16)(Model1___be16)(*(const Model1___be16 *)Model1_ipv6h))) ? ((Model1___u16)( (((Model1___u16)(( Model1___u16)(Model1___be16)(*(const Model1___be16 *)Model1_ipv6h)) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)(( Model1___u16)(Model1___be16)(*(const Model1___be16 *)Model1_ipv6h)) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16(( Model1___u16)(Model1___be16)(*(const Model1___be16 *)Model1_ipv6h))) >> 4;
}


static inline __attribute__((no_instrument_function)) void Model1_ipv4_change_dsfield(struct Model1_iphdr *Model1_iph,__u8 Model1_mask,
    __u8 Model1_value)
{
        __u32 Model1_check = (__builtin_constant_p((Model1___u16)(( Model1___u16)(Model1___be16)(( Model1___be16)Model1_iph->Model1_check))) ? ((Model1___u16)( (((Model1___u16)(( Model1___u16)(Model1___be16)(( Model1___be16)Model1_iph->Model1_check)) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)(( Model1___u16)(Model1___be16)(( Model1___be16)Model1_iph->Model1_check)) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16(( Model1___u16)(Model1___be16)(( Model1___be16)Model1_iph->Model1_check)));
 __u8 Model1_dsfield;

 Model1_dsfield = (Model1_iph->Model1_tos & Model1_mask) | Model1_value;
 Model1_check += Model1_iph->Model1_tos;
 if ((Model1_check+1) >> 16) Model1_check = (Model1_check+1) & 0xffff;
 Model1_check -= Model1_dsfield;
 Model1_check += Model1_check >> 16; /* adjust carry */
 Model1_iph->Model1_check = ( Model1___sum16)(( Model1___be16)(__builtin_constant_p((Model1___u16)((Model1_check))) ? ((Model1___u16)( (((Model1___u16)((Model1_check)) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)((Model1_check)) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16((Model1_check))));
 Model1_iph->Model1_tos = Model1_dsfield;
}


static inline __attribute__((no_instrument_function)) void Model1_ipv6_change_dsfield(struct Model1_ipv6hdr *Model1_ipv6h,__u8 Model1_mask,
    __u8 Model1_value)
{
 Model1___be16 *Model1_p = ( Model1___be16 *)Model1_ipv6h;

 *Model1_p = (*Model1_p & (( Model1___be16)(__builtin_constant_p((Model1___u16)(((((Model1_u16)Model1_mask << 4) | 0xf00f)))) ? ((Model1___u16)( (((Model1___u16)(((((Model1_u16)Model1_mask << 4) | 0xf00f))) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)(((((Model1_u16)Model1_mask << 4) | 0xf00f))) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16(((((Model1_u16)Model1_mask << 4) | 0xf00f)))))) | (( Model1___be16)(__builtin_constant_p((Model1___u16)(((Model1_u16)Model1_value << 4))) ? ((Model1___u16)( (((Model1___u16)(((Model1_u16)Model1_value << 4)) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)(((Model1_u16)Model1_value << 4)) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16(((Model1_u16)Model1_value << 4))));
}

enum {
 Model1_INET_ECN_NOT_ECT = 0,
 Model1_INET_ECN_ECT_1 = 1,
 Model1_INET_ECN_ECT_0 = 2,
 Model1_INET_ECN_CE = 3,
 Model1_INET_ECN_MASK = 3,
};

extern int Model1_sysctl_tunnel_ecn_log;

static inline __attribute__((no_instrument_function)) int Model1_INET_ECN_is_ce(__u8 Model1_dsfield)
{
 return (Model1_dsfield & Model1_INET_ECN_MASK) == Model1_INET_ECN_CE;
}

static inline __attribute__((no_instrument_function)) int Model1_INET_ECN_is_not_ect(__u8 Model1_dsfield)
{
 return (Model1_dsfield & Model1_INET_ECN_MASK) == Model1_INET_ECN_NOT_ECT;
}

static inline __attribute__((no_instrument_function)) int Model1_INET_ECN_is_capable(__u8 Model1_dsfield)
{
 return Model1_dsfield & Model1_INET_ECN_ECT_0;
}

/*
 * RFC 3168 9.1.1
 *  The full-functionality option for ECN encapsulation is to copy the
 *  ECN codepoint of the inside header to the outside header on
 *  encapsulation if the inside header is not-ECT or ECT, and to set the
 *  ECN codepoint of the outside header to ECT(0) if the ECN codepoint of
 *  the inside header is CE.
 */
static inline __attribute__((no_instrument_function)) __u8 Model1_INET_ECN_encapsulate(__u8 Model1_outer, __u8 Model1_inner)
{
 Model1_outer &= ~Model1_INET_ECN_MASK;
 Model1_outer |= !Model1_INET_ECN_is_ce(Model1_inner) ? (Model1_inner & Model1_INET_ECN_MASK) :
       Model1_INET_ECN_ECT_0;
 return Model1_outer;
}

static inline __attribute__((no_instrument_function)) void Model1_INET_ECN_xmit(struct Model1_sock *Model1_sk)
{
 Model1_inet_sk(Model1_sk)->Model1_tos |= Model1_INET_ECN_ECT_0;
 if (Model1_inet6_sk(Model1_sk) != ((void *)0))
  Model1_inet6_sk(Model1_sk)->Model1_tclass |= Model1_INET_ECN_ECT_0;
}

static inline __attribute__((no_instrument_function)) void Model1_INET_ECN_dontxmit(struct Model1_sock *Model1_sk)
{
 Model1_inet_sk(Model1_sk)->Model1_tos &= ~Model1_INET_ECN_MASK;
 if (Model1_inet6_sk(Model1_sk) != ((void *)0))
  Model1_inet6_sk(Model1_sk)->Model1_tclass &= ~Model1_INET_ECN_MASK;
}
static inline __attribute__((no_instrument_function)) int Model1_IP_ECN_set_ce(struct Model1_iphdr *Model1_iph)
{
 Model1_u32 Model1_check = ( Model1_u32)Model1_iph->Model1_check;
 Model1_u32 Model1_ecn = (Model1_iph->Model1_tos + 1) & Model1_INET_ECN_MASK;

 /*
	 * After the last operation we have (in binary):
	 * INET_ECN_NOT_ECT => 01
	 * INET_ECN_ECT_1   => 10
	 * INET_ECN_ECT_0   => 11
	 * INET_ECN_CE      => 00
	 */
 if (!(Model1_ecn & 2))
  return !Model1_ecn;

 /*
	 * The following gives us:
	 * INET_ECN_ECT_1 => check += htons(0xFFFD)
	 * INET_ECN_ECT_0 => check += htons(0xFFFE)
	 */
 Model1_check += ( Model1_u16)(( Model1___be16)(__builtin_constant_p((Model1___u16)((0xFFFB))) ? ((Model1___u16)( (((Model1___u16)((0xFFFB)) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)((0xFFFB)) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16((0xFFFB)))) + ( Model1_u16)(( Model1___be16)(__builtin_constant_p((Model1___u16)((Model1_ecn))) ? ((Model1___u16)( (((Model1___u16)((Model1_ecn)) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)((Model1_ecn)) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16((Model1_ecn))));

 Model1_iph->Model1_check = ( Model1___sum16)(Model1_check + (Model1_check>=0xFFFF));
 Model1_iph->Model1_tos |= Model1_INET_ECN_CE;
 return 1;
}

static inline __attribute__((no_instrument_function)) void Model1_IP_ECN_clear(struct Model1_iphdr *Model1_iph)
{
 Model1_iph->Model1_tos &= ~Model1_INET_ECN_MASK;
}

static inline __attribute__((no_instrument_function)) void Model1_ipv4_copy_dscp(unsigned int Model1_dscp, struct Model1_iphdr *Model1_inner)
{
 Model1_dscp &= ~Model1_INET_ECN_MASK;
 Model1_ipv4_change_dsfield(Model1_inner, Model1_INET_ECN_MASK, Model1_dscp);
}

struct Model1_ipv6hdr;

/* Note:
 * IP_ECN_set_ce() has to tweak IPV4 checksum when setting CE,
 * meaning both changes have no effect on skb->csum if/when CHECKSUM_COMPLETE
 * In IPv6 case, no checksum compensates the change in IPv6 header,
 * so we have to update skb->csum.
 */
static inline __attribute__((no_instrument_function)) int Model1_IP6_ECN_set_ce(struct Model1_sk_buff *Model1_skb, struct Model1_ipv6hdr *Model1_iph)
{
 Model1___be32 Model1_from, Model1_to;

 if (Model1_INET_ECN_is_not_ect(Model1_ipv6_get_dsfield(Model1_iph)))
  return 0;

 Model1_from = *(Model1___be32 *)Model1_iph;
 Model1_to = Model1_from | (( Model1___be32)(__builtin_constant_p((__u32)((Model1_INET_ECN_CE << 20))) ? ((__u32)( (((__u32)((Model1_INET_ECN_CE << 20)) & (__u32)0x000000ffUL) << 24) | (((__u32)((Model1_INET_ECN_CE << 20)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((Model1_INET_ECN_CE << 20)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((Model1_INET_ECN_CE << 20)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((Model1_INET_ECN_CE << 20))));
 *(Model1___be32 *)Model1_iph = Model1_to;
 if (Model1_skb->Model1_ip_summed == 2)
  Model1_skb->Model1_csum = Model1_csum_add(Model1_csum_sub(Model1_skb->Model1_csum, ( Model1___wsum)Model1_from),
         ( Model1___wsum)Model1_to);
 return 1;
}

static inline __attribute__((no_instrument_function)) void Model1_IP6_ECN_clear(struct Model1_ipv6hdr *Model1_iph)
{
 *(Model1___be32*)Model1_iph &= ~(( Model1___be32)(__builtin_constant_p((__u32)((Model1_INET_ECN_MASK << 20))) ? ((__u32)( (((__u32)((Model1_INET_ECN_MASK << 20)) & (__u32)0x000000ffUL) << 24) | (((__u32)((Model1_INET_ECN_MASK << 20)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((Model1_INET_ECN_MASK << 20)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((Model1_INET_ECN_MASK << 20)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((Model1_INET_ECN_MASK << 20))));
}

static inline __attribute__((no_instrument_function)) void Model1_ipv6_copy_dscp(unsigned int Model1_dscp, struct Model1_ipv6hdr *Model1_inner)
{
 Model1_dscp &= ~Model1_INET_ECN_MASK;
 Model1_ipv6_change_dsfield(Model1_inner, Model1_INET_ECN_MASK, Model1_dscp);
}

static inline __attribute__((no_instrument_function)) int Model1_INET_ECN_set_ce(struct Model1_sk_buff *Model1_skb)
{
 switch (Model1_skb->Model1_protocol) {
 case (( Model1___be16)(__builtin_constant_p((Model1___u16)((0x0800))) ? ((Model1___u16)( (((Model1___u16)((0x0800)) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)((0x0800)) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16((0x0800)))):
  if (Model1_skb_network_header(Model1_skb) + sizeof(struct Model1_iphdr) <=
      Model1_skb_tail_pointer(Model1_skb))
   return Model1_IP_ECN_set_ce(Model1_ip_hdr(Model1_skb));
  break;

 case (( Model1___be16)(__builtin_constant_p((Model1___u16)((0x86DD))) ? ((Model1___u16)( (((Model1___u16)((0x86DD)) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)((0x86DD)) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16((0x86DD)))):
  if (Model1_skb_network_header(Model1_skb) + sizeof(struct Model1_ipv6hdr) <=
      Model1_skb_tail_pointer(Model1_skb))
   return Model1_IP6_ECN_set_ce(Model1_skb, Model1_ipv6_hdr(Model1_skb));
  break;
 }

 return 0;
}

/*
 * RFC 6040 4.2
 *  To decapsulate the inner header at the tunnel egress, a compliant
 *  tunnel egress MUST set the outgoing ECN field to the codepoint at the
 *  intersection of the appropriate arriving inner header (row) and outer
 *  header (column) in Figure 4
 *
 *      +---------+------------------------------------------------+
 *      |Arriving |            Arriving Outer Header               |
 *      |   Inner +---------+------------+------------+------------+
 *      |  Header | Not-ECT | ECT(0)     | ECT(1)     |     CE     |
 *      +---------+---------+------------+------------+------------+
 *      | Not-ECT | Not-ECT |Not-ECT(!!!)|Not-ECT(!!!)| <drop>(!!!)|
 *      |  ECT(0) |  ECT(0) | ECT(0)     | ECT(1)     |     CE     |
 *      |  ECT(1) |  ECT(1) | ECT(1) (!) | ECT(1)     |     CE     |
 *      |    CE   |      CE |     CE     |     CE(!!!)|     CE     |
 *      +---------+---------+------------+------------+------------+
 *
 *             Figure 4: New IP in IP Decapsulation Behaviour
 *
 *  returns 0 on success
 *          1 if something is broken and should be logged (!!! above)
 *          2 if packet should be dropped
 */
static inline __attribute__((no_instrument_function)) int Model1_INET_ECN_decapsulate(struct Model1_sk_buff *Model1_skb,
           __u8 Model1_outer, __u8 Model1_inner)
{
 if (Model1_INET_ECN_is_not_ect(Model1_inner)) {
  switch (Model1_outer & Model1_INET_ECN_MASK) {
  case Model1_INET_ECN_NOT_ECT:
   return 0;
  case Model1_INET_ECN_ECT_0:
  case Model1_INET_ECN_ECT_1:
   return 1;
  case Model1_INET_ECN_CE:
   return 2;
  }
 }

 if (Model1_INET_ECN_is_ce(Model1_outer))
  Model1_INET_ECN_set_ce(Model1_skb);

 return 0;
}

static inline __attribute__((no_instrument_function)) int Model1_IP_ECN_decapsulate(const struct Model1_iphdr *Model1_oiph,
         struct Model1_sk_buff *Model1_skb)
{
 __u8 Model1_inner;

 if (Model1_skb->Model1_protocol == (( Model1___be16)(__builtin_constant_p((Model1___u16)((0x0800))) ? ((Model1___u16)( (((Model1___u16)((0x0800)) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)((0x0800)) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16((0x0800)))))
  Model1_inner = Model1_ip_hdr(Model1_skb)->Model1_tos;
 else if (Model1_skb->Model1_protocol == (( Model1___be16)(__builtin_constant_p((Model1___u16)((0x86DD))) ? ((Model1___u16)( (((Model1___u16)((0x86DD)) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)((0x86DD)) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16((0x86DD)))))
  Model1_inner = Model1_ipv6_get_dsfield(Model1_ipv6_hdr(Model1_skb));
 else
  return 0;

 return Model1_INET_ECN_decapsulate(Model1_skb, Model1_oiph->Model1_tos, Model1_inner);
}

static inline __attribute__((no_instrument_function)) int Model1_IP6_ECN_decapsulate(const struct Model1_ipv6hdr *Model1_oipv6h,
          struct Model1_sk_buff *Model1_skb)
{
 __u8 Model1_inner;

 if (Model1_skb->Model1_protocol == (( Model1___be16)(__builtin_constant_p((Model1___u16)((0x0800))) ? ((Model1___u16)( (((Model1___u16)((0x0800)) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)((0x0800)) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16((0x0800)))))
  Model1_inner = Model1_ip_hdr(Model1_skb)->Model1_tos;
 else if (Model1_skb->Model1_protocol == (( Model1___be16)(__builtin_constant_p((Model1___u16)((0x86DD))) ? ((Model1___u16)( (((Model1___u16)((0x86DD)) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)((0x86DD)) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16((0x86DD)))))
  Model1_inner = Model1_ipv6_get_dsfield(Model1_ipv6_hdr(Model1_skb));
 else
  return 0;

 return Model1_INET_ECN_decapsulate(Model1_skb, Model1_ipv6_get_dsfield(Model1_oipv6h), Model1_inner);
}





extern struct Model1_inet_hashinfo Model1_tcp_hashinfo;

extern struct Model1_percpu_counter Model1_tcp_orphan_count;
void Model1_tcp_time_wait(struct Model1_sock *Model1_sk, int Model1_state, int Model1_timeo);




/*
 * Never offer a window over 32767 without using window scaling. Some
 * poor stacks do signed 16bit maths!
 */


/* Minimal accepted MSS. It is (60+60+8) - (20+20). */


/* The least MTU to use for probing */


/* probing interval, default to 10 minutes as per RFC4821 */


/* Specify interval when tcp mtu probing will stop */


/* After receiving this amount of duplicate ACKs fast retransmit starts. */


/* Maximal number of ACKs sent quickly to accelerate slow-start. */


/* urg_data states */
                                 /* BSD style FIN_WAIT2 deadlock breaker.
				  * It used to be 3min, new value is 60sec,
				  * to combine FIN-WAIT-2 timeout with
				  * TIME-WAIT timer.
				  */
/*
 *	TCP option
 */
/* Magic number to be after the option value for sharing TCP
 * experimental options. See draft-ietf-tcpm-experimental-options-00.txt
 */


/*
 *     TCP option lengths
 */
/* But this is what stacks really send out. */
/* Flags in tp->nonagle */




/* TCP thin-stream limits */


/* TCP initial congestion window as per rfc6928 */


/* Bit Flags for sysctl_tcp_fastopen */




/* Accept SYN data w/o any cookie option */


/* Force enable TFO on all listeners, i.e., not requiring the
 * TCP_FASTOPEN socket option. SOCKOPT1/2 determine how to set max_qlen.
 */



extern struct Model1_inet_timewait_death_row Model1_tcp_death_row;

/* sysctl variables for tcp */
extern int Model1_sysctl_tcp_timestamps;
extern int Model1_sysctl_tcp_window_scaling;
extern int Model1_sysctl_tcp_sack;
extern int Model1_sysctl_tcp_fastopen;
extern int Model1_sysctl_tcp_retrans_collapse;
extern int Model1_sysctl_tcp_stdurg;
extern int Model1_sysctl_tcp_rfc1337;
extern int Model1_sysctl_tcp_abort_on_overflow;
extern int Model1_sysctl_tcp_max_orphans;
extern int Model1_sysctl_tcp_fack;
extern int Model1_sysctl_tcp_reordering;
extern int Model1_sysctl_tcp_max_reordering;
extern int Model1_sysctl_tcp_dsack;
extern long Model1_sysctl_tcp_mem[3];
extern int Model1_sysctl_tcp_wmem[3];
extern int Model1_sysctl_tcp_rmem[3];
extern int Model1_sysctl_tcp_app_win;
extern int Model1_sysctl_tcp_adv_win_scale;
extern int Model1_sysctl_tcp_tw_reuse;
extern int Model1_sysctl_tcp_frto;
extern int Model1_sysctl_tcp_low_latency;
extern int Model1_sysctl_tcp_nometrics_save;
extern int Model1_sysctl_tcp_moderate_rcvbuf;
extern int Model1_sysctl_tcp_tso_win_divisor;
extern int Model1_sysctl_tcp_workaround_signed_windows;
extern int Model1_sysctl_tcp_slow_start_after_idle;
extern int Model1_sysctl_tcp_thin_linear_timeouts;
extern int Model1_sysctl_tcp_thin_dupack;
extern int Model1_sysctl_tcp_early_retrans;
extern int Model1_sysctl_tcp_limit_output_bytes;
extern int Model1_sysctl_tcp_challenge_ack_limit;
extern int Model1_sysctl_tcp_min_tso_segs;
extern int Model1_sysctl_tcp_min_rtt_wlen;
extern int Model1_sysctl_tcp_autocorking;
extern int Model1_sysctl_tcp_invalid_ratelimit;
extern int Model1_sysctl_tcp_pacing_ss_ratio;
extern int Model1_sysctl_tcp_pacing_ca_ratio;

extern Model1_atomic_long_t Model1_tcp_memory_allocated;
extern struct Model1_percpu_counter Model1_tcp_sockets_allocated;
extern int Model1_tcp_memory_pressure;

/* optimized version of sk_under_memory_pressure() for TCP sockets */
static inline __attribute__((no_instrument_function)) bool Model1_tcp_under_memory_pressure(const struct Model1_sock *Model1_sk)
{
 if (0 && Model1_sk->Model1_sk_memcg &&
     Model1_mem_cgroup_under_socket_pressure(Model1_sk->Model1_sk_memcg))
  return true;

 return Model1_tcp_memory_pressure;
}
/*
 * The next routines deal with comparing 32 bit unsigned ints
 * and worry about wraparound (automatic with unsigned arithmetic).
 */

static inline __attribute__((no_instrument_function)) bool Model1_before(__u32 Model1_seq1, __u32 Model1_seq2)
{
        return (Model1___s32)(Model1_seq1-Model1_seq2) < 0;
}


/* is s2<=s1<=s3 ? */
static inline __attribute__((no_instrument_function)) bool Model1_between(__u32 Model1_seq1, __u32 Model1_seq2, __u32 Model1_seq3)
{
 return Model1_seq3 - Model1_seq2 >= Model1_seq1 - Model1_seq2;
}

static inline __attribute__((no_instrument_function)) bool Model1_tcp_out_of_memory(struct Model1_sock *Model1_sk)
{
 if (Model1_sk->Model1_sk_wmem_queued > ((2048 + ((((sizeof(struct Model1_sk_buff))) + ((typeof((sizeof(struct Model1_sk_buff))))(((1 << (6)))) - 1)) & ~((typeof((sizeof(struct Model1_sk_buff))))(((1 << (6)))) - 1))) * 2) &&
     Model1_sk_memory_allocated(Model1_sk) > Model1_sk_prot_mem_limits(Model1_sk, 2))
  return true;
 return false;
}

void Model1_sk_forced_mem_schedule(struct Model1_sock *Model1_sk, int Model1_size);

static inline __attribute__((no_instrument_function)) bool Model1_tcp_too_many_orphans(struct Model1_sock *Model1_sk, int Model1_shift)
{
 struct Model1_percpu_counter *Model1_ocp = Model1_sk->Model1___sk_common.Model1_skc_prot->Model1_orphan_count;
 int Model1_orphans = Model1_percpu_counter_read_positive(Model1_ocp);

 if (Model1_orphans << Model1_shift > Model1_sysctl_tcp_max_orphans) {
  Model1_orphans = Model1_percpu_counter_sum_positive(Model1_ocp);
  if (Model1_orphans << Model1_shift > Model1_sysctl_tcp_max_orphans)
   return true;
 }
 return false;
}

bool Model1_tcp_check_oom(struct Model1_sock *Model1_sk, int Model1_shift);


extern struct Model1_proto Model1_tcp_prot;
extern struct Model1_tcp_mib Model1_cy_tcp_mib;
//#define TCP_INC_STATS(net, field)	SNMP_INC_STATS((net)->mib.tcp_statistics, field)
//#define __TCP_INC_STATS(net, field)	__SNMP_INC_STATS((net)->mib.tcp_statistics, field)

//#define TCP_ADD_STATS(net, field, val)	SNMP_ADD_STATS((net)->mib.tcp_statistics, field, val)





void Model1_tcp_tasklet_init(void);

void Model1_tcp_v4_err(struct Model1_sk_buff *Model1_skb, Model1_u32);

void Model1_tcp_shutdown(struct Model1_sock *Model1_sk, int Model1_how);

void Model1_tcp_v4_early_demux(struct Model1_sk_buff *Model1_skb);
int Model1_tcp_v4_rcv(struct Model1_sk_buff *Model1_skb);

int Model1_tcp_v4_tw_remember_stamp(struct Model1_inet_timewait_sock *Model1_tw);
int Model1_tcp_sendmsg(struct Model1_sock *Model1_sk, struct Model1_msghdr *Model1_msg, Model1_size_t Model1_size);
int Model1_tcp_sendpage(struct Model1_sock *Model1_sk, struct Model1_page *Model1_page, int Model1_offset, Model1_size_t Model1_size,
   int Model1_flags);
void Model1_tcp_release_cb(struct Model1_sock *Model1_sk);
void Model1_tcp_wfree(struct Model1_sk_buff *Model1_skb);
void Model1_tcp_write_timer_handler(struct Model1_sock *Model1_sk);
void Model1_tcp_delack_timer_handler(struct Model1_sock *Model1_sk);
int Model1_tcp_ioctl(struct Model1_sock *Model1_sk, int Model1_cmd, unsigned long Model1_arg);
int Model1_tcp_rcv_state_process(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);
void Model1_tcp_rcv_established(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb,
    const struct Model1_tcphdr *Model1_th, unsigned int Model1_len);
void Model1_tcp_rcv_space_adjust(struct Model1_sock *Model1_sk);
int Model1_tcp_twsk_unique(struct Model1_sock *Model1_sk, struct Model1_sock *Model1_sktw, void *Model1_twp);
void Model1_tcp_twsk_destructor(struct Model1_sock *Model1_sk);
Model1_ssize_t Model1_tcp_splice_read(struct Model1_socket *Model1_sk, Model1_loff_t *Model1_ppos,
   struct Model1_pipe_inode_info *Model1_pipe, Model1_size_t Model1_len,
   unsigned int Model1_flags);

static inline __attribute__((no_instrument_function)) void Model1_tcp_dec_quickack_mode(struct Model1_sock *Model1_sk,
      const unsigned int Model1_pkts)
{
 struct Model1_inet_connection_sock *Model1_icsk = Model1_inet_csk(Model1_sk);

 if (Model1_icsk->Model1_icsk_ack.Model1_quick) {
  if (Model1_pkts >= Model1_icsk->Model1_icsk_ack.Model1_quick) {
   Model1_icsk->Model1_icsk_ack.Model1_quick = 0;
   /* Leaving quickack mode we deflate ATO. */
   Model1_icsk->Model1_icsk_ack.Model1_ato = ((unsigned)(1000/25));
  } else
   Model1_icsk->Model1_icsk_ack.Model1_quick -= Model1_pkts;
 }
}






enum Model1_tcp_tw_status {
 Model1_TCP_TW_SUCCESS = 0,
 Model1_TCP_TW_RST = 1,
 Model1_TCP_TW_ACK = 2,
 Model1_TCP_TW_SYN = 3
};


enum Model1_tcp_tw_status Model1_tcp_timewait_state_process(struct Model1_inet_timewait_sock *Model1_tw,
           struct Model1_sk_buff *Model1_skb,
           const struct Model1_tcphdr *Model1_th);
struct Model1_sock *Model1_tcp_check_req(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb,
      struct Model1_request_sock *Model1_req, bool Model1_fastopen);
int Model1_tcp_child_process(struct Model1_sock *Model1_parent, struct Model1_sock *Model1_child,
        struct Model1_sk_buff *Model1_skb);
void Model1_tcp_enter_loss(struct Model1_sock *Model1_sk);
void Model1_tcp_clear_retrans(struct Model1_tcp_sock *Model1_tp);
void Model1_tcp_update_metrics(struct Model1_sock *Model1_sk);
void Model1_tcp_init_metrics(struct Model1_sock *Model1_sk);
void Model1_tcp_metrics_init(void);
bool Model1_tcp_peer_is_proven(struct Model1_request_sock *Model1_req, struct Model1_dst_entry *Model1_dst,
   bool Model1_paws_check, bool Model1_timestamps);
bool Model1_tcp_remember_stamp(struct Model1_sock *Model1_sk);
bool Model1_tcp_tw_remember_stamp(struct Model1_inet_timewait_sock *Model1_tw);
void Model1_tcp_fetch_timewait_stamp(struct Model1_sock *Model1_sk, struct Model1_dst_entry *Model1_dst);
void Model1_tcp_disable_fack(struct Model1_tcp_sock *Model1_tp);
void Model1_tcp_close(struct Model1_sock *Model1_sk, long Model1_timeout);
void Model1_tcp_init_sock(struct Model1_sock *Model1_sk);
unsigned int Model1_tcp_poll(struct Model1_file *Model1_file, struct Model1_socket *Model1_sock,
        struct Model1_poll_table_struct *Model1_wait);
int Model1_tcp_getsockopt(struct Model1_sock *Model1_sk, int Model1_level, int Model1_optname,
     char *Model1_optval, int *Model1_optlen);
int Model1_tcp_setsockopt(struct Model1_sock *Model1_sk, int Model1_level, int Model1_optname,
     char *Model1_optval, unsigned int Model1_optlen);
int Model1_compat_tcp_getsockopt(struct Model1_sock *Model1_sk, int Model1_level, int Model1_optname,
     char *Model1_optval, int *Model1_optlen);
int Model1_compat_tcp_setsockopt(struct Model1_sock *Model1_sk, int Model1_level, int Model1_optname,
     char *Model1_optval, unsigned int Model1_optlen);
void Model1_tcp_set_keepalive(struct Model1_sock *Model1_sk, int Model1_val);
void Model1_tcp_syn_ack_timeout(const struct Model1_request_sock *Model1_req);
int Model1_tcp_recvmsg(struct Model1_sock *Model1_sk, struct Model1_msghdr *Model1_msg, Model1_size_t Model1_len, int Model1_nonblock,
  int Model1_flags, int *Model1_addr_len);
void Model1_tcp_parse_options(const struct Model1_sk_buff *Model1_skb,
         struct Model1_tcp_options_received *Model1_opt_rx,
         int Model1_estab, struct Model1_tcp_fastopen_cookie *Model1_foc);
const Model1_u8 *Model1_tcp_parse_md5sig_option(const struct Model1_tcphdr *Model1_th);

/*
 *	TCP v4 functions exported for the inet6 API
 */

void Model1_tcp_v4_send_check(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);
void Model1_tcp_v4_mtu_reduced(struct Model1_sock *Model1_sk);
void Model1_tcp_req_err(struct Model1_sock *Model1_sk, Model1_u32 Model1_seq, bool Model1_abort);
int Model1_tcp_v4_conn_request(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);
struct Model1_sock *Model1_tcp_create_openreq_child(const struct Model1_sock *Model1_sk,
          struct Model1_request_sock *Model1_req,
          struct Model1_sk_buff *Model1_skb);
void Model1_tcp_ca_openreq_child(struct Model1_sock *Model1_sk, const struct Model1_dst_entry *Model1_dst);
struct Model1_sock *Model1_tcp_v4_syn_recv_sock(const struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb,
      struct Model1_request_sock *Model1_req,
      struct Model1_dst_entry *Model1_dst,
      struct Model1_request_sock *Model1_req_unhash,
      bool *Model1_own_req);
int Model1_tcp_v4_do_rcv(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);
int Model1_tcp_v4_connect(struct Model1_sock *Model1_sk, struct Model1_sockaddr *Model1_uaddr, int Model1_addr_len);
int Model1_tcp_connect(struct Model1_sock *Model1_sk);
enum Model1_tcp_synack_type {
 Model1_TCP_SYNACK_NORMAL,
 Model1_TCP_SYNACK_FASTOPEN,
 Model1_TCP_SYNACK_COOKIE,
};
struct Model1_sk_buff *Model1_tcp_make_synack(const struct Model1_sock *Model1_sk, struct Model1_dst_entry *Model1_dst,
    struct Model1_request_sock *Model1_req,
    struct Model1_tcp_fastopen_cookie *Model1_foc,
    enum Model1_tcp_synack_type Model1_synack_type);
int Model1_tcp_disconnect(struct Model1_sock *Model1_sk, int Model1_flags);

void Model1_tcp_finish_connect(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);
int Model1_tcp_send_rcvq(struct Model1_sock *Model1_sk, struct Model1_msghdr *Model1_msg, Model1_size_t Model1_size);
void Model1_inet_sk_rx_dst_set(struct Model1_sock *Model1_sk, const struct Model1_sk_buff *Model1_skb);

/* From syncookies.c */
struct Model1_sock *Model1_tcp_get_cookie_sock(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb,
     struct Model1_request_sock *Model1_req,
     struct Model1_dst_entry *Model1_dst);
int Model1___cookie_v4_check(const struct Model1_iphdr *Model1_iph, const struct Model1_tcphdr *Model1_th,
        Model1_u32 Model1_cookie);
struct Model1_sock *Model1_cookie_v4_check(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);


/* Syncookies use a monotonic timer which increments every 60 seconds.
 * This counter is used both as a hash input and partially encoded into
 * the cookie value.  A cookie is only validated further if the delta
 * between the current counter value and the encoded one is less than this,
 * i.e. a sent cookie is valid only at most for 2*60 seconds (or less if
 * the counter advances immediately after a cookie is generated).
 */




/* syncookies: remember time of last synqueue overflow
 * But do not dirty this field too often (once per second is enough)
 * It is racy as we do not hold a lock, but race is very minor.
 */
static inline __attribute__((no_instrument_function)) void Model1_tcp_synq_overflow(const struct Model1_sock *Model1_sk)
{
 unsigned long Model1_last_overflow = Model1_tcp_sk(Model1_sk)->Model1_rx_opt.Model1_ts_recent_stamp;
 unsigned long Model1_now = Model1_jiffies;

 if ((({ unsigned long Model1___dummy; typeof(Model1_now) Model1___dummy2; (void)(&Model1___dummy == &Model1___dummy2); 1; }) && ({ unsigned long Model1___dummy; typeof(Model1_last_overflow + 1000) Model1___dummy2; (void)(&Model1___dummy == &Model1___dummy2); 1; }) && ((long)((Model1_last_overflow + 1000) - (Model1_now)) < 0)))
  Model1_tcp_sk(Model1_sk)->Model1_rx_opt.Model1_ts_recent_stamp = Model1_now;
}

/* syncookies: no recent synqueue overflow on this listening socket? */
static inline __attribute__((no_instrument_function)) bool Model1_tcp_synq_no_recent_overflow(const struct Model1_sock *Model1_sk)
{
 unsigned long Model1_last_overflow = Model1_tcp_sk(Model1_sk)->Model1_rx_opt.Model1_ts_recent_stamp;

 return (({ unsigned long Model1___dummy; typeof(Model1_jiffies) Model1___dummy2; (void)(&Model1___dummy == &Model1___dummy2); 1; }) && ({ unsigned long Model1___dummy; typeof(Model1_last_overflow + (2 * (60 * 1000))) Model1___dummy2; (void)(&Model1___dummy == &Model1___dummy2); 1; }) && ((long)((Model1_last_overflow + (2 * (60 * 1000))) - (Model1_jiffies)) < 0));
}

static inline __attribute__((no_instrument_function)) Model1_u32 Model1_tcp_cookie_time(void)
{
 Model1_u64 Model1_val = Model1_get_jiffies_64();

 ({ Model1_uint32_t Model1___base = ((60 * 1000)); Model1_uint32_t Model1___rem; Model1___rem = ((Model1_uint64_t)(Model1_val)) % Model1___base; (Model1_val) = ((Model1_uint64_t)(Model1_val)) / Model1___base; Model1___rem; });
 return Model1_val;
}

Model1_u32 Model1___cookie_v4_init_sequence(const struct Model1_iphdr *Model1_iph, const struct Model1_tcphdr *Model1_th,
         Model1_u16 *Model1_mssp);
__u32 Model1_cookie_v4_init_sequence(const struct Model1_sk_buff *Model1_skb, Model1___u16 *Model1_mss);
__u32 Model1_cookie_init_timestamp(struct Model1_request_sock *Model1_req);
bool Model1_cookie_timestamp_decode(struct Model1_tcp_options_received *Model1_opt);
bool Model1_cookie_ecn_ok(const struct Model1_tcp_options_received *Model1_opt,
     const struct Model1_net *Model1_net, const struct Model1_dst_entry *Model1_dst);

/* From net/ipv6/syncookies.c */
int Model1___cookie_v6_check(const struct Model1_ipv6hdr *Model1_iph, const struct Model1_tcphdr *Model1_th,
        Model1_u32 Model1_cookie);
struct Model1_sock *Model1_cookie_v6_check(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);

Model1_u32 Model1___cookie_v6_init_sequence(const struct Model1_ipv6hdr *Model1_iph,
         const struct Model1_tcphdr *Model1_th, Model1_u16 *Model1_mssp);
__u32 Model1_cookie_v6_init_sequence(const struct Model1_sk_buff *Model1_skb, Model1___u16 *Model1_mss);

/* tcp_output.c */

void Model1___tcp_push_pending_frames(struct Model1_sock *Model1_sk, unsigned int Model1_cur_mss,
          int Model1_nonagle);
bool Model1_tcp_may_send_now(struct Model1_sock *Model1_sk);
int Model1___tcp_retransmit_skb(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb, int Model1_segs);
int Model1_tcp_retransmit_skb(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb, int Model1_segs);
void Model1_tcp_retransmit_timer(struct Model1_sock *Model1_sk);
void Model1_tcp_xmit_retransmit_queue(struct Model1_sock *);
void Model1_tcp_simple_retransmit(struct Model1_sock *);
int Model1_tcp_trim_head(struct Model1_sock *, struct Model1_sk_buff *, Model1_u32);
int Model1_tcp_fragment(struct Model1_sock *, struct Model1_sk_buff *, Model1_u32, unsigned int, Model1_gfp_t);

void Model1_tcp_send_probe0(struct Model1_sock *);
void Model1_tcp_send_partial(struct Model1_sock *);
int Model1_tcp_write_wakeup(struct Model1_sock *, int Model1_mib);
void Model1_tcp_send_fin(struct Model1_sock *Model1_sk);
void Model1_tcp_send_active_reset(struct Model1_sock *Model1_sk, Model1_gfp_t Model1_priority);
int Model1_tcp_send_synack(struct Model1_sock *);
void Model1_tcp_push_one(struct Model1_sock *, unsigned int Model1_mss_now);
void Model1_tcp_send_ack(struct Model1_sock *Model1_sk);
void Model1_tcp_send_delayed_ack(struct Model1_sock *Model1_sk);
void Model1_tcp_send_loss_probe(struct Model1_sock *Model1_sk);
bool Model1_tcp_schedule_loss_probe(struct Model1_sock *Model1_sk);
void Model1_tcp_skb_collapse_tstamp(struct Model1_sk_buff *Model1_skb,
        const struct Model1_sk_buff *Model1_next_skb);

/* tcp_input.c */
void Model1_tcp_resume_early_retransmit(struct Model1_sock *Model1_sk);
void Model1_tcp_rearm_rto(struct Model1_sock *Model1_sk);
void Model1_tcp_synack_rtt_meas(struct Model1_sock *Model1_sk, struct Model1_request_sock *Model1_req);
void Model1_tcp_reset(struct Model1_sock *Model1_sk);
void Model1_tcp_skb_mark_lost_uncond_verify(struct Model1_tcp_sock *Model1_tp, struct Model1_sk_buff *Model1_skb);
void Model1_tcp_fin(struct Model1_sock *Model1_sk);

/* tcp_timer.c */
void Model1_tcp_init_xmit_timers(struct Model1_sock *);
static inline __attribute__((no_instrument_function)) void Model1_tcp_clear_xmit_timers(struct Model1_sock *Model1_sk)
{
 Model1_inet_csk_clear_xmit_timers(Model1_sk);
}

unsigned int Model1_tcp_sync_mss(struct Model1_sock *Model1_sk, Model1_u32 Model1_pmtu);
unsigned int Model1_tcp_current_mss(struct Model1_sock *Model1_sk);

/* Bound MSS / TSO packet size with the half of the window */
static inline __attribute__((no_instrument_function)) int Model1_tcp_bound_to_half_wnd(struct Model1_tcp_sock *Model1_tp, int Model1_pktsize)
{
 int Model1_cutoff;

 /* When peer uses tiny windows, there is no use in packetizing
	 * to sub-MSS pieces for the sake of SWS or making sure there
	 * are enough packets in the pipe for fast recovery.
	 *
	 * On the other hand, for extremely large MSS devices, handling
	 * smaller than MSS windows in this way does make sense.
	 */
 if (Model1_tp->Model1_max_window > 536U)
  Model1_cutoff = (Model1_tp->Model1_max_window >> 1);
 else
  Model1_cutoff = Model1_tp->Model1_max_window;

 if (Model1_cutoff && Model1_pktsize > Model1_cutoff)
  return ({ int Model1___max1 = (Model1_cutoff); int Model1___max2 = (68U - Model1_tp->Model1_tcp_header_len); Model1___max1 > Model1___max2 ? Model1___max1: Model1___max2; });
 else
  return Model1_pktsize;
}

/* tcp.c */
void Model1_tcp_get_info(struct Model1_sock *, struct Model1_tcp_info *);

/* Read 'sendfile()'-style from a TCP socket */
typedef int (*Model1_sk_read_actor_t)(Model1_read_descriptor_t *, struct Model1_sk_buff *,
    unsigned int, Model1_size_t);
int Model1_tcp_read_sock(struct Model1_sock *Model1_sk, Model1_read_descriptor_t *Model1_desc,
    Model1_sk_read_actor_t Model1_recv_actor);

void Model1_tcp_initialize_rcv_mss(struct Model1_sock *Model1_sk);

int Model1_tcp_mtu_to_mss(struct Model1_sock *Model1_sk, int Model1_pmtu);
int Model1_tcp_mss_to_mtu(struct Model1_sock *Model1_sk, int Model1_mss);
void Model1_tcp_mtup_init(struct Model1_sock *Model1_sk);
void Model1_tcp_init_buffer_space(struct Model1_sock *Model1_sk);

static inline __attribute__((no_instrument_function)) void Model1_tcp_bound_rto(const struct Model1_sock *Model1_sk)
{
 if (Model1_inet_csk(Model1_sk)->Model1_icsk_rto > ((unsigned)(120*1000)))
  Model1_inet_csk(Model1_sk)->Model1_icsk_rto = ((unsigned)(120*1000));
}

static inline __attribute__((no_instrument_function)) Model1_u32 Model1___tcp_set_rto(const struct Model1_tcp_sock *Model1_tp)
{
 return Model1_usecs_to_jiffies((Model1_tp->Model1_srtt_us >> 3) + Model1_tp->Model1_rttvar_us);
}

static inline __attribute__((no_instrument_function)) void Model1___tcp_fast_path_on(struct Model1_tcp_sock *Model1_tp, Model1_u32 Model1_snd_wnd)
{
 Model1_tp->Model1_pred_flags = (( Model1___be32)(__builtin_constant_p((__u32)(((Model1_tp->Model1_tcp_header_len << 26) | (__builtin_constant_p((__u32)(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK))) ? ((__u32)( (((__u32)(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK))) | Model1_snd_wnd))) ? ((__u32)( (((__u32)(((Model1_tp->Model1_tcp_header_len << 26) | (__builtin_constant_p((__u32)(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK))) ? ((__u32)( (((__u32)(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK))) | Model1_snd_wnd)) & (__u32)0x000000ffUL) << 24) | (((__u32)(((Model1_tp->Model1_tcp_header_len << 26) | (__builtin_constant_p((__u32)(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK))) ? ((__u32)( (((__u32)(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK))) | Model1_snd_wnd)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(((Model1_tp->Model1_tcp_header_len << 26) | (__builtin_constant_p((__u32)(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK))) ? ((__u32)( (((__u32)(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK))) | Model1_snd_wnd)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(((Model1_tp->Model1_tcp_header_len << 26) | (__builtin_constant_p((__u32)(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK))) ? ((__u32)( (((__u32)(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK))) | Model1_snd_wnd)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32(((Model1_tp->Model1_tcp_header_len << 26) | (__builtin_constant_p((__u32)(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK))) ? ((__u32)( (((__u32)(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK)) & (__u32)0x000000ffUL) << 24) | (((__u32)(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32(( __u32)(Model1___be32)(Model1_TCP_FLAG_ACK))) | Model1_snd_wnd))));


}

static inline __attribute__((no_instrument_function)) void Model1_tcp_fast_path_on(struct Model1_tcp_sock *Model1_tp)
{
 Model1___tcp_fast_path_on(Model1_tp, Model1_tp->Model1_snd_wnd >> Model1_tp->Model1_rx_opt.Model1_snd_wscale);
}

static inline __attribute__((no_instrument_function)) void Model1_tcp_fast_path_check(struct Model1_sock *Model1_sk)
{
 struct Model1_tcp_sock *Model1_tp = Model1_tcp_sk(Model1_sk);

 if (Model1_skb_queue_empty(&Model1_tp->Model1_out_of_order_queue) &&
     Model1_tp->Model1_rcv_wnd &&
     Model1_atomic_read(&Model1_sk->Model1_sk_backlog.Model1_rmem_alloc) < Model1_sk->Model1_sk_rcvbuf &&
     !Model1_tp->Model1_urg_data)
  Model1_tcp_fast_path_on(Model1_tp);
}

/* Compute the actual rto_min value */
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_tcp_rto_min(struct Model1_sock *Model1_sk)
{
 const struct Model1_dst_entry *Model1_dst = Model1___sk_dst_get(Model1_sk);
 Model1_u32 Model1_rto_min = ((unsigned)(1000/5));

 if (Model1_dst && Model1_dst_metric_locked(Model1_dst, Model1_RTAX_RTO_MIN))
  Model1_rto_min = Model1_dst_metric_rtt(Model1_dst, Model1_RTAX_RTO_MIN);
 return Model1_rto_min;
}

static inline __attribute__((no_instrument_function)) Model1_u32 Model1_tcp_rto_min_us(struct Model1_sock *Model1_sk)
{
 return Model1_jiffies_to_usecs(Model1_tcp_rto_min(Model1_sk));
}

static inline __attribute__((no_instrument_function)) bool Model1_tcp_ca_dst_locked(const struct Model1_dst_entry *Model1_dst)
{
 return Model1_dst_metric_locked(Model1_dst, Model1_RTAX_CC_ALGO);
}

/* Minimum RTT in usec. ~0 means not available. */
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_tcp_min_rtt(const struct Model1_tcp_sock *Model1_tp)
{
 return Model1_tp->Model1_rtt_min[0].Model1_rtt;
}

/* Compute the actual receive window we are currently advertising.
 * Rcv_nxt can be after the window if our peer push more data
 * than the offered window.
 */
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_tcp_receive_window(const struct Model1_tcp_sock *Model1_tp)
{
 Model1_s32 Model1_win = Model1_tp->Model1_rcv_wup + Model1_tp->Model1_rcv_wnd - Model1_tp->Model1_rcv_nxt;

 if (Model1_win < 0)
  Model1_win = 0;
 return (Model1_u32) Model1_win;
}

/* Choose a new window, without checks for shrinking, and without
 * scaling applied to the result.  The caller does these things
 * if necessary.  This is a "raw" window selection.
 */
Model1_u32 Model1___tcp_select_window(struct Model1_sock *Model1_sk);

void Model1_tcp_send_window_probe(struct Model1_sock *Model1_sk);

/* TCP timestamps are only 32-bits, this causes a slight
 * complication on 64-bit systems since we store a snapshot
 * of jiffies in the buffer control blocks below.  We decided
 * to use only the low 32-bits of jiffies and hide the ugly
 * casts with the following macro.
 */


static inline __attribute__((no_instrument_function)) Model1_u32 Model1_tcp_skb_timestamp(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb->Model1_skb_mstamp.Model1_stamp_jiffies;
}
/* This is what the send packet queuing engine uses to pass
 * TCP per-packet control information to the transmission code.
 * We also store the host-order sequence numbers in here too.
 * This is 44 bytes if IPV6 is enabled.
 * If this grows please adjust skbuff.h:skbuff->cb[xxx] size appropriately.
 */
struct Model1_tcp_skb_cb {
 __u32 Model1_seq; /* Starting sequence number	*/
 __u32 Model1_end_seq; /* SEQ + FIN + SYN + datalen	*/
 union {
  /* Note : tcp_tw_isn is used in input path only
		 *	  (isn chosen by tcp_timewait_state_process())
		 *
		 * 	  tcp_gso_segs/size are used in write queue only,
		 *	  cf tcp_skb_pcount()/tcp_skb_mss()
		 */
  __u32 Model1_tcp_tw_isn;
  struct {
   Model1_u16 Model1_tcp_gso_segs;
   Model1_u16 Model1_tcp_gso_size;
  };
 };
 __u8 Model1_tcp_flags; /* TCP header flags. (tcp[13])	*/

 __u8 Model1_sacked; /* State flags for SACK/FACK.	*/
 __u8 Model1_ip_dsfield; /* IPv4 tos or IPv6 dsfield	*/
 __u8 Model1_txstamp_ack:1, /* Record TX timestamp for ack? */
   Model1_eor:1, /* Is skb MSG_EOR marked? */
   unused:6;
 __u32 Model1_ack_seq; /* Sequence number ACK'd	*/
 union {
  struct {
   /* There is space for up to 20 bytes */
   __u32 Model1_in_flight;/* Bytes in flight when packet sent */
  } Model1_tx; /* only used for outgoing skbs */
  union {
   struct Model1_inet_skb_parm Model1_h4;

   struct Model1_inet6_skb_parm Model1_h6;

  } Model1_header; /* For incoming skbs */
 };
};





/* This is the variant of inet6_iif() that must be used by TCP,
 * as TCP moves IP6CB into a different location in skb->cb[]
 */
static inline __attribute__((no_instrument_function)) int Model1_tcp_v6_iif(const struct Model1_sk_buff *Model1_skb)
{
 bool Model1_l3_slave = Model1_skb_l3mdev_slave(((struct Model1_tcp_skb_cb *)&((Model1_skb)->Model1_cb[0]))->Model1_header.Model1_h6.Model1_flags);

 return Model1_l3_slave ? Model1_skb->Model1_skb_iif : ((struct Model1_tcp_skb_cb *)&((Model1_skb)->Model1_cb[0]))->Model1_header.Model1_h6.Model1_iif;
}


/* Due to TSO, an SKB can be composed of multiple actual
 * packets.  To keep these tracked properly, we use this.
 */
static inline __attribute__((no_instrument_function)) int Model1_tcp_skb_pcount(const struct Model1_sk_buff *Model1_skb)
{
 return ((struct Model1_tcp_skb_cb *)&((Model1_skb)->Model1_cb[0]))->Model1_tcp_gso_segs;
}

static inline __attribute__((no_instrument_function)) void Model1_tcp_skb_pcount_set(struct Model1_sk_buff *Model1_skb, int Model1_segs)
{
 ((struct Model1_tcp_skb_cb *)&((Model1_skb)->Model1_cb[0]))->Model1_tcp_gso_segs = Model1_segs;
}

static inline __attribute__((no_instrument_function)) void Model1_tcp_skb_pcount_add(struct Model1_sk_buff *Model1_skb, int Model1_segs)
{
 ((struct Model1_tcp_skb_cb *)&((Model1_skb)->Model1_cb[0]))->Model1_tcp_gso_segs += Model1_segs;
}

/* This is valid iff skb is in write queue and tcp_skb_pcount() > 1. */
static inline __attribute__((no_instrument_function)) int Model1_tcp_skb_mss(const struct Model1_sk_buff *Model1_skb)
{
 return ((struct Model1_tcp_skb_cb *)&((Model1_skb)->Model1_cb[0]))->Model1_tcp_gso_size;
}

static inline __attribute__((no_instrument_function)) bool Model1_tcp_skb_can_collapse_to(const struct Model1_sk_buff *Model1_skb)
{
 return __builtin_expect(!!(!((struct Model1_tcp_skb_cb *)&((Model1_skb)->Model1_cb[0]))->Model1_eor), 1);
}

/* Events passed to congestion control interface */
enum Model1_tcp_ca_event {
 Model1_CA_EVENT_TX_START, /* first transmit when no packets in flight */
 Model1_CA_EVENT_CWND_RESTART, /* congestion window restart */
 Model1_CA_EVENT_COMPLETE_CWR, /* end of congestion recovery */
 Model1_CA_EVENT_LOSS, /* loss timeout */
 Model1_CA_EVENT_ECN_NO_CE, /* ECT set, but not CE marked */
 Model1_CA_EVENT_ECN_IS_CE, /* received CE marked IP packet */
 Model1_CA_EVENT_DELAYED_ACK, /* Delayed ack is sent */
 Model1_CA_EVENT_NON_DELAYED_ACK,
};

/* Information about inbound ACK, passed to cong_ops->in_ack_event() */
enum Model1_tcp_ca_ack_event_flags {
 Model1_CA_ACK_SLOWPATH = (1 << 0), /* In slow path processing */
 Model1_CA_ACK_WIN_UPDATE = (1 << 1), /* ACK updated window */
 Model1_CA_ACK_ECE = (1 << 2), /* ECE bit is set on ack */
};

/*
 * Interface for adding new TCP congestion control handlers
 */






/* Algorithm can be set on socket without CAP_NET_ADMIN privileges */

/* Requires ECN/ECT set on all packets */


union Model1_tcp_cc_info;

struct Model1_ack_sample {
 Model1_u32 Model1_pkts_acked;
 Model1_s32 Model1_rtt_us;
 Model1_u32 Model1_in_flight;
};

struct Model1_tcp_congestion_ops {
 struct Model1_list_head Model1_list;
 Model1_u32 Model1_key;
 Model1_u32 Model1_flags;

 /* initialize private data (optional) */
 void (*Model1_init)(struct Model1_sock *Model1_sk);
 /* cleanup private data  (optional) */
 void (*Model1_release)(struct Model1_sock *Model1_sk);

 /* return slow start threshold (required) */
 Model1_u32 (*Model1_ssthresh)(struct Model1_sock *Model1_sk);
 /* do new cwnd calculation (required) */
 void (*Model1_cong_avoid)(struct Model1_sock *Model1_sk, Model1_u32 Model1_ack, Model1_u32 Model1_acked);
 /* call before changing ca_state (optional) */
 void (*Model1_set_state)(struct Model1_sock *Model1_sk, Model1_u8 Model1_new_state);
 /* call when cwnd event occurs (optional) */
 void (*Model1_cwnd_event)(struct Model1_sock *Model1_sk, enum Model1_tcp_ca_event Model1_ev);
 /* call when ack arrives (optional) */
 void (*Model1_in_ack_event)(struct Model1_sock *Model1_sk, Model1_u32 Model1_flags);
 /* new value of cwnd after loss (optional) */
 Model1_u32 (*Model1_undo_cwnd)(struct Model1_sock *Model1_sk);
 /* hook for packet ack accounting (optional) */
 void (*Model1_pkts_acked)(struct Model1_sock *Model1_sk, const struct Model1_ack_sample *Model1_sample);
 /* get info for inet_diag (optional) */
 Model1_size_t (*Model1_get_info)(struct Model1_sock *Model1_sk, Model1_u32 Model1_ext, int *Model1_attr,
      union Model1_tcp_cc_info *Model1_info);

 char Model1_name[16];
 struct Model1_module *Model1_owner;
};

int Model1_tcp_register_congestion_control(struct Model1_tcp_congestion_ops *Model1_type);
void Model1_tcp_unregister_congestion_control(struct Model1_tcp_congestion_ops *Model1_type);

void Model1_tcp_assign_congestion_control(struct Model1_sock *Model1_sk);
void Model1_tcp_init_congestion_control(struct Model1_sock *Model1_sk);
void Model1_tcp_cleanup_congestion_control(struct Model1_sock *Model1_sk);
int Model1_tcp_set_default_congestion_control(const char *Model1_name);
void Model1_tcp_get_default_congestion_control(char *Model1_name);
void Model1_tcp_get_available_congestion_control(char *Model1_buf, Model1_size_t Model1_len);
void Model1_tcp_get_allowed_congestion_control(char *Model1_buf, Model1_size_t Model1_len);
int Model1_tcp_set_allowed_congestion_control(char *Model1_allowed);
int Model1_tcp_set_congestion_control(struct Model1_sock *Model1_sk, const char *Model1_name);
Model1_u32 Model1_tcp_slow_start(struct Model1_tcp_sock *Model1_tp, Model1_u32 Model1_acked);
void Model1_tcp_cong_avoid_ai(struct Model1_tcp_sock *Model1_tp, Model1_u32 Model1_w, Model1_u32 Model1_acked);

Model1_u32 Model1_tcp_reno_ssthresh(struct Model1_sock *Model1_sk);
void Model1_tcp_reno_cong_avoid(struct Model1_sock *Model1_sk, Model1_u32 Model1_ack, Model1_u32 Model1_acked);
extern struct Model1_tcp_congestion_ops Model1_tcp_reno;

struct Model1_tcp_congestion_ops *Model1_tcp_ca_find_key(Model1_u32 Model1_key);
Model1_u32 Model1_tcp_ca_get_key_by_name(const char *Model1_name, bool *Model1_ecn_ca);

char *Model1_tcp_ca_get_name_by_key(Model1_u32 Model1_key, char *Model1_buffer);







static inline __attribute__((no_instrument_function)) bool Model1_tcp_ca_needs_ecn(const struct Model1_sock *Model1_sk)
{
 const struct Model1_inet_connection_sock *Model1_icsk = Model1_inet_csk(Model1_sk);

 return Model1_icsk->Model1_icsk_ca_ops->Model1_flags & 0x2;
}

static inline __attribute__((no_instrument_function)) void Model1_tcp_set_ca_state(struct Model1_sock *Model1_sk, const Model1_u8 Model1_ca_state)
{
 struct Model1_inet_connection_sock *Model1_icsk = Model1_inet_csk(Model1_sk);

#if CY_ABSTRACT7 //the function pointer is empty
#else
 if (Model1_icsk->Model1_icsk_ca_ops->Model1_set_state)
  Model1_icsk->Model1_icsk_ca_ops->Model1_set_state(Model1_sk, Model1_ca_state);
#endif
 Model1_icsk->Model1_icsk_ca_state = Model1_ca_state;
}

static inline __attribute__((no_instrument_function)) void Model1_tcp_ca_event(struct Model1_sock *Model1_sk, const enum Model1_tcp_ca_event Model1_event)
{
 const struct Model1_inet_connection_sock *Model1_icsk = Model1_inet_csk(Model1_sk);

#if !CY_ABSTRACT1
 if (Model1_icsk->Model1_icsk_ca_ops->Model1_cwnd_event)
  Model1_icsk->Model1_icsk_ca_ops->Model1_cwnd_event(Model1_sk, Model1_event);
#endif
}

/* These functions determine how the current flow behaves in respect of SACK
 * handling. SACK is negotiated with the peer, and therefore it can vary
 * between different flows.
 *
 * tcp_is_sack - SACK enabled
 * tcp_is_reno - No SACK
 * tcp_is_fack - FACK enabled, implies SACK enabled
 */
static inline __attribute__((no_instrument_function)) int Model1_tcp_is_sack(const struct Model1_tcp_sock *Model1_tp)
{
 return Model1_tp->Model1_rx_opt.Model1_sack_ok;
}

static inline __attribute__((no_instrument_function)) bool Model1_tcp_is_reno(const struct Model1_tcp_sock *Model1_tp)
{
 return !Model1_tcp_is_sack(Model1_tp);
}

static inline __attribute__((no_instrument_function)) bool Model1_tcp_is_fack(const struct Model1_tcp_sock *Model1_tp)
{
 return Model1_tp->Model1_rx_opt.Model1_sack_ok & (1 << 1);
}

static inline __attribute__((no_instrument_function)) void Model1_tcp_enable_fack(struct Model1_tcp_sock *Model1_tp)
{
 Model1_tp->Model1_rx_opt.Model1_sack_ok |= (1 << 1);
}

/* TCP early-retransmit (ER) is similar to but more conservative than
 * the thin-dupack feature.  Enable ER only if thin-dupack is disabled.
 */
static inline __attribute__((no_instrument_function)) void Model1_tcp_enable_early_retrans(struct Model1_tcp_sock *Model1_tp)
{
 struct Model1_net *Model1_net = Model1_sock_net((struct Model1_sock *)Model1_tp);

 Model1_tp->Model1_do_early_retrans = Model1_sysctl_tcp_early_retrans &&
  Model1_sysctl_tcp_early_retrans < 4 && !Model1_sysctl_tcp_thin_dupack &&
  Model1_net->Model1_ipv4.Model1_sysctl_tcp_reordering == 3;
}

static inline __attribute__((no_instrument_function)) void Model1_tcp_disable_early_retrans(struct Model1_tcp_sock *Model1_tp)
{
 Model1_tp->Model1_do_early_retrans = 0;
}

static inline __attribute__((no_instrument_function)) unsigned int Model1_tcp_left_out(const struct Model1_tcp_sock *Model1_tp)
{
 return Model1_tp->Model1_sacked_out + Model1_tp->Model1_lost_out;
}

/* This determines how many packets are "in the network" to the best
 * of our knowledge.  In many cases it is conservative, but where
 * detailed information is available from the receiver (via SACK
 * blocks etc.) we can make more aggressive calculations.
 *
 * Use this for decisions involving congestion control, use just
 * tp->packets_out to determine if the send queue is empty or not.
 *
 * Read this equation as:
 *
 *	"Packets sent once on transmission queue" MINUS
 *	"Packets left network, but not honestly ACKed yet" PLUS
 *	"Packets fast retransmitted"
 */
static inline __attribute__((no_instrument_function)) unsigned int Model1_tcp_packets_in_flight(const struct Model1_tcp_sock *Model1_tp)
{
 return Model1_tp->Model1_packets_out - Model1_tcp_left_out(Model1_tp) + Model1_tp->Model1_retrans_out;
}



static inline __attribute__((no_instrument_function)) bool Model1_tcp_in_slow_start(const struct Model1_tcp_sock *Model1_tp)
{
 return Model1_tp->Model1_snd_cwnd < Model1_tp->Model1_snd_ssthresh;
}

static inline __attribute__((no_instrument_function)) bool Model1_tcp_in_initial_slowstart(const struct Model1_tcp_sock *Model1_tp)
{
 return Model1_tp->Model1_snd_ssthresh >= 0x7fffffff;
}

static inline __attribute__((no_instrument_function)) bool Model1_tcp_in_cwnd_reduction(const struct Model1_sock *Model1_sk)
{
 return ((1<<Model1_TCP_CA_CWR) | (1<<Model1_TCP_CA_Recovery)) &
        (1 << Model1_inet_csk(Model1_sk)->Model1_icsk_ca_state);
}

/* If cwnd > ssthresh, we may raise ssthresh to be half-way to cwnd.
 * The exception is cwnd reduction phase, when cwnd is decreasing towards
 * ssthresh.
 */
static inline __attribute__((no_instrument_function)) __u32 Model1_tcp_current_ssthresh(const struct Model1_sock *Model1_sk)
{
 const struct Model1_tcp_sock *Model1_tp = Model1_tcp_sk(Model1_sk);

 if (Model1_tcp_in_cwnd_reduction(Model1_sk))
  return Model1_tp->Model1_snd_ssthresh;
 else
  return ({ typeof(Model1_tp->Model1_snd_ssthresh) Model1__max1 = (Model1_tp->Model1_snd_ssthresh); typeof(((Model1_tp->Model1_snd_cwnd >> 1) + (Model1_tp->Model1_snd_cwnd >> 2))) Model1__max2 = (((Model1_tp->Model1_snd_cwnd >> 1) + (Model1_tp->Model1_snd_cwnd >> 2))); (void) (&Model1__max1 == &Model1__max2); Model1__max1 > Model1__max2 ? Model1__max1 : Model1__max2; });


}

/* Use define here intentionally to get WARN_ON location shown at the caller */


void Model1_tcp_enter_cwr(struct Model1_sock *Model1_sk);
__u32 Model1_tcp_init_cwnd(const struct Model1_tcp_sock *Model1_tp, const struct Model1_dst_entry *Model1_dst);

/* The maximum number of MSS of available cwnd for which TSO defers
 * sending if not using sysctl_tcp_tso_win_divisor.
 */
static inline __attribute__((no_instrument_function)) __u32 Model1_tcp_max_tso_deferred_mss(const struct Model1_tcp_sock *Model1_tp)
{
 return 3;
}

/* Returns end sequence number of the receiver's advertised window */
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_tcp_wnd_end(const struct Model1_tcp_sock *Model1_tp)
{
 return Model1_tp->Model1_snd_una + Model1_tp->Model1_snd_wnd;
}

/* We follow the spirit of RFC2861 to validate cwnd but implement a more
 * flexible approach. The RFC suggests cwnd should not be raised unless
 * it was fully used previously. And that's exactly what we do in
 * congestion avoidance mode. But in slow start we allow cwnd to grow
 * as long as the application has used half the cwnd.
 * Example :
 *    cwnd is 10 (IW10), but application sends 9 frames.
 *    We allow cwnd to reach 18 when all frames are ACKed.
 * This check is safe because it's as aggressive as slow start which already
 * risks 100% overshoot. The advantage is that we discourage application to
 * either send more filler packets or data to artificially blow up the cwnd
 * usage, and allow application-limited process to probe bw more aggressively.
 */
static inline __attribute__((no_instrument_function)) bool Model1_tcp_is_cwnd_limited(const struct Model1_sock *Model1_sk)
{
 const struct Model1_tcp_sock *Model1_tp = Model1_tcp_sk(Model1_sk);

 /* If in slow start, ensure cwnd grows to twice what was ACKed. */
 if (Model1_tcp_in_slow_start(Model1_tp))
  return Model1_tp->Model1_snd_cwnd < 2 * Model1_tp->Model1_max_packets_out;

 return Model1_tp->Model1_is_cwnd_limited;
}

/* Something is really bad, we could not queue an additional packet,
 * because qdisc is full or receiver sent a 0 window.
 * We do not want to add fuel to the fire, or abort too early,
 * so make sure the timer we arm now is at least 200ms in the future,
 * regardless of current icsk_rto value (as it could be ~2ms)
 */
static inline __attribute__((no_instrument_function)) unsigned long Model1_tcp_probe0_base(const struct Model1_sock *Model1_sk)
{
 return ({ unsigned long Model1___max1 = (Model1_inet_csk(Model1_sk)->Model1_icsk_rto); unsigned long Model1___max2 = (((unsigned)(1000/5))); Model1___max1 > Model1___max2 ? Model1___max1: Model1___max2; });
}

/* Variant of inet_csk_rto_backoff() used for zero window probes */
static inline __attribute__((no_instrument_function)) unsigned long Model1_tcp_probe0_when(const struct Model1_sock *Model1_sk,
         unsigned long Model1_max_when)
{
 Model1_u64 Model1_when = (Model1_u64)Model1_tcp_probe0_base(Model1_sk) << Model1_inet_csk(Model1_sk)->Model1_icsk_backoff;

 return (unsigned long)({ Model1_u64 Model1___min1 = (Model1_when); Model1_u64 Model1___min2 = (Model1_max_when); Model1___min1 < Model1___min2 ? Model1___min1: Model1___min2; });
}

static inline __attribute__((no_instrument_function)) void Model1_tcp_check_probe_timer(struct Model1_sock *Model1_sk)
{
 if (!Model1_tcp_sk(Model1_sk)->Model1_packets_out && !Model1_inet_csk(Model1_sk)->Model1_icsk_pending)
  Model1_inet_csk_reset_xmit_timer(Model1_sk, 3,
       Model1_tcp_probe0_base(Model1_sk), ((unsigned)(120*1000)));
}

static inline __attribute__((no_instrument_function)) void Model1_tcp_init_wl(struct Model1_tcp_sock *Model1_tp, Model1_u32 Model1_seq)
{
 Model1_tp->Model1_snd_wl1 = Model1_seq;
}

static inline __attribute__((no_instrument_function)) void Model1_tcp_update_wl(struct Model1_tcp_sock *Model1_tp, Model1_u32 Model1_seq)
{
 Model1_tp->Model1_snd_wl1 = Model1_seq;
}

/*
 * Calculate(/check) TCP checksum
 */
static inline __attribute__((no_instrument_function)) Model1___sum16 Model1_tcp_v4_check(int Model1_len, Model1___be32 Model1_saddr,
       Model1___be32 Model1_daddr, Model1___wsum Model1_base)
{
 return Model1_csum_tcpudp_magic(Model1_saddr,Model1_daddr,Model1_len,Model1_IPPROTO_TCP,Model1_base);
}

static inline __attribute__((no_instrument_function)) Model1___sum16 Model1___tcp_checksum_complete(struct Model1_sk_buff *Model1_skb)
{
 return Model1___skb_checksum_complete(Model1_skb);
}

static inline __attribute__((no_instrument_function)) bool Model1_tcp_checksum_complete(struct Model1_sk_buff *Model1_skb)
{
 return !Model1_skb_csum_unnecessary(Model1_skb) &&
  Model1___tcp_checksum_complete(Model1_skb);
}

/* Prequeue for VJ style copy to user, combined with checksumming. */

static inline __attribute__((no_instrument_function)) void Model1_tcp_prequeue_init(struct Model1_tcp_sock *Model1_tp)
{
 Model1_tp->Model1_ucopy.Model1_task = ((void *)0);
 Model1_tp->Model1_ucopy.Model1_len = 0;
 Model1_tp->Model1_ucopy.Model1_memory = 0;
 Model1_skb_queue_head_init(&Model1_tp->Model1_ucopy.Model1_prequeue);
}

bool Model1_tcp_prequeue(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);
void Model1_tcp_set_state(struct Model1_sock *Model1_sk, int Model1_state);

void Model1_tcp_done(struct Model1_sock *Model1_sk);

int Model1_tcp_abort(struct Model1_sock *Model1_sk, int err);

static inline __attribute__((no_instrument_function)) void Model1_tcp_sack_reset(struct Model1_tcp_options_received *Model1_rx_opt)
{
 Model1_rx_opt->Model1_dsack = 0;
 Model1_rx_opt->Model1_num_sacks = 0;
}

Model1_u32 Model1_tcp_default_init_rwnd(Model1_u32 Model1_mss);
void Model1_tcp_cwnd_restart(struct Model1_sock *Model1_sk, Model1_s32 Model1_delta);

static inline __attribute__((no_instrument_function)) void Model1_tcp_slow_start_after_idle_check(struct Model1_sock *Model1_sk)
{
 struct Model1_tcp_sock *Model1_tp = Model1_tcp_sk(Model1_sk);
 Model1_s32 Model1_delta;

 if (!Model1_sysctl_tcp_slow_start_after_idle || Model1_tp->Model1_packets_out)
  return;
 Model1_delta = ((__u32)(Model1_jiffies)) - Model1_tp->Model1_lsndtime;
 if (Model1_delta > Model1_inet_csk(Model1_sk)->Model1_icsk_rto)
  Model1_tcp_cwnd_restart(Model1_sk, Model1_delta);
}

/* Determine a window scaling and initial window to offer. */
void Model1_tcp_select_initial_window(int Model1___space, __u32 Model1_mss, __u32 *Model1_rcv_wnd,
          __u32 *Model1_window_clamp, int Model1_wscale_ok,
          __u8 *Model1_rcv_wscale, __u32 Model1_init_rcv_wnd);

static inline __attribute__((no_instrument_function)) int Model1_tcp_win_from_space(int Model1_space)
{
 return Model1_sysctl_tcp_adv_win_scale<=0 ?
  (Model1_space>>(-Model1_sysctl_tcp_adv_win_scale)) :
  Model1_space - (Model1_space>>Model1_sysctl_tcp_adv_win_scale);
}

/* Note: caller must be prepared to deal with negative returns */
static inline __attribute__((no_instrument_function)) int Model1_tcp_space(const struct Model1_sock *Model1_sk)
{
 return Model1_tcp_win_from_space(Model1_sk->Model1_sk_rcvbuf -
      Model1_atomic_read(&Model1_sk->Model1_sk_backlog.Model1_rmem_alloc));
}

static inline __attribute__((no_instrument_function)) int Model1_tcp_full_space(const struct Model1_sock *Model1_sk)
{
 return Model1_tcp_win_from_space(Model1_sk->Model1_sk_rcvbuf);
}

extern void Model1_tcp_openreq_init_rwin(struct Model1_request_sock *Model1_req,
      const struct Model1_sock *Model1_sk_listener,
      const struct Model1_dst_entry *Model1_dst);

void Model1_tcp_enter_memory_pressure(struct Model1_sock *Model1_sk);

static inline __attribute__((no_instrument_function)) int Model1_keepalive_intvl_when(const struct Model1_tcp_sock *Model1_tp)
{
 struct Model1_net *Model1_net = Model1_sock_net((struct Model1_sock *)Model1_tp);

 return Model1_tp->Model1_keepalive_intvl ? : Model1_net->Model1_ipv4.Model1_sysctl_tcp_keepalive_intvl;
}

static inline __attribute__((no_instrument_function)) int Model1_keepalive_time_when(const struct Model1_tcp_sock *Model1_tp)
{
 struct Model1_net *Model1_net = Model1_sock_net((struct Model1_sock *)Model1_tp);

 return Model1_tp->Model1_keepalive_time ? : Model1_net->Model1_ipv4.Model1_sysctl_tcp_keepalive_time;
}

static inline __attribute__((no_instrument_function)) int Model1_keepalive_probes(const struct Model1_tcp_sock *Model1_tp)
{
 struct Model1_net *Model1_net = Model1_sock_net((struct Model1_sock *)Model1_tp);

 return Model1_tp->Model1_keepalive_probes ? : Model1_net->Model1_ipv4.Model1_sysctl_tcp_keepalive_probes;
}

static inline __attribute__((no_instrument_function)) Model1_u32 Model1_keepalive_time_elapsed(const struct Model1_tcp_sock *Model1_tp)
{
 const struct Model1_inet_connection_sock *Model1_icsk = &Model1_tp->Model1_inet_conn;

 return ({ Model1_u32 Model1___min1 = (((__u32)(Model1_jiffies)) - Model1_icsk->Model1_icsk_ack.Model1_lrcvtime); Model1_u32 Model1___min2 = (((__u32)(Model1_jiffies)) - Model1_tp->Model1_rcv_tstamp); Model1___min1 < Model1___min2 ? Model1___min1: Model1___min2; });

}

static inline __attribute__((no_instrument_function)) int Model1_tcp_fin_time(const struct Model1_sock *Model1_sk)
{
 int Model1_fin_timeout = Model1_tcp_sk(Model1_sk)->Model1_linger2 ? : Model1_sock_net(Model1_sk)->Model1_ipv4.Model1_sysctl_tcp_fin_timeout;
 const int Model1_rto = Model1_inet_csk(Model1_sk)->Model1_icsk_rto;

 if (Model1_fin_timeout < (Model1_rto << 2) - (Model1_rto >> 1))
  Model1_fin_timeout = (Model1_rto << 2) - (Model1_rto >> 1);

 return Model1_fin_timeout;
}

static inline __attribute__((no_instrument_function)) bool Model1_tcp_paws_check(const struct Model1_tcp_options_received *Model1_rx_opt,
      int Model1_paws_win)
{
 if ((Model1_s32)(Model1_rx_opt->Model1_ts_recent - Model1_rx_opt->Model1_rcv_tsval) <= Model1_paws_win)
  return true;
 if (__builtin_expect(!!(Model1_get_seconds() >= Model1_rx_opt->Model1_ts_recent_stamp + (60 * 60 * 24 * 24)), 0))
  return true;
 /*
	 * Some OSes send SYN and SYNACK messages with tsval=0 tsecr=0,
	 * then following tcp messages have valid values. Ignore 0 value,
	 * or else 'negative' tsval might forbid us to accept their packets.
	 */
 if (!Model1_rx_opt->Model1_ts_recent)
  return true;
 return false;
}

static inline __attribute__((no_instrument_function)) bool Model1_tcp_paws_reject(const struct Model1_tcp_options_received *Model1_rx_opt,
       int Model1_rst)
{
 if (Model1_tcp_paws_check(Model1_rx_opt, 0))
  return false;

 /* RST segments are not recommended to carry timestamp,
	   and, if they do, it is recommended to ignore PAWS because
	   "their cleanup function should take precedence over timestamps."
	   Certainly, it is mistake. It is necessary to understand the reasons
	   of this constraint to relax it: if peer reboots, clock may go
	   out-of-sync and half-open connections will not be reset.
	   Actually, the problem would be not existing if all
	   the implementations followed draft about maintaining clock
	   via reboots. Linux-2.2 DOES NOT!

	   However, we can relax time bounds for RST segments to MSL.
	 */
 if (Model1_rst && Model1_get_seconds() >= Model1_rx_opt->Model1_ts_recent_stamp + 60)
  return false;
 return true;
}

bool Model1_tcp_oow_rate_limited(struct Model1_net *Model1_net, const struct Model1_sk_buff *Model1_skb,
     int Model1_mib_idx, Model1_u32 *Model1_last_oow_ack_time);

#if CY_ABSTRACT0
extern struct Model1_tcp_mib Model1_cy_tcp_mib;
extern struct Model1_linux_mib Model1_cy_linux_mib;
#endif

static inline __attribute__((no_instrument_function)) void Model1_tcp_mib_init(struct Model1_net *Model1_net)
{
 /* See RFC 2012 */
 (Model1_cy_tcp_mib.Model1_mibs[Model1_TCP_MIB_RTOALGORITHM] += 1);
 (Model1_cy_tcp_mib.Model1_mibs[Model1_TCP_MIB_RTOMIN] += ((unsigned)(1000/5))*1000/1000);
 (Model1_cy_tcp_mib.Model1_mibs[Model1_TCP_MIB_RTOMAX] += ((unsigned)(120*1000))*1000/1000);
 (Model1_cy_tcp_mib.Model1_mibs[Model1_TCP_MIB_MAXCONN] += -1);
}

/* from STCP */
static inline __attribute__((no_instrument_function)) void Model1_tcp_clear_retrans_hints_partial(struct Model1_tcp_sock *Model1_tp)
{
 Model1_tp->Model1_lost_skb_hint = ((void *)0);
}

static inline __attribute__((no_instrument_function)) void Model1_tcp_clear_all_retrans_hints(struct Model1_tcp_sock *Model1_tp)
{
 Model1_tcp_clear_retrans_hints_partial(Model1_tp);
 Model1_tp->Model1_retransmit_skb_hint = ((void *)0);
}

union Model1_tcp_md5_addr {
 struct Model1_in_addr Model1_a4;

 struct Model1_in6_addr Model1_a6;

};

/* - key database */
struct Model1_tcp_md5sig_key {
 struct Model1_hlist_node Model1_node;
 Model1_u8 Model1_keylen;
 Model1_u8 Model1_family; /* AF_INET or AF_INET6 */
 union Model1_tcp_md5_addr Model1_addr;
 Model1_u8 Model1_key[80];
 struct Model1_callback_head Model1_rcu;
};

/* - sock block */
struct Model1_tcp_md5sig_info {
 struct Model1_hlist_head Model1_head;
 struct Model1_callback_head Model1_rcu;
};

/* - pseudo header */
struct Model1_tcp4_pseudohdr {
 Model1___be32 Model1_saddr;
 Model1___be32 Model1_daddr;
 __u8 Model1_pad;
 __u8 Model1_protocol;
 Model1___be16 Model1_len;
};

struct Model1_tcp6_pseudohdr {
 struct Model1_in6_addr Model1_saddr;
 struct Model1_in6_addr Model1_daddr;
 Model1___be32 Model1_len;
 Model1___be32 Model1_protocol; /* including padding */
};

union Model1_tcp_md5sum_block {
 struct Model1_tcp4_pseudohdr Model1_ip4;

 struct Model1_tcp6_pseudohdr Model1_ip6;

};

/* - pool: digest algorithm, hash description and scratch buffer */
struct Model1_tcp_md5sig_pool {
 struct Model1_ahash_request *Model1_md5_req;
 void *Model1_scratch;
};

/* - functions */
int Model1_tcp_v4_md5_hash_skb(char *Model1_md5_hash, const struct Model1_tcp_md5sig_key *Model1_key,
   const struct Model1_sock *Model1_sk, const struct Model1_sk_buff *Model1_skb);
int Model1_tcp_md5_do_add(struct Model1_sock *Model1_sk, const union Model1_tcp_md5_addr *Model1_addr,
     int Model1_family, const Model1_u8 *Model1_newkey, Model1_u8 Model1_newkeylen, Model1_gfp_t Model1_gfp);
int Model1_tcp_md5_do_del(struct Model1_sock *Model1_sk, const union Model1_tcp_md5_addr *Model1_addr,
     int Model1_family);
struct Model1_tcp_md5sig_key *Model1_tcp_v4_md5_lookup(const struct Model1_sock *Model1_sk,
      const struct Model1_sock *Model1_addr_sk);


static struct Model1_tcp_md5sig_key *Model1_tcp_md5_do_lookup(const struct Model1_sock *Model1_sk,
      const union Model1_tcp_md5_addr *Model1_addr,
      int Model1_family);
bool Model1_tcp_alloc_md5sig_pool(void);

struct Model1_tcp_md5sig_pool *Model1_tcp_get_md5sig_pool(void);
static inline __attribute__((no_instrument_function)) void Model1_tcp_put_md5sig_pool(void)
{
 Model1_local_bh_enable();
}

int Model1_tcp_md5_hash_skb_data(struct Model1_tcp_md5sig_pool *, const struct Model1_sk_buff *,
     unsigned int Model1_header_len);
int Model1_tcp_md5_hash_key(struct Model1_tcp_md5sig_pool *Model1_hp,
       const struct Model1_tcp_md5sig_key *Model1_key);

/* From tcp_fastopen.c */
void Model1_tcp_fastopen_cache_get(struct Model1_sock *Model1_sk, Model1_u16 *Model1_mss,
       struct Model1_tcp_fastopen_cookie *Model1_cookie, int *Model1_syn_loss,
       unsigned long *Model1_last_syn_loss);
void Model1_tcp_fastopen_cache_set(struct Model1_sock *Model1_sk, Model1_u16 Model1_mss,
       struct Model1_tcp_fastopen_cookie *Model1_cookie, bool Model1_syn_lost,
       Model1_u16 Model1_try_exp);
struct Model1_tcp_fastopen_request {
 /* Fast Open cookie. Size 0 means a cookie request */
 struct Model1_tcp_fastopen_cookie Model1_cookie;
 struct Model1_msghdr *Model1_data; /* data in MSG_FASTOPEN */
 Model1_size_t Model1_size;
 int Model1_copied; /* queued in tcp_connect() */
};
void Model1_tcp_free_fastopen_req(struct Model1_tcp_sock *Model1_tp);

extern struct Model1_tcp_fastopen_context *Model1_tcp_fastopen_ctx;
int Model1_tcp_fastopen_reset_cipher(void *Model1_key, unsigned int Model1_len);
void Model1_tcp_fastopen_add_skb(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);
struct Model1_sock *Model1_tcp_try_fastopen(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb,
         struct Model1_request_sock *Model1_req,
         struct Model1_tcp_fastopen_cookie *Model1_foc,
         struct Model1_dst_entry *Model1_dst);
void Model1_tcp_fastopen_init_key_once(bool Model1_publish);


/* Fastopen key context */
struct Model1_tcp_fastopen_context {
 struct Model1_crypto_cipher *Model1_tfm;
 __u8 Model1_key[16];
 struct Model1_callback_head Model1_rcu;
};

/* write queue abstraction */
static inline __attribute__((no_instrument_function)) void Model1_tcp_write_queue_purge(struct Model1_sock *Model1_sk)
{
 struct Model1_sk_buff *Model1_skb;

 while ((Model1_skb = Model1___skb_dequeue(&Model1_sk->Model1_sk_write_queue)) != ((void *)0))
  Model1_sk_wmem_free_skb(Model1_sk, Model1_skb);
 Model1_sk_mem_reclaim(Model1_sk);
 Model1_tcp_clear_all_retrans_hints(Model1_tcp_sk(Model1_sk));
}

static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1_tcp_write_queue_head(const struct Model1_sock *Model1_sk)
{
 return Model1_skb_peek(&Model1_sk->Model1_sk_write_queue);
}

static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1_tcp_write_queue_tail(const struct Model1_sock *Model1_sk)
{
 return Model1_skb_peek_tail(&Model1_sk->Model1_sk_write_queue);
}

static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1_tcp_write_queue_next(const struct Model1_sock *Model1_sk,
         const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb_queue_next(&Model1_sk->Model1_sk_write_queue, Model1_skb);
}

static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1_tcp_write_queue_prev(const struct Model1_sock *Model1_sk,
         const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb_queue_prev(&Model1_sk->Model1_sk_write_queue, Model1_skb);
}
static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1_tcp_send_head(const struct Model1_sock *Model1_sk)
{
 return Model1_sk->Model1_sk_send_head;
}

static inline __attribute__((no_instrument_function)) bool Model1_tcp_skb_is_last(const struct Model1_sock *Model1_sk,
       const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb_queue_is_last(&Model1_sk->Model1_sk_write_queue, Model1_skb);
}

static inline __attribute__((no_instrument_function)) void Model1_tcp_advance_send_head(struct Model1_sock *Model1_sk, const struct Model1_sk_buff *Model1_skb)
{
 if (Model1_tcp_skb_is_last(Model1_sk, Model1_skb))
  Model1_sk->Model1_sk_send_head = ((void *)0);
 else
  Model1_sk->Model1_sk_send_head = Model1_tcp_write_queue_next(Model1_sk, Model1_skb);
}

static inline __attribute__((no_instrument_function)) void Model1_tcp_check_send_head(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb_unlinked)
{
 if (Model1_sk->Model1_sk_send_head == Model1_skb_unlinked)
  Model1_sk->Model1_sk_send_head = ((void *)0);
 if (Model1_tcp_sk(Model1_sk)->Model1_highest_sack == Model1_skb_unlinked)
  Model1_tcp_sk(Model1_sk)->Model1_highest_sack = ((void *)0);
}

static inline __attribute__((no_instrument_function)) void Model1_tcp_init_send_head(struct Model1_sock *Model1_sk)
{
 Model1_sk->Model1_sk_send_head = ((void *)0);
}

static inline __attribute__((no_instrument_function)) void Model1___tcp_add_write_queue_tail(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb)
{
 Model1___skb_queue_tail(&Model1_sk->Model1_sk_write_queue, Model1_skb);
}

static inline __attribute__((no_instrument_function)) void Model1_tcp_add_write_queue_tail(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb)
{
 Model1___tcp_add_write_queue_tail(Model1_sk, Model1_skb);

 /* Queue it, remembering where we must start sending. */
 if (Model1_sk->Model1_sk_send_head == ((void *)0)) {
  Model1_sk->Model1_sk_send_head = Model1_skb;

  if (Model1_tcp_sk(Model1_sk)->Model1_highest_sack == ((void *)0))
   Model1_tcp_sk(Model1_sk)->Model1_highest_sack = Model1_skb;
 }
}

static inline __attribute__((no_instrument_function)) void Model1___tcp_add_write_queue_head(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb)
{
 Model1___skb_queue_head(&Model1_sk->Model1_sk_write_queue, Model1_skb);
}

/* Insert buff after skb on the write queue of sk.  */
static inline __attribute__((no_instrument_function)) void Model1_tcp_insert_write_queue_after(struct Model1_sk_buff *Model1_skb,
      struct Model1_sk_buff *Model1_buff,
      struct Model1_sock *Model1_sk)
{
 Model1___skb_queue_after(&Model1_sk->Model1_sk_write_queue, Model1_skb, Model1_buff);
}

/* Insert new before skb on the write queue of sk.  */
static inline __attribute__((no_instrument_function)) void Model1_tcp_insert_write_queue_before(struct Model1_sk_buff *Model1_new,
        struct Model1_sk_buff *Model1_skb,
        struct Model1_sock *Model1_sk)
{
 Model1___skb_queue_before(&Model1_sk->Model1_sk_write_queue, Model1_skb, Model1_new);

 if (Model1_sk->Model1_sk_send_head == Model1_skb)
  Model1_sk->Model1_sk_send_head = Model1_new;
}

static inline __attribute__((no_instrument_function)) void Model1_tcp_unlink_write_queue(struct Model1_sk_buff *Model1_skb, struct Model1_sock *Model1_sk)
{
 Model1___skb_unlink(Model1_skb, &Model1_sk->Model1_sk_write_queue);
}

static inline __attribute__((no_instrument_function)) bool Model1_tcp_write_queue_empty(struct Model1_sock *Model1_sk)
{
 return Model1_skb_queue_empty(&Model1_sk->Model1_sk_write_queue);
}

static inline __attribute__((no_instrument_function)) void Model1_tcp_push_pending_frames(struct Model1_sock *Model1_sk)
{
#if !CY_ABSTRACT1
 if (Model1_tcp_send_head(Model1_sk)) {
  struct Model1_tcp_sock *Model1_tp = Model1_tcp_sk(Model1_sk);

  Model1___tcp_push_pending_frames(Model1_sk, Model1_tcp_current_mss(Model1_sk), Model1_tp->Model1_nonagle);
 }
#endif
}

/* Start sequence of the skb just after the highest skb with SACKed
 * bit, valid only if sacked_out > 0 or when the caller has ensured
 * validity by itself.
 */
static inline __attribute__((no_instrument_function)) Model1_u32 Model1_tcp_highest_sack_seq(struct Model1_tcp_sock *Model1_tp)
{
 if (!Model1_tp->Model1_sacked_out)
  return Model1_tp->Model1_snd_una;

 if (Model1_tp->Model1_highest_sack == ((void *)0))
  return Model1_tp->Model1_snd_nxt;

 return ((struct Model1_tcp_skb_cb *)&((Model1_tp->Model1_highest_sack)->Model1_cb[0]))->Model1_seq;
}

static inline __attribute__((no_instrument_function)) void Model1_tcp_advance_highest_sack(struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb)
{
 Model1_tcp_sk(Model1_sk)->Model1_highest_sack = Model1_tcp_skb_is_last(Model1_sk, Model1_skb) ? ((void *)0) :
      Model1_tcp_write_queue_next(Model1_sk, Model1_skb);
}

static inline __attribute__((no_instrument_function)) struct Model1_sk_buff *Model1_tcp_highest_sack(struct Model1_sock *Model1_sk)
{
 return Model1_tcp_sk(Model1_sk)->Model1_highest_sack;
}

static inline __attribute__((no_instrument_function)) void Model1_tcp_highest_sack_reset(struct Model1_sock *Model1_sk)
{
 Model1_tcp_sk(Model1_sk)->Model1_highest_sack = Model1_tcp_write_queue_head(Model1_sk);
}

/* Called when old skb is about to be deleted (to be combined with new skb) */
static inline __attribute__((no_instrument_function)) void Model1_tcp_highest_sack_combine(struct Model1_sock *Model1_sk,
         struct Model1_sk_buff *old,
         struct Model1_sk_buff *Model1_new)
{
 if (Model1_tcp_sk(Model1_sk)->Model1_sacked_out && (old == Model1_tcp_sk(Model1_sk)->Model1_highest_sack))
  Model1_tcp_sk(Model1_sk)->Model1_highest_sack = Model1_new;
}

/* This helper checks if socket has IP_TRANSPARENT set */
static inline __attribute__((no_instrument_function)) bool Model1_inet_sk_transparent(const struct Model1_sock *Model1_sk)
{
 switch (Model1_sk->Model1___sk_common.Model1_skc_state) {
 case Model1_TCP_TIME_WAIT:
  return Model1_inet_twsk(Model1_sk)->Model1_tw_transparent;
 case Model1_TCP_NEW_SYN_RECV:
  return Model1_inet_rsk(Model1_inet_reqsk(Model1_sk))->Model1_no_srccheck;
 }
 return Model1_inet_sk(Model1_sk)->Model1_transparent;
}

/* Determines whether this is a thin stream (which may suffer from
 * increased latency). Used to trigger latency-reducing mechanisms.
 */
static inline __attribute__((no_instrument_function)) bool Model1_tcp_stream_is_thin(struct Model1_tcp_sock *Model1_tp)
{
 return Model1_tp->Model1_packets_out < 4 && !Model1_tcp_in_initial_slowstart(Model1_tp);
}

/* /proc */
enum Model1_tcp_seq_states {
 Model1_TCP_SEQ_STATE_LISTENING,
 Model1_TCP_SEQ_STATE_ESTABLISHED,
};

int Model1_tcp_seq_open(struct Model1_inode *Model1_inode, struct Model1_file *Model1_file);

struct Model1_tcp_seq_afinfo {
 char *Model1_name;
 Model1_sa_family_t Model1_family;
 const struct Model1_file_operations *Model1_seq_fops;
 struct Model1_seq_operations Model1_seq_ops;
};

struct Model1_tcp_iter_state {
 struct Model1_seq_net_private Model1_p;
 Model1_sa_family_t Model1_family;
 enum Model1_tcp_seq_states Model1_state;
 struct Model1_sock *Model1_syn_wait_sk;
 int Model1_bucket, Model1_offset, Model1_sbucket, Model1_num;
 Model1_loff_t Model1_last_pos;
};

int Model1_tcp_proc_register(struct Model1_net *Model1_net, struct Model1_tcp_seq_afinfo *Model1_afinfo);
void Model1_tcp_proc_unregister(struct Model1_net *Model1_net, struct Model1_tcp_seq_afinfo *Model1_afinfo);

extern struct Model1_request_sock_ops Model1_tcp_request_sock_ops;
extern struct Model1_request_sock_ops Model1_tcp6_request_sock_ops;

void Model1_tcp_v4_destroy_sock(struct Model1_sock *Model1_sk);

struct Model1_sk_buff *Model1_tcp_gso_segment(struct Model1_sk_buff *Model1_skb,
    Model1_netdev_features_t Model1_features);
struct Model1_sk_buff **Model1_tcp_gro_receive(struct Model1_sk_buff **Model1_head, struct Model1_sk_buff *Model1_skb);
int Model1_tcp_gro_complete(struct Model1_sk_buff *Model1_skb);

void Model1___tcp_v4_send_check(struct Model1_sk_buff *Model1_skb, Model1___be32 Model1_saddr, Model1___be32 Model1_daddr);

static inline __attribute__((no_instrument_function)) Model1_u32 Model1_tcp_notsent_lowat(const struct Model1_tcp_sock *Model1_tp)
{
 struct Model1_net *Model1_net = Model1_sock_net((struct Model1_sock *)Model1_tp);
 return Model1_tp->Model1_notsent_lowat ?: Model1_net->Model1_ipv4.Model1_sysctl_tcp_notsent_lowat;
}

static inline __attribute__((no_instrument_function)) bool Model1_tcp_stream_memory_free(const struct Model1_sock *Model1_sk)
{
 const struct Model1_tcp_sock *Model1_tp = Model1_tcp_sk(Model1_sk);
 Model1_u32 Model1_notsent_bytes = Model1_tp->Model1_write_seq - Model1_tp->Model1_snd_nxt;

 return Model1_notsent_bytes < Model1_tcp_notsent_lowat(Model1_tp);
}


int Model1_tcp4_proc_init(void);
void Model1_tcp4_proc_exit(void);


int Model1_tcp_rtx_synack(const struct Model1_sock *Model1_sk, struct Model1_request_sock *Model1_req);
int Model1_tcp_conn_request(struct Model1_request_sock_ops *Model1_rsk_ops,
       const struct Model1_tcp_request_sock_ops *Model1_af_ops,
       struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb);

/* TCP af-specific functions */
struct Model1_tcp_sock_af_ops {

 struct Model1_tcp_md5sig_key *(*Model1_md5_lookup) (const struct Model1_sock *Model1_sk,
      const struct Model1_sock *Model1_addr_sk);
 int (*Model1_calc_md5_hash)(char *Model1_location,
      const struct Model1_tcp_md5sig_key *Model1_md5,
      const struct Model1_sock *Model1_sk,
      const struct Model1_sk_buff *Model1_skb);
 int (*Model1_md5_parse)(struct Model1_sock *Model1_sk,
         char *Model1_optval,
         int Model1_optlen);

};

struct Model1_tcp_request_sock_ops {
 Model1_u16 Model1_mss_clamp;

 struct Model1_tcp_md5sig_key *(*Model1_req_md5_lookup)(const struct Model1_sock *Model1_sk,
       const struct Model1_sock *Model1_addr_sk);
 int (*Model1_calc_md5_hash) (char *Model1_location,
       const struct Model1_tcp_md5sig_key *Model1_md5,
       const struct Model1_sock *Model1_sk,
       const struct Model1_sk_buff *Model1_skb);

 void (*Model1_init_req)(struct Model1_request_sock *Model1_req,
    const struct Model1_sock *Model1_sk_listener,
    struct Model1_sk_buff *Model1_skb);

 __u32 (*Model1_cookie_init_seq)(const struct Model1_sk_buff *Model1_skb,
     Model1___u16 *Model1_mss);

 struct Model1_dst_entry *(*Model1_route_req)(const struct Model1_sock *Model1_sk, struct Model1_flowi *Model1_fl,
           const struct Model1_request_sock *Model1_req,
           bool *Model1_strict);
 __u32 (*Model1_init_seq)(const struct Model1_sk_buff *Model1_skb);
 int (*Model1_send_synack)(const struct Model1_sock *Model1_sk, struct Model1_dst_entry *Model1_dst,
      struct Model1_flowi *Model1_fl, struct Model1_request_sock *Model1_req,
      struct Model1_tcp_fastopen_cookie *Model1_foc,
      enum Model1_tcp_synack_type Model1_synack_type);
};


static inline __attribute__((no_instrument_function)) __u32 Model1_cookie_init_sequence(const struct Model1_tcp_request_sock_ops *Model1_ops,
      const struct Model1_sock *Model1_sk, struct Model1_sk_buff *Model1_skb,
      Model1___u16 *Model1_mss)
{
 Model1_tcp_synq_overflow(Model1_sk);
 (Model1_cy_linux_mib.Model1_mibs[Model1_LINUX_MIB_SYNCOOKIESSENT] += 1);
 return Model1_ops->Model1_cookie_init_seq(Model1_skb, Model1_mss);
}
int Model1_tcpv4_offload_init(void);

void Model1_tcp_v4_init(void);
void Model1_tcp_init(void);

/* tcp_recovery.c */

/* Flags to enable various loss recovery features. See below */
extern int Model1_sysctl_tcp_recovery;

/* Use TCP RACK to detect (some) tail and retransmit losses */


extern int Model1_tcp_rack_mark_lost(struct Model1_sock *Model1_sk);

extern void Model1_tcp_rack_advance(struct Model1_tcp_sock *Model1_tp,
        const struct Model1_skb_mstamp *Model1_xmit_time, Model1_u8 Model1_sacked);

/*
 * Save and compile IPv4 options, return a pointer to it
 */
static inline __attribute__((no_instrument_function)) struct Model1_ip_options_rcu *Model1_tcp_v4_save_options(struct Model1_sk_buff *Model1_skb)
{
 const struct Model1_ip_options *Model1_opt = &((struct Model1_tcp_skb_cb *)&((Model1_skb)->Model1_cb[0]))->Model1_header.Model1_h4.Model1_opt;
 struct Model1_ip_options_rcu *Model1_dopt = ((void *)0);

 if (Model1_opt->Model1_optlen) {
  int Model1_opt_size = sizeof(*Model1_dopt) + Model1_opt->Model1_optlen;

  Model1_dopt = Model1_kmalloc(Model1_opt_size, ((( Model1_gfp_t)0x20u)|(( Model1_gfp_t)0x80000u)|(( Model1_gfp_t)0x2000000u)));
  if (Model1_dopt && Model1___ip_options_echo(&Model1_dopt->Model1_opt, Model1_skb, Model1_opt)) {
   Model1_kfree(Model1_dopt);
   Model1_dopt = ((void *)0);
  }
 }
 return Model1_dopt;
}

/* locally generated TCP pure ACKs have skb->truesize == 2
 * (check tcp_send_ack() in net/ipv4/tcp_output.c )
 * This is much faster than dissecting the packet to find out.
 * (Think of GRE encapsulations, IPv4, IPv6, ...)
 */
static inline __attribute__((no_instrument_function)) bool Model1_skb_is_tcp_pure_ack(const struct Model1_sk_buff *Model1_skb)
{
 return Model1_skb->Model1_truesize == 2;
}

static inline __attribute__((no_instrument_function)) void Model1_skb_set_tcp_pure_ack(struct Model1_sk_buff *Model1_skb)
{
 Model1_skb->Model1_truesize = 2;
}

static inline __attribute__((no_instrument_function)) int Model1_tcp_inq(struct Model1_sock *Model1_sk)
{
 struct Model1_tcp_sock *Model1_tp = Model1_tcp_sk(Model1_sk);
 int Model1_answ;

 if ((1 << Model1_sk->Model1___sk_common.Model1_skc_state) & (Model1_TCPF_SYN_SENT | Model1_TCPF_SYN_RECV)) {
  Model1_answ = 0;
 } else if (Model1_sock_flag(Model1_sk, Model1_SOCK_URGINLINE) ||
     !Model1_tp->Model1_urg_data ||
     Model1_before(Model1_tp->Model1_urg_seq, Model1_tp->Model1_copied_seq) ||
     !Model1_before(Model1_tp->Model1_urg_seq, Model1_tp->Model1_rcv_nxt)) {

  Model1_answ = Model1_tp->Model1_rcv_nxt - Model1_tp->Model1_copied_seq;

  /* Subtract 1, if FIN was received */
  if (Model1_answ && Model1_sock_flag(Model1_sk, Model1_SOCK_DONE))
   Model1_answ--;
 } else {
  Model1_answ = Model1_tp->Model1_urg_seq - Model1_tp->Model1_copied_seq;
 }

 return Model1_answ;
}

static inline __attribute__((no_instrument_function)) void Model1_tcp_segs_in(struct Model1_tcp_sock *Model1_tp, const struct Model1_sk_buff *Model1_skb)
{
 Model1_u16 Model1_segs_in;

 Model1_segs_in = ({ Model1_u16 Model1___max1 = (1); Model1_u16 Model1___max2 = (((struct Model1_skb_shared_info *)(Model1_skb_end_pointer(Model1_skb)))->Model1_gso_segs); Model1___max1 > Model1___max2 ? Model1___max1: Model1___max2; });
 Model1_tp->Model1_segs_in += Model1_segs_in;
 if (Model1_skb->Model1_len > Model1_tcp_hdrlen(Model1_skb))
  Model1_tp->Model1_data_segs_in += Model1_segs_in;
}

/*
 * TCP listen path runs lockless.
 * We forced "struct sock" to be const qualified to make sure
 * we don't modify one of its field by mistake.
 * Here, we increment sk_drops which is an atomic_t, so we can safely
 * make sock writable again.
 */
static inline __attribute__((no_instrument_function)) void Model1_tcp_listendrop(const struct Model1_sock *Model1_sk)
{
 Model1_atomic_inc(&((struct Model1_sock *)Model1_sk)->Model1_sk_drops);
 (Model1_cy_linux_mib.Model1_mibs[Model1_LINUX_MIB_LISTENDROPS] += 1);
}



extern const struct Model1_proto_ops Model1_inet_stream_ops;
extern const struct Model1_proto_ops Model1_inet_dgram_ops;

/*
 *	INET4 prototypes used by INET6
 */

struct Model1_msghdr;
struct Model1_sock;
struct Model1_sockaddr;
struct Model1_socket;

int Model1_inet_release(struct Model1_socket *Model1_sock);
int Model1_inet_stream_connect(struct Model1_socket *Model1_sock, struct Model1_sockaddr *Model1_uaddr,
   int Model1_addr_len, int Model1_flags);
int Model1___inet_stream_connect(struct Model1_socket *Model1_sock, struct Model1_sockaddr *Model1_uaddr,
     int Model1_addr_len, int Model1_flags);
int Model1_inet_dgram_connect(struct Model1_socket *Model1_sock, struct Model1_sockaddr *Model1_uaddr,
         int Model1_addr_len, int Model1_flags);
int Model1_inet_accept(struct Model1_socket *Model1_sock, struct Model1_socket *Model1_newsock, int Model1_flags);
int Model1_inet_sendmsg(struct Model1_socket *Model1_sock, struct Model1_msghdr *Model1_msg, Model1_size_t Model1_size);
Model1_ssize_t Model1_inet_sendpage(struct Model1_socket *Model1_sock, struct Model1_page *Model1_page, int Model1_offset,
        Model1_size_t Model1_size, int Model1_flags);
int Model1_inet_recvmsg(struct Model1_socket *Model1_sock, struct Model1_msghdr *Model1_msg, Model1_size_t Model1_size,
   int Model1_flags);
int Model1_inet_shutdown(struct Model1_socket *Model1_sock, int Model1_how);
int Model1_inet_listen(struct Model1_socket *Model1_sock, int Model1_backlog);
void Model1_inet_sock_destruct(struct Model1_sock *Model1_sk);
int Model1_inet_bind(struct Model1_socket *Model1_sock, struct Model1_sockaddr *Model1_uaddr, int Model1_addr_len);
int Model1_inet_getname(struct Model1_socket *Model1_sock, struct Model1_sockaddr *Model1_uaddr, int *Model1_uaddr_len,
   int Model1_peer);
int Model1_inet_ioctl(struct Model1_socket *Model1_sock, unsigned int Model1_cmd, unsigned long Model1_arg);
int Model1_inet_ctl_sock_create(struct Model1_sock **Model1_sk, unsigned short Model1_family,
    unsigned short Model1_type, unsigned char Model1_protocol,
    struct Model1_net *Model1_net);
int Model1_inet_recv_error(struct Model1_sock *Model1_sk, struct Model1_msghdr *Model1_msg, int Model1_len,
      int *Model1_addr_len);

struct Model1_sk_buff **Model1_inet_gro_receive(struct Model1_sk_buff **Model1_head, struct Model1_sk_buff *Model1_skb);
int Model1_inet_gro_complete(struct Model1_sk_buff *Model1_skb, int Model1_nhoff);
struct Model1_sk_buff *Model1_inet_gso_segment(struct Model1_sk_buff *Model1_skb,
     Model1_netdev_features_t Model1_features);

static inline __attribute__((no_instrument_function)) void Model1_inet_ctl_sock_destroy(struct Model1_sock *Model1_sk)
{
 if (Model1_sk)
  Model1_sock_release(Model1_sk->Model1_sk_socket);
}



/* The definitions, required to talk to KAME racoon IKE. */


/* PF_KEY user interface, this is defined by rfc2367 so
 * do not make arbitrary modifications or else this header
 * file will not be compliant.
 */
struct Model1_sadb_msg {
 __u8 Model1_sadb_msg_version;
 __u8 Model1_sadb_msg_type;
 __u8 Model1_sadb_msg_errno;
 __u8 Model1_sadb_msg_satype;
 Model1___u16 Model1_sadb_msg_len;
 Model1___u16 Model1_sadb_msg_reserved;
 __u32 Model1_sadb_msg_seq;
 __u32 Model1_sadb_msg_pid;
} __attribute__((packed));
/* sizeof(struct sadb_msg) == 16 */

struct Model1_sadb_ext {
 Model1___u16 Model1_sadb_ext_len;
 Model1___u16 Model1_sadb_ext_type;
} __attribute__((packed));
/* sizeof(struct sadb_ext) == 4 */

struct Model1_sadb_sa {
 Model1___u16 Model1_sadb_sa_len;
 Model1___u16 Model1_sadb_sa_exttype;
 Model1___be32 Model1_sadb_sa_spi;
 __u8 Model1_sadb_sa_replay;
 __u8 Model1_sadb_sa_state;
 __u8 Model1_sadb_sa_auth;
 __u8 Model1_sadb_sa_encrypt;
 __u32 Model1_sadb_sa_flags;
} __attribute__((packed));
/* sizeof(struct sadb_sa) == 16 */

struct Model1_sadb_lifetime {
 Model1___u16 Model1_sadb_lifetime_len;
 Model1___u16 Model1_sadb_lifetime_exttype;
 __u32 Model1_sadb_lifetime_allocations;
 __u64 Model1_sadb_lifetime_bytes;
 __u64 Model1_sadb_lifetime_addtime;
 __u64 Model1_sadb_lifetime_usetime;
} __attribute__((packed));
/* sizeof(struct sadb_lifetime) == 32 */

struct Model1_sadb_address {
 Model1___u16 Model1_sadb_address_len;
 Model1___u16 Model1_sadb_address_exttype;
 __u8 Model1_sadb_address_proto;
 __u8 Model1_sadb_address_prefixlen;
 Model1___u16 Model1_sadb_address_reserved;
} __attribute__((packed));
/* sizeof(struct sadb_address) == 8 */

struct Model1_sadb_key {
 Model1___u16 Model1_sadb_key_len;
 Model1___u16 Model1_sadb_key_exttype;
 Model1___u16 Model1_sadb_key_bits;
 Model1___u16 Model1_sadb_key_reserved;
} __attribute__((packed));
/* sizeof(struct sadb_key) == 8 */

struct Model1_sadb_ident {
 Model1___u16 Model1_sadb_ident_len;
 Model1___u16 Model1_sadb_ident_exttype;
 Model1___u16 Model1_sadb_ident_type;
 Model1___u16 Model1_sadb_ident_reserved;
 __u64 Model1_sadb_ident_id;
} __attribute__((packed));
/* sizeof(struct sadb_ident) == 16 */

struct Model1_sadb_sens {
 Model1___u16 Model1_sadb_sens_len;
 Model1___u16 Model1_sadb_sens_exttype;
 __u32 Model1_sadb_sens_dpd;
 __u8 Model1_sadb_sens_sens_level;
 __u8 Model1_sadb_sens_sens_len;
 __u8 Model1_sadb_sens_integ_level;
 __u8 Model1_sadb_sens_integ_len;
 __u32 Model1_sadb_sens_reserved;
} __attribute__((packed));
/* sizeof(struct sadb_sens) == 16 */

/* followed by:
	__u64	sadb_sens_bitmap[sens_len];
	__u64	sadb_integ_bitmap[integ_len];  */

struct Model1_sadb_prop {
 Model1___u16 Model1_sadb_prop_len;
 Model1___u16 Model1_sadb_prop_exttype;
 __u8 Model1_sadb_prop_replay;
 __u8 Model1_sadb_prop_reserved[3];
} __attribute__((packed));
/* sizeof(struct sadb_prop) == 8 */

/* followed by:
	struct sadb_comb sadb_combs[(sadb_prop_len +
		sizeof(__u64) - sizeof(struct sadb_prop)) /
		sizeof(struct sadb_comb)]; */

struct Model1_sadb_comb {
 __u8 Model1_sadb_comb_auth;
 __u8 Model1_sadb_comb_encrypt;
 Model1___u16 Model1_sadb_comb_flags;
 Model1___u16 Model1_sadb_comb_auth_minbits;
 Model1___u16 Model1_sadb_comb_auth_maxbits;
 Model1___u16 Model1_sadb_comb_encrypt_minbits;
 Model1___u16 Model1_sadb_comb_encrypt_maxbits;
 __u32 Model1_sadb_comb_reserved;
 __u32 Model1_sadb_comb_soft_allocations;
 __u32 Model1_sadb_comb_hard_allocations;
 __u64 Model1_sadb_comb_soft_bytes;
 __u64 Model1_sadb_comb_hard_bytes;
 __u64 Model1_sadb_comb_soft_addtime;
 __u64 Model1_sadb_comb_hard_addtime;
 __u64 Model1_sadb_comb_soft_usetime;
 __u64 Model1_sadb_comb_hard_usetime;
} __attribute__((packed));
/* sizeof(struct sadb_comb) == 72 */

struct Model1_sadb_supported {
 Model1___u16 Model1_sadb_supported_len;
 Model1___u16 Model1_sadb_supported_exttype;
 __u32 Model1_sadb_supported_reserved;
} __attribute__((packed));
/* sizeof(struct sadb_supported) == 8 */

/* followed by:
	struct sadb_alg sadb_algs[(sadb_supported_len +
		sizeof(__u64) - sizeof(struct sadb_supported)) /
		sizeof(struct sadb_alg)]; */

struct Model1_sadb_alg {
 __u8 Model1_sadb_alg_id;
 __u8 Model1_sadb_alg_ivlen;
 Model1___u16 Model1_sadb_alg_minbits;
 Model1___u16 Model1_sadb_alg_maxbits;
 Model1___u16 Model1_sadb_alg_reserved;
} __attribute__((packed));
/* sizeof(struct sadb_alg) == 8 */

struct Model1_sadb_spirange {
 Model1___u16 Model1_sadb_spirange_len;
 Model1___u16 Model1_sadb_spirange_exttype;
 __u32 Model1_sadb_spirange_min;
 __u32 Model1_sadb_spirange_max;
 __u32 Model1_sadb_spirange_reserved;
} __attribute__((packed));
/* sizeof(struct sadb_spirange) == 16 */

struct Model1_sadb_x_kmprivate {
 Model1___u16 Model1_sadb_x_kmprivate_len;
 Model1___u16 Model1_sadb_x_kmprivate_exttype;
 __u32 Model1_sadb_x_kmprivate_reserved;
} __attribute__((packed));
/* sizeof(struct sadb_x_kmprivate) == 8 */

struct Model1_sadb_x_sa2 {
 Model1___u16 Model1_sadb_x_sa2_len;
 Model1___u16 Model1_sadb_x_sa2_exttype;
 __u8 Model1_sadb_x_sa2_mode;
 __u8 Model1_sadb_x_sa2_reserved1;
 Model1___u16 Model1_sadb_x_sa2_reserved2;
 __u32 Model1_sadb_x_sa2_sequence;
 __u32 Model1_sadb_x_sa2_reqid;
} __attribute__((packed));
/* sizeof(struct sadb_x_sa2) == 16 */

struct Model1_sadb_x_policy {
 Model1___u16 Model1_sadb_x_policy_len;
 Model1___u16 Model1_sadb_x_policy_exttype;
 Model1___u16 Model1_sadb_x_policy_type;
 __u8 Model1_sadb_x_policy_dir;
 __u8 Model1_sadb_x_policy_reserved;
 __u32 Model1_sadb_x_policy_id;
 __u32 Model1_sadb_x_policy_priority;
} __attribute__((packed));
/* sizeof(struct sadb_x_policy) == 16 */

struct Model1_sadb_x_ipsecrequest {
 Model1___u16 Model1_sadb_x_ipsecrequest_len;
 Model1___u16 Model1_sadb_x_ipsecrequest_proto;
 __u8 Model1_sadb_x_ipsecrequest_mode;
 __u8 Model1_sadb_x_ipsecrequest_level;
 Model1___u16 Model1_sadb_x_ipsecrequest_reserved1;
 __u32 Model1_sadb_x_ipsecrequest_reqid;
 __u32 Model1_sadb_x_ipsecrequest_reserved2;
} __attribute__((packed));
/* sizeof(struct sadb_x_ipsecrequest) == 16 */

/* This defines the TYPE of Nat Traversal in use.  Currently only one
 * type of NAT-T is supported, draft-ietf-ipsec-udp-encaps-06
 */
struct Model1_sadb_x_nat_t_type {
 Model1___u16 Model1_sadb_x_nat_t_type_len;
 Model1___u16 Model1_sadb_x_nat_t_type_exttype;
 __u8 Model1_sadb_x_nat_t_type_type;
 __u8 Model1_sadb_x_nat_t_type_reserved[3];
} __attribute__((packed));
/* sizeof(struct sadb_x_nat_t_type) == 8 */

/* Pass a NAT Traversal port (Source or Dest port) */
struct Model1_sadb_x_nat_t_port {
 Model1___u16 Model1_sadb_x_nat_t_port_len;
 Model1___u16 Model1_sadb_x_nat_t_port_exttype;
 Model1___be16 Model1_sadb_x_nat_t_port_port;
 Model1___u16 Model1_sadb_x_nat_t_port_reserved;
} __attribute__((packed));
/* sizeof(struct sadb_x_nat_t_port) == 8 */

/* Generic LSM security context */
struct Model1_sadb_x_sec_ctx {
 Model1___u16 Model1_sadb_x_sec_len;
 Model1___u16 Model1_sadb_x_sec_exttype;
 __u8 Model1_sadb_x_ctx_alg; /* LSMs: e.g., selinux == 1 */
 __u8 Model1_sadb_x_ctx_doi;
 Model1___u16 Model1_sadb_x_ctx_len;
} __attribute__((packed));
/* sizeof(struct sadb_sec_ctx) = 8 */

/* Used by MIGRATE to pass addresses IKE will use to perform
 * negotiation with the peer */
struct Model1_sadb_x_kmaddress {
 Model1___u16 Model1_sadb_x_kmaddress_len;
 Model1___u16 Model1_sadb_x_kmaddress_exttype;
 __u32 Model1_sadb_x_kmaddress_reserved;
} __attribute__((packed));
/* sizeof(struct sadb_x_kmaddress) == 8 */

/* To specify the SA dump filter */
struct Model1_sadb_x_filter {
 Model1___u16 Model1_sadb_x_filter_len;
 Model1___u16 Model1_sadb_x_filter_exttype;
 __u32 Model1_sadb_x_filter_saddr[4];
 __u32 Model1_sadb_x_filter_daddr[4];
 Model1___u16 Model1_sadb_x_filter_family;
 __u8 Model1_sadb_x_filter_splen;
 __u8 Model1_sadb_x_filter_dplen;
} __attribute__((packed));
/* sizeof(struct sadb_x_filter) == 40 */

/* Message types */
/* Security Association flags */





/* Security Association states */






/* Security Association types */
/* Authentication algorithms */
/* Encryption algorithms */
/* private allocations should use 249-255 (RFC2407) */



/* Compression algorithms */







/* Extension Header values */
/* The next four entries are for setting up NAT Traversal */





/* Used with MIGRATE to pass @ to IKE for negotiation */




/* Identity Extension values */





enum {
 Model1_IPSEC_MODE_ANY = 0, /* We do not support this for SA */
 Model1_IPSEC_MODE_TRANSPORT = 1,
 Model1_IPSEC_MODE_TUNNEL = 2,
 Model1_IPSEC_MODE_BEET = 3
};

enum {
 Model1_IPSEC_DIR_ANY = 0,
 Model1_IPSEC_DIR_INBOUND = 1,
 Model1_IPSEC_DIR_OUTBOUND = 2,
 Model1_IPSEC_DIR_FWD = 3, /* It is our own */
 Model1_IPSEC_DIR_MAX = 4,
 Model1_IPSEC_DIR_INVALID = 5
};

enum {
 Model1_IPSEC_POLICY_DISCARD = 0,
 Model1_IPSEC_POLICY_NONE = 1,
 Model1_IPSEC_POLICY_IPSEC = 2,
 Model1_IPSEC_POLICY_ENTRUST = 3,
 Model1_IPSEC_POLICY_BYPASS = 4
};

enum {
 Model1_IPSEC_LEVEL_DEFAULT = 0,
 Model1_IPSEC_LEVEL_USE = 1,
 Model1_IPSEC_LEVEL_REQUIRE = 2,
 Model1_IPSEC_LEVEL_UNIQUE = 3
};



/*
 * The x86 can do unaligned accesses itself.
 */








static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model1_u16 Model1_get_unaligned_le16(const void *Model1_p)
{
 return Model1___le16_to_cpup((Model1___le16 *)Model1_p);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model1_u32 Model1_get_unaligned_le32(const void *Model1_p)
{
 return Model1___le32_to_cpup((Model1___le32 *)Model1_p);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model1_u64 Model1_get_unaligned_le64(const void *Model1_p)
{
 return Model1___le64_to_cpup((Model1___le64 *)Model1_p);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model1_u16 Model1_get_unaligned_be16(const void *Model1_p)
{
 return Model1___be16_to_cpup((Model1___be16 *)Model1_p);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model1_u32 Model1_get_unaligned_be32(const void *Model1_p)
{
 return Model1___be32_to_cpup((Model1___be32 *)Model1_p);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) Model1_u64 Model1_get_unaligned_be64(const void *Model1_p)
{
 return Model1___be64_to_cpup((Model1___be64 *)Model1_p);
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_put_unaligned_le16(Model1_u16 Model1_val, void *Model1_p)
{
 *((Model1___le16 *)Model1_p) = (( Model1___le16)(Model1___u16)(Model1_val));
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_put_unaligned_le32(Model1_u32 Model1_val, void *Model1_p)
{
 *((Model1___le32 *)Model1_p) = (( Model1___le32)(__u32)(Model1_val));
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_put_unaligned_le64(Model1_u64 Model1_val, void *Model1_p)
{
 *((Model1___le64 *)Model1_p) = (( Model1___le64)(__u64)(Model1_val));
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_put_unaligned_be16(Model1_u16 Model1_val, void *Model1_p)
{
 *((Model1___be16 *)Model1_p) = (( Model1___be16)(__builtin_constant_p((Model1___u16)((Model1_val))) ? ((Model1___u16)( (((Model1___u16)((Model1_val)) & (Model1___u16)0x00ffU) << 8) | (((Model1___u16)((Model1_val)) & (Model1___u16)0xff00U) >> 8))) : Model1___fswab16((Model1_val))));
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_put_unaligned_be32(Model1_u32 Model1_val, void *Model1_p)
{
 *((Model1___be32 *)Model1_p) = (( Model1___be32)(__builtin_constant_p((__u32)((Model1_val))) ? ((__u32)( (((__u32)((Model1_val)) & (__u32)0x000000ffUL) << 24) | (((__u32)((Model1_val)) & (__u32)0x0000ff00UL) << 8) | (((__u32)((Model1_val)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)((Model1_val)) & (__u32)0xff000000UL) >> 24))) : Model1___fswab32((Model1_val))));
}

static inline __attribute__((no_instrument_function)) __attribute__((always_inline)) void Model1_put_unaligned_be64(Model1_u64 Model1_val, void *Model1_p)
{
 *((Model1___be64 *)Model1_p) = (( Model1___be64)(__builtin_constant_p((__u64)((Model1_val))) ? ((__u64)( (((__u64)((Model1_val)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)((Model1_val)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)((Model1_val)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)((Model1_val)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)((Model1_val)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)((Model1_val)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)((Model1_val)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)((Model1_val)) & (__u64)0xff00000000000000ULL) >> 56))) : Model1___fswab64((Model1_val))));
}



/*
 * Cause a link-time error if we try an unaligned access other than
 * 1,2,4 or 8 bytes long
 */
extern void Model1___bad_unaligned_access_size(void);













struct Model1_sock_extended_err {
 __u32 Model1_ee_errno;
 __u8 Model1_ee_origin;
 __u8 Model1_ee_type;
 __u8 Model1_ee_code;
 __u8 Model1_ee_pad;
 __u32 Model1_ee_info;
 __u32 Model1_ee_data;
};
/**
 *	struct scm_timestamping - timestamps exposed through cmsg
 *
 *	The timestamping interfaces SO_TIMESTAMPING, MSG_TSTAMP_*
 *	communicate network timestamps by passing this struct in a cmsg with
 *	recvmsg(). See Documentation/networking/timestamping.txt for details.
 */
struct Model1_scm_timestamping {
 struct Model1_timespec Model1_ts[3];
};

/* The type of scm_timestamping, passed in sock_extended_err ee_info.
 * This defines the type of ts[0]. For SCM_TSTAMP_SND only, if ts[0]
 * is zero, then this is a hardware timestamp and recorded in ts[2].
 */
enum {
 Model1_SCM_TSTAMP_SND, /* driver passed skb to NIC, or HW */
 Model1_SCM_TSTAMP_SCHED, /* data entered the packet scheduler */
 Model1_SCM_TSTAMP_ACK, /* data acknowledged by peer */
};



struct Model1_sock_exterr_skb {
 union {
  struct Model1_inet_skb_parm Model1_h4;

  struct Model1_inet6_skb_parm Model1_h6;

 } Model1_header;
 struct Model1_sock_extended_err Model1_ee;
 Model1_u16 Model1_addr_offset;
 Model1___be16 Model1_port;
};
#endif
